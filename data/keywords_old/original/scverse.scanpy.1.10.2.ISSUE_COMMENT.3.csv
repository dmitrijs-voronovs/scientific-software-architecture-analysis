id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/pull/1182#issuecomment-618699783:26,Testability,test,test,26,I'm also not sure why the test is failing -- it works interactively locally for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-618699783
https://github.com/scverse/scanpy/pull/1182#issuecomment-619321412:85,Availability,error,error,85,"@ivirshup is it possible that Travis has cached pbmc3k and that's what's causing the error? I really don't have it running pytest locally either. . Also as far as the code review -- I understand code is duplicated, but this code does not really fit in the existing implementation because it works a bit differently and requires raw data. Let me know how you'd like to address this. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-619321412
https://github.com/scverse/scanpy/pull/1182#issuecomment-619321412:41,Performance,cache,cached,41,"@ivirshup is it possible that Travis has cached pbmc3k and that's what's causing the error? I really don't have it running pytest locally either. . Also as far as the code review -- I understand code is duplicated, but this code does not really fit in the existing implementation because it works a bit differently and requires raw data. Let me know how you'd like to address this. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-619321412
https://github.com/scverse/scanpy/pull/1182#issuecomment-624403497:59,Testability,test,tests,59,This got a bit messy -- travis is defeating me because the tests work locally. I'm going to close this and figure it out on my fork and make a new PR with a clean history.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-624403497
https://github.com/scverse/scanpy/pull/1182#issuecomment-624644914:22,Usability,feedback,feedback,22,Sorry for the lack of feedback! I got caught up in my own work. * Have you made any progress on the travis issue?; * Is it an issue of getting different results from computing variance?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-624644914
https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512:84,Availability,error,error,84,"Hey @adamgayoso ,. I really love this HVG method, but sometimes I get the following error from the loess fit:. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-244-764977f87ce6> in <module>; ----> 1 sc.pp.highly_variable_genes(ad_sub, n_top_genes=1500, flavor='seurat_v3', layer='counts'); 2 sc.pp.pca(ad_sub); 3 sc.pp.neighbors(ad_sub); 4 sc.tl.umap(ad_sub); 5 sc.tl.leiden(ad_sub, resolution=2.0). ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 82 x = np.log10(mean[not_const]); 83 model = loess(x, y, span=span, degree=2); ---> 84 model.fit(); 85 estimat_var[not_const] = model.outputs.fitted_values; 86 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'reciprocal condition number 7.4971e-16\n'; ```. This is due to the very low but non-zero variance genes, I think. It goes away when I run `sc.pp.filter_genes(ad_sub, min_cells=5)` but not when I run only `sc.pp.filter_genes(ad_sub, min_cells=1)`. Maybe we can make the variance check more stringent, or we can print a better error message for the users?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512
https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512:1605,Availability,error,error,1605,"Hey @adamgayoso ,. I really love this HVG method, but sometimes I get the following error from the loess fit:. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-244-764977f87ce6> in <module>; ----> 1 sc.pp.highly_variable_genes(ad_sub, n_top_genes=1500, flavor='seurat_v3', layer='counts'); 2 sc.pp.pca(ad_sub); 3 sc.pp.neighbors(ad_sub); 4 sc.tl.umap(ad_sub); 5 sc.tl.leiden(ad_sub, resolution=2.0). ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 82 x = np.log10(mean[not_const]); 83 model = loess(x, y, span=span, degree=2); ---> 84 model.fit(); 85 estimat_var[not_const] = model.outputs.fitted_values; 86 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'reciprocal condition number 7.4971e-16\n'; ```. This is due to the very low but non-zero variance genes, I think. It goes away when I run `sc.pp.filter_genes(ad_sub, min_cells=5)` but not when I run only `sc.pp.filter_genes(ad_sub, min_cells=1)`. Maybe we can make the variance check more stringent, or we can print a better error message for the users?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512
https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512:1611,Integrability,message,message,1611,"Hey @adamgayoso ,. I really love this HVG method, but sometimes I get the following error from the loess fit:. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-244-764977f87ce6> in <module>; ----> 1 sc.pp.highly_variable_genes(ad_sub, n_top_genes=1500, flavor='seurat_v3', layer='counts'); 2 sc.pp.pca(ad_sub); 3 sc.pp.neighbors(ad_sub); 4 sc.tl.umap(ad_sub); 5 sc.tl.leiden(ad_sub, resolution=2.0). ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 82 x = np.log10(mean[not_const]); 83 model = loess(x, y, span=span, degree=2); ---> 84 model.fit(); 85 estimat_var[not_const] = model.outputs.fitted_values; 86 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'reciprocal condition number 7.4971e-16\n'; ```. This is due to the very low but non-zero variance genes, I think. It goes away when I run `sc.pp.filter_genes(ad_sub, min_cells=5)` but not when I run only `sc.pp.filter_genes(ad_sub, min_cells=1)`. Maybe we can make the variance check more stringent, or we can print a better error message for the users?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512
https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477:21,Availability,error,error,21,"I encounter the same error as well when i tried sc.pp.normalize_total(adata, target_sum=5e4). . Environment:; scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. [conda list]; https://github.com/phamidko/codesnippets/blob/master/scanpy-conda-list.txt; [ipynb]; https://github.com/phamidko/codesnippets/blob/master/Tissue-Tcell-activation.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477
https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477:199,Usability,learn,learn,199,"I encounter the same error as well when i tried sc.pp.normalize_total(adata, target_sum=5e4). . Environment:; scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. [conda list]; https://github.com/phamidko/codesnippets/blob/master/scanpy-conda-list.txt; [ipynb]; https://github.com/phamidko/codesnippets/blob/master/Tissue-Tcell-activation.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620365477
https://github.com/scverse/scanpy/issues/1183#issuecomment-620631436:52,Availability,down,downgrading,52,"Hi Issac,; Thank you for looking into this. I tried downgrading to python 3.7 but the; error still persists. Best,; Philip. On Tue, Apr 28, 2020 at 12:16 AM Isaac Virshup <notifications@github.com>; wrote:. > @phamidko <https://github.com/phamidko>, could you export that conda; > environment with conda list --export? I'd like to see if I can recreate; > with your environment.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1183#issuecomment-620426227>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AJ7QRZ6CCZW74XVYGDEPRFDROZ7CRANCNFSM4MRFHJHQ>; > .; >. <envlist moved to next post by @ivirshup>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620631436
https://github.com/scverse/scanpy/issues/1183#issuecomment-620631436:87,Availability,error,error,87,"Hi Issac,; Thank you for looking into this. I tried downgrading to python 3.7 but the; error still persists. Best,; Philip. On Tue, Apr 28, 2020 at 12:16 AM Isaac Virshup <notifications@github.com>; wrote:. > @phamidko <https://github.com/phamidko>, could you export that conda; > environment with conda list --export? I'd like to see if I can recreate; > with your environment.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1183#issuecomment-620426227>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AJ7QRZ6CCZW74XVYGDEPRFDROZ7CRANCNFSM4MRFHJHQ>; > .; >. <envlist moved to next post by @ivirshup>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620631436
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:4504,Availability,error,error,4504,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:5082,Availability,error,error,5082,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:5135,Availability,error,error,5135,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:4886,Deployability,install,install,4886,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:1747,Integrability,wrap,wrap,1747,; cycler=0.10.0=pypi_0; decorator=4.4.2=py_0; defusedxml=0.6.0=py_0; entrypoints=0.3=py37hc8dfbb8_1001; fontconfig=2.13.1=h86ecdb6_1001; freetype=2.10.1=he06d7ca_0; get-version=2.1=pypi_0; gettext=0.19.8.1=hc5be6a0_1002; glib=2.64.2=h6f030ca_0; gmp=6.2.0=he1b5a44_2; h5py=2.10.0=pypi_0; icu=64.2=he1b5a44_1; idna=2.9=py_1; importlib-metadata=1.6.0=py37hc8dfbb8_0; importlib_metadata=1.6.0=0; ipykernel=5.2.1=py37h43977f1_0; ipython=7.13.0=py37hc8dfbb8_2; ipython_genutils=0.2.0=py_1; jedi=0.17.0=py37hc8dfbb8_0; jinja2=2.11.2=pyh9f0ad1d_0; joblib=0.14.1=pypi_0; json5=0.9.0=py_0; jsonschema=3.2.0=py37hc8dfbb8_1; jupyter_client=6.1.3=py_0; jupyter_contrib_core=0.3.3=py_2; jupyter_contrib_nbextensions=0.5.1=py37_0; jupyter_core=4.6.3=py37hc8dfbb8_1; jupyter_highlight_selected_word=0.2.0=py37_1000; jupyter_latex_envs=1.4.6=py37_1000; jupyter_nbextensions_configurator=0.4.1=py37_0; jupyterlab=2.1.1=py_0; jupyterlab_server=1.1.1=py_0; kiwisolver=1.2.0=pypi_0; ld_impl_linux-64=2.33.1=h53a641e_7; legacy-api-wrap=1.2=pypi_0; leidenalg=0.8.0=py37h43df1e8_0; libedit=3.1.20181209=hc058e9b_0; libffi=3.2.1=hd88cf55_4; libgcc-ng=9.1.0=hdf63c60_0; libgfortran-ng=7.3.0=hdf63c60_5; libiconv=1.15=h516909a_1006; libpng=1.6.37=hed695b0_1; libsodium=1.0.17=h516909a_0; libstdcxx-ng=9.1.0=hdf63c60_0; libuuid=2.32.1=h14c3975_1000; libxcb=1.13=h14c3975_1002; libxml2=2.9.10=hee79883_0; libxslt=1.1.33=h31b3aaa_0; llvmlite=0.32.0=pypi_0; lxml=4.5.0=py37he3881c9_1; markupsafe=1.1.1=py37h8f50634_1; matplotlib=3.2.1=pypi_0; mistune=0.8.4=py37h8f50634_1001; natsort=7.0.1=pypi_0; nbconvert=5.6.1=py37hc8dfbb8_1; nbformat=5.0.6=py_0; ncurses=6.2=he6710b0_0; networkx=2.4=pypi_0; notebook=6.0.3=py37_0; numba=0.49.0=pypi_0; numexpr=2.7.1=pypi_0; numpy=1.18.3=pypi_0; openssl=1.1.1g=h516909a_0; packaging=20.3=pypi_0; pandas=1.0.3=pypi_0; pandoc=2.9.2.1=0; pandocfilters=1.4.2=py_1; parso=0.7.0=pyh9f0ad1d_0; patsy=0.5.1=pypi_0; pcre=8.44=he1b5a44_0; pexpect=4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:545,Security,certificate,certificates,545,"@phamidko I've modified your comment, and placed the output in the collapsed section below so I don't have to scroll so much on this issue. Unfortunately replies sent from email don't support markdown :(. <details>; <summary> phamidko's environment </summary>. ```; # This file may be used to create an environment using:; # $ conda create --name <env> --file <this file>; # platform: linux-64; _libgcc_mutex=0.1=main; anndata=0.7.1=pypi_0; attrs=19.3.0=py_0; backcall=0.1.0=py_0; bleach=3.1.4=pyh9f0ad1d_0; brotlipy=0.7.0=py37h8f50634_1000; ca-certificates=2020.4.5.1=hecc5488_0; cairo=1.16.0=hcf35c78_1003; certifi=2020.4.5.1=py37hc8dfbb8_0; cffi=1.14.0=py37hd463f26_0; chardet=3.0.4=py37hc8dfbb8_1006; cryptography=2.9.2=py37hb09aad4_0; cycler=0.10.0=pypi_0; decorator=4.4.2=py_0; defusedxml=0.6.0=py_0; entrypoints=0.3=py37hc8dfbb8_1001; fontconfig=2.13.1=h86ecdb6_1001; freetype=2.10.1=he06d7ca_0; get-version=2.1=pypi_0; gettext=0.19.8.1=hc5be6a0_1002; glib=2.64.2=h6f030ca_0; gmp=6.2.0=he1b5a44_2; h5py=2.10.0=pypi_0; icu=64.2=he1b5a44_1; idna=2.9=py_1; importlib-metadata=1.6.0=py37hc8dfbb8_0; importlib_metadata=1.6.0=0; ipykernel=5.2.1=py37h43977f1_0; ipython=7.13.0=py37hc8dfbb8_2; ipython_genutils=0.2.0=py_1; jedi=0.17.0=py37hc8dfbb8_0; jinja2=2.11.2=pyh9f0ad1d_0; joblib=0.14.1=pypi_0; json5=0.9.0=py_0; jsonschema=3.2.0=py37hc8dfbb8_1; jupyter_client=6.1.3=py_0; jupyter_contrib_core=0.3.3=py_2; jupyter_contrib_nbextensions=0.5.1=py37_0; jupyter_core=4.6.3=py37hc8dfbb8_1; jupyter_highlight_selected_word=0.2.0=py37_1000; jupyter_latex_envs=1.4.6=py37_1000; jupyter_nbextensions_configurator=0.4.1=py37_0; jupyterlab=2.1.1=py_0; jupyterlab_server=1.1.1=py_0; kiwisolver=1.2.0=pypi_0; ld_impl_linux-64=2.33.1=h53a641e_7; legacy-api-wrap=1.2=pypi_0; leidenalg=0.8.0=py37h43df1e8_0; libedit=3.1.20181209=hc058e9b_0; libffi=3.2.1=hd88cf55_4; libgcc-ng=9.1.0=hdf63c60_0; libgfortran-ng=7.3.0=hdf63c60_5; libiconv=1.15=h516909a_1006; libpng=1.6.37=hed695b0_1; libsodium=1.0.17=h516909a_0; li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:2882,Testability,stub,stubs,2882,63c60_0; libgfortran-ng=7.3.0=hdf63c60_5; libiconv=1.15=h516909a_1006; libpng=1.6.37=hed695b0_1; libsodium=1.0.17=h516909a_0; libstdcxx-ng=9.1.0=hdf63c60_0; libuuid=2.32.1=h14c3975_1000; libxcb=1.13=h14c3975_1002; libxml2=2.9.10=hee79883_0; libxslt=1.1.33=h31b3aaa_0; llvmlite=0.32.0=pypi_0; lxml=4.5.0=py37he3881c9_1; markupsafe=1.1.1=py37h8f50634_1; matplotlib=3.2.1=pypi_0; mistune=0.8.4=py37h8f50634_1001; natsort=7.0.1=pypi_0; nbconvert=5.6.1=py37hc8dfbb8_1; nbformat=5.0.6=py_0; ncurses=6.2=he6710b0_0; networkx=2.4=pypi_0; notebook=6.0.3=py37_0; numba=0.49.0=pypi_0; numexpr=2.7.1=pypi_0; numpy=1.18.3=pypi_0; openssl=1.1.1g=h516909a_0; packaging=20.3=pypi_0; pandas=1.0.3=pypi_0; pandoc=2.9.2.1=0; pandocfilters=1.4.2=py_1; parso=0.7.0=pyh9f0ad1d_0; patsy=0.5.1=pypi_0; pcre=8.44=he1b5a44_0; pexpect=4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001; pip=20.0.2=py37_1; pixman=0.38.0=h516909a_1003; prometheus_client=0.7.1=py_0; prompt-toolkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:3685,Testability,test,testpath,3685,"4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001; pip=20.0.2=py37_1; pixman=0.38.0=h516909a_1003; prometheus_client=0.7.1=py_0; prompt-toolkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:4930,Testability,test,tested,4930,"z=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanpy_1183_env_nopip.txt; $ conda activate scanpy1183; $ pip install -r scanpy_1183_pip.txt; ```. Then I tested this using:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. But did not get an error. ~Could you send a snippet that reproduces the error for you?~ Oops, forgot that you already did this in the notebook. Taking a look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:3391,Usability,learn,learn,3391,orkx=2.4=pypi_0; notebook=6.0.3=py37_0; numba=0.49.0=pypi_0; numexpr=2.7.1=pypi_0; numpy=1.18.3=pypi_0; openssl=1.1.1g=h516909a_0; packaging=20.3=pypi_0; pandas=1.0.3=pypi_0; pandoc=2.9.2.1=0; pandocfilters=1.4.2=py_1; parso=0.7.0=pyh9f0ad1d_0; patsy=0.5.1=pypi_0; pcre=8.44=he1b5a44_0; pexpect=4.8.0=py37hc8dfbb8_1; pickleshare=0.7.5=py37hc8dfbb8_1001; pip=20.0.2=py37_1; pixman=0.38.0=h516909a_1003; prometheus_client=0.7.1=py_0; prompt-toolkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575:3836,Usability,learn,learn,3836,"lkit=3.0.5=py_0; psutil=5.7.0=py37h8f50634_1; pthread-stubs=0.4=h14c3975_1001; ptyprocess=0.6.0=py_1001; pycairo=1.19.1=py37h01af8b0_3; pycparser=2.20=py_0; pygments=2.6.1=py_0; pyopenssl=19.1.0=py_1; pyparsing=2.4.7=pypi_0; pyrsistent=0.16.0=py37h8f50634_0; pysocks=1.7.1=py37hc8dfbb8_1; python=3.7.7=hcf32534_0_cpython; python-dateutil=2.8.1=py_0; python-igraph=0.8.1=pypi_0; python_abi=3.7=1_cp37m; pytz=2019.3=pypi_0; pyyaml=5.3.1=py37h8f50634_0; pyzmq=19.0.0=py37hac76be4_1; readline=8.0=h7b6447c_0; requests=2.23.0=pyh8c360ce_2; scanpy=1.4.6=pypi_0; scikit-learn=0.22.2.post1=pypi_0; scipy=1.4.1=pypi_0; seaborn=0.10.1=pypi_0; send2trash=1.5.0=py_0; setuptools=46.1.3=py37_0; setuptools-scm=3.5.0=pypi_0; six=1.14.0=py_1; sqlite=3.31.1=h62c20be_1; statsmodels=0.11.1=pypi_0; tables=3.6.1=pypi_0; tbb=2020.0.133=pypi_0; terminado=0.8.3=py37hc8dfbb8_1; testpath=0.4.4=py_0; texttable=1.6.2=py_0; tk=8.6.8=hbc83047_0; tornado=6.0.4=py37h8f50634_1; tqdm=4.45.0=pypi_0; traitlets=4.3.3=py37hc8dfbb8_1; umap-learn=0.4.1=pypi_0; urllib3=1.25.9=py_0; wcwidth=0.1.9=pyh9f0ad1d_0; webencodings=0.5.1=py_1; wheel=0.34.2=py37_0; xorg-kbproto=1.0.7=h14c3975_1002; xorg-libice=1.0.10=h516909a_0; xorg-libsm=1.2.3=h84519dc_1000; xorg-libx11=1.6.9=h516909a_0; xorg-libxau=1.0.9=h14c3975_0; xorg-libxdmcp=1.1.3=h516909a_0; xorg-libxext=1.3.4=h516909a_0; xorg-libxrender=0.9.10=h516909a_1002; xorg-renderproto=0.11.1=h14c3975_1002; xorg-xextproto=7.3.0=h14c3975_1002; xorg-xproto=7.0.31=h14c3975_1007; xz=5.2.5=h7b6447c_0; yaml=0.2.4=h516909a_0; zeromq=4.3.2=he1b5a44_2; zipp=3.1.0=py_0; zlib=1.2.11=h7b6447c_3; ```. </details>. I've recreated your environment, but cannot reproduce this error. Here's how I created the environment:. ```bash; # Where the output you pasted above is in scanpy_1183_env.txt; $ grep -v pypi_0 scanpy_1183_env.txt > scanpy_1183_env_nopip.txt; $ grep pypi_0 scanpy_1183_env.txt | sed 's/=pypi_0//' | sed 's/=/==/' > scanpy_1183_pip.txt; $ conda create -y --name scanpy1183 --file scanp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-620988575
https://github.com/scverse/scanpy/issues/1183#issuecomment-621000991:41,Availability,error,error,41,"Hmm. I'm not able to replicate the exact error, but I get a different one. Could you try running:. ```python; adata = adata.copy(); ```. right before `normalize_total` and see if that works?. ----------------------. Update: tried on a different machine and could replicate your error. The issue is that `normalize_total` doesn't make sure the anndata object isn't a view before assigning to it. As a work-around, you can just run `adata = adata.copy()` before `sc.pp.normalize_total(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-621000991
https://github.com/scverse/scanpy/issues/1183#issuecomment-621000991:278,Availability,error,error,278,"Hmm. I'm not able to replicate the exact error, but I get a different one. Could you try running:. ```python; adata = adata.copy(); ```. right before `normalize_total` and see if that works?. ----------------------. Update: tried on a different machine and could replicate your error. The issue is that `normalize_total` doesn't make sure the anndata object isn't a view before assigning to it. As a work-around, you can just run `adata = adata.copy()` before `sc.pp.normalize_total(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-621000991
https://github.com/scverse/scanpy/issues/1183#issuecomment-621000991:216,Deployability,Update,Update,216,"Hmm. I'm not able to replicate the exact error, but I get a different one. Could you try running:. ```python; adata = adata.copy(); ```. right before `normalize_total` and see if that works?. ----------------------. Update: tried on a different machine and could replicate your error. The issue is that `normalize_total` doesn't make sure the anndata object isn't a view before assigning to it. As a work-around, you can just run `adata = adata.copy()` before `sc.pp.normalize_total(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1183#issuecomment-621000991
https://github.com/scverse/scanpy/pull/1186#issuecomment-623326183:118,Security,access,accessible,118,We'll see 😝. I'm hoping I might be able to help out with other things along these things which would make Scanpy more accessible for R users and hopefully expand the user base.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1186#issuecomment-623326183
https://github.com/scverse/scanpy/issues/1187#issuecomment-620657306:68,Modifiability,variab,variable,68,"That issue report mentions setting the `PYTHONHASHSEED` environment variable to `0` (next to all the seed setting) worked to create a fully reproducible workflow. If that doesn't work for you, it might be good to continue the discussion there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620657306
https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:1160,Availability,echo,echo,1160," as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Running on a machine with 16 CPUs, evaluate the differences betw",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409
https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:2074,Availability,echo,echo,2074,"ighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Running on a machine with 16 CPUs, evaluate the differences between the results first from the arpack solver; adata8 = sc.read('test8.h5ad'); adata16 = sc.read('test16.h5ad'); print((adata8.X != adata16.X).sum()); print((adata8.obsm['X_pca'] != adata16.obsm['X_pca']).sum()); print((adata8.uns['neighbors']['connectivities'] != adata16.uns['neighbors']['connectivities']).sum()); sc.tl.leiden(adata8, random_state=14); sc.tl.leiden(adata16, random_state=14); display(adata8.obs['leiden'].value_counts()); display(adata16.obs['leiden'].value_counts()). # Running on a machine with 16 CPUs, evaluate the differences between the results first from the randomized solver; adata8 = sc.read('test8_randomized.h5ad'); adata16 = sc.read('test16_randomized.h5ad'); print((adata8.X != adata16.X).sum()); print((adata8.obsm['X_pca'] != adata16.obsm['X_pca']).sum()); print((adata8.uns['neighbors']['connectivities'] != adata16.uns['neighbors']['connectivities']).sum()); sc.tl.leiden(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409
https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:255,Deployability,pipeline,pipeline,255,"I confirmed that setting the PYTHONHASHSEED environmental variable to 0 did not change the results. The code run below (in jupyter notebook) gave the same results as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409
https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:58,Modifiability,variab,variable,58,"I confirmed that setting the PYTHONHASHSEED environmental variable to 0 did not change the results. The code run below (in jupyter notebook) gave the same results as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409
https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:214,Modifiability,variab,variable,214,"I confirmed that setting the PYTHONHASHSEED environmental variable to 0 did not change the results. The code run below (in jupyter notebook) gave the same results as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409
https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:493,Performance,cache,cache,493,"I confirmed that setting the PYTHONHASHSEED environmental variable to 0 did not change the results. The code run below (in jupyter notebook) gave the same results as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409
https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:1405,Performance,cache,cache,1405,"port scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Running on a machine with 16 CPUs, evaluate the differences between the results first from the arpack solver; adata8 = sc.read('test8.h5ad'); adata16 = sc.read('test16.h5ad'); print((adata8.X != adata16.X).sum()); print((adata8.obsm['X_pca'] != adata16.obsm['X_pca']).sum()); pri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409
https://github.com/scverse/scanpy/issues/1187#issuecomment-620862704:124,Modifiability,variab,variability,124,"That is interesting... do you know where the randomness is coming in? I think `sc.pp.highly_variable_genes()` can have some variability. These two VMs have the same operating system and hardware otherwise, right? I've had reproducibility issues moving between Fedora 25 and 28. In the end the libraries we use rely on underlying kernel numerics. There's a limit to how reproducible one can be. This only really becomes an issue if the biological interpretation is no longer consistent. Of course we'd like to be reproducible before then as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620862704
https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:356,Deployability,pipeline,pipeline,356,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096
https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:29,Modifiability,variab,variable,29,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096
https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:502,Usability,learn,learn,502,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096
https://github.com/scverse/scanpy/issues/1187#issuecomment-621186718:110,Performance,load,loading,110,I am actually booting using the exact same disks so identical OS (Ubuntu 16.04) and BLAS libraries. I am just loading them up with different virtual machines with different numbers of CPUs. In both cases the CPUs are Intel Xeon E5 v3 (Haswell). Have not tried limiting the number of CPUs used by arpack. I didn't know that was something I could do! Do you have a tip on how to do so? I'll look this up and give it a shot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-621186718
https://github.com/scverse/scanpy/issues/1187#issuecomment-621660499:172,Modifiability,variab,variables,172,"IIRC, you can limit the number of CPUs used through blas. This works on my machine:. ```; export OMP_NUM_THREADS=1; ```. Different blas libraries use different environment variables for this, so I'd check to make sure it's actually restricting the number of threads used.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-621660499
https://github.com/scverse/scanpy/issues/1189#issuecomment-621299538:100,Modifiability,variab,variables,100,"Yes, that is what I currently do. It is just a matter of aesthetics. Since I have a large number of variables that I generate with custom functions, I wanted to store them separately based on what they represent in separate modules under (`adata.uns`). . Adding everything to `adata.obs` quickly gets cluttered. No worries just wanted to see if it was an option. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1189#issuecomment-621299538
https://github.com/scverse/scanpy/issues/1189#issuecomment-621466673:140,Testability,test,tests,140,"I think `adata.obsm` could make sense, but `adata.uns` would maybe be a bit too messy given the unstructured nature and the assumptions and tests that would have to be added.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1189#issuecomment-621466673
https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839:63,Modifiability,layers,layers,63,"Yeah, that makes absolute sense. . Even better would be to add layers similar to `adata.X` and pass that into any function. Something like `adata.obs_custom`. In which case `adata.obs_custom` will inherit the same properties as that of `adata.obs` and users can make as many as they need in an organized manner. It will also allow users to store different values with the same column name (of course in different layers). e.g. `adata.obs_custom['same_column_name']` and `adata.obs_custom2['same_column_name']`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839
https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839:197,Modifiability,inherit,inherit,197,"Yeah, that makes absolute sense. . Even better would be to add layers similar to `adata.X` and pass that into any function. Something like `adata.obs_custom`. In which case `adata.obs_custom` will inherit the same properties as that of `adata.obs` and users can make as many as they need in an organized manner. It will also allow users to store different values with the same column name (of course in different layers). e.g. `adata.obs_custom['same_column_name']` and `adata.obs_custom2['same_column_name']`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839
https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839:413,Modifiability,layers,layers,413,"Yeah, that makes absolute sense. . Even better would be to add layers similar to `adata.X` and pass that into any function. Something like `adata.obs_custom`. In which case `adata.obs_custom` will inherit the same properties as that of `adata.obs` and users can make as many as they need in an organized manner. It will also allow users to store different values with the same column name (of course in different layers). e.g. `adata.obs_custom['same_column_name']` and `adata.obs_custom2['same_column_name']`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839
https://github.com/scverse/scanpy/issues/1190#issuecomment-623035955:27,Deployability,install,install,27,"same here, wish I tried to install earlier....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190#issuecomment-623035955
https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806:193,Deployability,install,installation,193,"The bio conda builds look like their broken at the moment, and we haven't had the bandwidth to fix them yet (we are not the direct maintainers of the bio-conda builds). You can find up to date installation instructions which avoid this on the [latest docs](https://scanpy.readthedocs.io/en/latest/installation.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806
https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806:297,Deployability,install,installation,297,"The bio conda builds look like their broken at the moment, and we haven't had the bandwidth to fix them yet (we are not the direct maintainers of the bio-conda builds). You can find up to date installation instructions which avoid this on the [latest docs](https://scanpy.readthedocs.io/en/latest/installation.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806
https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806:225,Safety,avoid,avoid,225,"The bio conda builds look like their broken at the moment, and we haven't had the bandwidth to fix them yet (we are not the direct maintainers of the bio-conda builds). You can find up to date installation instructions which avoid this on the [latest docs](https://scanpy.readthedocs.io/en/latest/installation.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190#issuecomment-623272806
https://github.com/scverse/scanpy/issues/1191#issuecomment-622229246:68,Availability,down,downgrading,68,This looks like an bug in the most recent release of `louvain`. Try downgrading?. I would also recommend using `leiden` clustering instead.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-622229246
https://github.com/scverse/scanpy/issues/1191#issuecomment-622229246:42,Deployability,release,release,42,This looks like an bug in the most recent release of `louvain`. Try downgrading?. I would also recommend using `leiden` clustering instead.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-622229246
https://github.com/scverse/scanpy/issues/1191#issuecomment-627466608:743,Testability,log,logg,743,"FYI, it appears that this bug remains in louvain version 0.7 ; ```python; In [1]: import numpy as np; ...: import scanpy as sc; ...:; ...: adata = sc.AnnData(np.random.normal(size=(100,3))); ...:; ...: sc.pp.neighbors(adata); ...: sc.tl.louvain(adata); ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-1-3505d1878068> in <module>; 5; 6 sc.pp.neighbors(adata); ----> 7 sc.tl.louvain(adata). ~/.local/lib/python3.7/site-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy); 136 partition_kwargs[""weights""] = weights; 137 logg.info(' using the ""louvain"" package of Traag (2017)'); --> 138 louvain.set_rng_seed(random_state); 139 part = louvain.find_partition(; 140 g, partition_type,. AttributeError: module 'louvain' has no attribute 'set_rng_seed'. In [2]: import louvain. In [3]: louvain.__version__; Out[3]: '0.7.0'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-627466608
https://github.com/scverse/scanpy/issues/1191#issuecomment-628933774:56,Availability,error,error,56,"Hello,. I am having the same issue, here is my code and error :. sc.tl.louvain(adata,resolution=0.4) ; running Louvain clustering; using the ""louvain"" package of Traag (2017); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/Morgane/anaconda3/lib/python3.7/site-packages/scanpy/tools/_louvain.py"", line 138, in louvain; louvain.set_rng_seed(random_state); AttributeError: module 'louvain' has no attribute 'set_rng_seed'. I am using Louvain version 0.7.0. Did you fix this issue in that version?. thanks for your help,; Morgane",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-628933774
https://github.com/scverse/scanpy/issues/1191#issuecomment-628969212:58,Deployability,release,release,58,"The API for setting the random seed changed in the recent release (`v0.7`) of `louvain`, this is fixed on master, which should see a release soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-628969212
https://github.com/scverse/scanpy/issues/1191#issuecomment-628969212:133,Deployability,release,release,133,"The API for setting the random seed changed in the recent release (`v0.7`) of `louvain`, this is fixed on master, which should see a release soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-628969212
https://github.com/scverse/scanpy/issues/1191#issuecomment-630174509:39,Deployability,release,release,39,"If anyone is stuck waiting for the new release, you can edit your `.../lib/python3.7/site-packages/scanpy/tools/_louvain.py` with these changes:. Add: `partition_kwargs[""seed""] = random_state` ; Remove: `louvain.set_rng_seed(random_state)`. From:; https://github.com/theislab/scanpy/pull/1197/commits/b54d67b9d6b41269c1612df0242210d1279ede85",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-630174509
https://github.com/scverse/scanpy/issues/1191#issuecomment-638817267:41,Deployability,release,release,41,"> If anyone is stuck waiting for the new release, you can edit your `.../lib/python3.7/site-packages/scanpy/tools/_louvain.py` with these changes:; > ; > Add: `partition_kwargs[""seed""] = random_state`; > Remove: `louvain.set_rng_seed(random_state)`; > ; > From:; > [b54d67b](https://github.com/theislab/scanpy/commit/b54d67b9d6b41269c1612df0242210d1279ede85). Thankx this worked",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-638817267
https://github.com/scverse/scanpy/issues/1191#issuecomment-659515808:0,Deployability,Install,Install,0,Install old louvain package will solve the problem: pip install louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-659515808
https://github.com/scverse/scanpy/issues/1191#issuecomment-659515808:56,Deployability,install,install,56,Install old louvain package will solve the problem: pip install louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1191#issuecomment-659515808
https://github.com/scverse/scanpy/issues/1193#issuecomment-622662852:164,Availability,error,error,164,"I don't think this is a segfault, but a `TypeError`. I believe this is due to using an out of date version of `numba`. Could you update that and let me know if the error persists?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622662852
https://github.com/scverse/scanpy/issues/1193#issuecomment-622662852:129,Deployability,update,update,129,"I don't think this is a segfault, but a `TypeError`. I believe this is due to using an out of date version of `numba`. Could you update that and let me know if the error persists?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622662852
https://github.com/scverse/scanpy/issues/1193#issuecomment-622663649:104,Availability,error,error,104,"This is not related, and most certainly separate issue. <can start a new thread > ; I am getting memory error with sc.tl.pca; What is your recommendation? . Traceback (most recent call last):; File ""timeseriesScanpy.py"", line 111, in <module>; sc.tl.pca(adata, svd_solver='arpack') # svd_solver='arpack' is important for reproducibility; File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 498, in pca; X = adata_comp.X.toarray() # Copying the whole adata_comp.X here, could cause memory problems; File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scipy/sparse/compressed.py"", line 1024, in toarray; out = self._process_toarray_args(order, out); File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scipy/sparse/base.py"", line 1186, in _process_toarray_args; return np.zeros(self.shape, dtype=self.dtype, order=order); MemoryError",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622663649
https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255:188,Deployability,release,release,188,"I think that might be due to the dense array being to large to fit in memory on your machine. Just to be sure, how large is your dataset? And how much memory do you have?. For the current release, you could either try using the incremental PCA, using a subset of the data, or using a machine with more memory. In the next scanpy release, there will be a much more memory efficient PCA implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255
https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255:329,Deployability,release,release,329,"I think that might be due to the dense array being to large to fit in memory on your machine. Just to be sure, how large is your dataset? And how much memory do you have?. For the current release, you could either try using the incremental PCA, using a subset of the data, or using a machine with more memory. In the next scanpy release, there will be a much more memory efficient PCA implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255
https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255:371,Energy Efficiency,efficient,efficient,371,"I think that might be due to the dense array being to large to fit in memory on your machine. Just to be sure, how large is your dataset? And how much memory do you have?. For the current release, you could either try using the incremental PCA, using a subset of the data, or using a machine with more memory. In the next scanpy release, there will be a much more memory efficient PCA implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193#issuecomment-622666255
https://github.com/scverse/scanpy/pull/1196#issuecomment-623272610:138,Testability,test,tests,138,"hi,. - yeah, it should work if the original array contain `nan` (i.e. it would just ignore these entries); - ah, just saw that there's a `tests/test_score_genes.py` file, I'll add a few tests the next days!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1196#issuecomment-623272610
https://github.com/scverse/scanpy/pull/1196#issuecomment-623272610:186,Testability,test,tests,186,"hi,. - yeah, it should work if the original array contain `nan` (i.e. it would just ignore these entries); - ah, just saw that there's a `tests/test_score_genes.py` file, I'll add a few tests the next days!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1196#issuecomment-623272610
https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235:47,Modifiability,refactor,refactor,47,"hi,. finally managed to add some tests. Had to refactor the original `test_score_genes.py` a little, I hope that's ok: The one test that was already there still exists, I just pulled out the creation of the adata into a separate function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235
https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235:33,Testability,test,tests,33,"hi,. finally managed to add some tests. Had to refactor the original `test_score_genes.py` a little, I hope that's ok: The one test that was already there still exists, I just pulled out the creation of the adata into a separate function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235
https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235:127,Testability,test,test,127,"hi,. finally managed to add some tests. Had to refactor the original `test_score_genes.py` a little, I hope that's ok: The one test that was already there still exists, I just pulled out the creation of the adata into a separate function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235
https://github.com/scverse/scanpy/issues/1198#issuecomment-623974282:197,Deployability,update,updated,197,"Hi @honghh2018,. This might be an issue you have to raise with Seurat about their ReadH5AD function. From `AnnData` 0.7 the h5ad format has changed a little on disk, so maybe their function is not updated to this yet? Other ways you can go between Scanpy and Seurat are loom files or `anndata2ri` as shown [here](https://github.com/LuckyMD/Code_snippets/blob/master/Seurat_to_anndata.ipynb)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1198#issuecomment-623974282
https://github.com/scverse/scanpy/issues/1199#issuecomment-800058390:0,Testability,Test,Test,0,Test case is included in https://github.com/theislab/scanpy/pull/1669,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1199#issuecomment-800058390
https://github.com/scverse/scanpy/issues/1199#issuecomment-826288598:17,Testability,test,tests,17,"If it passes the tests I'm sure it works, feel free to pull it in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1199#issuecomment-826288598
https://github.com/scverse/scanpy/issues/1201#issuecomment-658658158:70,Testability,test,test,70,"I think it's normally just the string `""euclidean""`, but you can just test what is stored in `.uns['neighbors']['params']['metric']` after running `sc.pp.neighbors()` on some test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1201#issuecomment-658658158
https://github.com/scverse/scanpy/issues/1201#issuecomment-658658158:175,Testability,test,test,175,"I think it's normally just the string `""euclidean""`, but you can just test what is stored in `.uns['neighbors']['params']['metric']` after running `sc.pp.neighbors()` on some test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1201#issuecomment-658658158
https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:17,Availability,error,error,17,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006
https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:48,Deployability,install,installing,48,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006
https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:64,Usability,learn,learn,64,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006
https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006:96,Usability,learn,learn,96,"ok, I solved the error by uninstalling umap and installing umap-learn; it only worked with umap-learn v. 0.3.9, as was suggested here: https://github.com/theislab/scanpy/issues/1181",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1202#issuecomment-624926006
https://github.com/scverse/scanpy/pull/1204#issuecomment-624831279:139,Availability,error,error,139,"@ivirshup -- I still can't tell why Travis is failing. For some reason on Travis, loess is outputting a zero for the gene mentioned in the error message, but this doesn't happen locally for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-624831279
https://github.com/scverse/scanpy/pull/1204#issuecomment-624831279:145,Integrability,message,message,145,"@ivirshup -- I still can't tell why Travis is failing. For some reason on Travis, loess is outputting a zero for the gene mentioned in the error message, but this doesn't happen locally for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-624831279
https://github.com/scverse/scanpy/pull/1204#issuecomment-644976141:184,Energy Efficiency,reduce,reduce,184,"@gokceneraslan why are the defaults for the main function all None (e.g., dispersion cutoffs)? It seems like if scanpydoc is picking up the defaults then we can make them not None and reduce some code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-644976141
https://github.com/scverse/scanpy/pull/1204#issuecomment-645459822:186,Energy Efficiency,reduce,reduce,186,"> @gokceneraslan why are the defaults for the main function all None (e.g., dispersion cutoffs)? It seems like if scanpydoc is picking up the defaults then we can make them not None and reduce some code?. Honestly, I don't know. But @ivirshup also brought it up here https://github.com/theislab/scanpy/pull/1180#discussion_r412871280 and here https://github.com/theislab/scanpy/pull/1180#discussion_r413445806, I just didn't have time to address it. I am fine with making them not None.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-645459822
https://github.com/scverse/scanpy/pull/1204#issuecomment-645460915:258,Availability,error,error,258,"But I think scanpydoc is very confused now for some reason. Documentation build is broken, it's visible in ~all~ some recent PRs too and there is not much we can do without the help of @falexwolf or @flying-sheep or @ivirshup, because we cannot even see the error message. My local builds are just fine 🤷",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-645460915
https://github.com/scverse/scanpy/pull/1204#issuecomment-645460915:264,Integrability,message,message,264,"But I think scanpydoc is very confused now for some reason. Documentation build is broken, it's visible in ~all~ some recent PRs too and there is not much we can do without the help of @falexwolf or @flying-sheep or @ivirshup, because we cannot even see the error message. My local builds are just fine 🤷",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-645460915
https://github.com/scverse/scanpy/pull/1204#issuecomment-650924757:120,Testability,test,test,120,"The docs should now be fixed on master. Updating the branch should fix that. I've added a commit to fix that formatting test. @adamgayoso could you compress the data files you're using for testing, and remove to uncompressed files from the git history? This is to keep the repo size as small as possible. Otherwise, @gokceneraslan were you happy with the state this PR is in?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-650924757
https://github.com/scverse/scanpy/pull/1204#issuecomment-650924757:189,Testability,test,testing,189,"The docs should now be fixed on master. Updating the branch should fix that. I've added a commit to fix that formatting test. @adamgayoso could you compress the data files you're using for testing, and remove to uncompressed files from the git history? This is to keep the repo size as small as possible. Otherwise, @gokceneraslan were you happy with the state this PR is in?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-650924757
https://github.com/scverse/scanpy/pull/1204#issuecomment-651244499:45,Modifiability,rewrite,rewrite,45,I think I messed something up when trying to rewrite the history @ivirshup :(,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-651244499
https://github.com/scverse/scanpy/pull/1204#issuecomment-654765480:51,Availability,error,error,51,@ivirshup @gokceneraslan Do you have access to the error details for readthedocs? I get 'page does not exists' error,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-654765480
https://github.com/scverse/scanpy/pull/1204#issuecomment-654765480:111,Availability,error,error,111,@ivirshup @gokceneraslan Do you have access to the error details for readthedocs? I get 'page does not exists' error,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-654765480
https://github.com/scverse/scanpy/pull/1204#issuecomment-654765480:37,Security,access,access,37,@ivirshup @gokceneraslan Do you have access to the error details for readthedocs? I get 'page does not exists' error,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-654765480
https://github.com/scverse/scanpy/pull/1204#issuecomment-663879113:81,Deployability,release,release,81,"@gokceneraslan @ivirshup just checking in on this. Will this be part of the next release? Do you need anything else from me? It might be nice to note somewhere that when `batch_key` is not None, results aren't absolutely consistent with Seurat. > The problem appears to be due to the fact that many genes have the same normalized variance in a given batch and the merging method uses ranks. So I believe the difference is due to genes being sorted differently with the same normalized variance. Perhaps this merging scheme is not ideal.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-663879113
https://github.com/scverse/scanpy/pull/1204#issuecomment-665509704:54,Deployability,release,release,54,"Awesome, thanks for this! I've added something to the release notes, let me know if you'd like to say more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-665509704
https://github.com/scverse/scanpy/issues/1205#issuecomment-702378075:113,Usability,learn,learning,113,"Hi @jorvis ,. ProjectR as far as I'm aware relies on some form of matrix decompositon (PCA, NMF) and do transfer learning with learnt weights. In some sense, it's similar to ingest. However, it would be a bit complicated to port it from R. ; It doesn't seem super complicated to have it in scanpy as an additional tool, but it's not really a priority now. If you have anything in mind and would want to try with submitting a PR, it would be very much appreciated and we would definitely have a look! ; I'll close this for now, but pls feel free to re-open if you want to continue discussion or would like to discuss implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1205#issuecomment-702378075
https://github.com/scverse/scanpy/issues/1205#issuecomment-702378075:127,Usability,learn,learnt,127,"Hi @jorvis ,. ProjectR as far as I'm aware relies on some form of matrix decompositon (PCA, NMF) and do transfer learning with learnt weights. In some sense, it's similar to ingest. However, it would be a bit complicated to port it from R. ; It doesn't seem super complicated to have it in scanpy as an additional tool, but it's not really a priority now. If you have anything in mind and would want to try with submitting a PR, it would be very much appreciated and we would definitely have a look! ; I'll close this for now, but pls feel free to re-open if you want to continue discussion or would like to discuss implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1205#issuecomment-702378075
https://github.com/scverse/scanpy/issues/1208#issuecomment-638496235:272,Safety,safe,safeguards,272,"@shendong124 @ivirshup I assume `normalize_geometric` was intended to be similar to Seurat's centered log ratio transformation, which is implemented as follows in R: `log1p(x = x / (exp(x = sum(log1p(x = x[x > 0]), na.rm = TRUE) / length(x = x))))`. This is CLR with some safeguards for 0 counts. Here's a reimplementation of the Seurat CLR transformation for scanpy. Call this with `clr_normalize_each_cell(adata)`:. ```; def clr_normalize_each_cell(adata, inplace=True):; """"""Normalize count vector for each cell, i.e. for each row of .X"""""". import numpy as np; import scipy. def seurat_clr(x):; # TODO: support sparseness; s = np.sum(np.log1p(x[x > 0])); exp = np.exp(s / len(x)); return np.log1p(x / exp). if not inplace:; adata = adata.copy(). # apply to dense or sparse matrix, along axis. returns dense matrix; adata.X = np.apply_along_axis(; seurat_clr, 1, (adata.X.A if scipy.sparse.issparse(adata.X) else adata.X); ); return adata; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1208#issuecomment-638496235
https://github.com/scverse/scanpy/issues/1208#issuecomment-638496235:102,Testability,log,log,102,"@shendong124 @ivirshup I assume `normalize_geometric` was intended to be similar to Seurat's centered log ratio transformation, which is implemented as follows in R: `log1p(x = x / (exp(x = sum(log1p(x = x[x > 0]), na.rm = TRUE) / length(x = x))))`. This is CLR with some safeguards for 0 counts. Here's a reimplementation of the Seurat CLR transformation for scanpy. Call this with `clr_normalize_each_cell(adata)`:. ```; def clr_normalize_each_cell(adata, inplace=True):; """"""Normalize count vector for each cell, i.e. for each row of .X"""""". import numpy as np; import scipy. def seurat_clr(x):; # TODO: support sparseness; s = np.sum(np.log1p(x[x > 0])); exp = np.exp(s / len(x)); return np.log1p(x / exp). if not inplace:; adata = adata.copy(). # apply to dense or sparse matrix, along axis. returns dense matrix; adata.X = np.apply_along_axis(; seurat_clr, 1, (adata.X.A if scipy.sparse.issparse(adata.X) else adata.X); ); return adata; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1208#issuecomment-638496235
https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104:127,Modifiability,Extend,Extended,127,"@ivirshup @flying-sheep @falexwolf . I would like to merge this branch soon and I would like your opinion on the following:; * Extended functionality and fine tuning of the plots is achieved by using the new plot objects. However, I don't know how they can be documented in readthedocs (or if we want that). Note: the object methods are well documented and can be accessed, for example in Jupyter notebooks. * I am using `return_fig` as an argument to return the plot object in `sc.pl.dotplot` etc. Is this a good name choice?. As mentioned previously, the current PR tries to keep the current functionality with minimal changes to the way functions like `sc.pl.dotplot` are called. On the background, the code was refactored to remove much repetition as possible and allow new functionally. . The current progress on the visualization tutorial is here: https://nbviewer.jupyter.org/github/fidelram/scanpy-tutorials/blob/marker_genes_vis_tutorial/visualizing-marker-genes.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104
https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104:715,Modifiability,refactor,refactored,715,"@ivirshup @flying-sheep @falexwolf . I would like to merge this branch soon and I would like your opinion on the following:; * Extended functionality and fine tuning of the plots is achieved by using the new plot objects. However, I don't know how they can be documented in readthedocs (or if we want that). Note: the object methods are well documented and can be accessed, for example in Jupyter notebooks. * I am using `return_fig` as an argument to return the plot object in `sc.pl.dotplot` etc. Is this a good name choice?. As mentioned previously, the current PR tries to keep the current functionality with minimal changes to the way functions like `sc.pl.dotplot` are called. On the background, the code was refactored to remove much repetition as possible and allow new functionally. . The current progress on the visualization tutorial is here: https://nbviewer.jupyter.org/github/fidelram/scanpy-tutorials/blob/marker_genes_vis_tutorial/visualizing-marker-genes.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104
https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104:364,Security,access,accessed,364,"@ivirshup @flying-sheep @falexwolf . I would like to merge this branch soon and I would like your opinion on the following:; * Extended functionality and fine tuning of the plots is achieved by using the new plot objects. However, I don't know how they can be documented in readthedocs (or if we want that). Note: the object methods are well documented and can be accessed, for example in Jupyter notebooks. * I am using `return_fig` as an argument to return the plot object in `sc.pl.dotplot` etc. Is this a good name choice?. As mentioned previously, the current PR tries to keep the current functionality with minimal changes to the way functions like `sc.pl.dotplot` are called. On the background, the code was refactored to remove much repetition as possible and allow new functionally. . The current progress on the visualization tutorial is here: https://nbviewer.jupyter.org/github/fidelram/scanpy-tutorials/blob/marker_genes_vis_tutorial/visualizing-marker-genes.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104
https://github.com/scverse/scanpy/pull/1210#issuecomment-644215713:219,Safety,avoid,avoid,219,"In the dotplots, when the color_on == 'square', fixed dot edge color might lead to some trouble in visibility when the square color and dot edge color happen to be similar. There is a nice feature of seaborn heatmap to avoid exactly that, where the annotation text color is determined conditionally on the square color, see [here](https://seaborn.pydata.org/generated/seaborn.heatmap.html) for the documentation and [here](https://github.com/mwaskom/seaborn/blob/master/seaborn/matrix.py#L261) for the relevant code:. ![image](https://user-images.githubusercontent.com/1140359/84678062-8f28dc00-aefd-11ea-84f5-d1b4f1496814.png). It can be too much work, feel free to ignore but just wanted to highlight.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-644215713
https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250:176,Energy Efficiency,adapt,adapted,176,"@gokceneraslan Thanks for pointing this out! The `relative_luminance` function required for this can be easily imported from seaborn, therefore, I imagine that the code can be adapted easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250
https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250:176,Modifiability,adapt,adapted,176,"@gokceneraslan Thanks for pointing this out! The `relative_luminance` function required for this can be easily imported from seaborn, therefore, I imagine that the code can be adapted easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250
https://github.com/scverse/scanpy/pull/1210#issuecomment-651513742:350,Availability,error,errors,350,"I think the issue that there are references to the `DotPlot` class, but no doc page get's generated for that class. I think this can be fixed by adding something like:. ```rst; Classes used for these plots:. .. autosummary::; :toctree: . pl._dotplot.DotPlot; ```. to the doc-string of the plotting module. Once this exists, there might be some other errors that pop up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-651513742
https://github.com/scverse/scanpy/pull/1210#issuecomment-682371282:63,Deployability,release,release,63,"Is because we changed dot edge the defaults shortly before the release. Time to add a test for this. I will make a fix but meanwhile you can trigger the dynamic coloring by setting `dot_edge_color` and `dot_edge_lw` as `None`:. ```PYTHON; sc.pl.dotplot(adata, markers, groupby='bulk_labels', return_fig=True)\; .style(color_on='square', dot_edge_color=None, dot_edge_lw=None).show(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-682371282
https://github.com/scverse/scanpy/pull/1210#issuecomment-682371282:86,Testability,test,test,86,"Is because we changed dot edge the defaults shortly before the release. Time to add a test for this. I will make a fix but meanwhile you can trigger the dynamic coloring by setting `dot_edge_color` and `dot_edge_lw` as `None`:. ```PYTHON; sc.pl.dotplot(adata, markers, groupby='bulk_labels', return_fig=True)\; .style(color_on='square', dot_edge_color=None, dot_edge_lw=None).show(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-682371282
https://github.com/scverse/scanpy/issues/1211#issuecomment-702374955:17,Availability,ping,pinging,17,"I believe it is, pinging @Koncopd who probably knows more about it. If this is desired behaviour, we could close this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1211#issuecomment-702374955
https://github.com/scverse/scanpy/issues/1213#issuecomment-629970781:512,Deployability,update,updated,512,"It makes sense to use AND logic, because the function keeps genes that satisfy all three conditions. ; 1) Fraction of cells inside the cluster expressing the gene must be greater than `min_in_group_fraction`; 2) Fractions of cells outside the cluster expressing the gene must be less than `max_out_group_fraction`; 3) Fold change must be greater than `min_fold_change`. But there are remaining issues (calculation of fold change and using the absolute value of the fold change) in this function that needs to be updated #863",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-629970781
https://github.com/scverse/scanpy/issues/1213#issuecomment-629970781:26,Testability,log,logic,26,"It makes sense to use AND logic, because the function keeps genes that satisfy all three conditions. ; 1) Fraction of cells inside the cluster expressing the gene must be greater than `min_in_group_fraction`; 2) Fractions of cells outside the cluster expressing the gene must be less than `max_out_group_fraction`; 3) Fold change must be greater than `min_fold_change`. But there are remaining issues (calculation of fold change and using the absolute value of the fold change) in this function that needs to be updated #863",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-629970781
https://github.com/scverse/scanpy/issues/1213#issuecomment-630475750:684,Deployability,update,updated,684,"Thanks for clarification. I had thought it is for filtering out genes. On Mon, May 18, 2020 at 2:21 AM Rachel Ng <notifications@github.com> wrote:. > It makes sense to use AND logic, because the function keeps genes that; > satisfy all three conditions.; >; > 1. Fraction of cells inside the cluster expressing the gene must be; > greater than min_in_group_fraction; > 2. Fractions of cells outside the cluster expressing the gene must be; > less than max_out_group_fraction; > 3. Fold change must be greater than min_fold_change; >; > But there are remaining issues (calculation of fold change and using the; > absolute value of the fold change) in this function that needs to be; > updated #863 <https://github.com/theislab/scanpy/issues/863>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1213#issuecomment-629970781>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADG2PVZVFSYU3ST4ESJFS33RSDHVXANCNFSM4NAA5V2A>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-630475750
https://github.com/scverse/scanpy/issues/1213#issuecomment-630475750:176,Testability,log,logic,176,"Thanks for clarification. I had thought it is for filtering out genes. On Mon, May 18, 2020 at 2:21 AM Rachel Ng <notifications@github.com> wrote:. > It makes sense to use AND logic, because the function keeps genes that; > satisfy all three conditions.; >; > 1. Fraction of cells inside the cluster expressing the gene must be; > greater than min_in_group_fraction; > 2. Fractions of cells outside the cluster expressing the gene must be; > less than max_out_group_fraction; > 3. Fold change must be greater than min_fold_change; >; > But there are remaining issues (calculation of fold change and using the; > absolute value of the fold change) in this function that needs to be; > updated #863 <https://github.com/theislab/scanpy/issues/863>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1213#issuecomment-629970781>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADG2PVZVFSYU3ST4ESJFS33RSDHVXANCNFSM4NAA5V2A>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-630475750
https://github.com/scverse/scanpy/issues/1213#issuecomment-630597880:58,Deployability,update,update,58,After https://github.com/theislab/scanpy/pull/1156 I will update the function.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-630597880
https://github.com/scverse/scanpy/issues/1213#issuecomment-696776848:21,Deployability,update,update,21,"> After #1156 I will update the function. Wait, does it use OR logic now?? Doesn't AND logic make more sense???",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-696776848
https://github.com/scverse/scanpy/issues/1213#issuecomment-696776848:63,Testability,log,logic,63,"> After #1156 I will update the function. Wait, does it use OR logic now?? Doesn't AND logic make more sense???",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-696776848
https://github.com/scverse/scanpy/issues/1213#issuecomment-696776848:87,Testability,log,logic,87,"> After #1156 I will update the function. Wait, does it use OR logic now?? Doesn't AND logic make more sense???",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1213#issuecomment-696776848
https://github.com/scverse/scanpy/pull/1216#issuecomment-632506015:7,Deployability,install,install,7,leiden install via conda code is wrong in the current page. it should be:. ```; conda install -c conda-forge leidenalg ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1216#issuecomment-632506015
https://github.com/scverse/scanpy/pull/1216#issuecomment-632506015:86,Deployability,install,install,86,leiden install via conda code is wrong in the current page. it should be:. ```; conda install -c conda-forge leidenalg ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1216#issuecomment-632506015
https://github.com/scverse/scanpy/pull/1217#issuecomment-630021238:243,Deployability,release,release,243,"@giovp, I'll merge this. I'm merging a couple other things first though. I'm not super happy with the logic flow here at the moment. Could we aim for separating out the code for scatter plots, and overlaying grids on-top of images in the next release cycle?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1217#issuecomment-630021238
https://github.com/scverse/scanpy/pull/1217#issuecomment-630021238:102,Testability,log,logic,102,"@giovp, I'll merge this. I'm merging a couple other things first though. I'm not super happy with the logic flow here at the moment. Could we aim for separating out the code for scatter plots, and overlaying grids on-top of images in the next release cycle?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1217#issuecomment-630021238
https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437:61,Availability,error,error,61,"it's not clear what the problem here sorry, can you copy the error and report a reproducible example? thank you!; I'll close this for now, feel free to reopen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437
https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437:9,Usability,clear,clear,9,"it's not clear what the problem here sorry, can you copy the error and report a reproducible example? thank you!; I'll close this for now, feel free to reopen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1220#issuecomment-702374437
https://github.com/scverse/scanpy/issues/1220#issuecomment-702550857:320,Availability,error,error,320,"Hi @JayalalKJ ,; as you also pointed out, this issue is related to an environment in https://github.com/theislab/single-cell-tutorial; It's best if you open an issue there and directly address maintainers of that repo. ; Beside that, we can't really help you in this case because we don't have enough information on the error and also it relates to an external package. We could provide you with more help if you post the complete error log, but pls do so not here but in the other repo.; Hope this is helpful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1220#issuecomment-702550857
https://github.com/scverse/scanpy/issues/1220#issuecomment-702550857:431,Availability,error,error,431,"Hi @JayalalKJ ,; as you also pointed out, this issue is related to an environment in https://github.com/theislab/single-cell-tutorial; It's best if you open an issue there and directly address maintainers of that repo. ; Beside that, we can't really help you in this case because we don't have enough information on the error and also it relates to an external package. We could provide you with more help if you post the complete error log, but pls do so not here but in the other repo.; Hope this is helpful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1220#issuecomment-702550857
https://github.com/scverse/scanpy/issues/1220#issuecomment-702550857:437,Testability,log,log,437,"Hi @JayalalKJ ,; as you also pointed out, this issue is related to an environment in https://github.com/theislab/single-cell-tutorial; It's best if you open an issue there and directly address maintainers of that repo. ; Beside that, we can't really help you in this case because we don't have enough information on the error and also it relates to an external package. We could provide you with more help if you post the complete error log, but pls do so not here but in the other repo.; Hope this is helpful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1220#issuecomment-702550857
https://github.com/scverse/scanpy/pull/1224#issuecomment-631242191:121,Deployability,release,release,121,"Thanks for this PR, this looks interesting! Sorry for taking a while to get back to you, we've been quite busy getting a release out. We'll try and get back to you with more in the next couple weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1224#issuecomment-631242191
https://github.com/scverse/scanpy/pull/1226#issuecomment-630023263:43,Testability,log,logging,43,"In my opinion, we'll likely move away from logging everything. Isaac built this in so that one can conveniently visualize things in seaborn; I added the switch to turn it off so that the basic tutorial of v1.5 doesn't lead to a completely cluttered AnnData object. But, I guess, we all agree that this here isn't the final solution. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1226#issuecomment-630023263
https://github.com/scverse/scanpy/issues/1227#issuecomment-659856395:45,Deployability,install,install,45,I meet the same problem. You could use:. pip install matplotlib==2.2.3. I tried just. And it done.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1227#issuecomment-659856395
https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650:121,Availability,error,error,121,"We require matplotlib 3.x for other parts of scanpy, so that’s not a real solution. As you can see in the traceback, the error happens in `networkx`. It has been fixed in networkx/networkx#3179 ([networkx 2.3](https://networkx.github.io/documentation/stable/release/release_2.3.html)) in April 2019. So you should upgrade networkx instead of downgrading matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650
https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650:342,Availability,down,downgrading,342,"We require matplotlib 3.x for other parts of scanpy, so that’s not a real solution. As you can see in the traceback, the error happens in `networkx`. It has been fixed in networkx/networkx#3179 ([networkx 2.3](https://networkx.github.io/documentation/stable/release/release_2.3.html)) in April 2019. So you should upgrade networkx instead of downgrading matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650
https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650:258,Deployability,release,release,258,"We require matplotlib 3.x for other parts of scanpy, so that’s not a real solution. As you can see in the traceback, the error happens in `networkx`. It has been fixed in networkx/networkx#3179 ([networkx 2.3](https://networkx.github.io/documentation/stable/release/release_2.3.html)) in April 2019. So you should upgrade networkx instead of downgrading matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650
https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650:314,Deployability,upgrade,upgrade,314,"We require matplotlib 3.x for other parts of scanpy, so that’s not a real solution. As you can see in the traceback, the error happens in `networkx`. It has been fixed in networkx/networkx#3179 ([networkx 2.3](https://networkx.github.io/documentation/stable/release/release_2.3.html)) in April 2019. So you should upgrade networkx instead of downgrading matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1227#issuecomment-661039650
https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:169,Availability,avail,available,169,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395
https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:382,Availability,down,downstream,382,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395
https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:257,Deployability,install,installed,257,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395
https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:215,Integrability,depend,depend,215,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395
https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:1649,Integrability,wrap,wrapped,1649,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395
https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:846,Modifiability,flexible,flexible,846,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395
https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:970,Performance,perform,performance,970,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395
https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:1251,Usability,learn,learning,1251,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395
https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550:201,Integrability,depend,depend,201,"Thanks! Can I ask for two clarifications before replying:. > Right now, we tend to use a connectivity graph built by UMAP ... UMAP uses Pynndescent to construct the kNN graph. So does it mean that you depend on Pynndescent to construct the kNN graph, and then if the user calls UMAP, it's run on this previously constructred kNN graph?. By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy (like Euclidean or cosine). It seems to work quite a bit faster. For sparse input data and/or fancy metrics, it uses Pynndescent. > ... but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. What are the use cases here that you thinking of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550
https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550:422,Usability,simpl,simple,422,"Thanks! Can I ask for two clarifications before replying:. > Right now, we tend to use a connectivity graph built by UMAP ... UMAP uses Pynndescent to construct the kNN graph. So does it mean that you depend on Pynndescent to construct the kNN graph, and then if the user calls UMAP, it's run on this previously constructred kNN graph?. By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy (like Euclidean or cosine). It seems to work quite a bit faster. For sparse input data and/or fancy metrics, it uses Pynndescent. > ... but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. What are the use cases here that you thinking of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550
https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092:112,Deployability,install,installed,112,"That sounds mostly right. We use the `nearest_neighbors` function from `umap`, which uses `pynndescent` if it's installed. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/neighbors/__init__.py#L270-L280. > By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy. I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. I'm definitely for being more generic about how the neighbors graph is generated and weighted. I haven't seen anything yet which looks at the character of the inaccuracies for each method, something that's probably important when they're used for classification. > What are the use cases here that you thinking of?. Mainly cases of merged graphs, like when you have multiple datasets or modalities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092
https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092:331,Usability,simpl,simple,331,"That sounds mostly right. We use the `nearest_neighbors` function from `umap`, which uses `pynndescent` if it's installed. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/neighbors/__init__.py#L270-L280. > By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy. I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. I'm definitely for being more generic about how the neighbors graph is generated and weighted. I haven't seen anything yet which looks at the character of the inaccuracies for each method, something that's probably important when they're used for classification. > What are the use cases here that you thinking of?. Mainly cases of merged graphs, like when you have multiple datasets or modalities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633348092
https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:581,Modifiability,flexible,flexible,581,"> I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. You can see some quick comparisons between Pynndescent and Annoy here: https://github.com/pavlin-policar/openTSNE/issues/101#issuecomment-597178379. But I have not investigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833
https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:548,Testability,benchmark,benchmarks,548,"> I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. You can see some quick comparisons between Pynndescent and Annoy here: https://github.com/pavlin-policar/openTSNE/issues/101#issuecomment-597178379. But I have not investigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833
https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:1733,Usability,learn,learning,1733,"tigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal tSNE embeddings. The same about initialization: UMAP smartly uses Laplacian Eigenmaps to initialize, but sklearn/multicore tSNE use random init, which is simply a bad choice (as again shown in that paper). openTSNE now uses PCA init by default which is much more sensible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833
https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:1781,Usability,simpl,simply,1781,"tigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal tSNE embeddings. The same about initialization: UMAP smartly uses Laplacian Eigenmaps to initialize, but sklearn/multicore tSNE use random init, which is simply a bad choice (as again shown in that paper). openTSNE now uses PCA init by default which is much more sensible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833
https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:2154,Usability,simpl,simply,2154,"tigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal tSNE embeddings. The same about initialization: UMAP smartly uses Laplacian Eigenmaps to initialize, but sklearn/multicore tSNE use random init, which is simply a bad choice (as again shown in that paper). openTSNE now uses PCA init by default which is much more sensible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833
https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724:38,Deployability,Integrat,Integrating,38,"Glad to see this discussion going on. Integrating openTSNE into scanpy should be fairly straightforward but may require some thought. I think Dmitry has already pointed out the most important things such as improved defaults, which other t-SNE implementations are lagging behind in. Apart from that, we package prebuilt binaries, so adding openTSNE as a dependency would be incredibly easy. The main thing we'd have to agree on is how to deal with the KNN graphs. UMAP by default calculates 15 nearest neighbors, and from what I can tell, louvain and leiden clustering both use those 15 neighbors as well by default. t-SNE, on the other hand, calculates 90 nearest neighbors by default. This is every single t-SNE implementation, not just openTSNE. Dmitry suggested using a uniform kernel with 15 neighbors, which would fit elegantly, but then again, this isn't truly t-SNE anymore, but rather something very close. The same goes for the `ingest` functionality. openTSNE does something similar to UMAP for adding new samples to existing embeddings, and then we'd again have to figure out how to calculate nearest neighbors from the new data to the reference data. I don't know how you do this currently for UMAP. I'm not exactly sure how these neighbors are meant to be used in scanpy, since there are several different algorithms that use them. Graph-based clustering uses the KNNG, UMAP uses it, forceatlas2 uses it, PAGA probably as well? Is relying on a single k=15 from UMAP for everything really ok? For example, seurat defaults to using 30 neighbors for clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724
https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724:38,Integrability,Integrat,Integrating,38,"Glad to see this discussion going on. Integrating openTSNE into scanpy should be fairly straightforward but may require some thought. I think Dmitry has already pointed out the most important things such as improved defaults, which other t-SNE implementations are lagging behind in. Apart from that, we package prebuilt binaries, so adding openTSNE as a dependency would be incredibly easy. The main thing we'd have to agree on is how to deal with the KNN graphs. UMAP by default calculates 15 nearest neighbors, and from what I can tell, louvain and leiden clustering both use those 15 neighbors as well by default. t-SNE, on the other hand, calculates 90 nearest neighbors by default. This is every single t-SNE implementation, not just openTSNE. Dmitry suggested using a uniform kernel with 15 neighbors, which would fit elegantly, but then again, this isn't truly t-SNE anymore, but rather something very close. The same goes for the `ingest` functionality. openTSNE does something similar to UMAP for adding new samples to existing embeddings, and then we'd again have to figure out how to calculate nearest neighbors from the new data to the reference data. I don't know how you do this currently for UMAP. I'm not exactly sure how these neighbors are meant to be used in scanpy, since there are several different algorithms that use them. Graph-based clustering uses the KNNG, UMAP uses it, forceatlas2 uses it, PAGA probably as well? Is relying on a single k=15 from UMAP for everything really ok? For example, seurat defaults to using 30 neighbors for clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724
https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724:354,Integrability,depend,dependency,354,"Glad to see this discussion going on. Integrating openTSNE into scanpy should be fairly straightforward but may require some thought. I think Dmitry has already pointed out the most important things such as improved defaults, which other t-SNE implementations are lagging behind in. Apart from that, we package prebuilt binaries, so adding openTSNE as a dependency would be incredibly easy. The main thing we'd have to agree on is how to deal with the KNN graphs. UMAP by default calculates 15 nearest neighbors, and from what I can tell, louvain and leiden clustering both use those 15 neighbors as well by default. t-SNE, on the other hand, calculates 90 nearest neighbors by default. This is every single t-SNE implementation, not just openTSNE. Dmitry suggested using a uniform kernel with 15 neighbors, which would fit elegantly, but then again, this isn't truly t-SNE anymore, but rather something very close. The same goes for the `ingest` functionality. openTSNE does something similar to UMAP for adding new samples to existing embeddings, and then we'd again have to figure out how to calculate nearest neighbors from the new data to the reference data. I don't know how you do this currently for UMAP. I'm not exactly sure how these neighbors are meant to be used in scanpy, since there are several different algorithms that use them. Graph-based clustering uses the KNNG, UMAP uses it, forceatlas2 uses it, PAGA probably as well? Is relying on a single k=15 from UMAP for everything really ok? For example, seurat defaults to using 30 neighbors for clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724
https://github.com/scverse/scanpy/issues/1233#issuecomment-657406330:890,Usability,simpl,simplicity,890,"> UMAP by default calculates 15 nearest neighbors, and from what I can tell, louvain and leiden clustering both use those 15 neighbors as well by default. Both of those clustering algorithms just use whatever graph is passed, so this shouldn't be an issue. > t-SNE, on the other hand, calculates 90 nearest neighbors by default.; > openTSNE does something similar to UMAP for adding new samples to existing embeddings. Could there just be a separate function for computing neighbors for tsne? `sc.pp.neighbors` can be considered to be ""compute nearest neighbors and connectivity as expected by UMAP"", while a separate function could use methods and defaults appropriate to openTSNE. @Koncopd would have more to say on how this should work w.r.t. `ingest`. > Is relying on a single k=15 from UMAP for everything really ok?. Ultimately, up to the user. There is an element of consistency and simplicity to using the same representation of the data for multiple parts of the analysis. I think there would have to be a good reason for using a different connectivity matrix for the 2d embedding and for the clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-657406330
https://github.com/scverse/scanpy/issues/1233#issuecomment-748543070:1348,Availability,avail,available,1348,"@ivirshup @pavlin-policar I'd like to get back to this issue. I think maybe we should postpone discussing `ingest` until later (as well as ""recipes"" based on our Nat Comms paper, and other topics raised above) and focus on switching to openTSNE first. Scanpy's architecture is to compute kNN graph by calling `sc.pp.neighbors` and then run dimensionality reduction (UMAP) and clustering (Leiden) on this graph. I think this is very neat and makes everything consistent and other functions should follow this approach as much as possible. So IMHO if it's possible to run t-SNE on the kNN graph with k=15, then that's what we should do. And luckily it is possible! I can even see two approaches. (1) Either use k=15 kNN graph with the uniform similarity kernel. As I said, and as Pavlin knows, this yields result that is *very* similar to using perplexity=30. (2) Or use k=15 kNN graph with UMAP weights, normalize it as t-SNE expects it to be normalized and use that. My expectation is that it would yield very similar results, but I haven't actually tried it. Admittedly, this is not the ""vanilla"" t-SNE. But it's very close. And I think advantages outweigh the disadvantages. Actually this is quite a bit faster than the standard t-SNE, because it only uses k=15 instead of k=90 (3 times perplexity=30). Moreover, we could make the standard t-SNE available by extending `sc.pp.neighors` with `method=""tsne""` (there are several `method`s there already). What I mean is that . ```; sc.pp.neighors(); sc.tl.tsne(); ```; would use let's say uniform kernel on k=15 kNN neighbor graph (and maybe print a warning about it, but I am not even sure it's needed), while. ```; sc.pp.neighbors(method=""tsne"", perplexity=30); sc.tl.tsne(); ```; would construct k=90 weighted kNN graph as standard t-SNE does and then use that. Either way, `sc.tl.tsne()` runs openTSNE with the pre-defined affinity matrix. Thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748543070
https://github.com/scverse/scanpy/issues/1233#issuecomment-748543070:1361,Modifiability,extend,extending,1361,"@ivirshup @pavlin-policar I'd like to get back to this issue. I think maybe we should postpone discussing `ingest` until later (as well as ""recipes"" based on our Nat Comms paper, and other topics raised above) and focus on switching to openTSNE first. Scanpy's architecture is to compute kNN graph by calling `sc.pp.neighbors` and then run dimensionality reduction (UMAP) and clustering (Leiden) on this graph. I think this is very neat and makes everything consistent and other functions should follow this approach as much as possible. So IMHO if it's possible to run t-SNE on the kNN graph with k=15, then that's what we should do. And luckily it is possible! I can even see two approaches. (1) Either use k=15 kNN graph with the uniform similarity kernel. As I said, and as Pavlin knows, this yields result that is *very* similar to using perplexity=30. (2) Or use k=15 kNN graph with UMAP weights, normalize it as t-SNE expects it to be normalized and use that. My expectation is that it would yield very similar results, but I haven't actually tried it. Admittedly, this is not the ""vanilla"" t-SNE. But it's very close. And I think advantages outweigh the disadvantages. Actually this is quite a bit faster than the standard t-SNE, because it only uses k=15 instead of k=90 (3 times perplexity=30). Moreover, we could make the standard t-SNE available by extending `sc.pp.neighors` with `method=""tsne""` (there are several `method`s there already). What I mean is that . ```; sc.pp.neighors(); sc.tl.tsne(); ```; would use let's say uniform kernel on k=15 kNN neighbor graph (and maybe print a warning about it, but I am not even sure it's needed), while. ```; sc.pp.neighbors(method=""tsne"", perplexity=30); sc.tl.tsne(); ```; would construct k=90 weighted kNN graph as standard t-SNE does and then use that. Either way, `sc.tl.tsne()` runs openTSNE with the pre-defined affinity matrix. Thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748543070
https://github.com/scverse/scanpy/issues/1233#issuecomment-748670360:1452,Availability,avail,available,1452,"itch the t-SNE implementation to openTSNE at the very least. For the recipes, there' already something similar in the preprocessing module. So I'd imagine calling standard t-SNE with `sc.tl.tsne` and the recipes like `sc.tl.tsne.recipe_multiscale`. > And luckily it is possible! I can even see two approaches. (1) Either use k=15 kNN graph with the uniform similarity kernel. As I said, and as Pavlin knows, this yields result that is very similar to using perplexity=30. (2) Or use k=15 kNN graph with UMAP weights, normalize it as t-SNE expects it to be normalized and use that. My expectation is that it would yield very similar results, but I haven't actually tried it. I very much prefer option 1. If I understand option 2 correctly, we would normalize the 15 neighbors to essentially `perplexity=5`. I've never once found a case where that is useful, so having this as the default behaviour in scanpy seems like a really bad idea (I foresee a lot of issues in the style ""why is t-SNE not working?""). Using a uniform kernel produces results that are virtually indistinguishable from vanilla t-SNE, so that's fine IMO, and it's faster as well. It's still less than the default `perplexity=30`, but this seems like the best option. Whatever we agree on, the same can be applied to the ingest functionality, so adding that would also be straightforward. > Moreover, we could make the standard t-SNE available by extending sc.pp.neighors with method=""tsne"" (there are several methods there already). I don't understand this, why would this belong on `sc.pp.neighbors`? The graph weighing should go into the `sc.tl.tsne` call. Are the UMAP weights assigned to the graph in `sc.pp.neighbors`? That seems questionable. I would expect the output to be a directed, unweighted graph, and let each method take care of the graph. If anything, I'd expect it to weight it using the Jaccard index of shared nearest neighbors, which seems to me like pretty much the standard thing to do in single-cell analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748670360
https://github.com/scverse/scanpy/issues/1233#issuecomment-748670360:1465,Modifiability,extend,extending,1465,"itch the t-SNE implementation to openTSNE at the very least. For the recipes, there' already something similar in the preprocessing module. So I'd imagine calling standard t-SNE with `sc.tl.tsne` and the recipes like `sc.tl.tsne.recipe_multiscale`. > And luckily it is possible! I can even see two approaches. (1) Either use k=15 kNN graph with the uniform similarity kernel. As I said, and as Pavlin knows, this yields result that is very similar to using perplexity=30. (2) Or use k=15 kNN graph with UMAP weights, normalize it as t-SNE expects it to be normalized and use that. My expectation is that it would yield very similar results, but I haven't actually tried it. I very much prefer option 1. If I understand option 2 correctly, we would normalize the 15 neighbors to essentially `perplexity=5`. I've never once found a case where that is useful, so having this as the default behaviour in scanpy seems like a really bad idea (I foresee a lot of issues in the style ""why is t-SNE not working?""). Using a uniform kernel produces results that are virtually indistinguishable from vanilla t-SNE, so that's fine IMO, and it's faster as well. It's still less than the default `perplexity=30`, but this seems like the best option. Whatever we agree on, the same can be applied to the ingest functionality, so adding that would also be straightforward. > Moreover, we could make the standard t-SNE available by extending sc.pp.neighors with method=""tsne"" (there are several methods there already). I don't understand this, why would this belong on `sc.pp.neighbors`? The graph weighing should go into the `sc.tl.tsne` call. Are the UMAP weights assigned to the graph in `sc.pp.neighbors`? That seems questionable. I would expect the output to be a directed, unweighted graph, and let each method take care of the graph. If anything, I'd expect it to weight it using the Jaccard index of shared nearest neighbors, which seems to me like pretty much the standard thing to do in single-cell analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748670360
https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742:464,Energy Efficiency,adapt,adaptive,464,"> I don't understand this, why would this belong on sc.pp.neighbors? The graph weighing should go into the sc.tl.tsne call. Are the UMAP weights assigned to the graph in sc.pp.neighbors?. Yes, this is my understanding of how it works in scanpy. See https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.neighbors.html:. ```; method : {‘umap’, ‘gauss’, ‘rapids’}, None (default: 'umap'). Use ‘umap’ [McInnes18] or ‘gauss’ (Gauss kernel following [Coifman05] with ; adaptive width [Haghverdi16]) for computing connectivities. Use ‘rapids’ for the; RAPIDS implementation of UMAP (experimental, GPU only).; ```. > If I understand option 2 correctly, we would normalize the 15 neighbors to essentially perplexity=5. That's not what I meant. I meant taking UMAP's weights for k=15 and normalizing the matrix so that it sums to 1, as in t-SNE. That said, I would also prefer option 1 because I don't want anything that sounds like it's a UMAP/t-SNE hybrid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742
https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742:464,Modifiability,adapt,adaptive,464,"> I don't understand this, why would this belong on sc.pp.neighbors? The graph weighing should go into the sc.tl.tsne call. Are the UMAP weights assigned to the graph in sc.pp.neighbors?. Yes, this is my understanding of how it works in scanpy. See https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.neighbors.html:. ```; method : {‘umap’, ‘gauss’, ‘rapids’}, None (default: 'umap'). Use ‘umap’ [McInnes18] or ‘gauss’ (Gauss kernel following [Coifman05] with ; adaptive width [Haghverdi16]) for computing connectivities. Use ‘rapids’ for the; RAPIDS implementation of UMAP (experimental, GPU only).; ```. > If I understand option 2 correctly, we would normalize the 15 neighbors to essentially perplexity=5. That's not what I meant. I meant taking UMAP's weights for k=15 and normalizing the matrix so that it sums to 1, as in t-SNE. That said, I would also prefer option 1 because I don't want anything that sounds like it's a UMAP/t-SNE hybrid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742
https://github.com/scverse/scanpy/issues/1235#issuecomment-649557649:15,Deployability,update,update,15,"I just made an update in the PR #1210 that will solve the issue. Now you can do:. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); # make a heatmap with all the 765 genes in the dataset, highlight each 50th gene; ax_dict = sc.pl.heatmap(adata, adata.var_names, groupby='louvain', show=False, show_gene_labels=True, figsize=(7,4)); ax_dict['heatmap_ax'].set_xticks(range(len(adata.var_names))[::50]); ax_dict['heatmap_ax'].set_xticklabels(adata.var_names[::50]) ; ```. ![image](https://user-images.githubusercontent.com/4964309/85733220-4db5df00-b6fc-11ea-9c5c-d657ebb136c8.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1235#issuecomment-649557649
https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992:44,Deployability,update,updated,44,I am facing the same issue. I have recently updated my scanpy to the latest version.; I think that it was working before that. Here is rest of my software versions; scanpy==1.4.6 anndata==0.7.1 umap==0.3.9 numpy==1.17.4 scipy==1.3.1 pandas==0.25.3 scikit-learn==0.22 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992
https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992:255,Usability,learn,learn,255,I am facing the same issue. I have recently updated my scanpy to the latest version.; I think that it was working before that. Here is rest of my software versions; scanpy==1.4.6 anndata==0.7.1 umap==0.3.9 numpy==1.17.4 scipy==1.3.1 pandas==0.25.3 scikit-learn==0.22 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1237#issuecomment-632650992
https://github.com/scverse/scanpy/issues/1237#issuecomment-638408390:138,Deployability,release,release,138,@gokceneraslan . A fix is provided here https://github.com/theislab/scanpy/pull/1245. In the meanwhile until this PR is merged into a new release you can follow this https://github.com/dpeerlab/Palantir/issues/34#issuecomment-632933449,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1237#issuecomment-638408390
https://github.com/scverse/scanpy/issues/1239#issuecomment-631872010:111,Availability,down,downstream,111,"I found that after doing deep copy, sc.tl.pca doesn't change the PC values in the object, which may affect the downstream umap and Leiden clustering. . But why? I thought a deep copied object was supposed to behave the same as the non-deep copy one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631872010
https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443:507,Deployability,release,release,507,"Copying using the `copy` module is a bit ill defined for `AnnData` objects currently. This has to do with some internals of how we do views of arrays. In general I'd recommend doing copies via `adata.copy()`, which performs a deep copy. But it looks like there might be another problem with the PCA not being exactly reproducible. After a fair amount of checking that it was exactly reproducible, it looks like we forgot to actually pass the random seed... There has been fixed, and there will be a bug-fix release soon (#1240). This still does not fix the issue of reproducibility if you've made a shallow copy of a AnnData view with `copy`. I'll have to look into this a bit more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443
https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443:215,Performance,perform,performs,215,"Copying using the `copy` module is a bit ill defined for `AnnData` objects currently. This has to do with some internals of how we do views of arrays. In general I'd recommend doing copies via `adata.copy()`, which performs a deep copy. But it looks like there might be another problem with the PCA not being exactly reproducible. After a fair amount of checking that it was exactly reproducible, it looks like we forgot to actually pass the random seed... There has been fixed, and there will be a bug-fix release soon (#1240). This still does not fix the issue of reproducibility if you've made a shallow copy of a AnnData view with `copy`. I'll have to look into this a bit more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443
https://github.com/scverse/scanpy/issues/1239#issuecomment-631969151:4,Deployability,release,release,4,The release with PCA bug fix is now on pypi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631969151
https://github.com/scverse/scanpy/issues/1239#issuecomment-631984912:118,Deployability,update,updated,118,Hmm. Did your original object already have a pca computed on it? I'm not sure if the values in `obsm` would have been updated when you made a shallow copy with `copy`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631984912
https://github.com/scverse/scanpy/pull/1241#issuecomment-631952736:0,Availability,Ping,Ping,0,Ping @flying-sheep,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1241#issuecomment-631952736
https://github.com/scverse/scanpy/pull/1241#issuecomment-635926781:25,Availability,failure,failure,25,"can you link a doc build failure, please? Then I can check out why it fails. I assume they changed the documented location of the class. We have it in `qualname_overrides` and should probably just update the location there (or remove it that line if the new documented location now matches the qualified name). https://github.com/theislab/scanpy/blob/e5d246aacc71fb9ed71d49e2e7d5e26743fd4acb/docs/conf.py#L136-L140",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1241#issuecomment-635926781
https://github.com/scverse/scanpy/pull/1241#issuecomment-635926781:197,Deployability,update,update,197,"can you link a doc build failure, please? Then I can check out why it fails. I assume they changed the documented location of the class. We have it in `qualname_overrides` and should probably just update the location there (or remove it that line if the new documented location now matches the qualified name). https://github.com/theislab/scanpy/blob/e5d246aacc71fb9ed71d49e2e7d5e26743fd4acb/docs/conf.py#L136-L140",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1241#issuecomment-635926781
https://github.com/scverse/scanpy/issues/1242#issuecomment-632446507:82,Deployability,install,installed,82,@icml-compbio The `draw_graph` function calls out to `forceatlas2` if you have it installed. This does seem slower than using UMAP. @YubinXie I see some multithreading being used on my machine when I run `neighbors`. Is there none on yours? One thing I'd check first is to make sure UMAP is up to date and install `pynndescent`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1242#issuecomment-632446507
https://github.com/scverse/scanpy/issues/1242#issuecomment-632446507:306,Deployability,install,install,306,@icml-compbio The `draw_graph` function calls out to `forceatlas2` if you have it installed. This does seem slower than using UMAP. @YubinXie I see some multithreading being used on my machine when I run `neighbors`. Is there none on yours? One thing I'd check first is to make sure UMAP is up to date and install `pynndescent`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1242#issuecomment-632446507
https://github.com/scverse/scanpy/issues/1242#issuecomment-632470036:81,Deployability,release,release,81,"Ah, my bad, I read the path wrong on my phone. In general, the most recent numba release cycle had a lot of deprecations, so many packages are throwing numba warnings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1242#issuecomment-632470036
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:31,Deployability,install,installation,31,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:66,Deployability,install,install,66,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:191,Deployability,install,installations,191,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:279,Deployability,install,installation,279,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:327,Deployability,install,installing,327,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:353,Deployability,Install,Installing,353,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:393,Deployability,install,install,393,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:456,Deployability,install,install,456,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:547,Deployability,install,installation,547,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:595,Deployability,install,installing,595,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:621,Deployability,Install,Installing,621,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:661,Deployability,install,install,661,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825:416,Usability,learn,learn,416,"What is wrong with the current installation instructions?. `conda install -c conda-forge python-igraph leidenalg`. Is python-igraph not required? It's also bad practice to mix Conda and PyPI installations (yes it works). . What about changing:; ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; ```. to. ```; If you do not have a working installation of Python 3.6 (or later), consider installing Miniconda (see Installing Miniconda). Then run:. conda install -c conda-forge scanpy python-igraph leidenalg; ```. The other packages are already included in the recipe: https://bioconda.github.io/recipes/scanpy/README.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1243#issuecomment-823304825
https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038:42,Testability,log,logging,42,OS: Windows 10; Python version: 3.7.7; sc.logging.print_versions() gives; scanpy==1.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.18.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038
https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038:165,Usability,learn,learn,165,OS: Windows 10; Python version: 3.7.7; sc.logging.print_versions() gives; scanpy==1.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.18.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633439038
https://github.com/scverse/scanpy/issues/1246#issuecomment-633443990:15,Deployability,upgrade,upgrade,15,"Maybe it's the upgrade to version 1.5.1 that leads to this bug, I can run this piece of code under 1.4.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633443990
https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019:29,Deployability,update,update,29,"Yes, it looks like we didn't update our dependency requirements correctly. It looks like the `rmatmat` argument for `LinearOperators` was only added as of `1.4`. I believe using `scanpy 1.5.1` with `scipy>1.4` should fix this. Could you let me know if that solves your problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019
https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019:40,Integrability,depend,dependency,40,"Yes, it looks like we didn't update our dependency requirements correctly. It looks like the `rmatmat` argument for `LinearOperators` was only added as of `1.4`. I believe using `scanpy 1.5.1` with `scipy>1.4` should fix this. Could you let me know if that solves your problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019
https://github.com/scverse/scanpy/pull/1247#issuecomment-636059236:83,Deployability,update,update,83,"I think it's very likely people will hit this bug, because 1) they typically don't update packages like scipy very often 2) most pipelines use sparse datasets + PCA. I think it deserves a new release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1247#issuecomment-636059236
https://github.com/scverse/scanpy/pull/1247#issuecomment-636059236:129,Deployability,pipeline,pipelines,129,"I think it's very likely people will hit this bug, because 1) they typically don't update packages like scipy very often 2) most pipelines use sparse datasets + PCA. I think it deserves a new release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1247#issuecomment-636059236
https://github.com/scverse/scanpy/pull/1247#issuecomment-636059236:192,Deployability,release,release,192,"I think it's very likely people will hit this bug, because 1) they typically don't update packages like scipy very often 2) most pipelines use sparse datasets + PCA. I think it deserves a new release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1247#issuecomment-636059236
https://github.com/scverse/scanpy/pull/1248#issuecomment-702660644:253,Testability,test,test,253,"Hi, @awnimo , sorry for the delay.; It seems that this PR breaks test_harmony_timeseries.py. I get ; ```; E ValueError: 'time_points' column does not contain Categorical data. ../../external/tl/_harmony_timeseries.py:140: ValueError; ```; On master the test works fine.; Could you check and fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1248#issuecomment-702660644
https://github.com/scverse/scanpy/pull/1248#issuecomment-703754388:19,Availability,error,error,19,"Hi @Koncopd ,. The error fixed here [df8bbaf](https://github.com/theislab/scanpy/pull/1248/commits/df8bbaf5ffb58eb37d4b80ef62819f69b8fce023). Thank you!. > Hi, @awnimo , sorry for the delay.; > It seems that this PR breaks test_harmony_timeseries.py. I get; > ; > ```; > E ValueError: 'time_points' column does not contain Categorical data; > ; > ../../external/tl/_harmony_timeseries.py:140: ValueError; > ```; > ; > On master the test works fine.; > Could you check and fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1248#issuecomment-703754388
https://github.com/scverse/scanpy/pull/1248#issuecomment-703754388:432,Testability,test,test,432,"Hi @Koncopd ,. The error fixed here [df8bbaf](https://github.com/theislab/scanpy/pull/1248/commits/df8bbaf5ffb58eb37d4b80ef62819f69b8fce023). Thank you!. > Hi, @awnimo , sorry for the delay.; > It seems that this PR breaks test_harmony_timeseries.py. I get; > ; > ```; > E ValueError: 'time_points' column does not contain Categorical data; > ; > ../../external/tl/_harmony_timeseries.py:140: ValueError; > ```; > ; > On master the test works fine.; > Could you check and fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1248#issuecomment-703754388
https://github.com/scverse/scanpy/pull/1250#issuecomment-634537829:218,Deployability,pipeline,pipelines,218,"Good point @ivirshup , by just asking around a bit it seems that no, that's not the case and it seems there is no prefix. From the space ranger [output](https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview) it seems there shouldn't be such prefix added. However, to be honest not super confident that this is not gonna happen in the future, since so far there are not do many visum datasets around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1250#issuecomment-634537829
https://github.com/scverse/scanpy/pull/1250#issuecomment-635277251:15,Testability,test,tests,15,"@ivirshup, the tests now copies the prefix data into a temp dir.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1250#issuecomment-635277251
https://github.com/scverse/scanpy/issues/1251#issuecomment-702373149:44,Performance,perform,performed,44,you can check here that the log is actually performed as you suggest: https://github.com/theislab/scanpy/blob/5533b644e796379fd146bf8e659fd49f92f718cd/scanpy/preprocessing/_recipes.py#L66-L95. I'll close this for now,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1251#issuecomment-702373149
https://github.com/scverse/scanpy/issues/1251#issuecomment-702373149:28,Testability,log,log,28,you can check here that the log is actually performed as you suggest: https://github.com/theislab/scanpy/blob/5533b644e796379fd146bf8e659fd49f92f718cd/scanpy/preprocessing/_recipes.py#L66-L95. I'll close this for now,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1251#issuecomment-702373149
https://github.com/scverse/scanpy/issues/1252#issuecomment-635084967:10,Deployability,update,update,10,Could you update your scipy with `pip install -U scipy`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635084967
https://github.com/scverse/scanpy/issues/1252#issuecomment-635084967:38,Deployability,install,install,38,Could you update your scipy with `pip install -U scipy`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635084967
https://github.com/scverse/scanpy/issues/1252#issuecomment-635089683:173,Deployability,install,installed,173,When I do this it tells me I am using version 1.4.1 but when I run sc.logging.print_versions() it comes up as version 1.01. I assumed this is somehow related to the version installed with scanpy that it uses by default maybe? But I feel I am not proficient enough in python to determine that,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635089683
https://github.com/scverse/scanpy/issues/1252#issuecomment-635089683:70,Testability,log,logging,70,When I do this it tells me I am using version 1.4.1 but when I run sc.logging.print_versions() it comes up as version 1.01. I assumed this is somehow related to the version installed with scanpy that it uses by default maybe? But I feel I am not proficient enough in python to determine that,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635089683
https://github.com/scverse/scanpy/issues/1252#issuecomment-635099440:29,Deployability,install,installing,29,"Huh. It sounds like `pip` is installing to a different environment than the one you're using for scanpy. How are you starting the relevant python session? Also, are you sure you're restarted that session after updating scipy?. To check to see if you're in the same environment, these commands should tell you where scipy is installed:. ```sh; pip show scipy. python -c ""import scipy; print(scipy.__file__)""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635099440
https://github.com/scverse/scanpy/issues/1252#issuecomment-635099440:324,Deployability,install,installed,324,"Huh. It sounds like `pip` is installing to a different environment than the one you're using for scanpy. How are you starting the relevant python session? Also, are you sure you're restarted that session after updating scipy?. To check to see if you're in the same environment, these commands should tell you where scipy is installed:. ```sh; pip show scipy. python -c ""import scipy; print(scipy.__file__)""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635099440
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:474,Availability,avail,available,474,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1346,Availability,avail,available,1346,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:529,Deployability,install,install,529,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:539,Deployability,upgrade,upgrade,539,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1401,Deployability,install,install,1401,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1411,Deployability,upgrade,upgrade,1411,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1452,Deployability,install,install,1452,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1711,Deployability,install,install,1711,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1760,Deployability,update,updated,1760,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1100,Integrability,wrap,wrap,1100,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1799,Testability,log,logging,1799,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:288,Usability,learn,learn,288,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:315,Usability,learn,learn,315,"When I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct vers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1223,Usability,learn,learn,1223,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1237,Usability,learn,learn,1237,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, Andrés R. Muñoz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942
https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015:27,Deployability,install,install,27,"That you are calling `!pip install scanpy --user` in your session definitely suggest you are not using the same environment as you were installing to before. I don't think I can help too much with this, since it sounds like there are some deeper problems with your python environment. Looking back at the traceback from your previous example, it looks like your environment is in a very strange state. You're using `scanpy` installed to your user python site packages, but importing `scipy` that's been installed via `conda`. If I were you, I think I would just try uninstalling everything and starting a new environment from scratch, possibly all through `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015
https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015:136,Deployability,install,installing,136,"That you are calling `!pip install scanpy --user` in your session definitely suggest you are not using the same environment as you were installing to before. I don't think I can help too much with this, since it sounds like there are some deeper problems with your python environment. Looking back at the traceback from your previous example, it looks like your environment is in a very strange state. You're using `scanpy` installed to your user python site packages, but importing `scipy` that's been installed via `conda`. If I were you, I think I would just try uninstalling everything and starting a new environment from scratch, possibly all through `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015
https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015:424,Deployability,install,installed,424,"That you are calling `!pip install scanpy --user` in your session definitely suggest you are not using the same environment as you were installing to before. I don't think I can help too much with this, since it sounds like there are some deeper problems with your python environment. Looking back at the traceback from your previous example, it looks like your environment is in a very strange state. You're using `scanpy` installed to your user python site packages, but importing `scipy` that's been installed via `conda`. If I were you, I think I would just try uninstalling everything and starting a new environment from scratch, possibly all through `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015
https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015:503,Deployability,install,installed,503,"That you are calling `!pip install scanpy --user` in your session definitely suggest you are not using the same environment as you were installing to before. I don't think I can help too much with this, since it sounds like there are some deeper problems with your python environment. Looking back at the traceback from your previous example, it looks like your environment is in a very strange state. You're using `scanpy` installed to your user python site packages, but importing `scipy` that's been installed via `conda`. If I were you, I think I would just try uninstalling everything and starting a new environment from scratch, possibly all through `conda`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635754015
https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:227,Availability,error,error,227,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302
https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:74,Deployability,install,installed,74,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302
https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:89,Deployability,install,installing,89,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302
https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:112,Deployability,Install,Installing,112,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302
https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:170,Deployability,install,install,170,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302
https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:291,Deployability,install,installed,291,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302
https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:233,Integrability,message,message,233,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302
https://github.com/scverse/scanpy/issues/1252#issuecomment-747903214:974,Availability,error,error,974,"File ""C:\Users\Reda\Anaconda3\lib\site-packages\hdbscan\hdbscan_.py"", line 941, in fit_predict; self.fit(X); File ""C:\Users\Reda\Anaconda3\lib\site-packages\hdbscan\hdbscan_.py"", line 919, in fit; self._min_spanning_tree) = hdbscan(X, **kwargs); File ""C:\Users\Reda\Anaconda3\lib\site-packages\hdbscan\hdbscan_.py"", line 615, in hdbscan; core_dist_n_jobs, **kwargs); File ""C:\Users\Reda\Anaconda3\lib\site-packages\joblib\memory.py"", line 352, in __call__; return self.func(*args, **kwargs); File ""C:\Users\Reda\Anaconda3\lib\site-packages\hdbscan\hdbscan_.py"", line 274, in _hdbscan_boruvka_kdtree; tree = KDTree(X, metric=metric, leaf_size=leaf_size, **kwargs); File ""sklearn\neighbors\_binary_tree.pxi"", line 1061, in sklearn.neighbors._kd_tree.BinaryTree.__init__; File ""sklearn\neighbors\_dist_metrics.pyx"", line 289, in sklearn.neighbors._dist_metrics.DistanceMetric.get_metric; TypeError: __init__() got an unexpected keyword argument 'check_pickle'. How to fix this error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-747903214
https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317:214,Deployability,integrat,integration-scanorama,214,"Much of the spatial data is stored in `uns`, which does not get combined by default. There is an example of concatenating visium datasets [in the tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html#Data-integration) and more information on concatenating `.uns` [in the latest anndata docs](https://anndata.readthedocs.io/en/latest/concatenation.html#merging-uns).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317
https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317:246,Deployability,integrat,integration,246,"Much of the spatial data is stored in `uns`, which does not get combined by default. There is an example of concatenating visium datasets [in the tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html#Data-integration) and more information on concatenating `.uns` [in the latest anndata docs](https://anndata.readthedocs.io/en/latest/concatenation.html#merging-uns).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317
https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317:214,Integrability,integrat,integration-scanorama,214,"Much of the spatial data is stored in `uns`, which does not get combined by default. There is an example of concatenating visium datasets [in the tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html#Data-integration) and more information on concatenating `.uns` [in the latest anndata docs](https://anndata.readthedocs.io/en/latest/concatenation.html#merging-uns).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317
https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317:246,Integrability,integrat,integration,246,"Much of the spatial data is stored in `uns`, which does not get combined by default. There is an example of concatenating visium datasets [in the tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html#Data-integration) and more information on concatenating `.uns` [in the latest anndata docs](https://anndata.readthedocs.io/en/latest/concatenation.html#merging-uns).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317
https://github.com/scverse/scanpy/issues/1254#issuecomment-635702014:9,Availability,error,error,9,"I get an error trying to merge multiples slides using the code in the tutorial. Is it possible to install the scanpy version the tutorial is using?. ```python; adata = adata.concatenate(; list(slides.values()),; batch_key=""sample"",; uns_merge=""unique"",; batch_categories=list(sample_data['sample_name'].values), ; index_unique=None; ); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-4-fe8a54a66c17> in <module>; 40 uns_merge=""unique"",; 41 batch_categories=list(sample_data['sample_name'].values),; ---> 42 index_unique=None; 43 ); 44 . TypeError: concatenate() got an unexpected keyword argument 'uns_merge'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635702014
https://github.com/scverse/scanpy/issues/1254#issuecomment-635702014:98,Deployability,install,install,98,"I get an error trying to merge multiples slides using the code in the tutorial. Is it possible to install the scanpy version the tutorial is using?. ```python; adata = adata.concatenate(; list(slides.values()),; batch_key=""sample"",; uns_merge=""unique"",; batch_categories=list(sample_data['sample_name'].values), ; index_unique=None; ); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-4-fe8a54a66c17> in <module>; 40 uns_merge=""unique"",; 41 batch_categories=list(sample_data['sample_name'].values),; ---> 42 index_unique=None; 43 ); 44 . TypeError: concatenate() got an unexpected keyword argument 'uns_merge'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635702014
https://github.com/scverse/scanpy/issues/1254#issuecomment-635750301:23,Deployability,update,update,23,I think you'll want to update your version of anndata for that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635750301
https://github.com/scverse/scanpy/pull/1255#issuecomment-635249580:86,Testability,test,test,86,"This seems like a very drastic change for default output, I feel like this deserves a test. Any idea what happened here? I'm very surprised this wasn't caught by a test or the tutorials.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1255#issuecomment-635249580
https://github.com/scverse/scanpy/pull/1255#issuecomment-635249580:164,Testability,test,test,164,"This seems like a very drastic change for default output, I feel like this deserves a test. Any idea what happened here? I'm very surprised this wasn't caught by a test or the tutorials.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1255#issuecomment-635249580
https://github.com/scverse/scanpy/pull/1255#issuecomment-640493235:53,Testability,test,test,53,"Sorry for late reply, wasn't catched by the previous test because the test set the size of the spot for plotting.; Current test:; ```python; sc.pl.spatial(; adata,; color=""array_row"",; groups=[""24"", ""33""],; crop_coord=(100, 400, 400, 100),; alpha=0.5,; size=1.3,; ); ```; Shall i remove the `size` param from this test or make a new one?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1255#issuecomment-640493235
https://github.com/scverse/scanpy/pull/1255#issuecomment-640493235:70,Testability,test,test,70,"Sorry for late reply, wasn't catched by the previous test because the test set the size of the spot for plotting.; Current test:; ```python; sc.pl.spatial(; adata,; color=""array_row"",; groups=[""24"", ""33""],; crop_coord=(100, 400, 400, 100),; alpha=0.5,; size=1.3,; ); ```; Shall i remove the `size` param from this test or make a new one?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1255#issuecomment-640493235
https://github.com/scverse/scanpy/pull/1255#issuecomment-640493235:123,Testability,test,test,123,"Sorry for late reply, wasn't catched by the previous test because the test set the size of the spot for plotting.; Current test:; ```python; sc.pl.spatial(; adata,; color=""array_row"",; groups=[""24"", ""33""],; crop_coord=(100, 400, 400, 100),; alpha=0.5,; size=1.3,; ); ```; Shall i remove the `size` param from this test or make a new one?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1255#issuecomment-640493235
https://github.com/scverse/scanpy/pull/1255#issuecomment-640493235:314,Testability,test,test,314,"Sorry for late reply, wasn't catched by the previous test because the test set the size of the spot for plotting.; Current test:; ```python; sc.pl.spatial(; adata,; color=""array_row"",; groups=[""24"", ""33""],; crop_coord=(100, 400, 400, 100),; alpha=0.5,; size=1.3,; ); ```; Shall i remove the `size` param from this test or make a new one?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1255#issuecomment-640493235
https://github.com/scverse/scanpy/pull/1255#issuecomment-657567661:52,Testability,test,test,52,"@ivirshup sorry super late on this, added a default test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1255#issuecomment-657567661
https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:0,Deployability,install,install,0,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692
https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:43,Deployability,install,installing,43,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692
https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:64,Deployability,install,install,64,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692
https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:213,Deployability,install,install,213,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692
https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:361,Deployability,install,install,361,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692
https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:471,Deployability,install,install,471,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692
https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:586,Deployability,install,install,586,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692
https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692:245,Energy Efficiency,Power,Powershell,245,"install cython in anaconda jupyter lab for installing fa2; !pip install Cython. this scanpy trajectory tutorial needs package 'fa2' (not 'forceatlas2'), otherwise the plot made by sc.pl.draw_graph() is not right. install method 1; open Anaconda Powershell Promopt; > conda activate Py36R36 (Py36R36 is the enviroment you create in anaconda for scanpy); > conda install -c conda-forge fa2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; run scanpy trajectory. install method 2; open Anaconda navigator; choose Py36R36; open Jupyter Lab; open Terminal in Jupyter Lab; > conda install -c conda-forge fa2; run scanpy trajectory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256#issuecomment-962562692
https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516:73,Availability,echo,echo,73,"Assuming you're on a debian based linux, please check the following:; - `echo $PATH` shows your PATH variable.; - `which git` shows you the location of your git installation. If nothing is shown, you need to install it.; - `apt install git` if you haven't installed it yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516
https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516:161,Deployability,install,installation,161,"Assuming you're on a debian based linux, please check the following:; - `echo $PATH` shows your PATH variable.; - `which git` shows you the location of your git installation. If nothing is shown, you need to install it.; - `apt install git` if you haven't installed it yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516
https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516:208,Deployability,install,install,208,"Assuming you're on a debian based linux, please check the following:; - `echo $PATH` shows your PATH variable.; - `which git` shows you the location of your git installation. If nothing is shown, you need to install it.; - `apt install git` if you haven't installed it yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516
https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516:228,Deployability,install,install,228,"Assuming you're on a debian based linux, please check the following:; - `echo $PATH` shows your PATH variable.; - `which git` shows you the location of your git installation. If nothing is shown, you need to install it.; - `apt install git` if you haven't installed it yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516
https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516:256,Deployability,install,installed,256,"Assuming you're on a debian based linux, please check the following:; - `echo $PATH` shows your PATH variable.; - `which git` shows you the location of your git installation. If nothing is shown, you need to install it.; - `apt install git` if you haven't installed it yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516
https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516:101,Modifiability,variab,variable,101,"Assuming you're on a debian based linux, please check the following:; - `echo $PATH` shows your PATH variable.; - `which git` shows you the location of your git installation. If nothing is shown, you need to install it.; - `apt install git` if you haven't installed it yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516
https://github.com/scverse/scanpy/issues/1258#issuecomment-713492283:215,Modifiability,variab,variables,215,"Totally forgot about this one sorry :(. **The problem** ; In `scatter_base`: https://github.com/theislab/scanpy/blob/040e61ff50836d4a6cdd7da7482dcb4ee50d05ae/scanpy/plotting/_utils.py#L736-L740. For non categorical variables, this code gets the current figure and adds a separate axis on which the colorbar is plotted.; Therefore, the axes objects on which the data is plotted do not contain a legend object.; Instead, `fig` should contain the colorbar axis and we could maybe manage to manipulate it as a workaround. There is also this DeprecationWarning popping up.; ```pytb; MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance. In a future version, a new instance will always be created and returned. Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.; ax_cb = fig.add_axes(rectangle); ```. **Current workaround (also for all other sort of plots)**; The problem here is really that we don't have two separate figures / axes aren't handled correctly.; Good news is, that there is a way around using `plt.subplots` and using given `Axes` objects. even if we want to plot 2 plots side by side in a jupyter notebook (original post here: https://stackoverflow.com/questions/21754976/ipython-notebook-arrange-plots-horizontally).; However, `sc.pl.scatter` isn't exposing the figure object but only the axis. But if we specify `show=False`, it returns the axis and we can obtain the figure object using `matplotlib.pyplot.gcf()`.; Store these figures in a list and pass them to the `plot_nice()` function which will plot all your figures side by side until it runs out of space, after which it will create a linebreak and continue. Therefore, you can specify how many figures you want to plot per line, using the individual `figsize` argument. For my example it would look like this:; ```python; from flow_layout import plot_nice # import the required plot",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1258#issuecomment-713492283
https://github.com/scverse/scanpy/issues/1258#issuecomment-1853651085:263,Availability,ERROR,ERROR,263,"I had some issue with `io.BytesIO()` from the fix proposed above. . So, I used `R` to generate scatter plots as below:. ```py; import anndata2ri; import logging. import rpy2.rinterface_lib.callbacks as rcb; import rpy2.robjects as ro. rcb.logger.setLevel(logging.ERROR); ro.pandas2ri.activate(); anndata2ri.activate(). %load_ext rpy2.ipython; ```; Convert adata_p and adata_g to R objects. ```r; ro.globalenv['r_adata_p'] = adata_p; ro.globalenv['r_adata_g'] = adata_g; ```. ```r; %%R -w 800 -h 400 -u px. library(Seurat); library(viridis); library(viridisLite); library(ggplot2); library(cowplot). df_poor= data.frame(; total_counts = colData(r_adata_p)$total_counts,; n_genes_by_counts = colData(r_adata_p)$n_genes_by_counts,; pct_counts_mt = colData(r_adata_p)$pct_counts_mt; ). df_good= data.frame(; total_counts = colData(r_adata_g)$total_counts,; n_genes_by_counts = colData(r_adata_g)$n_genes_by_counts,; pct_counts_mt = colData(r_adata_g)$pct_counts_mt; ). #head(df); # Create a scatter plot using ggplot2; p2 <- ggplot(data = df_poor, aes(x = total_counts, y = n_genes_by_counts, color = pct_counts_mt)) +; geom_point() +; scale_color_viridis() +; labs(title = ""poor (after outlier and mitochrondrial gene removal)"") +; theme_minimal(). g2 <- ggplot(data = df_good, aes(x = total_counts, y = n_genes_by_counts, color = pct_counts_mt)) +; geom_point() +; scale_color_viridis() +; labs(title = ""good (after outlier and mitochrondrial gene removal)"") +; theme_minimal(). p2 + g2; ```. ![Screenshot from 2023-12-13 11-25-03](https://github.com/scverse/scanpy/assets/3212461/f016798e-aa7a-4601-9fad-f85d54877c2d)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1258#issuecomment-1853651085
https://github.com/scverse/scanpy/issues/1258#issuecomment-1853651085:153,Testability,log,logging,153,"I had some issue with `io.BytesIO()` from the fix proposed above. . So, I used `R` to generate scatter plots as below:. ```py; import anndata2ri; import logging. import rpy2.rinterface_lib.callbacks as rcb; import rpy2.robjects as ro. rcb.logger.setLevel(logging.ERROR); ro.pandas2ri.activate(); anndata2ri.activate(). %load_ext rpy2.ipython; ```; Convert adata_p and adata_g to R objects. ```r; ro.globalenv['r_adata_p'] = adata_p; ro.globalenv['r_adata_g'] = adata_g; ```. ```r; %%R -w 800 -h 400 -u px. library(Seurat); library(viridis); library(viridisLite); library(ggplot2); library(cowplot). df_poor= data.frame(; total_counts = colData(r_adata_p)$total_counts,; n_genes_by_counts = colData(r_adata_p)$n_genes_by_counts,; pct_counts_mt = colData(r_adata_p)$pct_counts_mt; ). df_good= data.frame(; total_counts = colData(r_adata_g)$total_counts,; n_genes_by_counts = colData(r_adata_g)$n_genes_by_counts,; pct_counts_mt = colData(r_adata_g)$pct_counts_mt; ). #head(df); # Create a scatter plot using ggplot2; p2 <- ggplot(data = df_poor, aes(x = total_counts, y = n_genes_by_counts, color = pct_counts_mt)) +; geom_point() +; scale_color_viridis() +; labs(title = ""poor (after outlier and mitochrondrial gene removal)"") +; theme_minimal(). g2 <- ggplot(data = df_good, aes(x = total_counts, y = n_genes_by_counts, color = pct_counts_mt)) +; geom_point() +; scale_color_viridis() +; labs(title = ""good (after outlier and mitochrondrial gene removal)"") +; theme_minimal(). p2 + g2; ```. ![Screenshot from 2023-12-13 11-25-03](https://github.com/scverse/scanpy/assets/3212461/f016798e-aa7a-4601-9fad-f85d54877c2d)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1258#issuecomment-1853651085
https://github.com/scverse/scanpy/issues/1258#issuecomment-1853651085:239,Testability,log,logger,239,"I had some issue with `io.BytesIO()` from the fix proposed above. . So, I used `R` to generate scatter plots as below:. ```py; import anndata2ri; import logging. import rpy2.rinterface_lib.callbacks as rcb; import rpy2.robjects as ro. rcb.logger.setLevel(logging.ERROR); ro.pandas2ri.activate(); anndata2ri.activate(). %load_ext rpy2.ipython; ```; Convert adata_p and adata_g to R objects. ```r; ro.globalenv['r_adata_p'] = adata_p; ro.globalenv['r_adata_g'] = adata_g; ```. ```r; %%R -w 800 -h 400 -u px. library(Seurat); library(viridis); library(viridisLite); library(ggplot2); library(cowplot). df_poor= data.frame(; total_counts = colData(r_adata_p)$total_counts,; n_genes_by_counts = colData(r_adata_p)$n_genes_by_counts,; pct_counts_mt = colData(r_adata_p)$pct_counts_mt; ). df_good= data.frame(; total_counts = colData(r_adata_g)$total_counts,; n_genes_by_counts = colData(r_adata_g)$n_genes_by_counts,; pct_counts_mt = colData(r_adata_g)$pct_counts_mt; ). #head(df); # Create a scatter plot using ggplot2; p2 <- ggplot(data = df_poor, aes(x = total_counts, y = n_genes_by_counts, color = pct_counts_mt)) +; geom_point() +; scale_color_viridis() +; labs(title = ""poor (after outlier and mitochrondrial gene removal)"") +; theme_minimal(). g2 <- ggplot(data = df_good, aes(x = total_counts, y = n_genes_by_counts, color = pct_counts_mt)) +; geom_point() +; scale_color_viridis() +; labs(title = ""good (after outlier and mitochrondrial gene removal)"") +; theme_minimal(). p2 + g2; ```. ![Screenshot from 2023-12-13 11-25-03](https://github.com/scverse/scanpy/assets/3212461/f016798e-aa7a-4601-9fad-f85d54877c2d)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1258#issuecomment-1853651085
https://github.com/scverse/scanpy/issues/1258#issuecomment-1853651085:255,Testability,log,logging,255,"I had some issue with `io.BytesIO()` from the fix proposed above. . So, I used `R` to generate scatter plots as below:. ```py; import anndata2ri; import logging. import rpy2.rinterface_lib.callbacks as rcb; import rpy2.robjects as ro. rcb.logger.setLevel(logging.ERROR); ro.pandas2ri.activate(); anndata2ri.activate(). %load_ext rpy2.ipython; ```; Convert adata_p and adata_g to R objects. ```r; ro.globalenv['r_adata_p'] = adata_p; ro.globalenv['r_adata_g'] = adata_g; ```. ```r; %%R -w 800 -h 400 -u px. library(Seurat); library(viridis); library(viridisLite); library(ggplot2); library(cowplot). df_poor= data.frame(; total_counts = colData(r_adata_p)$total_counts,; n_genes_by_counts = colData(r_adata_p)$n_genes_by_counts,; pct_counts_mt = colData(r_adata_p)$pct_counts_mt; ). df_good= data.frame(; total_counts = colData(r_adata_g)$total_counts,; n_genes_by_counts = colData(r_adata_g)$n_genes_by_counts,; pct_counts_mt = colData(r_adata_g)$pct_counts_mt; ). #head(df); # Create a scatter plot using ggplot2; p2 <- ggplot(data = df_poor, aes(x = total_counts, y = n_genes_by_counts, color = pct_counts_mt)) +; geom_point() +; scale_color_viridis() +; labs(title = ""poor (after outlier and mitochrondrial gene removal)"") +; theme_minimal(). g2 <- ggplot(data = df_good, aes(x = total_counts, y = n_genes_by_counts, color = pct_counts_mt)) +; geom_point() +; scale_color_viridis() +; labs(title = ""good (after outlier and mitochrondrial gene removal)"") +; theme_minimal(). p2 + g2; ```. ![Screenshot from 2023-12-13 11-25-03](https://github.com/scverse/scanpy/assets/3212461/f016798e-aa7a-4601-9fad-f85d54877c2d)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1258#issuecomment-1853651085
https://github.com/scverse/scanpy/issues/1259#issuecomment-638585609:551,Usability,learn,learn,551,"> Thanks for the report.; > ; > Do you have NaN values in your expression matrix? If so, try filtering them out and seeing if that works. If not, could you report the versions of the library you're working with, and try to make a self contained example that I could run on my machine?. Hi, . Thank you for your reply. I think there is no NaN data in the matrix of the mito genes. Because I have already plotted the mitochondrial genes as follows. The version is scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.0. I am sorry I don't know how to make a self-contained example. The plot is: ; ![highest_expre_genes_BHCF](https://user-images.githubusercontent.com/49381637/83712829-3ea3ab80-a5ec-11ea-8497-cc70a95a216e.png). Thank you. Best wishes,. Shangyu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-638585609
https://github.com/scverse/scanpy/issues/1259#issuecomment-639309900:193,Availability,error,error,193,"The idea behind a self contained example is to give me something that I can run on my machine. Ideally you'd be able to put something together with randomly generated data that still gave this error. If that's difficult, you could keep removing elements from your data until you find the minimal object that can reproduce this. [Here is a good blog post on how to do this](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports). Right now, I'm unable to reproduce the error you're seeing. Do you think you could try and create an example you could share with me? This could even be sharing your data as an `h5ad` file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-639309900
https://github.com/scverse/scanpy/issues/1259#issuecomment-639309900:482,Availability,error,error,482,"The idea behind a self contained example is to give me something that I can run on my machine. Ideally you'd be able to put something together with randomly generated data that still gave this error. If that's difficult, you could keep removing elements from your data until you find the minimal object that can reproduce this. [Here is a good blog post on how to do this](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports). Right now, I'm unable to reproduce the error you're seeing. Do you think you could try and create an example you could share with me? This could even be sharing your data as an `h5ad` file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-639309900
https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437:649,Availability,error,error,649,"Hi, Isaac,. Thank you for your reply. The matrix that I load is the output of; cellranger, we use 10X generate the library. Follow your; recommended tutorial, I can't small size it. Do you have any other; suggestions? The code is: adata = sc.read_10x_mtx(; 'D:/.../.../filtered_feature_bc_matrix/', var_names='gene_symbols',; cache=True) . Thank you so much. Best regards,. Shangyu. Isaac Virshup <notifications@github.com> 于2020年6月5日周五 上午2:31写道：. > The idea behind a self contained example is to give me something that I; > can run on my machine. Ideally you'd be able to put something together with; > randomly generated data that still gave this error. If that's difficult,; > you could keep removing elements from your data until you find the minimal; > object that can reproduce this. Here is a good blog post on how to do this; > <https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports>.; >; > Right now, I'm unable to reproduce the error you're seeing. Do you think; > you could try and create an example you could share with me? This could; > even be sharing your data as an h5ad file.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1259#issuecomment-639309900>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALYYCBLMIG7FAT7MMJIDC2DRVCNM3ANCNFSM4NOZJRCQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437
https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437:952,Availability,error,error,952,"Hi, Isaac,. Thank you for your reply. The matrix that I load is the output of; cellranger, we use 10X generate the library. Follow your; recommended tutorial, I can't small size it. Do you have any other; suggestions? The code is: adata = sc.read_10x_mtx(; 'D:/.../.../filtered_feature_bc_matrix/', var_names='gene_symbols',; cache=True) . Thank you so much. Best regards,. Shangyu. Isaac Virshup <notifications@github.com> 于2020年6月5日周五 上午2:31写道：. > The idea behind a self contained example is to give me something that I; > can run on my machine. Ideally you'd be able to put something together with; > randomly generated data that still gave this error. If that's difficult,; > you could keep removing elements from your data until you find the minimal; > object that can reproduce this. Here is a good blog post on how to do this; > <https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports>.; >; > Right now, I'm unable to reproduce the error you're seeing. Do you think; > you could try and create an example you could share with me? This could; > even be sharing your data as an h5ad file.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1259#issuecomment-639309900>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALYYCBLMIG7FAT7MMJIDC2DRVCNM3ANCNFSM4NOZJRCQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437
https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437:56,Performance,load,load,56,"Hi, Isaac,. Thank you for your reply. The matrix that I load is the output of; cellranger, we use 10X generate the library. Follow your; recommended tutorial, I can't small size it. Do you have any other; suggestions? The code is: adata = sc.read_10x_mtx(; 'D:/.../.../filtered_feature_bc_matrix/', var_names='gene_symbols',; cache=True) . Thank you so much. Best regards,. Shangyu. Isaac Virshup <notifications@github.com> 于2020年6月5日周五 上午2:31写道：. > The idea behind a self contained example is to give me something that I; > can run on my machine. Ideally you'd be able to put something together with; > randomly generated data that still gave this error. If that's difficult,; > you could keep removing elements from your data until you find the minimal; > object that can reproduce this. Here is a good blog post on how to do this; > <https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports>.; >; > Right now, I'm unable to reproduce the error you're seeing. Do you think; > you could try and create an example you could share with me? This could; > even be sharing your data as an h5ad file.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1259#issuecomment-639309900>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALYYCBLMIG7FAT7MMJIDC2DRVCNM3ANCNFSM4NOZJRCQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437
https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437:326,Performance,cache,cache,326,"Hi, Isaac,. Thank you for your reply. The matrix that I load is the output of; cellranger, we use 10X generate the library. Follow your; recommended tutorial, I can't small size it. Do you have any other; suggestions? The code is: adata = sc.read_10x_mtx(; 'D:/.../.../filtered_feature_bc_matrix/', var_names='gene_symbols',; cache=True) . Thank you so much. Best regards,. Shangyu. Isaac Virshup <notifications@github.com> 于2020年6月5日周五 上午2:31写道：. > The idea behind a self contained example is to give me something that I; > can run on my machine. Ideally you'd be able to put something together with; > randomly generated data that still gave this error. If that's difficult,; > you could keep removing elements from your data until you find the minimal; > object that can reproduce this. Here is a good blog post on how to do this; > <https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports>.; >; > Right now, I'm unable to reproduce the error you're seeing. Do you think; > you could try and create an example you could share with me? This could; > even be sharing your data as an h5ad file.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1259#issuecomment-639309900>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALYYCBLMIG7FAT7MMJIDC2DRVCNM3ANCNFSM4NOZJRCQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437
https://github.com/scverse/scanpy/issues/1259#issuecomment-799095321:166,Testability,log,log,166,"NaN is a special floating point sentinel value, meaning ""Not a Number."" In general, Python prefers raising an exception to returning NaN, so things like sqrt(-1) and log(0.0) will generally raise instead of returning NaN. However, you may get this value back from some other library. From v0.24, you actually can. [Pandas](http://net-informations.com/ds/pd/default.htm) introduces Nullable Integer Data Types which allows integers to coexist with NaNs. You need to say what you want to do with nans. You can either drop those rows (df.dropna()) or replace nans with something else (0 for instance: df.fillna(0)). My suggestion would be to specifically try to identify this problem (why are you getting this particular NaN), and then write some code to provide a replacement. Also, even at the lastest versions of pandas if the column is object type you would have to convert into float first, something like:. `df['column_name'].astype(np.float).astype(""Int32"")`. NB: You have to go through numpy float first and then to nullable Int32, for some reason.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-799095321
https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183:168,Availability,error,error,168,"HI everyone, . I have the excat same issue, which prevents me from performing further analysis. ; What I did : ; - dropna(), still boolean values, which poses the same error again (boolean values are NANs appearently); - fillna(0) : replaced all NAN values with 0, but this poses a problem later in the analysis when i lognormalize the data (log(0) = inf).; How do you guys deal with these sorts of problems with your data ? . I don't think the mt colum should contain boolean values... (cf. screeshot); Please correct me if i am wrong, and thank you in advance for your help. ![Screenshot from 2021-12-13 17-17-56](https://user-images.githubusercontent.com/45742503/145848639-6d7c6ee6-a38f-4c48-b38a-c8339984e360.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183
https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183:67,Performance,perform,performing,67,"HI everyone, . I have the excat same issue, which prevents me from performing further analysis. ; What I did : ; - dropna(), still boolean values, which poses the same error again (boolean values are NANs appearently); - fillna(0) : replaced all NAN values with 0, but this poses a problem later in the analysis when i lognormalize the data (log(0) = inf).; How do you guys deal with these sorts of problems with your data ? . I don't think the mt colum should contain boolean values... (cf. screeshot); Please correct me if i am wrong, and thank you in advance for your help. ![Screenshot from 2021-12-13 17-17-56](https://user-images.githubusercontent.com/45742503/145848639-6d7c6ee6-a38f-4c48-b38a-c8339984e360.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183
https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183:319,Testability,log,lognormalize,319,"HI everyone, . I have the excat same issue, which prevents me from performing further analysis. ; What I did : ; - dropna(), still boolean values, which poses the same error again (boolean values are NANs appearently); - fillna(0) : replaced all NAN values with 0, but this poses a problem later in the analysis when i lognormalize the data (log(0) = inf).; How do you guys deal with these sorts of problems with your data ? . I don't think the mt colum should contain boolean values... (cf. screeshot); Please correct me if i am wrong, and thank you in advance for your help. ![Screenshot from 2021-12-13 17-17-56](https://user-images.githubusercontent.com/45742503/145848639-6d7c6ee6-a38f-4c48-b38a-c8339984e360.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183
https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183:342,Testability,log,log,342,"HI everyone, . I have the excat same issue, which prevents me from performing further analysis. ; What I did : ; - dropna(), still boolean values, which poses the same error again (boolean values are NANs appearently); - fillna(0) : replaced all NAN values with 0, but this poses a problem later in the analysis when i lognormalize the data (log(0) = inf).; How do you guys deal with these sorts of problems with your data ? . I don't think the mt colum should contain boolean values... (cf. screeshot); Please correct me if i am wrong, and thank you in advance for your help. ![Screenshot from 2021-12-13 17-17-56](https://user-images.githubusercontent.com/45742503/145848639-6d7c6ee6-a38f-4c48-b38a-c8339984e360.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183
https://github.com/scverse/scanpy/issues/1260#issuecomment-645395239:207,Usability,learn,learn,207,"Hello, I am running into exactly the same bug when using both **scanpy-1.5.1** or **scanpy-1.5.0**. . **Versions**: ; scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.3.2 pandas==1.0.4 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.0. **Input**: ; `import time; t0 = time.time(); sc.external.exporting.spring_project(adata, './SPRING',; 'umap', subplot_name='all', overwrite=True, cell_groupings=['leiden'],; custom_color_tracks=['total_counts']); print(time.time() - t0)`. **Output**: ; `WARNING: root:Overwriting the files in SPRING.; Writing subplot to SPRING\all; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-59-9c683583ff59> in <module>; 1 import time; 2 t0 = time.time(); ----> 3 sc.external.exporting.spring_project(adata, './SPRING',; 4 'umap', subplot_name='all', overwrite=True, cell_groupings=['leiden'],; 5 custom_color_tracks=['total_counts']). ~\Anaconda3\envs\sfn-workshop\lib\site-packages\scanpy\external\exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, neighbors_key, overwrite); 179 ; 180 # Write graph in two formats for backwards compatibility; --> 181 edges = _get_edges(adata, neighbors_key); 182 _write_graph(subplot_dir / 'graph_data.json', E.shape[0], edges); 183 _write_edges(subplot_dir / 'edges.csv', edges). ~\Anaconda3\envs\sfn-workshop\lib\site-packages\scanpy\external\exporting.py in _get_edges(adata, neighbors_key); 217 ; 218 def _get_edges(adata, neighbors_key=None):; --> 219 neighbors = NeighborsView(adata, neighbors_key); 220 if 'distances' in neighbors: # these are sparse matrices; 221 matrix = neighbors['distances']. NameError: name 'NeighborsView' is not defined`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1260#issuecomment-645395239
https://github.com/scverse/scanpy/issues/1263#issuecomment-637843007:149,Testability,test,tests,149,"Yes, exactly :). I would have a default that turns it to random. I don't think it should be too hard either. However, this will probably break a few tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-637843007
https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155:2190,Energy Efficiency,sensor,sensor,2190,"ot being made. . As an aside, I've also tried coloring the pixel by which group showed up the most under it, but this can look weird (less so, if density is used to calculate the alpha level). ![image](https://user-images.githubusercontent.com/8238804/83601513-16985600-a5b4-11ea-8f0d-68a15a3fbf96.png). <details>; <summary> Example without accounting for density </summary>. ![image](https://user-images.githubusercontent.com/8238804/83601587-362f7e80-a5b4-11ea-8e1a-b1bc20948504.png). </details>. <details>; <summary> Snippet to reproduce </summary>. ```python; import datashader as ds; from datashader import transfer_functions as tf; import scanpy as sc; import numpy as np; import xarray as xr. # Where you load your AnnData, I was using a preprocessed set of 1.3 million mouse braincells. df = sc.get.obs_df(; adata,; [""Sox17"", ""louvain""],; obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]; ); louvain_colors = dict(; zip(; adata.obs[""louvain""].cat.categories, ; adata.uns[""louvain_colors""]; ); ). pts = (; ds.Canvas(500, 500); .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")); ). newpts = xr.zeros_like(pts); newpts[:, :, pts.argmax(dim=""louvain"")] = pts.sum(dim=""louvain""); tf.shade(newpts, color_key=louvain_colors); ```. </details>. What datashader does by default is takes the average of the RGB values for the categories under a pixel, weighted by number of samples, and calculates an alpha level based on the number of samples present. This looks like:. ![image](https://user-images.githubusercontent.com/8238804/83599943-c9ff4b80-a5b0-11ea-8acf-3cfc640a9abb.png). <details>; <summary> Addendum to previous snippet for plotting this </summary>. ```python; tf.shade(pts, color_key=louvain_colors); ```; </details>. </details>. I've also been wondering if there's a good way to show ""colors cannot be trusted in this region"". This could be done like how camera's do zebra stripes – where a texture is overlaid on the viewfinder for the sensor pixels which are saturated with light.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155
https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155:947,Performance,load,load,947,"Sounds good to me. How are you thinking of handling reproducibility w.r.t. random seeds?. To me, the best solution here is to make it easy to do small multiples for categorical plots like this, but that's a big change in the kind of plot being made. . As an aside, I've also tried coloring the pixel by which group showed up the most under it, but this can look weird (less so, if density is used to calculate the alpha level). ![image](https://user-images.githubusercontent.com/8238804/83601513-16985600-a5b4-11ea-8f0d-68a15a3fbf96.png). <details>; <summary> Example without accounting for density </summary>. ![image](https://user-images.githubusercontent.com/8238804/83601587-362f7e80-a5b4-11ea-8e1a-b1bc20948504.png). </details>. <details>; <summary> Snippet to reproduce </summary>. ```python; import datashader as ds; from datashader import transfer_functions as tf; import scanpy as sc; import numpy as np; import xarray as xr. # Where you load your AnnData, I was using a preprocessed set of 1.3 million mouse braincells. df = sc.get.obs_df(; adata,; [""Sox17"", ""louvain""],; obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]; ); louvain_colors = dict(; zip(; adata.obs[""louvain""].cat.categories, ; adata.uns[""louvain_colors""]; ); ). pts = (; ds.Canvas(500, 500); .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")); ). newpts = xr.zeros_like(pts); newpts[:, :, pts.argmax(dim=""louvain"")] = pts.sum(dim=""louvain""); tf.shade(newpts, color_key=louvain_colors); ```. </details>. What datashader does by default is takes the average of the RGB values for the categories under a pixel, weighted by number of samples, and calculates an alpha level based on the number of samples present. This looks like:. ![image](https://user-images.githubusercontent.com/8238804/83599943-c9ff4b80-a5b0-11ea-8acf-3cfc640a9abb.png). <details>; <summary> Addendum to previous snippet for plotting this </summary>. ```python; tf.shade(pts, color_key=louvain_colors); ```; </details>. </details>. I've also been wond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155
https://github.com/scverse/scanpy/issues/1263#issuecomment-760657953:2817,Deployability,Update,Update,2817,""", ""louvain""],; obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]; ); louvain_colors = dict(; zip(; adata.obs[""louvain""].cat.categories, ; adata.uns[""louvain_colors""]; ); ); pts = (; ds.Canvas(1000, 1000); .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")); ). # Make images; pts_ncats = (pts != 0).sum(axis=2); overlap_idx = pts_ncats == 1; zebra_source = xr.DataArray(; diagonal_bands_like(overlap_idx, 13),; coords=overlap_idx.coords; ). color_by_cluster = tf.shade(pts, color_key=louvain_colors); tf.Images(; color_by_cluster,; tf.stack(; tf.Image(xr.where(pts_ncats == 1, color_by_cluster, 0)),; tf.Image(tf.shade(xr.where(pts_ncats > 1, zebra_source, False), cmap=""black"")); ),; tf.stack(; color_by_cluster,; tf.Image(tf.shade(xr.where(pts_ncats > 1, zebra_source, False), cmap=""black"")); ),; ); ```. </details>. > I do think that randomization would result in sth similar to the datashader example you show though, except that it wouldn't change alphas by density. I wonder how either of these are effected by number of points. Say you have two cell types (A and B) in an overlapping region. ; A has 10x the representation of B in this region, but it's only 10% of the A in this dataset, while this region has all of B. What the fair way to color this? If it were random, or purely by count this would look mostly like A. > I'm not sure if doing that is so helpful as it can lead to hardly being able to see small clusters in less dense regions of the plot. I think bin size would be helpful here. Additionally [datashader has methods](https://pyviz-dev.github.io/datashader/api.html#datashader.transfer_functions.dynspread) for exaggerating points in less dense regions so they are visible. This could be worth looking into. Update: Turns out `dynspread` uses global density, not local. The spread operators could still be of help here. Also, minimum alpha values can be set. Overall, I do like that there is a sense of density with the alpha levels, and wouldn't want to miss out on it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-760657953
https://github.com/scverse/scanpy/issues/1263#issuecomment-760657953:639,Testability,assert,assert,639,"As an additional example, I was thinking about using [zebra-stripes (like a camera)](https://en.wikipedia.org/wiki/Zebra_patterning) for showing when information was hidden. Not sure if it's quite there yet, but its something:. <img width=""1318"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104683802-c2f60980-574b-11eb-9a96-8d65e853739d.png"">. <details>; <summary> Code </summary>. ```python; import datashader as ds; from datashader import transfer_functions as tf. import numpy as np; import pandas as pd; from scipy import sparse; import xarray as xr. import scanpy as sc. def diagonal_bands_like(arr, width=3):; assert arr.ndim == 2; a = np.zeros_like(arr, dtype=bool); step = a.shape[1] + 1; # Not sure why end isn't making a difference; end = None; # end = a.shape[1] * a.shape[1]; fill = True; for i in range(arr.shape[0]):; if (i + width // 2) % width == 0:; fill = not fill; if fill:; a.flat[i:end:step] = True; return a. # Setup; adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""); df = sc.get.obs_df(; adata,; [""Sox17"", ""louvain""],; obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]; ); louvain_colors = dict(; zip(; adata.obs[""louvain""].cat.categories, ; adata.uns[""louvain_colors""]; ); ); pts = (; ds.Canvas(1000, 1000); .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")); ). # Make images; pts_ncats = (pts != 0).sum(axis=2); overlap_idx = pts_ncats == 1; zebra_source = xr.DataArray(; diagonal_bands_like(overlap_idx, 13),; coords=overlap_idx.coords; ). color_by_cluster = tf.shade(pts, color_key=louvain_colors); tf.Images(; color_by_cluster,; tf.stack(; tf.Image(xr.where(pts_ncats == 1, color_by_cluster, 0)),; tf.Image(tf.shade(xr.where(pts_ncats > 1, zebra_source, False), cmap=""black"")); ),; tf.stack(; color_by_cluster,; tf.Image(tf.shade(xr.where(pts_ncats > 1, zebra_source, False), cmap=""black"")); ),; ); ```. </details>. > I do think that randomization would result in sth similar to the datashader example you show tho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-760657953
https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279:389,Safety,avoid,avoid,389,"We should make dynamic 3D plots ;-) . If I remember correctly, in the past we have the issue that the categorical colors were given by the adata.obs order and we change them such that they follow the order of the categories. Yet, I agree that a good mix of categorical colors is good sometimes. To address this issue I think that we can simply randomize the order if `sort_order=False` to avoid adding any new parameters. . Isaac's solution looks great for dealing with of lots of cells, something that I imagine will become more frequent. I think we should have a 'cookbook' where we can keep this and other information. I find this better than adding more and more functionality to the scatter plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279
https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279:337,Usability,simpl,simply,337,"We should make dynamic 3D plots ;-) . If I remember correctly, in the past we have the issue that the categorical colors were given by the adata.obs order and we change them such that they follow the order of the categories. Yet, I agree that a good mix of categorical colors is good sometimes. To address this issue I think that we can simply randomize the order if `sort_order=False` to avoid adding any new parameters. . Isaac's solution looks great for dealing with of lots of cells, something that I imagine will become more frequent. I think we should have a 'cookbook' where we can keep this and other information. I find this better than adding more and more functionality to the scatter plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761095279
https://github.com/scverse/scanpy/issues/1263#issuecomment-761598626:518,Deployability,update,update,518,"> What the fair way to color this? If it were random, or purely by count this would look mostly like A. I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. And rare cell types shouldn't be up-weighted in that in an unbiased representation (if there is such a thing). In general I do like the idea of density being linked to transparency though. We could do a quick fix based on random order for now though, and then look into transparency for a larger update that would have to do with updating scanpy plotting to larger cell numbers?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761598626
https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1465,Deployability,update,update,1465,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895
https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1692,Integrability,depend,dependencies,1692,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895
https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1368,Modifiability,flexible,flexible,1368,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895
https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:848,Usability,simpl,simplest,848,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push 😆. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895
https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554:737,Deployability,continuous,continuous,737,"> The only advantage of sort_order=order_array is that is explicit for the user. Another advantage is that it could be user specified per plot when there are multiple plots. -------------------. I think there is another issue, which is that `sort_order` currently just applies to numeric values while here we are trying to deal with issues around categorical values. To me this suggests a need to have separate arguments for the two cases (`order_categorical`, `order_continuous`), though this raises issues with ""vectorizing"" the argument. Docstrings for these arguments would look something like:. ```rst; order_continuous: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""ascending""; How to order points in plots colored by continuous values. Options include:; * ""current"": use current ordering of AnnData object; * ""random"": randomize the order; * ""ascending"": points with the highest value are plotted on top; * ""descending"": points with lowest value are plotted on top; order_categorical: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""random""; How to order non-null categorical points in the plot. Uses same options as order_continuous.; ```. In this case, `sort_order` would be deprecated, and tell the user to use `order_continuous` instead. ## Potential extensions. * We could also allow users to pass `Callable[Vector, Vector[int]]`s (e.g. function which takes color vector, returns vector of integers) as arguments. ## Possible issues. ### Vectorization could be complicated. Vectorization of argument unclear/ maybe not possible. That is, what if I want the same variable twice, but ordered differently? This would look like: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8""], order_continuous=[""ascending"", ""descending""]); ```. Now what if I wanted to also plot a categorical value? Is this: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8"", ""leiden""], order_continuous=[""ascending"", ""descending"", None]); ```. ### Null values. This solution assumes we still wa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554
https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554:1598,Modifiability,variab,variable,1598,"th issues around categorical values. To me this suggests a need to have separate arguments for the two cases (`order_categorical`, `order_continuous`), though this raises issues with ""vectorizing"" the argument. Docstrings for these arguments would look something like:. ```rst; order_continuous: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""ascending""; How to order points in plots colored by continuous values. Options include:; * ""current"": use current ordering of AnnData object; * ""random"": randomize the order; * ""ascending"": points with the highest value are plotted on top; * ""descending"": points with lowest value are plotted on top; order_categorical: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""random""; How to order non-null categorical points in the plot. Uses same options as order_continuous.; ```. In this case, `sort_order` would be deprecated, and tell the user to use `order_continuous` instead. ## Potential extensions. * We could also allow users to pass `Callable[Vector, Vector[int]]`s (e.g. function which takes color vector, returns vector of integers) as arguments. ## Possible issues. ### Vectorization could be complicated. Vectorization of argument unclear/ maybe not possible. That is, what if I want the same variable twice, but ordered differently? This would look like: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8""], order_continuous=[""ascending"", ""descending""]); ```. Now what if I wanted to also plot a categorical value? Is this: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8"", ""leiden""], order_continuous=[""ascending"", ""descending"", None]); ```. ### Null values. This solution assumes we still want null values plotted on bottom. Should there be control over that?. ## Some references for other libraries:. * [`altair.Sort`](https://altair-viz.github.io/user_guide/generated/core/altair.Sort.html#altair.Sort); * (I'm actually not sure if other libraries do this, datashader does `max`/ `min`/ `mean` which is sorta similar?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554
https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245:811,Performance,perform,perform,811,"Hi @preetida,. I think this question is more directed towards the `single-cell-tutorial` github [here](github.com/theislab/single-cell-tutorial). I assume that's where you got the above sentence from. In case you haven't done so already, you can check out the accompanying paper with that tutorial [here](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746). In general whatever you store in `adata.raw` is what is used when you set `use_raw=True`. In that tutorial I have stored log-normalized data in `adata.raw.X` and I store log-normalized and batch corrected data in `adata.X`. Thus, you are plotting two different versions of the data when you set `use_raw` differently. In general, if you set up your `adata.raw` as I did in the tutorial, it is advisable to plot with `use_raw=False`, but when you perform a DE test, you shouldn't use the corrected data stored in `adata.X`, so the default is `use_raw=True`. I hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245
https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245:486,Testability,log,log-normalized,486,"Hi @preetida,. I think this question is more directed towards the `single-cell-tutorial` github [here](github.com/theislab/single-cell-tutorial). I assume that's where you got the above sentence from. In case you haven't done so already, you can check out the accompanying paper with that tutorial [here](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746). In general whatever you store in `adata.raw` is what is used when you set `use_raw=True`. In that tutorial I have stored log-normalized data in `adata.raw.X` and I store log-normalized and batch corrected data in `adata.X`. Thus, you are plotting two different versions of the data when you set `use_raw` differently. In general, if you set up your `adata.raw` as I did in the tutorial, it is advisable to plot with `use_raw=False`, but when you perform a DE test, you shouldn't use the corrected data stored in `adata.X`, so the default is `use_raw=True`. I hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245
https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245:535,Testability,log,log-normalized,535,"Hi @preetida,. I think this question is more directed towards the `single-cell-tutorial` github [here](github.com/theislab/single-cell-tutorial). I assume that's where you got the above sentence from. In case you haven't done so already, you can check out the accompanying paper with that tutorial [here](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746). In general whatever you store in `adata.raw` is what is used when you set `use_raw=True`. In that tutorial I have stored log-normalized data in `adata.raw.X` and I store log-normalized and batch corrected data in `adata.X`. Thus, you are plotting two different versions of the data when you set `use_raw` differently. In general, if you set up your `adata.raw` as I did in the tutorial, it is advisable to plot with `use_raw=False`, but when you perform a DE test, you shouldn't use the corrected data stored in `adata.X`, so the default is `use_raw=True`. I hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245
https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245:824,Testability,test,test,824,"Hi @preetida,. I think this question is more directed towards the `single-cell-tutorial` github [here](github.com/theislab/single-cell-tutorial). I assume that's where you got the above sentence from. In case you haven't done so already, you can check out the accompanying paper with that tutorial [here](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746). In general whatever you store in `adata.raw` is what is used when you set `use_raw=True`. In that tutorial I have stored log-normalized data in `adata.raw.X` and I store log-normalized and batch corrected data in `adata.X`. Thus, you are plotting two different versions of the data when you set `use_raw` differently. In general, if you set up your `adata.raw` as I did in the tutorial, it is advisable to plot with `use_raw=False`, but when you perform a DE test, you shouldn't use the corrected data stored in `adata.X`, so the default is `use_raw=True`. I hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245
https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813:72,Modifiability,refactor,refactoring,72,"Sorry but the question is not clear. The plotting functions underwent a refactoring recently but that one should still work no? I'll close this for the moment, feel free to reopen it but please do so with a reproducible example, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813
https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813:30,Usability,clear,clear,30,"Sorry but the question is not clear. The plotting functions underwent a refactoring recently but that one should still work no? I'll close this for the moment, feel free to reopen it but please do so with a reproducible example, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813
https://github.com/scverse/scanpy/pull/1271#issuecomment-641221919:98,Testability,benchmark,benchmark,98,This is awesome! And much more elaborate than my version of the same:; https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-641221919
https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854:313,Security,access,access,313,"Do you guys think this PR makes sense or is it too much to add R packages to the travis setup? @ivirshup @flying-sheep . There are bunch of useful R packages out there that will most likely not be reimplemented in Python (limma-voom pseudobulk DE, Liger, MAST etc.). I think it'd be cool to revive rtools and add access to such packages. I'm not sure if this is the right way but, any guidance is appreciated :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854
https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854:385,Usability,guid,guidance,385,"Do you guys think this PR makes sense or is it too much to add R packages to the travis setup? @ivirshup @flying-sheep . There are bunch of useful R packages out there that will most likely not be reimplemented in Python (limma-voom pseudobulk DE, Liger, MAST etc.). I think it'd be cool to revive rtools and add access to such packages. I'm not sure if this is the right way but, any guidance is appreciated :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-666481854
https://github.com/scverse/scanpy/pull/1271#issuecomment-666969144:144,Integrability,depend,dependencies,144,I don't think we should have to worry about dealing with CI for R in this project. This only becomes harder if the intent is to have many other dependencies. I think doing anything like this makes much more sense in seperate dedicated package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-666969144
https://github.com/scverse/scanpy/pull/1271#issuecomment-694955289:14,Deployability,update,updates,14,Are there any updates concerning this PR? scTransform functionality in scanpy would be much appreciated :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-694955289
https://github.com/scverse/scanpy/issues/1273#issuecomment-651518306:251,Deployability,install,installing,251,"I think your issue here is due to having multiple versions of various packages in your path. In general, that will cause problems. I think I can only recommend creating an isolated environment using something like `conda` or `virtualenv` and say that installing with `pip` tends to have the fewest problems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-651518306
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:162,Availability,ERROR,ERROR,162,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:328,Availability,down,download,328,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:488,Availability,error,error,488,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:592,Availability,error,error,592,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:675,Availability,error,error,675,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:682,Availability,echo,echo,682,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:96,Deployability,install,install,96,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:149,Deployability,install,installed,149,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:428,Deployability,install,install,428,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:458,Deployability,install,install,458,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:509,Deployability,install,install,509,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:562,Deployability,install,install,562,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:613,Deployability,install,install,613,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:645,Deployability,install,install,645,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:312,Performance,cache,cache,312,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:113,Usability,learn,learn,113,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:174,Usability,learn,learn,174,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:337,Usability,clear,clearer,337,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039
https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084:266,Availability,ERROR,ERROR,266,"Also, set up johnnydep and then do:. `johnnydep --output-format pinned scanpy_scripts; `; and after trundling for a very long time and emitting a lot of messages it gives up with:. ```. Given no hashes to check 0 links for project 'scipy': discarding no candidates; ERROR: Could not find a version that satisfies the requirement scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0rc2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0); ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084
https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084:813,Availability,ERROR,ERROR,813,"Also, set up johnnydep and then do:. `johnnydep --output-format pinned scanpy_scripts; `; and after trundling for a very long time and emitting a lot of messages it gives up with:. ```. Given no hashes to check 0 links for project 'scipy': discarding no candidates; ERROR: Could not find a version that satisfies the requirement scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0rc2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0); ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084
https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084:153,Integrability,message,messages,153,"Also, set up johnnydep and then do:. `johnnydep --output-format pinned scanpy_scripts; `; and after trundling for a very long time and emitting a lot of messages it gives up with:. ```. Given no hashes to check 0 links for project 'scipy': discarding no candidates; ERROR: Could not find a version that satisfies the requirement scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0rc2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0); ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084
https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084:195,Security,hash,hashes,195,"Also, set up johnnydep and then do:. `johnnydep --output-format pinned scanpy_scripts; `; and after trundling for a very long time and emitting a lot of messages it gives up with:. ```. Given no hashes to check 0 links for project 'scipy': discarding no candidates; ERROR: Could not find a version that satisfies the requirement scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0rc2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0); ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084
https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532:266,Deployability,install,installs,266,"It's definitely a problem that you are seeing all of these version restrictions at once. This may be related to having too many entries in your PYTHONPATH environment variable. `PYTHONPATH` should probably just be empty, since python already knows to look where pip installs packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532
https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532:167,Modifiability,variab,variable,167,"It's definitely a problem that you are seeing all of these version restrictions at once. This may be related to having too many entries in your PYTHONPATH environment variable. `PYTHONPATH` should probably just be empty, since python already knows to look where pip installs packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532
https://github.com/scverse/scanpy/issues/1273#issuecomment-659028734:122,Availability,down,downloads,122,"Look at the virtualenv example - PYTHONPATH was empty. The johnnydep application does not actually do an install, it just downloads all the pieces a package calls for and looks at all the listed requirements - and it gives the same version restriction conflict as an actual installation attemp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-659028734
https://github.com/scverse/scanpy/issues/1273#issuecomment-659028734:105,Deployability,install,install,105,"Look at the virtualenv example - PYTHONPATH was empty. The johnnydep application does not actually do an install, it just downloads all the pieces a package calls for and looks at all the listed requirements - and it gives the same version restriction conflict as an actual installation attemp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-659028734
https://github.com/scverse/scanpy/issues/1273#issuecomment-659028734:274,Deployability,install,installation,274,"Look at the virtualenv example - PYTHONPATH was empty. The johnnydep application does not actually do an install, it just downloads all the pieces a package calls for and looks at all the listed requirements - and it gives the same version restriction conflict as an actual installation attemp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-659028734
https://github.com/scverse/scanpy/issues/1273#issuecomment-661095384:36,Integrability,depend,dependency,36,Which package has the `scipy<1.3.0` dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661095384
https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497:920,Availability,ERROR,ERROR,920,"It seems to be scanpy-scripts itself. johnnydep analysis shows these (99% of lines removed):; ```. 2020-07-20 18:57:50 [info ] init johnnydist [johnnydep.lib] dist=scipy<1.3.0,>=1.2.0 parent=scanpy-scripts; 2020-07-20 18:58:10 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata<0.6.20; 2020-07-20 18:59:17 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata>=0.6.15; 2020-07-20 18:59:26 [info ] init johnnydist [johnnydep.lib] dist=scipy>=0.19.1 parent=scikit-learn>=0.19.1; 2020-07-20 18:59:58 [info ] init johnnydist [johnnydep.lib] dist=scipy>=1.3.1 parent=umap-learn>=0.3.0; ```. and later. ```; 2020-07-20 19:00:14 [info ] merged specs [johnnydep.lib] dist=scanpy-scripts extras=; set() name=scipy spec=<SpecifierSet('<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0', prereleases=True)>. ```. It cannot match both <1.3.0 and >= 1.3.1, and eventually bails out with:. ```; ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; pip._internal.exceptions.DistributionNotFound: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'wheel', '-vvv', '--no-deps', '--no-cache-dir', '--disable-pip-version-check', '--pro; gress-bar=off', 'scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497
https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497:1270,Performance,cache,cache-dir,1270,"It seems to be scanpy-scripts itself. johnnydep analysis shows these (99% of lines removed):; ```. 2020-07-20 18:57:50 [info ] init johnnydist [johnnydep.lib] dist=scipy<1.3.0,>=1.2.0 parent=scanpy-scripts; 2020-07-20 18:58:10 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata<0.6.20; 2020-07-20 18:59:17 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata>=0.6.15; 2020-07-20 18:59:26 [info ] init johnnydist [johnnydep.lib] dist=scipy>=0.19.1 parent=scikit-learn>=0.19.1; 2020-07-20 18:59:58 [info ] init johnnydist [johnnydep.lib] dist=scipy>=1.3.1 parent=umap-learn>=0.3.0; ```. and later. ```; 2020-07-20 19:00:14 [info ] merged specs [johnnydep.lib] dist=scanpy-scripts extras=; set() name=scipy spec=<SpecifierSet('<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0', prereleases=True)>. ```. It cannot match both <1.3.0 and >= 1.3.1, and eventually bails out with:. ```; ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; pip._internal.exceptions.DistributionNotFound: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'wheel', '-vvv', '--no-deps', '--no-cache-dir', '--disable-pip-version-check', '--pro; gress-bar=off', 'scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497
https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497:499,Usability,learn,learn,499,"It seems to be scanpy-scripts itself. johnnydep analysis shows these (99% of lines removed):; ```. 2020-07-20 18:57:50 [info ] init johnnydist [johnnydep.lib] dist=scipy<1.3.0,>=1.2.0 parent=scanpy-scripts; 2020-07-20 18:58:10 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata<0.6.20; 2020-07-20 18:59:17 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata>=0.6.15; 2020-07-20 18:59:26 [info ] init johnnydist [johnnydep.lib] dist=scipy>=0.19.1 parent=scikit-learn>=0.19.1; 2020-07-20 18:59:58 [info ] init johnnydist [johnnydep.lib] dist=scipy>=1.3.1 parent=umap-learn>=0.3.0; ```. and later. ```; 2020-07-20 19:00:14 [info ] merged specs [johnnydep.lib] dist=scanpy-scripts extras=; set() name=scipy spec=<SpecifierSet('<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0', prereleases=True)>. ```. It cannot match both <1.3.0 and >= 1.3.1, and eventually bails out with:. ```; ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; pip._internal.exceptions.DistributionNotFound: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'wheel', '-vvv', '--no-deps', '--no-cache-dir', '--disable-pip-version-check', '--pro; gress-bar=off', 'scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497
https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497:604,Usability,learn,learn,604,"It seems to be scanpy-scripts itself. johnnydep analysis shows these (99% of lines removed):; ```. 2020-07-20 18:57:50 [info ] init johnnydist [johnnydep.lib] dist=scipy<1.3.0,>=1.2.0 parent=scanpy-scripts; 2020-07-20 18:58:10 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata<0.6.20; 2020-07-20 18:59:17 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata>=0.6.15; 2020-07-20 18:59:26 [info ] init johnnydist [johnnydep.lib] dist=scipy>=0.19.1 parent=scikit-learn>=0.19.1; 2020-07-20 18:59:58 [info ] init johnnydist [johnnydep.lib] dist=scipy>=1.3.1 parent=umap-learn>=0.3.0; ```. and later. ```; 2020-07-20 19:00:14 [info ] merged specs [johnnydep.lib] dist=scanpy-scripts extras=; set() name=scipy spec=<SpecifierSet('<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0', prereleases=True)>. ```. It cannot match both <1.3.0 and >= 1.3.1, and eventually bails out with:. ```; ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; pip._internal.exceptions.DistributionNotFound: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'wheel', '-vvv', '--no-deps', '--no-cache-dir', '--disable-pip-version-check', '--pro; gress-bar=off', 'scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497
https://github.com/scverse/scanpy/issues/1273#issuecomment-661986559:182,Deployability,upgrade,upgrade,182,"Yeah, please file your issue with [scanpy-scripts](https://github.com/ebi-gene-expression-group/scanpy-scripts) then, and ask them why they want an old scipy version and if they can upgrade their code to work with scipy 1.3+",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661986559
https://github.com/scverse/scanpy/issues/1275#issuecomment-647275662:58,Integrability,depend,dependencies,58,"Oh, by version numbers I meant not just `anndata` but the dependencies as well. I'm wondering in particular about the version of `h5py`. The output of something like [`sinfo(dependencies=True)`](https://pypi.org/project/sinfo/) would be great. If you try writing to a different path, are you able too? It kind of looks like you're writing to a file that already exists, though that should just overwrite the file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275#issuecomment-647275662
https://github.com/scverse/scanpy/issues/1275#issuecomment-647275662:174,Integrability,depend,dependencies,174,"Oh, by version numbers I meant not just `anndata` but the dependencies as well. I'm wondering in particular about the version of `h5py`. The output of something like [`sinfo(dependencies=True)`](https://pypi.org/project/sinfo/) would be great. If you try writing to a different path, are you able too? It kind of looks like you're writing to a file that already exists, though that should just overwrite the file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275#issuecomment-647275662
https://github.com/scverse/scanpy/issues/1275#issuecomment-654493614:36,Availability,error,error,36,"I am not sure what was causing this error, but it must be somewhat idiosyncratic as the issue is resolved in a fresh env. Thanks! Closing this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275#issuecomment-654493614
https://github.com/scverse/scanpy/issues/1275#issuecomment-996448667:38,Availability,error,error,38,"> I am not sure what was causing this error, but it must be somewhat idiosyncratic as the issue is resolved in a fresh env. Thanks! Closing this.; Hi, I met same problem, how did you solve it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275#issuecomment-996448667
https://github.com/scverse/scanpy/issues/1275#issuecomment-996451713:702,Availability,robust,robust,702,"Hi @GouQiao - it's been a while since this specific incident so I don't 100% remember / have the code anymore. However, I have run into this problem in general when using AnnData and it's usually resolved by one of two paths:. 1. Check the version of `h5py` that you have installed and perhaps it is too new and an older version resolves the issue. ; 2. Some components of AnnData are not implemented in the function, `.write_h5ad()`. One example that comes to mind is the umap or pca transformer. These objects are not handled well by `.h5py` (at least natively in my experience) and are better off saved independently as dictionaries using `pickle`. That being said, I think there is probably a more robust solution I am not aware of - I know in several instances transformers are able to be saved (e.g., the Scanpy tutorials). . Does this help at all? Happy to be of further assistance if possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275#issuecomment-996451713
https://github.com/scverse/scanpy/issues/1275#issuecomment-996451713:272,Deployability,install,installed,272,"Hi @GouQiao - it's been a while since this specific incident so I don't 100% remember / have the code anymore. However, I have run into this problem in general when using AnnData and it's usually resolved by one of two paths:. 1. Check the version of `h5py` that you have installed and perhaps it is too new and an older version resolves the issue. ; 2. Some components of AnnData are not implemented in the function, `.write_h5ad()`. One example that comes to mind is the umap or pca transformer. These objects are not handled well by `.h5py` (at least natively in my experience) and are better off saved independently as dictionaries using `pickle`. That being said, I think there is probably a more robust solution I am not aware of - I know in several instances transformers are able to be saved (e.g., the Scanpy tutorials). . Does this help at all? Happy to be of further assistance if possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275#issuecomment-996451713
https://github.com/scverse/scanpy/issues/1277#issuecomment-701367442:114,Availability,error,error,114,This feature is not used by pl.umap() or pl.draw_graph(). These functions do not search in the raw and return the error` IndexError: index 0 is out of bounds for axis 0 with size 0` when using gene_symbols that are only present in raw.; Could this feature also be implemented for these funtions?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-701367442
https://github.com/scverse/scanpy/issues/1277#issuecomment-701591572:102,Availability,error,error,102,"Sure, I can do this. I'll submit a PR in a bit. @bfurtwa could you post the full stack trace for this error to help me find the problem please? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-701591572
https://github.com/scverse/scanpy/issues/1277#issuecomment-703164973:1649,Availability,error,error,1649,"Sure:. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-161-7b672fc51046> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False). ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 724 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 725 """"""; --> 726 return embedding(adata, 'pca', **kwargs); 727 ; 728 . ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 226 itertools.product(color, idx_components); 227 ):; --> 228 color_vector, categorical = _get_color_values(; 229 adata,; 230 value_to_plot,. ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer); 1031 ):; 1032 # We should probably just make an index for this, and share it over runs; -> 1033 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1034 0; 1035 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bounds for axis 0 with size 0; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703164973
https://github.com/scverse/scanpy/issues/1277#issuecomment-703164973:244,Security,Access,Accession,244,"Sure:. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-161-7b672fc51046> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False). ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 724 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 725 """"""; --> 726 return embedding(adata, 'pca', **kwargs); 727 ; 728 . ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 226 itertools.product(color, idx_components); 227 ):; --> 228 color_vector, categorical = _get_color_values(; 229 adata,; 230 value_to_plot,. ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer); 1031 ):; 1032 # We should probably just make an index for this, and share it over runs; -> 1033 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1034 0; 1035 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bounds for axis 0 with size 0; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703164973
https://github.com/scverse/scanpy/issues/1277#issuecomment-703829011:111,Availability,error,error,111,It looks like in your initial call to `sc.pl.pca` you are not specifying `use_raw=True`. Do you still get this error when you do?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703829011
https://github.com/scverse/scanpy/issues/1277#issuecomment-703860357:1661,Availability,error,error,1661,"; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-39-2ee11f6b7699> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False, use_raw=True). ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 724 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 725 """"""; --> 726 return embedding(adata, 'pca', **kwargs); 727 ; 728 . ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 226 itertools.product(color, idx_components); 227 ):; --> 228 color_vector, categorical = _get_color_values(; 229 adata,; 230 value_to_plot,. ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer); 1031 ):; 1032 # We should probably just make an index for this, and share it over runs; -> 1033 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1034 0; 1035 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bounds for axis 0 with size 0; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703860357
https://github.com/scverse/scanpy/issues/1277#issuecomment-703860357:242,Security,Access,Accession,242,"Yes:. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-39-2ee11f6b7699> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False, use_raw=True). ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 724 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 725 """"""; --> 726 return embedding(adata, 'pca', **kwargs); 727 ; 728 . ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 226 itertools.product(color, idx_components); 227 ):; --> 228 color_vector, categorical = _get_color_values(; 229 adata,; 230 value_to_plot,. ~/miniconda3/envs/sc/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw, gene_symbols, layer); 1031 ):; 1032 # We should probably just make an index for this, and share it over runs; -> 1033 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1034 0; 1035 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bounds for axis 0 with siz",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703860357
https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451:208,Availability,error,error,208,What version of scanpy are you using? I don't see a function called `_get_color_values` in `scatterplots.py` in the HEAD version. Can you try installing the development version and seeing if you get the same error? The issue might be that the version you are currently using does not include the fix I made above. ```; git clone https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451
https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451:142,Deployability,install,installing,142,What version of scanpy are you using? I don't see a function called `_get_color_values` in `scatterplots.py` in the HEAD version. Can you try installing the development version and seeing if you get the same error? The issue might be that the version you are currently using does not include the fix I made above. ```; git clone https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451
https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451:384,Deployability,install,install,384,What version of scanpy are you using? I don't see a function called `_get_color_values` in `scatterplots.py` in the HEAD version. Can you try installing the development version and seeing if you get the same error? The issue might be that the version you are currently using does not include the fix I made above. ```; git clone https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703887451
https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344:12,Availability,error,error,12,"This is the error with the development version:. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-36-2ee11f6b7699> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False, use_raw=True). /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 732 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 733 """"""; --> 734 return embedding(adata, 'pca', **kwargs); 735 ; 736 . /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 243 itertools.product(color, idx_components); 244 ):; --> 245 color_source_vector = _get_color_source_vector(; 246 adata,; 247 value_to_plot,. /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_source_vector(adata, value_to_plot, use_raw, gene_symbols, layer, groups); 1016 ):; 1017 # We should probably just make an index for this, and share it over runs; -> 1018 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1019 0; 1020 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bound",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344
https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344:393,Availability,Down,Downloads,393,"This is the error with the development version:. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-36-2ee11f6b7699> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False, use_raw=True). /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 732 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 733 """"""; --> 734 return embedding(adata, 'pca', **kwargs); 735 ; 736 . /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 243 itertools.product(color, idx_components); 244 ):; --> 245 color_source_vector = _get_color_source_vector(; 246 adata,; 247 value_to_plot,. /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_source_vector(adata, value_to_plot, use_raw, gene_symbols, layer, groups); 1016 ):; 1017 # We should probably just make an index for this, and share it over runs; -> 1018 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1019 0; 1020 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bound",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344
https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344:632,Availability,Down,Downloads,632,"This is the error with the development version:. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-36-2ee11f6b7699> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False, use_raw=True). /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 732 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 733 """"""; --> 734 return embedding(adata, 'pca', **kwargs); 735 ; 736 . /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 243 itertools.product(color, idx_components); 244 ):; --> 245 color_source_vector = _get_color_source_vector(; 246 adata,; 247 value_to_plot,. /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_source_vector(adata, value_to_plot, use_raw, gene_symbols, layer, groups); 1016 ):; 1017 # We should probably just make an index for this, and share it over runs; -> 1018 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1019 0; 1020 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bound",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344
https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344:1328,Availability,Down,Downloads,1328,"elopment version:. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-36-2ee11f6b7699> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False, use_raw=True). /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 732 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 733 """"""; --> 734 return embedding(adata, 'pca', **kwargs); 735 ; 736 . /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 243 itertools.product(color, idx_components); 244 ):; --> 245 color_source_vector = _get_color_source_vector(; 246 adata,; 247 value_to_plot,. /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_source_vector(adata, value_to_plot, use_raw, gene_symbols, layer, groups); 1016 ):; 1017 # We should probably just make an index for this, and share it over runs; -> 1018 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1019 0; 1020 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bounds for axis 0 with size 0; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344
https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344:1682,Availability,error,error,1682,"elopment version:. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-36-2ee11f6b7699> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False, use_raw=True). /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 732 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 733 """"""; --> 734 return embedding(adata, 'pca', **kwargs); 735 ; 736 . /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 243 itertools.product(color, idx_components); 244 ):; --> 245 color_source_vector = _get_color_source_vector(; 246 adata,; 247 value_to_plot,. /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_source_vector(adata, value_to_plot, use_raw, gene_symbols, layer, groups); 1016 ):; 1017 # We should probably just make an index for this, and share it over runs; -> 1018 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1019 0; 1020 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bounds for axis 0 with size 0; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344
https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344:285,Security,Access,Accession,285,"This is the error with the development version:. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-36-2ee11f6b7699> in <module>; ----> 1 axs = sc.pl.pca(adata, color=['P36957'], gene_symbols='Accession', size=cellsize, wspace=wspace, hspace=hspace, ncols=3, show=False, use_raw=True). /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 732 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 733 """"""; --> 734 return embedding(adata, 'pca', **kwargs); 735 ; 736 . /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, img_key, crop_coord, alpha_img, bw, library_id, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 243 itertools.product(color, idx_components); 244 ):; --> 245 color_source_vector = _get_color_source_vector(; 246 adata,; 247 value_to_plot,. /mnt/c/Users/q/Downloads/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_color_source_vector(adata, value_to_plot, use_raw, gene_symbols, layer, groups); 1016 ):; 1017 # We should probably just make an index for this, and share it over runs; -> 1018 value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][; 1019 0; 1020 ] # TODO: Throw helpful error if this doesn't work. ~/miniconda3/envs/sc/lib/python3.8/site-packages/pandas/core/indexes/base.py in __getitem__(self, key); 4095 if is_scalar(key):; 4096 key = com.cast_scalar_indexer(key, warn_float=True); -> 4097 return getitem(key); 4098 ; 4099 if isinstance(key, slice):. IndexError: index 0 is out of bound",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703912344
https://github.com/scverse/scanpy/issues/1277#issuecomment-703931637:312,Deployability,install,install,312,"Alright, I think I found the problem and hopefully fixed it, although I am not sure as I don't completely understand how the `_get_color_source_vector` function works. If you'd like to try my fix, it's in my fork:. ```; git clone https://github.com/WarrenLab/scanpy.git; cd scanpy; git checkout use_raw_fix; pip install .; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1277#issuecomment-703931637
https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898:14,Deployability,install,install,14,just use `pip install louvain` to install the louvain package and use this functionality. . @ivirshup @flying-sheep I noticed that the louvain install suggestion in the documentation has been replaced by a `pip install scanpy[leiden]` suggestion. However `louvain` is still the default in the tutorials. Maybe the louvain install should be added again?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898
https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898:34,Deployability,install,install,34,just use `pip install louvain` to install the louvain package and use this functionality. . @ivirshup @flying-sheep I noticed that the louvain install suggestion in the documentation has been replaced by a `pip install scanpy[leiden]` suggestion. However `louvain` is still the default in the tutorials. Maybe the louvain install should be added again?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898
https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898:143,Deployability,install,install,143,just use `pip install louvain` to install the louvain package and use this functionality. . @ivirshup @flying-sheep I noticed that the louvain install suggestion in the documentation has been replaced by a `pip install scanpy[leiden]` suggestion. However `louvain` is still the default in the tutorials. Maybe the louvain install should be added again?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898
https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898:211,Deployability,install,install,211,just use `pip install louvain` to install the louvain package and use this functionality. . @ivirshup @flying-sheep I noticed that the louvain install suggestion in the documentation has been replaced by a `pip install scanpy[leiden]` suggestion. However `louvain` is still the default in the tutorials. Maybe the louvain install should be added again?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898
https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898:322,Deployability,install,install,322,just use `pip install louvain` to install the louvain package and use this functionality. . @ivirshup @flying-sheep I noticed that the louvain install suggestion in the documentation has been replaced by a `pip install scanpy[leiden]` suggestion. However `louvain` is still the default in the tutorials. Maybe the louvain install should be added again?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-645269898
https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756:132,Availability,failure,failure,132,"I also have the same problem and I tried to use `pip install louvain`, but I cannot install the package and it says `legacy-install-failure`. The GitHub for the [louvain](https://github.com/vtraag/louvain-igraph/tree/master) says,. > Warning; > ; > This package has been superseded by the [leidenalg](https://github.com/vtraag/leidenalg) package and will no longer be maintained.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756
https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756:53,Deployability,install,install,53,"I also have the same problem and I tried to use `pip install louvain`, but I cannot install the package and it says `legacy-install-failure`. The GitHub for the [louvain](https://github.com/vtraag/louvain-igraph/tree/master) says,. > Warning; > ; > This package has been superseded by the [leidenalg](https://github.com/vtraag/leidenalg) package and will no longer be maintained.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756
https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756:84,Deployability,install,install,84,"I also have the same problem and I tried to use `pip install louvain`, but I cannot install the package and it says `legacy-install-failure`. The GitHub for the [louvain](https://github.com/vtraag/louvain-igraph/tree/master) says,. > Warning; > ; > This package has been superseded by the [leidenalg](https://github.com/vtraag/leidenalg) package and will no longer be maintained.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756
https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756:124,Deployability,install,install-failure,124,"I also have the same problem and I tried to use `pip install louvain`, but I cannot install the package and it says `legacy-install-failure`. The GitHub for the [louvain](https://github.com/vtraag/louvain-igraph/tree/master) says,. > Warning; > ; > This package has been superseded by the [leidenalg](https://github.com/vtraag/leidenalg) package and will no longer be maintained.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637853756
https://github.com/scverse/scanpy/issues/1283#issuecomment-1637879203:183,Deployability,install,install,183,"Check out scanpy’s optional features:. https://github.com/scverse/scanpy/blob/21ca328672646d7d0ba42b64eee2823babc2d2ed/pyproject.toml#L133-L145. as you can see `scanpy[louvain]` will install it, while `scanpy[leiden]` will install the successor.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637879203
https://github.com/scverse/scanpy/issues/1283#issuecomment-1637879203:223,Deployability,install,install,223,"Check out scanpy’s optional features:. https://github.com/scverse/scanpy/blob/21ca328672646d7d0ba42b64eee2823babc2d2ed/pyproject.toml#L133-L145. as you can see `scanpy[louvain]` will install it, while `scanpy[leiden]` will install the successor.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637879203
https://github.com/scverse/scanpy/issues/1283#issuecomment-1637898938:62,Deployability,install,installed,62,"Thank you for the comment. Do you mean these packages will be installed when we install Scanpy? Sorry that I don't understand. When I try to use `scanpy.tl.louvain`, it says `ModuleNotFoundError: No module named 'louvain'`, and I don't know how to solve it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637898938
https://github.com/scverse/scanpy/issues/1283#issuecomment-1637898938:80,Deployability,install,install,80,"Thank you for the comment. Do you mean these packages will be installed when we install Scanpy? Sorry that I don't understand. When I try to use `scanpy.tl.louvain`, it says `ModuleNotFoundError: No module named 'louvain'`, and I don't know how to solve it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1637898938
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248:14,Deployability,install,install,14,"As said: `pip install scanpy[leiden]`, and use `scanpy.tl.leiden()` instead. See here for how to install scanpy and its dependencies: https://scanpy.readthedocs.io/en/stable/installation.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248:97,Deployability,install,install,97,"As said: `pip install scanpy[leiden]`, and use `scanpy.tl.leiden()` instead. See here for how to install scanpy and its dependencies: https://scanpy.readthedocs.io/en/stable/installation.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248:174,Deployability,install,installation,174,"As said: `pip install scanpy[leiden]`, and use `scanpy.tl.leiden()` instead. See here for how to install scanpy and its dependencies: https://scanpy.readthedocs.io/en/stable/installation.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248:120,Integrability,depend,dependencies,120,"As said: `pip install scanpy[leiden]`, and use `scanpy.tl.leiden()` instead. See here for how to install scanpy and its dependencies: https://scanpy.readthedocs.io/en/stable/installation.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:158,Availability,error,error,158,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:279,Availability,error,error,279,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:301,Availability,failure,failure,301,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:107,Deployability,install,install,107,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:242,Deployability,install,install,242,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:293,Deployability,install,install-failure,293,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:164,Integrability,message,message,164,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295
https://github.com/scverse/scanpy/issues/1283#issuecomment-1638297190:95,Safety,detect,detection,95,"louvain is deprecated. leiden is its successor. So unless you want compare different community detection methods, you should use leiden instead of louvain. Just leave out the `flavor`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638297190
https://github.com/scverse/scanpy/issues/1284#issuecomment-646467845:96,Deployability,install,installation,96,"Are you able to run `import tables` in this environment? If not, I think the issue will be with installation of the `pytables` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1284#issuecomment-646467845
https://github.com/scverse/scanpy/issues/1285#issuecomment-650102141:79,Deployability,release,release,79,"Hi, @AlejandraRodelaRo ; It was fixed on github master. You can wait for a new release or install scanpy from github.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1285#issuecomment-650102141
https://github.com/scverse/scanpy/issues/1285#issuecomment-650102141:90,Deployability,install,install,90,"Hi, @AlejandraRodelaRo ; It was fixed on github master. You can wait for a new release or install scanpy from github.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1285#issuecomment-650102141
https://github.com/scverse/scanpy/issues/1285#issuecomment-660627731:35,Deployability,release,release,35,"Hi @Koncopd, any idea when the new release will be out?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1285#issuecomment-660627731
https://github.com/scverse/scanpy/issues/1285#issuecomment-660998267:73,Deployability,release,release,73,"Yes, this is a duplicate of #1260. Please follow #1319 to track the next release",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1285#issuecomment-660998267
https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990:181,Deployability,Install,Installed,181,"I'm not able to reproduce this. Here's what I tried. * Made a conda environment with `conda create -yn torch-scanpy ""python=3.8""`, and activated it `conda activate torch-scanpy`; * Installed: `pip install scanpy torch`; * Imported: `python3 -c ""import torch; import scanpy""`. IIRC, there has been an issue with the order of importing numba and pytorch due to how they require their LLVM dependency. I would make sure your version of pytorch and numba are up to date (I believe your pytorch is a few versions old) and trying again. If the issue persists, could you check if you run into problems with this?. ```python; import torch; import numba; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990
https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990:197,Deployability,install,install,197,"I'm not able to reproduce this. Here's what I tried. * Made a conda environment with `conda create -yn torch-scanpy ""python=3.8""`, and activated it `conda activate torch-scanpy`; * Installed: `pip install scanpy torch`; * Imported: `python3 -c ""import torch; import scanpy""`. IIRC, there has been an issue with the order of importing numba and pytorch due to how they require their LLVM dependency. I would make sure your version of pytorch and numba are up to date (I believe your pytorch is a few versions old) and trying again. If the issue persists, could you check if you run into problems with this?. ```python; import torch; import numba; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990
https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990:387,Integrability,depend,dependency,387,"I'm not able to reproduce this. Here's what I tried. * Made a conda environment with `conda create -yn torch-scanpy ""python=3.8""`, and activated it `conda activate torch-scanpy`; * Installed: `pip install scanpy torch`; * Imported: `python3 -c ""import torch; import scanpy""`. IIRC, there has been an issue with the order of importing numba and pytorch due to how they require their LLVM dependency. I would make sure your version of pytorch and numba are up to date (I believe your pytorch is a few versions old) and trying again. If the issue persists, could you check if you run into problems with this?. ```python; import torch; import numba; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990
https://github.com/scverse/scanpy/issues/1286#issuecomment-702367885:120,Usability,clear,clear,120,"I have pytorch and tensorflow alongside scanpy in several conda envs. I would close this for now, also because it's not clear what ""probably it does not finish"" means. ; Feel free to reopen it if problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286#issuecomment-702367885
https://github.com/scverse/scanpy/issues/1287#issuecomment-706180849:346,Usability,simpl,simply,346,"Sounds like a good idea. Since we have a hexagonal grid, we can just connect the centers of the hexagons in a regular fashion instead of running delauney triangulation. But it’s fast enough to do that too if we want to have it easy and there’s a delauney implementation in something we already import (e.g. scipy maybe?). @giovp the result would simply be a smoothly changing shading. Like this, but with a hex grid instead of a square grid:. ![](https://upload.wikimedia.org/wikipedia/commons/f/f5/Interpolation-bicubic.svg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1287#issuecomment-706180849
https://github.com/scverse/scanpy/issues/1287#issuecomment-706185841:225,Availability,avail,available,225,"Hello,. Yes that is exactly what @flying-sheep mentioned. Instead of having a dot plot of gene expression, we would have the option of a surface plot with smoothed gene expression values.; I will try to run this on a pubicly available Visium dataset (mentioned in one of the scanpy tutorials) to show the outcome. On the other hand, it is not necessary to limit this option to regular grids (although in Visium datasets, it is regular). In this way, the function can be used in a more general case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1287#issuecomment-706185841
https://github.com/scverse/scanpy/issues/1288#issuecomment-702366534:53,Availability,ping,ping,53,mmh true that should probably be 1-corr_matrix. I'll ping @flying-sheep he might have a better answer,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1288#issuecomment-702366534
https://github.com/scverse/scanpy/issues/1289#issuecomment-660090356:184,Availability,ping,pinging,184,"Hi, thanks for the suggestion! Are you referring to [this function](https://rdrr.io/bioc/batchelor/man/multiBatchPCA.html) ?; It sounds a bit like `ingest` but with multiple datasets, pinging @Koncopd to see what's his take on this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-660090356
https://github.com/scverse/scanpy/issues/1289#issuecomment-660125494:188,Availability,ping,pinging,188,"> Hi, thanks for the suggestion! Are you referring to [this function](https://rdrr.io/bioc/batchelor/man/multiBatchPCA.html) ?; > It sounds a bit like `ingest` but with multiple datasets, pinging @Koncopd to see what's his take on this. Yes that's the function.; I think it is doing something similar to `ingest`. I think this sort of batch-balanced PCA could be a useful addition addition where batches are very uneven in terms of number of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-660125494
https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353:309,Availability,avail,available,309,"Like you say, the difference between this and `ingest` is joint PCA calculation vs asymmetric batch integration. This function is the first step in the `fastMNN` function, which I have found in some cases yields very sensible batch correction results. It would be awesome to see `multiBatchPCA` +/- `fastMNN` available in scanpy. I am aware of the python implementation of `mnncorrect`, but I think this still operates on expression values rather than a PCA representation (correct me if I am wrong..). Without going all the way the batch correction, `multiBatchPCA` is useful where different experiments have very different numbers of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353
https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353:100,Deployability,integrat,integration,100,"Like you say, the difference between this and `ingest` is joint PCA calculation vs asymmetric batch integration. This function is the first step in the `fastMNN` function, which I have found in some cases yields very sensible batch correction results. It would be awesome to see `multiBatchPCA` +/- `fastMNN` available in scanpy. I am aware of the python implementation of `mnncorrect`, but I think this still operates on expression values rather than a PCA representation (correct me if I am wrong..). Without going all the way the batch correction, `multiBatchPCA` is useful where different experiments have very different numbers of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353
https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353:100,Integrability,integrat,integration,100,"Like you say, the difference between this and `ingest` is joint PCA calculation vs asymmetric batch integration. This function is the first step in the `fastMNN` function, which I have found in some cases yields very sensible batch correction results. It would be awesome to see `multiBatchPCA` +/- `fastMNN` available in scanpy. I am aware of the python implementation of `mnncorrect`, but I think this still operates on expression values rather than a PCA representation (correct me if I am wrong..). Without going all the way the batch correction, `multiBatchPCA` is useful where different experiments have very different numbers of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353
https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967:39,Deployability,integrat,integrating,39,"Hi all,. I am trying to use ScanPy for integrating multiple scRNA-Seq samples (~20). Doing so that I can look at RNA Velocity with SCVelo, and want to use MNN as I got good batch effect removal previously in monocle using MNN. Is it true - as stated above, that the current implementation of mnncorrect with ScanPy is only operating on expression values? I have run through a ScanPy MNN [tutorial ](https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html) provided by NBI Sweden. The results are improved, but it doesn't appear to work as well as in monocle - some separation by batch is still going on. . I'm wondering what the difference might be? Whether it could be due to the difference in PCA (multi-batch), or the actual MNN / batch effect removal step. Alternatively, I could use the corrected expression matrix, and add the UMAP coordinates/clusters from monocle, although I wonder if this is advisable. . If you have any info please let me know, or if I should raise a separate issue etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967
https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967:39,Integrability,integrat,integrating,39,"Hi all,. I am trying to use ScanPy for integrating multiple scRNA-Seq samples (~20). Doing so that I can look at RNA Velocity with SCVelo, and want to use MNN as I got good batch effect removal previously in monocle using MNN. Is it true - as stated above, that the current implementation of mnncorrect with ScanPy is only operating on expression values? I have run through a ScanPy MNN [tutorial ](https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html) provided by NBI Sweden. The results are improved, but it doesn't appear to work as well as in monocle - some separation by batch is still going on. . I'm wondering what the difference might be? Whether it could be due to the difference in PCA (multi-batch), or the actual MNN / batch effect removal step. Alternatively, I could use the corrected expression matrix, and add the UMAP coordinates/clusters from monocle, although I wonder if this is advisable. . If you have any info please let me know, or if I should raise a separate issue etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967
https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:484,Deployability,integrat,integration,484,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157
https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:702,Deployability,integrat,integrated,702,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157
https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:895,Deployability,pipeline,pipeline,895,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157
https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:484,Integrability,integrat,integration,484,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157
https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:702,Integrability,integrat,integrated,702,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157
https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:437,Performance,perform,performed,437,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157
https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:1066,Performance,perform,perform,1066,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157
https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:466,Testability,benchmark,benchmark,466,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157
https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:308,Deployability,integrat,integration,308,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds – when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation – I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then – my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916
https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:357,Deployability,integrat,integrated,357,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds – when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation – I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then – my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916
https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:550,Deployability,pipeline,pipeline,550,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds – when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation – I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then – my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916
https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:308,Integrability,integrat,integration,308,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds – when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation – I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then – my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916
https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:357,Integrability,integrat,integrated,357,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds – when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation – I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then – my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916
https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:795,Performance,perform,performing,795,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds – when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation – I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then – my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916
https://github.com/scverse/scanpy/issues/1292#issuecomment-704767951:44,Availability,Ping,Pinging,44,good catch! Thank you for reporting this. ; Pinging @Koncopd since I believe you were involved in major refactoring of this. ; Thank you @rpeys,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1292#issuecomment-704767951
https://github.com/scverse/scanpy/issues/1292#issuecomment-704767951:104,Modifiability,refactor,refactoring,104,good catch! Thank you for reporting this. ; Pinging @Koncopd since I believe you were involved in major refactoring of this. ; Thank you @rpeys,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1292#issuecomment-704767951
https://github.com/scverse/scanpy/issues/1293#issuecomment-702362311:163,Availability,ping,pinging,163,"this is not related to scanpy, but to sam (scanpy external). Please report the bug in the original repo: https://github.com/atarashansky/self-assembling-manifold; pinging @atarashansky who is possibly most helpful in this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293#issuecomment-702362311
https://github.com/scverse/scanpy/issues/1293#issuecomment-702387945:122,Availability,down,downgrade,122,"Hi @alkhairohr! Strange bug -- never seen it before. The latest UMAP version I've been using with SAM is `0.4.1`. Can you downgrade UMAP to that version and try again? If that fixes your issue, then I'll add `umap<=0.4.1` requirement to the `setup.py` file for SAM as a stopgap until I figure out the issue. Meanwhile, I'll try upgrading to `0.4.4` and see if I can reproduce the error. As @giovp said, because this is a SAM issue, I'll follow up on this issue with a thread on my github repo and ping you there. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293#issuecomment-702387945
https://github.com/scverse/scanpy/issues/1293#issuecomment-702387945:380,Availability,error,error,380,"Hi @alkhairohr! Strange bug -- never seen it before. The latest UMAP version I've been using with SAM is `0.4.1`. Can you downgrade UMAP to that version and try again? If that fixes your issue, then I'll add `umap<=0.4.1` requirement to the `setup.py` file for SAM as a stopgap until I figure out the issue. Meanwhile, I'll try upgrading to `0.4.4` and see if I can reproduce the error. As @giovp said, because this is a SAM issue, I'll follow up on this issue with a thread on my github repo and ping you there. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293#issuecomment-702387945
https://github.com/scverse/scanpy/issues/1293#issuecomment-702387945:497,Availability,ping,ping,497,"Hi @alkhairohr! Strange bug -- never seen it before. The latest UMAP version I've been using with SAM is `0.4.1`. Can you downgrade UMAP to that version and try again? If that fixes your issue, then I'll add `umap<=0.4.1` requirement to the `setup.py` file for SAM as a stopgap until I figure out the issue. Meanwhile, I'll try upgrading to `0.4.4` and see if I can reproduce the error. As @giovp said, because this is a SAM issue, I'll follow up on this issue with a thread on my github repo and ping you there. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293#issuecomment-702387945
https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766:58,Availability,error,error,58,"When using **scanpy.pl.paga_path**, I experience the same error as @plrlhb12 (TypeError: **float() argument must be a string or a number, not 'csr_matrix'**) and I can also only generate a plot after deleting adata.raw. As a consequence, I can only plot genes that are filtered for high variability during preprocessing and still present in adata.var.gene_ids. ; I would be glad if there was a way to make it work without deleting adata.raw and therefore being able to plot also non-highly variable genes! Thank you!. **Versions:**; > anndata==0.7.4 matplotlib==3.3.0 numpy==1.19.1 pandas==1.1.0 scanpy==1.6.0 scipy==1.5.2 sklearn==0.23.1 igraph==0.8.2 leidenalg==0.8.1 umap==0.4.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766
https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766:287,Modifiability,variab,variability,287,"When using **scanpy.pl.paga_path**, I experience the same error as @plrlhb12 (TypeError: **float() argument must be a string or a number, not 'csr_matrix'**) and I can also only generate a plot after deleting adata.raw. As a consequence, I can only plot genes that are filtered for high variability during preprocessing and still present in adata.var.gene_ids. ; I would be glad if there was a way to make it work without deleting adata.raw and therefore being able to plot also non-highly variable genes! Thank you!. **Versions:**; > anndata==0.7.4 matplotlib==3.3.0 numpy==1.19.1 pandas==1.1.0 scanpy==1.6.0 scipy==1.5.2 sklearn==0.23.1 igraph==0.8.2 leidenalg==0.8.1 umap==0.4.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766
https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766:490,Modifiability,variab,variable,490,"When using **scanpy.pl.paga_path**, I experience the same error as @plrlhb12 (TypeError: **float() argument must be a string or a number, not 'csr_matrix'**) and I can also only generate a plot after deleting adata.raw. As a consequence, I can only plot genes that are filtered for high variability during preprocessing and still present in adata.var.gene_ids. ; I would be glad if there was a way to make it work without deleting adata.raw and therefore being able to plot also non-highly variable genes! Thank you!. **Versions:**; > anndata==0.7.4 matplotlib==3.3.0 numpy==1.19.1 pandas==1.1.0 scanpy==1.6.0 scipy==1.5.2 sklearn==0.23.1 igraph==0.8.2 leidenalg==0.8.1 umap==0.4.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766
https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557:9,Availability,error,error,9,"The same error happens to me, @Blohrer . **Versions:**. > scanpy==1.6.0 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557
https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557:147,Usability,learn,learn,147,"The same error happens to me, @Blohrer . **Versions:**. > scanpy==1.6.0 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-705222557
https://github.com/scverse/scanpy/issues/1295#issuecomment-797806911:22,Availability,error,error,22,"I've encountered this error as well. deleting the raw attributes did not help but what worked was changing the CSR matrix to an array. so `adata.X = adata.X.toarray()`. Hopefully, this will help others.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-797806911
https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843:41,Deployability,install,installing,41,"@pati-ni ; I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843
https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843:71,Deployability,install,install,71,"@pati-ni ; I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843
https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843:244,Deployability,Install,Installing,244,"@pati-ni ; I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843
https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843:268,Deployability,install,install,268,"@pati-ni ; I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843
https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843:121,Integrability,depend,dependencies,121,"@pati-ni ; I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843
https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843:192,Testability,test,tested,192,"@pati-ni ; I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843
https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011:110,Deployability,install,installing,110,"I can confirm this as a working workaround. Thank you @michalk8 . > @pati-ni; > I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; > Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011
https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011:140,Deployability,install,install,140,"I can confirm this as a working workaround. Thank you @michalk8 . > @pati-ni; > I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; > Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011
https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011:315,Deployability,Install,Installing,315,"I can confirm this as a working workaround. Thank you @michalk8 . > @pati-ni; > I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; > Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011
https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011:339,Deployability,install,install,339,"I can confirm this as a working workaround. Thank you @michalk8 . > @pati-ni; > I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; > Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011
https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011:190,Integrability,depend,dependencies,190,"I can confirm this as a working workaround. Thank you @michalk8 . > @pati-ni; > I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; > Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011
https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011:261,Testability,test,tested,261,"I can confirm this as a working workaround. Thank you @michalk8 . > @pati-ni; > I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; > Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:193,Availability,error,error,193,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:1090,Availability,Avail,Available,1090,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:54,Deployability,install,install,54,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:115,Deployability,install,installed,115,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:204,Deployability,install,install,204,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:318,Deployability,install,install,318,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:357,Deployability,install,installing,357,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:1283,Deployability,install,installed,1283,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:523,Modifiability,flexible,flexible,523,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:775,Modifiability,flexible,flexible,775,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:923,Safety,abort,abort,923,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859
https://github.com/scverse/scanpy/issues/1298#issuecomment-1009027580:147,Deployability,install,install,147,"What command are you running?. You might be running into the fact that we no longer put scanpy on bioconda, but instead use conda-forge. So `conda install -c conda-forge scanpy` should work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1009027580
https://github.com/scverse/scanpy/issues/1299#issuecomment-651928411:21,Deployability,install,install,21,"aha, sorry, I didn't install the 'fa2' package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1299#issuecomment-651928411
https://github.com/scverse/scanpy/issues/1300#issuecomment-655340510:22,Availability,down,downgrade,22,A temporary fix is to downgrade scipy to 1.4.1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1300#issuecomment-655340510
https://github.com/scverse/scanpy/issues/1300#issuecomment-718771033:136,Usability,learn,learn,136,"I could not reproduce this bug, I am using . `scanpy==1.5.1 anndata==0.7.4 umap==0.3.10 numpy==1.19.2 scipy==1.5.2 pandas==1.1.2 scikit-learn==0.23.2 statsmodels==0.12.0 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.7.0`. @giovp maybe be a good idea to close this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1300#issuecomment-718771033
https://github.com/scverse/scanpy/issues/1301#issuecomment-654741567:80,Modifiability,layers,layers,80,"Hi, you can also do this directly; `adata.obsm[""mylayer_pca""] = sc.tl.pca(adata.layers[""mylayer""])`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654741567
https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:187,Modifiability,variab,variable,187,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068
https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:167,Performance,load,loadings,167,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068
https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:196,Performance,load,loadings,196,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068
https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:584,Usability,clear,clearer,584,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068
https://github.com/scverse/scanpy/issues/1301#issuecomment-943977687:13,Deployability,update,update,13,Is there any update on the progress of this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-943977687
https://github.com/scverse/scanpy/issues/1301#issuecomment-2458452960:39,Modifiability,layers,layers,39,"Recently I need to run PCA on multiple layers of AnnData. If no one is working on this, I can work on a pull request for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-2458452960
https://github.com/scverse/scanpy/issues/1302#issuecomment-653022657:125,Deployability,install,installed,125,Are you sure you haven't set wxpython as the matplotlib backend somehow? Many of us use scanpy on macOS but I never manually installed wxpython...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302#issuecomment-653022657
https://github.com/scverse/scanpy/issues/1302#issuecomment-653096483:53,Deployability,install,install,53,"Thanks, what is the default backend on macOS? I only install matplotlib via the scanpy/anndata depenancies in a conda environment (and `MPLBACKEND` is unset). Upon installing wxpython, I can confirm the used backend. ```; >>> matplotlib.__version__; '3.2.2'; >>> matplotlib.rcParams['backend']; 'WXAgg'; >>> matplotlib.get_backend(); 'WXAgg'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302#issuecomment-653096483
https://github.com/scverse/scanpy/issues/1302#issuecomment-653096483:164,Deployability,install,installing,164,"Thanks, what is the default backend on macOS? I only install matplotlib via the scanpy/anndata depenancies in a conda environment (and `MPLBACKEND` is unset). Upon installing wxpython, I can confirm the used backend. ```; >>> matplotlib.__version__; '3.2.2'; >>> matplotlib.rcParams['backend']; 'WXAgg'; >>> matplotlib.get_backend(); 'WXAgg'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302#issuecomment-653096483
https://github.com/scverse/scanpy/pull/1304#issuecomment-658032666:168,Safety,detect,detecting,168,"Sure! @michalk8 , could you please add the below sentence:. ""CellRank is a toolkit to uncover cellular dynamics based on scRNA-seq data with RNA velocity annotation by detecting initial and terminal populations, inferring fate potentials and uncovering gene expression trends towards specific terminal populations. """,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1304#issuecomment-658032666
https://github.com/scverse/scanpy/pull/1305#issuecomment-661712907:246,Availability,error,error,246,"I like that CI for notebooks!. Question, what should be happening here? I'm a little confused about how the path isn't a directory, but you're going to write a file inside of it? If this is the problem, I think having `exists_ok=True` will still error. In general, I believe we'd previously decided to not create parent directories for writing automatically (mentioned in https://github.com/theislab/anndata/pull/364, and I think talked about in a zoom call?). This follows other tools better, which just throw an error if your parent directory doesn't exist. I personally like this approach better because it'll throw an error for typos, instead of doing the wrong thing. The only exception here is for the `datasets` module, which should create the directory it will store data in. I would recommend you explicitly create the directory in your code if needed. @falexwolf @flying-sheep, I would like to remove the directory creation code from `_check_datafile_present_and_download`. Do you think this will be a problem? It's not documented, so I'm leaning towards just removal instead of deprecation, but could be convinced to just warn for a cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1305#issuecomment-661712907
https://github.com/scverse/scanpy/pull/1305#issuecomment-661712907:514,Availability,error,error,514,"I like that CI for notebooks!. Question, what should be happening here? I'm a little confused about how the path isn't a directory, but you're going to write a file inside of it? If this is the problem, I think having `exists_ok=True` will still error. In general, I believe we'd previously decided to not create parent directories for writing automatically (mentioned in https://github.com/theislab/anndata/pull/364, and I think talked about in a zoom call?). This follows other tools better, which just throw an error if your parent directory doesn't exist. I personally like this approach better because it'll throw an error for typos, instead of doing the wrong thing. The only exception here is for the `datasets` module, which should create the directory it will store data in. I would recommend you explicitly create the directory in your code if needed. @falexwolf @flying-sheep, I would like to remove the directory creation code from `_check_datafile_present_and_download`. Do you think this will be a problem? It's not documented, so I'm leaning towards just removal instead of deprecation, but could be convinced to just warn for a cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1305#issuecomment-661712907
https://github.com/scverse/scanpy/pull/1305#issuecomment-661712907:622,Availability,error,error,622,"I like that CI for notebooks!. Question, what should be happening here? I'm a little confused about how the path isn't a directory, but you're going to write a file inside of it? If this is the problem, I think having `exists_ok=True` will still error. In general, I believe we'd previously decided to not create parent directories for writing automatically (mentioned in https://github.com/theislab/anndata/pull/364, and I think talked about in a zoom call?). This follows other tools better, which just throw an error if your parent directory doesn't exist. I personally like this approach better because it'll throw an error for typos, instead of doing the wrong thing. The only exception here is for the `datasets` module, which should create the directory it will store data in. I would recommend you explicitly create the directory in your code if needed. @falexwolf @flying-sheep, I would like to remove the directory creation code from `_check_datafile_present_and_download`. Do you think this will be a problem? It's not documented, so I'm leaning towards just removal instead of deprecation, but could be convinced to just warn for a cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1305#issuecomment-661712907
https://github.com/scverse/scanpy/pull/1306#issuecomment-661049767:354,Availability,error,errors,354,"Looks good then! Great that you followed https://github.com/theislab/scanpy/pull/503#issuecomment-471331400 and named it `harmony_integrate` instead of generally `harmony`. Not sure if `obsm_{in,out}_field` are good names. Maybe use something more speaking? I think we use `basis` or `rep` for something like `X_pca` elsewhere. Please also fix doc build errors and the `.travis.yml` conflict. (“Kor**s**unsky19” is a typo I guess, and I think it should be `kwargs` instead of `**kwargs`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-661049767
https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338:106,Availability,error,errors,106,"Great, thanks for the feedback. Hopefully this merge and commit fix everything. I wasn't able to see what errors were causing the readthedocs build fail as the ""Details"" link just took me to a page that said ""SORRY / This page does not exist yet."", so let me know if there are any other issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338
https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338:22,Usability,feedback,feedback,22,"Great, thanks for the feedback. Hopefully this merge and commit fix everything. I wasn't able to see what errors were causing the readthedocs build fail as the ""Details"" link just took me to a page that said ""SORRY / This page does not exist yet."", so let me know if there are any other issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-661224338
https://github.com/scverse/scanpy/pull/1306#issuecomment-662064808:575,Deployability,continuous,continuously,575,"Ah, yeah we need to fix the visibility of the builds. Hmm, so you can’t see the build [here](https://icb-scanpy--1306.com.readthedocs.build/en/1306/external/scanpy.external.pp.harmony_integrate.html)?. It still has issues:. ![grafik](https://user-images.githubusercontent.com/291575/88098047-6dc0ad00-cb99-11ea-8c30-f11ee3a820fb.png). you need to add blank lines before the `>>>` i guess. Also ![grafik](https://user-images.githubusercontent.com/291575/88098511-30a8ea80-cb9a-11ea-987b-c2fa929f36d6.png) again. You need to either give us acess to WarrenLab/scanpy@harmony or continuously merge master until you pressing the “merge upstream changes” button and us hitting the “squash & merge” button happens fast enough. forcing up-to-date branches this way is a bit annoying, but it’s the only way to be sure conflicts in changes don’t break everything. (well, the only way without switching to [bors](https://bors.tech/))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-662064808
https://github.com/scverse/scanpy/pull/1306#issuecomment-662072716:99,Availability,error,error,99,"Nope, that link brings me to a login page, and when I log in with my github account it gives me an error. I merged master again and added newlines; hopefully this fixes the issue. I'll give you access to my fork as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-662072716
https://github.com/scverse/scanpy/pull/1306#issuecomment-662072716:194,Security,access,access,194,"Nope, that link brings me to a login page, and when I log in with my github account it gives me an error. I merged master again and added newlines; hopefully this fixes the issue. I'll give you access to my fork as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-662072716
https://github.com/scverse/scanpy/pull/1306#issuecomment-662072716:31,Testability,log,login,31,"Nope, that link brings me to a login page, and when I log in with my github account it gives me an error. I merged master again and added newlines; hopefully this fixes the issue. I'll give you access to my fork as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-662072716
https://github.com/scverse/scanpy/pull/1306#issuecomment-662072716:54,Testability,log,log,54,"Nope, that link brings me to a login page, and when I log in with my github account it gives me an error. I merged master again and added newlines; hopefully this fixes the issue. I'll give you access to my fork as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306#issuecomment-662072716
https://github.com/scverse/scanpy/issues/1307#issuecomment-655012955:424,Testability,log,log,424,"Hi @ivirshup ,. Actually we can't see the PR doc builds, for https://readthedocs.com/projects/icb-scanpy/builds/361157/ for example this is what I see:. ![image](https://user-images.githubusercontent.com/1140359/86819388-93a46880-c055-11ea-8f62-458508a6f614.png). It's a bit annoying especially when it build fine locally but fails at readthedocs.com, it would be great if the person who sends the PR can also see the build log on the website.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307#issuecomment-655012955
https://github.com/scverse/scanpy/issues/1307#issuecomment-655013333:35,Security,access,access,35,or at least the people with commit access to scanpy should be able to see the PR doc builds.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307#issuecomment-655013333
https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522:224,Deployability,Update,Updates,224,"Yeah, I had thought anyone could see that. I'll look into checking if we can make that happen. If not, I'll at least try and give everyone with commit rights to scanpy access. Can you see the docs when the build succeeds?. *Updates* . * You can at least see some of the most recent build logs [here](https://readthedocs.org/projects/icb-scanpy/builds/11406014/).; * It looks like you can see the PR logs for other projects, like [pip](https://readthedocs.org/projects/pip/builds/). I think the difference here is that we have a paid readthedocs account (pip is on `.org`, we are on `.com`), which may [make some things private](https://docs.readthedocs.io/en/latest/choosing-a-site.html). Hopefully we can make this not private?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522
https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522:168,Security,access,access,168,"Yeah, I had thought anyone could see that. I'll look into checking if we can make that happen. If not, I'll at least try and give everyone with commit rights to scanpy access. Can you see the docs when the build succeeds?. *Updates* . * You can at least see some of the most recent build logs [here](https://readthedocs.org/projects/icb-scanpy/builds/11406014/).; * It looks like you can see the PR logs for other projects, like [pip](https://readthedocs.org/projects/pip/builds/). I think the difference here is that we have a paid readthedocs account (pip is on `.org`, we are on `.com`), which may [make some things private](https://docs.readthedocs.io/en/latest/choosing-a-site.html). Hopefully we can make this not private?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522
https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522:288,Testability,log,logs,288,"Yeah, I had thought anyone could see that. I'll look into checking if we can make that happen. If not, I'll at least try and give everyone with commit rights to scanpy access. Can you see the docs when the build succeeds?. *Updates* . * You can at least see some of the most recent build logs [here](https://readthedocs.org/projects/icb-scanpy/builds/11406014/).; * It looks like you can see the PR logs for other projects, like [pip](https://readthedocs.org/projects/pip/builds/). I think the difference here is that we have a paid readthedocs account (pip is on `.org`, we are on `.com`), which may [make some things private](https://docs.readthedocs.io/en/latest/choosing-a-site.html). Hopefully we can make this not private?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522
https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522:399,Testability,log,logs,399,"Yeah, I had thought anyone could see that. I'll look into checking if we can make that happen. If not, I'll at least try and give everyone with commit rights to scanpy access. Can you see the docs when the build succeeds?. *Updates* . * You can at least see some of the most recent build logs [here](https://readthedocs.org/projects/icb-scanpy/builds/11406014/).; * It looks like you can see the PR logs for other projects, like [pip](https://readthedocs.org/projects/pip/builds/). I think the difference here is that we have a paid readthedocs account (pip is on `.org`, we are on `.com`), which may [make some things private](https://docs.readthedocs.io/en/latest/choosing-a-site.html). Hopefully we can make this not private?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307#issuecomment-655302522
https://github.com/scverse/scanpy/pull/1308#issuecomment-654812288:105,Testability,test,tests,105,"Oh, no need to do this, I've already got this working a bit more generically (also supports `obsm`) with tests. Just wasn't sure about how to do the keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1308#issuecomment-654812288
https://github.com/scverse/scanpy/pull/1308#issuecomment-665610999:70,Modifiability,layers,layers,70,So what is now the recommended way of calculating and plotting PCA on layers? Is it still the same as suggested in https://github.com/theislab/scanpy/issues/1301? With this I am still unable to use sc.pl.pca_variance_ratio.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1308#issuecomment-665610999
https://github.com/scverse/scanpy/pull/1308#issuecomment-943177025:4,Deployability,update,update,4,Any update on this? I can't figure out a way to run PCA on a specified layer & get the additional returns beyond the PCA plot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1308#issuecomment-943177025
https://github.com/scverse/scanpy/pull/1309#issuecomment-655334219:398,Availability,error,errors,398,"I think the main issue with the docs building right now is that there are references to the `DotPlot` class, but there isn't any page built for those to link to. I'm not sure the `**kwargs` argument is causing an issue, since the docs for `sc.queries.enrich` have this, and seem to work fine: https://scanpy.readthedocs.io/en/stable/api/scanpy.queries.enrich.html. I have had issues with doc build errors failing to show up if you just try and build the docs again. It might be worth running `make clean` before each `make build` to see what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1309#issuecomment-655334219
https://github.com/scverse/scanpy/pull/1309#issuecomment-656556087:52,Availability,error,errors,52,@flying-sheep Thanks! I will take care of the other errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1309#issuecomment-656556087
https://github.com/scverse/scanpy/pull/1309#issuecomment-656592229:9,Testability,test,tests,9,Now that tests are passing I will replace the outdated doc images.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1309#issuecomment-656592229
https://github.com/scverse/scanpy/pull/1310#issuecomment-657413409:105,Testability,test,test,105,"Yeah, I was thinking `scipy.stats.mannwhitneyu` though I'm not sure if that does an approximate or exact test. It would be good if we could match some gold standard implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1310#issuecomment-657413409
https://github.com/scverse/scanpy/issues/1312#issuecomment-662452670:54,Integrability,Depend,Depending,54,"It is difficult to find a one-size-fits-all solution. Depending on the number of legends, the length of the labels and the DPI used the space required will vary. Thus, I think is is better for the user to adjust the plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1312#issuecomment-662452670
https://github.com/scverse/scanpy/issues/1313#issuecomment-656318884:49,Availability,down,downgrade,49,"Hi, this is fixed on master. You can temporarily downgrade scipy to avoid this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1313#issuecomment-656318884
https://github.com/scverse/scanpy/issues/1313#issuecomment-656318884:79,Availability,error,error,79,"Hi, this is fixed on master. You can temporarily downgrade scipy to avoid this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1313#issuecomment-656318884
https://github.com/scverse/scanpy/issues/1313#issuecomment-656318884:68,Safety,avoid,avoid,68,"Hi, this is fixed on master. You can temporarily downgrade scipy to avoid this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1313#issuecomment-656318884
https://github.com/scverse/scanpy/issues/1313#issuecomment-656337276:64,Availability,down,downgrade,64,"Hi, was it just fixed? If I clone right now, will I not have to downgrade scipy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1313#issuecomment-656337276
https://github.com/scverse/scanpy/issues/1313#issuecomment-656362456:28,Availability,down,downgrading,28,"Yes, it should work without downgrading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1313#issuecomment-656362456
https://github.com/scverse/scanpy/issues/1318#issuecomment-658987394:68,Performance,perform,performance,68,"@Koncopd ; hi, sc.pp.neighbors doesn't have hsnw which has superior performance from what I've seen in my data and literature https://arxiv.org/abs/1603.09320",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1318#issuecomment-658987394
https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:96,Deployability,upgrade,upgraded,96,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855
https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:134,Deployability,upgrade,upgraded,134,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855
https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:74,Integrability,depend,dependencies,74,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855
https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:461,Integrability,depend,dependencies,461,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855
https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:481,Integrability,depend,dependencies,481,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855
https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:732,Integrability,depend,dependency,732,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855
https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:182,Usability,learn,learn,182,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855
https://github.com/scverse/scanpy/pull/1320#issuecomment-659921938:101,Integrability,depend,depend,101,"Okay, this is weird. My current theory is that it's a bug in pip. It looks like if multiple packages depend on a single dependency, pip doesn't necessarily check if all requirements are satisfied. It just checks if one packages version requirements are. It seems non-deterministic which package is used to check. --------------------------------------. I was working on an example, but then I just found this: https://github.com/pypa/pip/issues/8218. There's an unstable feature for this (`--unstable-feature=resolver`), which does work, but I think adding a requirement for numpy is a bit more justifiable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659921938
https://github.com/scverse/scanpy/pull/1320#issuecomment-659921938:120,Integrability,depend,dependency,120,"Okay, this is weird. My current theory is that it's a bug in pip. It looks like if multiple packages depend on a single dependency, pip doesn't necessarily check if all requirements are satisfied. It just checks if one packages version requirements are. It seems non-deterministic which package is used to check. --------------------------------------. I was working on an example, but then I just found this: https://github.com/pypa/pip/issues/8218. There's an unstable feature for this (`--unstable-feature=resolver`), which does work, but I think adding a requirement for numpy is a bit more justifiable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659921938
https://github.com/scverse/scanpy/pull/1320#issuecomment-661004708:151,Deployability,install,installed,151,"I’m really happy that they’re finally adding a resolver. Without officially having a resolver, it technically wasn’t a bug that incompatible stuff got installed, but instead just a missing (if very vital) feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-661004708
https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536:299,Performance,tune,tune,299,"Should be possible to turn the y ticks legends on. But I just tested it and didn't work. I will try to fix it. The syntax is:; ```PYTHON; sc.pl.stacked_violin(adata,marker_genes,groupby='louvain', return_fig=True).style(yticklabels=True,row_palette='muted').show(); ```. `style` needs to be used to tune the graphical parameters to avoid overcrowding the parameters list. But I am open to have a discussion on what the users think is best. Documentation is here: https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536
https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536:332,Safety,avoid,avoid,332,"Should be possible to turn the y ticks legends on. But I just tested it and didn't work. I will try to fix it. The syntax is:; ```PYTHON; sc.pl.stacked_violin(adata,marker_genes,groupby='louvain', return_fig=True).style(yticklabels=True,row_palette='muted').show(); ```. `style` needs to be used to tune the graphical parameters to avoid overcrowding the parameters list. But I am open to have a discussion on what the users think is best. Documentation is here: https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536
https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536:62,Testability,test,tested,62,"Should be possible to turn the y ticks legends on. But I just tested it and didn't work. I will try to fix it. The syntax is:; ```PYTHON; sc.pl.stacked_violin(adata,marker_genes,groupby='louvain', return_fig=True).style(yticklabels=True,row_palette='muted').show(); ```. `style` needs to be used to tune the graphical parameters to avoid overcrowding the parameters list. But I am open to have a discussion on what the users think is best. Documentation is here: https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536
https://github.com/scverse/scanpy/issues/1325#issuecomment-777203347:357,Testability,log,logfoldchanges,357,"Hi @rpeys, sorry for being slow on this. What are you trying to do exactly? Are you looking to use scanpy plotting features afterwards, or just get tables of genes? If it's the latter, I would suggest using `sc.get.rank_genes_groups_df`, where you can do something like:. ```python; de_df = sc.get.rank_genes_groups_df(adata, group=""CD8""); de_df.query(""abs(logfoldchanges) > 1""); ```. You should then be able to use these genes to plot by passing the genes names to `var_names` parameter of the `rank_genes_groups` plotting functions. To be honest, a lot of us on the team are not really happy with the differential expression API – but also haven't had the time to completely redo it. Progress on this area has generally been slow. @fidelram has commented on `filter_rank_genes_groups` in particular here: https://github.com/theislab/scanpy/pull/1529#issuecomment-738733928.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1325#issuecomment-777203347
https://github.com/scverse/scanpy/issues/1325#issuecomment-1662404711:39,Deployability,release,release,39,Looks like this fix was made in a 2021 release - thanks! Will close the issue,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1325#issuecomment-1662404711
https://github.com/scverse/scanpy/issues/1326#issuecomment-661826106:64,Deployability,release,release,64,Is this using the development version? The fix hasn't been in a release yet.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-661826106
https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955:15,Deployability,release,release,15,"The last minor release of Scanpy was version 1.5.1 in the end of May, so it hasn't been released yet. You would need to update scanpy to the development version on master. This can be done via `pip install git+https://www.github.com/theislab/scanpy@master` or by having a local copy of the scanpy repo, updating this and installing from source via `pip install .` :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955
https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955:88,Deployability,release,released,88,"The last minor release of Scanpy was version 1.5.1 in the end of May, so it hasn't been released yet. You would need to update scanpy to the development version on master. This can be done via `pip install git+https://www.github.com/theislab/scanpy@master` or by having a local copy of the scanpy repo, updating this and installing from source via `pip install .` :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955
https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955:120,Deployability,update,update,120,"The last minor release of Scanpy was version 1.5.1 in the end of May, so it hasn't been released yet. You would need to update scanpy to the development version on master. This can be done via `pip install git+https://www.github.com/theislab/scanpy@master` or by having a local copy of the scanpy repo, updating this and installing from source via `pip install .` :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955
https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955:198,Deployability,install,install,198,"The last minor release of Scanpy was version 1.5.1 in the end of May, so it hasn't been released yet. You would need to update scanpy to the development version on master. This can be done via `pip install git+https://www.github.com/theislab/scanpy@master` or by having a local copy of the scanpy repo, updating this and installing from source via `pip install .` :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955
https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955:321,Deployability,install,installing,321,"The last minor release of Scanpy was version 1.5.1 in the end of May, so it hasn't been released yet. You would need to update scanpy to the development version on master. This can be done via `pip install git+https://www.github.com/theislab/scanpy@master` or by having a local copy of the scanpy repo, updating this and installing from source via `pip install .` :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955
https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955:353,Deployability,install,install,353,"The last minor release of Scanpy was version 1.5.1 in the end of May, so it hasn't been released yet. You would need to update scanpy to the development version on master. This can be done via `pip install git+https://www.github.com/theislab/scanpy@master` or by having a local copy of the scanpy repo, updating this and installing from source via `pip install .` :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1326#issuecomment-664923955
https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723:197,Deployability,integrat,integration,197,"Hi @brianhie,. It's great that you're contributing to Scanpy to make the interoperability even easier (I guess it was already quite good given you built on `AnnData`). We have been evaluating data integration methods and in which Scanorama performed quite well (you may have seen the [preprint](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2)). One aspect that would make it even easier to use the tool that we were missing in the comparison is a small tutorial. The example in the function docstring is already very helpful, but do you think it would be possible to add a quite jupyter notebook in this direction? This is obviously a request outside of this PR. On the topic of the PR, I wonder if `adata.obsm['X_pca_scanorama']` is a good default name for the generated embedding, and not just `adata.obsm['X_scanorama']` as the standard user may not have delved into the methodology as much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723
https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723:73,Integrability,interoperab,interoperability,73,"Hi @brianhie,. It's great that you're contributing to Scanpy to make the interoperability even easier (I guess it was already quite good given you built on `AnnData`). We have been evaluating data integration methods and in which Scanorama performed quite well (you may have seen the [preprint](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2)). One aspect that would make it even easier to use the tool that we were missing in the comparison is a small tutorial. The example in the function docstring is already very helpful, but do you think it would be possible to add a quite jupyter notebook in this direction? This is obviously a request outside of this PR. On the topic of the PR, I wonder if `adata.obsm['X_pca_scanorama']` is a good default name for the generated embedding, and not just `adata.obsm['X_scanorama']` as the standard user may not have delved into the methodology as much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723
https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723:197,Integrability,integrat,integration,197,"Hi @brianhie,. It's great that you're contributing to Scanpy to make the interoperability even easier (I guess it was already quite good given you built on `AnnData`). We have been evaluating data integration methods and in which Scanorama performed quite well (you may have seen the [preprint](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2)). One aspect that would make it even easier to use the tool that we were missing in the comparison is a small tutorial. The example in the function docstring is already very helpful, but do you think it would be possible to add a quite jupyter notebook in this direction? This is obviously a request outside of this PR. On the topic of the PR, I wonder if `adata.obsm['X_pca_scanorama']` is a good default name for the generated embedding, and not just `adata.obsm['X_scanorama']` as the standard user may not have delved into the methodology as much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723
https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723:240,Performance,perform,performed,240,"Hi @brianhie,. It's great that you're contributing to Scanpy to make the interoperability even easier (I guess it was already quite good given you built on `AnnData`). We have been evaluating data integration methods and in which Scanorama performed quite well (you may have seen the [preprint](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2)). One aspect that would make it even easier to use the tool that we were missing in the comparison is a small tutorial. The example in the function docstring is already very helpful, but do you think it would be possible to add a quite jupyter notebook in this direction? This is obviously a request outside of this PR. On the topic of the PR, I wonder if `adata.obsm['X_pca_scanorama']` is a good default name for the generated embedding, and not just `adata.obsm['X_scanorama']` as the standard user may not have delved into the methodology as much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723
https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:61,Deployability,release,release,61,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954
https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:883,Performance,perform,performance,883,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954
https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:203,Testability,benchmark,benchmarking,203,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954
https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:848,Testability,test,tests,848,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954
https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:336,Usability,simpl,simpler,336,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954
https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535:91,Modifiability,layers,layers,91,"Hi @gheimberg,. In your example you are not using a deepcopy to assign `adata.X` to `adata.layers['other']`. So when you log transform the data in the layer, it automatically log transforms the data in `adata.X` as well, as you just passed the reference. That being said, this is still a bug as even with a `adata.X.copy()` the warning is given.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535
https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535:121,Testability,log,log,121,"Hi @gheimberg,. In your example you are not using a deepcopy to assign `adata.X` to `adata.layers['other']`. So when you log transform the data in the layer, it automatically log transforms the data in `adata.X` as well, as you just passed the reference. That being said, this is still a bug as even with a `adata.X.copy()` the warning is given.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535
https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535:175,Testability,log,log,175,"Hi @gheimberg,. In your example you are not using a deepcopy to assign `adata.X` to `adata.layers['other']`. So when you log transform the data in the layer, it automatically log transforms the data in `adata.X` as well, as you just passed the reference. That being said, this is still a bug as even with a `adata.X.copy()` the warning is given.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535
https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:61,Integrability,Depend,Depending,61,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748
https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:464,Safety,avoid,avoid,464,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748
https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:304,Testability,log,logaritmize,304,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748
https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:321,Testability,Test,Test,321,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748
https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:695,Testability,log,logaritmize,695,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748
https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:712,Testability,Test,Test,712,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748
https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:782,Testability,test,testing,782,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748
https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:991,Testability,log,log-transformed,991,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748
https://github.com/scverse/scanpy/issues/1333#issuecomment-1210570611:295,Testability,log,log-transformed,295,"I must also mention that upon reading in the data:; - running `adata.uns['log1p']` returns `{}`;; - setting `adata.uns['log1p'][""base""] = None` after reading doesn't help.; - running `del adata.uns['log1p']` solves the problem. Visual inspection of expression values in `adata.X` seem to not be log-transformed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1210570611
https://github.com/scverse/scanpy/pull/1334#issuecomment-664837370:181,Availability,down,download,181,"@Koncopd, this seems to work for me. But also it looks like `install_opener` modifies global state and we shouldn't do that. In theory it shouldn't be too difficult to both start a download with a header, and write the result to a file, but it's not obvious `urllib` has anything to help do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-664837370
https://github.com/scverse/scanpy/pull/1334#issuecomment-664924095:30,Availability,down,downloading,30,"@ivirshup The idea is that if downloading with a browser or wget works but python requests fail, then the problem should be somewhere in the header. Changing user agent (to avoid blacklist \ whitelist on the server) is the most obvious thing to try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-664924095
https://github.com/scverse/scanpy/pull/1334#issuecomment-664924095:173,Safety,avoid,avoid,173,"@ivirshup The idea is that if downloading with a browser or wget works but python requests fail, then the problem should be somewhere in the header. Changing user agent (to avoid blacklist \ whitelist on the server) is the most obvious thing to try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-664924095
https://github.com/scverse/scanpy/pull/1334#issuecomment-705184936:23,Availability,Error,Error,23,How to avoid the 'HTTP Error 403: Forbidden' exactly? I reinstalled scanpy and still have this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-705184936
https://github.com/scverse/scanpy/pull/1334#issuecomment-705184936:7,Safety,avoid,avoid,7,How to avoid the 'HTTP Error 403: Forbidden' exactly? I reinstalled scanpy and still have this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-705184936
https://github.com/scverse/scanpy/pull/1334#issuecomment-733637978:52,Availability,Error,Error,52,I installed it now but still have the issue of HTTP Error 403.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-733637978
https://github.com/scverse/scanpy/pull/1334#issuecomment-733637978:2,Deployability,install,installed,2,I installed it now but still have the issue of HTTP Error 403.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-733637978
https://github.com/scverse/scanpy/pull/1334#issuecomment-733647903:190,Availability,down,downloading,190,"Hi @ivirshup ,; It just fixed when I installed the library directly from pip. Since I was following the documentation for library installation, the command mentioned in the documentation is downloading an outdated version. . Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-733647903
https://github.com/scverse/scanpy/pull/1334#issuecomment-733647903:37,Deployability,install,installed,37,"Hi @ivirshup ,; It just fixed when I installed the library directly from pip. Since I was following the documentation for library installation, the command mentioned in the documentation is downloading an outdated version. . Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-733647903
https://github.com/scverse/scanpy/pull/1334#issuecomment-733647903:130,Deployability,install,installation,130,"Hi @ivirshup ,; It just fixed when I installed the library directly from pip. Since I was following the documentation for library installation, the command mentioned in the documentation is downloading an outdated version. . Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1334#issuecomment-733647903
https://github.com/scverse/scanpy/issues/1335#issuecomment-665417204:21,Testability,test,test,21,"It'd be great have a test for this to catch such bugs next time, like checking the range of the scores.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1335#issuecomment-665417204
https://github.com/scverse/scanpy/issues/1335#issuecomment-665499836:677,Availability,ping,ping,677,"My suspicion is that this is more likely to do with the plotting functions, than the calculation. I think the issue is that previously the axis limits weren't being set (though they were calculated). Now they are set, but it turns out they were calculated on the full range of scores – not just the plotted ones. Here's what a potential fix looks like:. <details>; <summary> Big images </summary>. Without fix:. ![tmp](https://user-images.githubusercontent.com/8238804/88772041-7cbfe480-d1c3-11ea-80e9-9ea26c3d4cea.png). With fix:. ![tmp_new](https://user-images.githubusercontent.com/8238804/88772070-88aba680-d1c3-11ea-8872-16a26103b892.png). </details>. What do you think? (ping @fidelram)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1335#issuecomment-665499836
https://github.com/scverse/scanpy/issues/1335#issuecomment-666158255:129,Testability,test,tests,129,"Ah, that makes sense. Either way, I don't think the intent of the function was to have the axis bounds determined by how many DE tests were saved.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1335#issuecomment-666158255
https://github.com/scverse/scanpy/pull/1337#issuecomment-665564920:0,Deployability,Update,Updated,0,Updated plots. <details>; <summary> ranked_genes </summary>. Orig:. ![master_ranked_genes](https://user-images.githubusercontent.com/8238804/88785867-1b087600-d1d5-11ea-85dc-e8ea31303ae7.png). Current:. ![master_ranked_genes](https://user-images.githubusercontent.com/8238804/88785705-e0064280-d1d4-11ea-966e-f856ee2468f6.png). </details>. <details>; <summary> ranked_genes_sharey </summary>. Orig:. ![master_ranked_genes_sharey](https://user-images.githubusercontent.com/8238804/88785853-13e16800-d1d5-11ea-997a-d72955c5b3ce.png). Current:. ![master_ranked_genes_sharey](https://user-images.githubusercontent.com/8238804/88785819-06c47900-d1d5-11ea-8a74-22ef2b4803d0.png). </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1337#issuecomment-665564920
https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:252,Availability,down,downloaded,252,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817
https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:385,Availability,down,download,385,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817
https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:806,Availability,down,downstream,806,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817
https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:213,Energy Efficiency,reduce,reduced,213,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817
https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:582,Energy Efficiency,reduce,reduced,582,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817
https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:732,Energy Efficiency,reduce,reduce,732,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817
https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:620,Modifiability,variab,variable,620,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817
https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053:300,Availability,avail,available,300,"Hi @LuckyMD ,. Thanks so much for getting back to me this quickly. I just want to clarify that I am not running this analysis with the built-in 10x data set, I have followed the tutorial as seen on the link in the report, which says: ""The data consist in 3k PBMCs from a Healthy Donor and are freely available from 10x Genomics"". I have downloaded the file from the following URL, as seen in the tutorial:. http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz. This is also the same URL found on this link, directly from 10x:. https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k. The 10x summary [here](https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_web_summary.html) mentions LYZ as one of the most differentially expressed genes, yet it is missed by the sample analysis as performed in the Scanpy tutorial. As both use the exact same count matrix as a source, there are two possibilities here as far as I can see: either the thresholds and filtering parameters in the tutorial are inaccurate and miss important marker genes, or there is a bug that drops these genes. My question is which of the following is true. From your answer I would assume it's the former, in which case maybe a disclaimer pointing this out would be helpful in the tutorial page? I think, as it stands, the average user would assume important marker genes such as LYZ would not be missed by even a rough analysis of a PBMC data set. For reference, the [tutorial](https://satijalab.org/seurat/v3.2/pbmc3k_tutorial.html) which the Scanpy one is apparently based on finds LYZ as a very important contributor to the first principal component.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053
https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053:337,Availability,down,downloaded,337,"Hi @LuckyMD ,. Thanks so much for getting back to me this quickly. I just want to clarify that I am not running this analysis with the built-in 10x data set, I have followed the tutorial as seen on the link in the report, which says: ""The data consist in 3k PBMCs from a Healthy Donor and are freely available from 10x Genomics"". I have downloaded the file from the following URL, as seen in the tutorial:. http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz. This is also the same URL found on this link, directly from 10x:. https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k. The 10x summary [here](https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_web_summary.html) mentions LYZ as one of the most differentially expressed genes, yet it is missed by the sample analysis as performed in the Scanpy tutorial. As both use the exact same count matrix as a source, there are two possibilities here as far as I can see: either the thresholds and filtering parameters in the tutorial are inaccurate and miss important marker genes, or there is a bug that drops these genes. My question is which of the following is true. From your answer I would assume it's the former, in which case maybe a disclaimer pointing this out would be helpful in the tutorial page? I think, as it stands, the average user would assume important marker genes such as LYZ would not be missed by even a rough analysis of a PBMC data set. For reference, the [tutorial](https://satijalab.org/seurat/v3.2/pbmc3k_tutorial.html) which the Scanpy one is apparently based on finds LYZ as a very important contributor to the first principal component.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053
https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053:865,Performance,perform,performed,865,"Hi @LuckyMD ,. Thanks so much for getting back to me this quickly. I just want to clarify that I am not running this analysis with the built-in 10x data set, I have followed the tutorial as seen on the link in the report, which says: ""The data consist in 3k PBMCs from a Healthy Donor and are freely available from 10x Genomics"". I have downloaded the file from the following URL, as seen in the tutorial:. http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz. This is also the same URL found on this link, directly from 10x:. https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k. The 10x summary [here](https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_web_summary.html) mentions LYZ as one of the most differentially expressed genes, yet it is missed by the sample analysis as performed in the Scanpy tutorial. As both use the exact same count matrix as a source, there are two possibilities here as far as I can see: either the thresholds and filtering parameters in the tutorial are inaccurate and miss important marker genes, or there is a bug that drops these genes. My question is which of the following is true. From your answer I would assume it's the former, in which case maybe a disclaimer pointing this out would be helpful in the tutorial page? I think, as it stands, the average user would assume important marker genes such as LYZ would not be missed by even a rough analysis of a PBMC data set. For reference, the [tutorial](https://satijalab.org/seurat/v3.2/pbmc3k_tutorial.html) which the Scanpy one is apparently based on finds LYZ as a very important contributor to the first principal component.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053
https://github.com/scverse/scanpy/issues/1338#issuecomment-665595284:349,Testability,test,test,349,"The tutorial was built quite a while ago to mirror the old Seurat tutorial in that direction. If you remove the line `adata = adata[:, adata.var.highly_variable]` you should have all the genes still there. . @ivirshup Maybe this line should generally be removed from the tutorial, given that we now no longer need to filter genes anyway? Is there a test to ensure tutorial results are consistent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665595284
https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276:803,Modifiability,variab,variable,803,"Thank you for this. I took a dive into the data set to figure out what filtering step is causing this problem, and it seems to be the conditions set for `sc.pp.highly_variable_genes`. To carry on with `LYZ`, it does not show up because its `mean_counts` is too high, above the *maximum* of 3 set in the analysis:. ```; >>> adata.raw.to_adata().var.loc['LYZ']; gene_ids ENSG00000090382; n_cells 1631; mt False; n_cells_by_counts 1631; mean_counts 10.2467; pct_dropout_by_counts 39.5926; total_counts 27666; highly_variable False; means 3.68714; dispersions 5.12101; dispersions_norm 3.65908; Name: LYZ, dtype: object; ```. Is this filtering on `max_mean` as described in the tutorial a reasonable thing to do? That said, even if I were to not filter the matrix to restrict it to genes detected as highly variable, by default `scanpy.tl.pca` would not even use `LYZ` as a potential contributor to a principal component, because it is not highly variable. Again, the equivalent Seurat tutorial does have LYZ in it, but I assume that is because they are now using a different way to classify which genes are variable. Would you say my interpretation is correct? If so, would a better implementation of `sc.pp.highly_variable_genes` solve the problem? Would be happy to contribute if that is something that would be needed and there's no good Python-based alternative around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276
https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276:943,Modifiability,variab,variable,943,"Thank you for this. I took a dive into the data set to figure out what filtering step is causing this problem, and it seems to be the conditions set for `sc.pp.highly_variable_genes`. To carry on with `LYZ`, it does not show up because its `mean_counts` is too high, above the *maximum* of 3 set in the analysis:. ```; >>> adata.raw.to_adata().var.loc['LYZ']; gene_ids ENSG00000090382; n_cells 1631; mt False; n_cells_by_counts 1631; mean_counts 10.2467; pct_dropout_by_counts 39.5926; total_counts 27666; highly_variable False; means 3.68714; dispersions 5.12101; dispersions_norm 3.65908; Name: LYZ, dtype: object; ```. Is this filtering on `max_mean` as described in the tutorial a reasonable thing to do? That said, even if I were to not filter the matrix to restrict it to genes detected as highly variable, by default `scanpy.tl.pca` would not even use `LYZ` as a potential contributor to a principal component, because it is not highly variable. Again, the equivalent Seurat tutorial does have LYZ in it, but I assume that is because they are now using a different way to classify which genes are variable. Would you say my interpretation is correct? If so, would a better implementation of `sc.pp.highly_variable_genes` solve the problem? Would be happy to contribute if that is something that would be needed and there's no good Python-based alternative around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276
https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276:1104,Modifiability,variab,variable,1104,"Thank you for this. I took a dive into the data set to figure out what filtering step is causing this problem, and it seems to be the conditions set for `sc.pp.highly_variable_genes`. To carry on with `LYZ`, it does not show up because its `mean_counts` is too high, above the *maximum* of 3 set in the analysis:. ```; >>> adata.raw.to_adata().var.loc['LYZ']; gene_ids ENSG00000090382; n_cells 1631; mt False; n_cells_by_counts 1631; mean_counts 10.2467; pct_dropout_by_counts 39.5926; total_counts 27666; highly_variable False; means 3.68714; dispersions 5.12101; dispersions_norm 3.65908; Name: LYZ, dtype: object; ```. Is this filtering on `max_mean` as described in the tutorial a reasonable thing to do? That said, even if I were to not filter the matrix to restrict it to genes detected as highly variable, by default `scanpy.tl.pca` would not even use `LYZ` as a potential contributor to a principal component, because it is not highly variable. Again, the equivalent Seurat tutorial does have LYZ in it, but I assume that is because they are now using a different way to classify which genes are variable. Would you say my interpretation is correct? If so, would a better implementation of `sc.pp.highly_variable_genes` solve the problem? Would be happy to contribute if that is something that would be needed and there's no good Python-based alternative around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276
https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276:784,Safety,detect,detected,784,"Thank you for this. I took a dive into the data set to figure out what filtering step is causing this problem, and it seems to be the conditions set for `sc.pp.highly_variable_genes`. To carry on with `LYZ`, it does not show up because its `mean_counts` is too high, above the *maximum* of 3 set in the analysis:. ```; >>> adata.raw.to_adata().var.loc['LYZ']; gene_ids ENSG00000090382; n_cells 1631; mt False; n_cells_by_counts 1631; mean_counts 10.2467; pct_dropout_by_counts 39.5926; total_counts 27666; highly_variable False; means 3.68714; dispersions 5.12101; dispersions_norm 3.65908; Name: LYZ, dtype: object; ```. Is this filtering on `max_mean` as described in the tutorial a reasonable thing to do? That said, even if I were to not filter the matrix to restrict it to genes detected as highly variable, by default `scanpy.tl.pca` would not even use `LYZ` as a potential contributor to a principal component, because it is not highly variable. Again, the equivalent Seurat tutorial does have LYZ in it, but I assume that is because they are now using a different way to classify which genes are variable. Would you say my interpretation is correct? If so, would a better implementation of `sc.pp.highly_variable_genes` solve the problem? Would be happy to contribute if that is something that would be needed and there's no good Python-based alternative around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276
https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151:397,Deployability,update,updated,397,"Thanks for diving in deeper. I would agree with you that settig the `max_mean` is not a great idea. I have never done this in any of my analyses. As mentioned, this tutorial was a copy of an early Seurat tutorial and does not represent a recommendation on what is the best way to perform a single-cell analysis. Instead it is designed to showcase the tools that exist in Scanpy. Indeed Seurat has updated its tutorials since then, but we have not. This should probably be considered, but at the moment it would be at the end of a long to-do list. . Instead, our recommendation for how a single-cell analysis workflow should be structured would be the notebook in the best-practices tutorial [here](https://github.com/theislab/single-cell-tutorial). This should probably be linked on the scanpy front page, although it doesn't only include Scanpy analysis tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151
https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151:280,Performance,perform,perform,280,"Thanks for diving in deeper. I would agree with you that settig the `max_mean` is not a great idea. I have never done this in any of my analyses. As mentioned, this tutorial was a copy of an early Seurat tutorial and does not represent a recommendation on what is the best way to perform a single-cell analysis. Instead it is designed to showcase the tools that exist in Scanpy. Indeed Seurat has updated its tutorials since then, but we have not. This should probably be considered, but at the moment it would be at the end of a long to-do list. . Instead, our recommendation for how a single-cell analysis workflow should be structured would be the notebook in the best-practices tutorial [here](https://github.com/theislab/single-cell-tutorial). This should probably be linked on the scanpy front page, although it doesn't only include Scanpy analysis tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151
https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348:392,Deployability,update,updated,392,"And in terms of the `sc.pp.highly_variable_genes` function. We typically don't use the `max_mean` and `disperson` based parametrization anymore, but instead just select `n_top_genes`, which avoids this problem altogether. That being said, there is a PR with the VST-based highly-variable genes implementation from Seurat that will be added into scanpy soon. If you would like to reproduce an updated pbmc3k tutorial from Seurat using scanpy functions, that would be very welcome of course!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348
https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348:279,Modifiability,variab,variable,279,"And in terms of the `sc.pp.highly_variable_genes` function. We typically don't use the `max_mean` and `disperson` based parametrization anymore, but instead just select `n_top_genes`, which avoids this problem altogether. That being said, there is a PR with the VST-based highly-variable genes implementation from Seurat that will be added into scanpy soon. If you would like to reproduce an updated pbmc3k tutorial from Seurat using scanpy functions, that would be very welcome of course!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348
https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348:190,Safety,avoid,avoids,190,"And in terms of the `sc.pp.highly_variable_genes` function. We typically don't use the `max_mean` and `disperson` based parametrization anymore, but instead just select `n_top_genes`, which avoids this problem altogether. That being said, there is a PR with the VST-based highly-variable genes implementation from Seurat that will be added into scanpy soon. If you would like to reproduce an updated pbmc3k tutorial from Seurat using scanpy functions, that would be very welcome of course!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348
https://github.com/scverse/scanpy/issues/1340#issuecomment-666128751:289,Security,access,access,289,"It's definitely been discussed and is something I would like to do, but it has not been implemented. I think the main blocker here is that it would be a lot of effort to go through every plotting function and make it work with that structure. A good step towards this would be normalizing access to colors through a single function. In general, I think the colors could be stored as something like:. ```; .uns[""colors""][dim][key] = {cat1: color1, ...}; ``` . though I'm open to other solutions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666128751
https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760:287,Energy Efficiency,reduce,reduces,287,"I think we should allow for categorical colors along either axis, and right now it's becomes ambiguous. A good example of an annotation that can apply to both observations and variables is `species`. I'd like to shift to a nested model to limit the amount of reserved keys in `.uns`. It reduces that chance of unintentional naming collisions. As for the amount of things that would need to change, a lot has to change anyways. Hardly any code that works with the current setup will work with mappings (`len` is all I can think of). If we're already making a breaking change, might as well take advantage and future proof it a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760
https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760:176,Modifiability,variab,variables,176,"I think we should allow for categorical colors along either axis, and right now it's becomes ambiguous. A good example of an annotation that can apply to both observations and variables is `species`. I'd like to shift to a nested model to limit the amount of reserved keys in `.uns`. It reduces that chance of unintentional naming collisions. As for the amount of things that would need to change, a lot has to change anyways. Hardly any code that works with the current setup will work with mappings (`len` is all I can think of). If we're already making a breaking change, might as well take advantage and future proof it a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760
https://github.com/scverse/scanpy/issues/1340#issuecomment-666331545:0,Integrability,Depend,Depends,0,"Depends on how we're feeling about semantic versioning. 2.0 for a complete switch, but we could internally switch over with a compatibility layer and deprecation warnings anytime before then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666331545
https://github.com/scverse/scanpy/issues/1341#issuecomment-666246043:117,Deployability,install,install,117,"@taopeng1100: This is a numba bug. Please report this there, but only if you use the newest numba version (otherwise install it and try to reproduce this with the newest version). Give them this link so they see the code that triggers their bug:. https://github.com/theislab/scanpy/blob/2f160ea403d124d237fc2138c0aa0d175fbad22a/scanpy/preprocessing/_qc.py#L402-L428. @team: We should include numba in the package versions list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-666246043
https://github.com/scverse/scanpy/issues/1341#issuecomment-668005293:44,Availability,error,error,44,"Thank you @stuartarchibald, it sure is! The error happens when numba tries to JIT-compile `top_segment_proportions_sparse_csr`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-668005293
https://github.com/scverse/scanpy/issues/1341#issuecomment-670189681:6,Availability,down,downgrade,6,Maybe downgrade numba for the time being? IDK to which version though. @stuartarchibald has more insight here. Please follow numba/numba#5955 for updates!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670189681
https://github.com/scverse/scanpy/issues/1341#issuecomment-670189681:146,Deployability,update,updates,146,Maybe downgrade numba for the time being? IDK to which version though. @stuartarchibald has more insight here. Please follow numba/numba#5955 for updates!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670189681
https://github.com/scverse/scanpy/issues/1341#issuecomment-670191957:52,Availability,down,downgrade,52,"I am using Anaconda/Jupyter in my PC. When I try to downgrade numba, I run into issues of numba dependency packages in Anaconda so I am stuck!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670191957
https://github.com/scverse/scanpy/issues/1341#issuecomment-670191957:96,Integrability,depend,dependency,96,"I am using Anaconda/Jupyter in my PC. When I try to downgrade numba, I run into issues of numba dependency packages in Anaconda so I am stuck!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670191957
https://github.com/scverse/scanpy/issues/1341#issuecomment-670198281:91,Deployability,install,installing,91,"Sure, thank you! Care to do a quick PR? Then we can point @taopeng1100 in the direction of installing scanpy’s dev version and everyone’s happy. @taopeng1100 please reply by GitHub comment and not by email anymore, it spams up this comment section. I always have to remove some junk your email program adds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670198281
https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278:40,Deployability,install,install,40,"Okay, it’s merged! @taopeng1100, please install the dev version of scanpy like this, and retry:. ```bash; pip install git+https://github.com/theislab/scanpy.git; # or; pip install --user git+https://github.com/theislab/scanpy.git; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278
https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278:110,Deployability,install,install,110,"Okay, it’s merged! @taopeng1100, please install the dev version of scanpy like this, and retry:. ```bash; pip install git+https://github.com/theislab/scanpy.git; # or; pip install --user git+https://github.com/theislab/scanpy.git; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278
https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278:172,Deployability,install,install,172,"Okay, it’s merged! @taopeng1100, please install the dev version of scanpy like this, and retry:. ```bash; pip install git+https://github.com/theislab/scanpy.git; # or; pip install --user git+https://github.com/theislab/scanpy.git; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670525278
https://github.com/scverse/scanpy/pull/1342#issuecomment-666974308:187,Testability,log,log,187,"I tried to improve the ticklabel location which now looks like this. Also, I added a parameter to turn on the labels. ```PYTHON; sc.pl.stacked_violin(adata,marker_genes,groupby='louvain',log=False, yticklabels=True, row_palette='muted'); ```; ![image](https://user-images.githubusercontent.com/4964309/89010208-6a05f680-d30e-11ea-995b-bd5b1a673c51.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1342#issuecomment-666974308
https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831:1540,Deployability,update,updated,1540,"I was thinking we could go a bit further. We could add `sinfo` as a dependency and make `print_versions` just call: `sinfo.sinfo(dependencies=True)` which will always be comprehensive. <details>; <summary> Example output: </summary>. ```; -----; IPython 7.16.1; scanpy 1.5.2.dev38+g6728bdab; sinfo 0.3.1; -----; IPython 7.16.1; PIL 7.2.0; anndata 0.7.5.dev0+g58886f0.d20200729; asciitree NA; backcall 0.2.0; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2.21.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; louvain 0.7.0; matplotlib 3.3.0; monotonic NA; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; numba 0.50.1; numcodecs 0.6.4; numexpr 2.7.1; numpy 1.19.0; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.5.2.dev38+g6728bdab; scipy 1.5.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.2; tlz 0.10.0; toolz 0.10.0; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zarr 2.4.0; -----; Python 3.8.5 (default, Jul 23 2020, 15:50:11) [Clang 11.0.3 (clang-1103.0.32.62)]; macOS-10.15.6-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2020-07-30 19:28; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831
https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831:68,Integrability,depend,dependency,68,"I was thinking we could go a bit further. We could add `sinfo` as a dependency and make `print_versions` just call: `sinfo.sinfo(dependencies=True)` which will always be comprehensive. <details>; <summary> Example output: </summary>. ```; -----; IPython 7.16.1; scanpy 1.5.2.dev38+g6728bdab; sinfo 0.3.1; -----; IPython 7.16.1; PIL 7.2.0; anndata 0.7.5.dev0+g58886f0.d20200729; asciitree NA; backcall 0.2.0; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2.21.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; louvain 0.7.0; matplotlib 3.3.0; monotonic NA; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; numba 0.50.1; numcodecs 0.6.4; numexpr 2.7.1; numpy 1.19.0; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.5.2.dev38+g6728bdab; scipy 1.5.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.2; tlz 0.10.0; toolz 0.10.0; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zarr 2.4.0; -----; Python 3.8.5 (default, Jul 23 2020, 15:50:11) [Clang 11.0.3 (clang-1103.0.32.62)]; macOS-10.15.6-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2020-07-30 19:28; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831
https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831:129,Integrability,depend,dependencies,129,"I was thinking we could go a bit further. We could add `sinfo` as a dependency and make `print_versions` just call: `sinfo.sinfo(dependencies=True)` which will always be comprehensive. <details>; <summary> Example output: </summary>. ```; -----; IPython 7.16.1; scanpy 1.5.2.dev38+g6728bdab; sinfo 0.3.1; -----; IPython 7.16.1; PIL 7.2.0; anndata 0.7.5.dev0+g58886f0.d20200729; asciitree NA; backcall 0.2.0; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2.21.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; louvain 0.7.0; matplotlib 3.3.0; monotonic NA; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; numba 0.50.1; numcodecs 0.6.4; numexpr 2.7.1; numpy 1.19.0; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.5.2.dev38+g6728bdab; scipy 1.5.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.2; tlz 0.10.0; toolz 0.10.0; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zarr 2.4.0; -----; Python 3.8.5 (default, Jul 23 2020, 15:50:11) [Clang 11.0.3 (clang-1103.0.32.62)]; macOS-10.15.6-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2020-07-30 19:28; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831
https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831:1488,Testability,log,logical,1488,"I was thinking we could go a bit further. We could add `sinfo` as a dependency and make `print_versions` just call: `sinfo.sinfo(dependencies=True)` which will always be comprehensive. <details>; <summary> Example output: </summary>. ```; -----; IPython 7.16.1; scanpy 1.5.2.dev38+g6728bdab; sinfo 0.3.1; -----; IPython 7.16.1; PIL 7.2.0; anndata 0.7.5.dev0+g58886f0.d20200729; asciitree NA; backcall 0.2.0; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2.21.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; louvain 0.7.0; matplotlib 3.3.0; monotonic NA; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; numba 0.50.1; numcodecs 0.6.4; numexpr 2.7.1; numpy 1.19.0; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.5.2.dev38+g6728bdab; scipy 1.5.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.2; tlz 0.10.0; toolz 0.10.0; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zarr 2.4.0; -----; Python 3.8.5 (default, Jul 23 2020, 15:50:11) [Clang 11.0.3 (clang-1103.0.32.62)]; macOS-10.15.6-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2020-07-30 19:28; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831
https://github.com/scverse/scanpy/pull/1343#issuecomment-666268228:43,Integrability,wrap,wrap,43,"How would you change it? I'd probably just wrap the section that says ""paste here"" with a `<details>` tag.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666268228
https://github.com/scverse/scanpy/pull/1343#issuecomment-666343302:6,Usability,guid,guide,6,"Their guide is also applicable to us, even if it uses a lot of pandas examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666343302
https://github.com/scverse/scanpy/pull/1344#issuecomment-666259323:5,Availability,ping,ping,5,Also ping @flying-sheep since I believe you wrote the initial version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666259323
https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642:145,Deployability,install,installed,145,Shouldn’t we just depend on `requests` if it’s so complicated and we have to resort to code copying?. Basically every Python user should have it installed anyway.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642
https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642:18,Integrability,depend,depend,18,Shouldn’t we just depend on `requests` if it’s so complicated and we have to resort to code copying?. Basically every Python user should have it installed anyway.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642
https://github.com/scverse/scanpy/pull/1344#issuecomment-666336406:123,Deployability,update,update,123,"I looked into that, but I'm not sure it actually makes this any less complicated. The issue is getting the `tqdm` thing to update, and requests would need all of the same logic to do that as far as I can tell. Plus, at that point it's copying from stack overflow vs. copying from python's stdlib. You'd think this would be a convenience function somewhere. Or you'd think that `urlretrieve` could take a `Request` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666336406
https://github.com/scverse/scanpy/pull/1344#issuecomment-666336406:171,Testability,log,logic,171,"I looked into that, but I'm not sure it actually makes this any less complicated. The issue is getting the `tqdm` thing to update, and requests would need all of the same logic to do that as far as I can tell. Plus, at that point it's copying from stack overflow vs. copying from python's stdlib. You'd think this would be a convenience function somewhere. Or you'd think that `urlretrieve` could take a `Request` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666336406
https://github.com/scverse/scanpy/pull/1344#issuecomment-666510858:79,Integrability,depend,dependency,79,"It’s more code we have to maintain. If I had to decide between adding a common dependency, feature regression, or complex code, I’d go with the first one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666510858
https://github.com/scverse/scanpy/issues/1346#issuecomment-669056636:368,Testability,assert,assert,368,"Thanks for the report! I believe the results should be the same for multiple runs. Could you provide an example of this? I'm having a bit of trouble reproducing. Here's what I ran:. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc68k_reduced(); a = pbmc.copy(); b = pbmc.copy(). sc.pp.regress_out(a, ""phase""); sc.pp.regress_out(b, ""phase""). assert np.array_equal(a.X, b.X); assert not np.array_equal(a.X, pbmc.X); ```. Is this what you meant?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1346#issuecomment-669056636
https://github.com/scverse/scanpy/issues/1346#issuecomment-669056636:401,Testability,assert,assert,401,"Thanks for the report! I believe the results should be the same for multiple runs. Could you provide an example of this? I'm having a bit of trouble reproducing. Here's what I ran:. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc68k_reduced(); a = pbmc.copy(); b = pbmc.copy(). sc.pp.regress_out(a, ""phase""); sc.pp.regress_out(b, ""phase""). assert np.array_equal(a.X, b.X); assert not np.array_equal(a.X, pbmc.X); ```. Is this what you meant?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1346#issuecomment-669056636
https://github.com/scverse/scanpy/issues/1347#issuecomment-666988129:73,Testability,log,logo,73,"Yeah, Alex came up with the name and I came up with the idea to make the logo a shrimp. I think it was the logo’s designer [Daniela Barreto from südakzente](http://suedakzente.de/) who came up with the body-segments-are-cells idea, but it might also have been one of us, as we were floating ideas about combining something techy with something biological in the logo. The logo page is here btw: https://scverse.org/scanpy/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1347#issuecomment-666988129
https://github.com/scverse/scanpy/issues/1347#issuecomment-666988129:107,Testability,log,logo,107,"Yeah, Alex came up with the name and I came up with the idea to make the logo a shrimp. I think it was the logo’s designer [Daniela Barreto from südakzente](http://suedakzente.de/) who came up with the body-segments-are-cells idea, but it might also have been one of us, as we were floating ideas about combining something techy with something biological in the logo. The logo page is here btw: https://scverse.org/scanpy/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1347#issuecomment-666988129
https://github.com/scverse/scanpy/issues/1347#issuecomment-666988129:362,Testability,log,logo,362,"Yeah, Alex came up with the name and I came up with the idea to make the logo a shrimp. I think it was the logo’s designer [Daniela Barreto from südakzente](http://suedakzente.de/) who came up with the body-segments-are-cells idea, but it might also have been one of us, as we were floating ideas about combining something techy with something biological in the logo. The logo page is here btw: https://scverse.org/scanpy/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1347#issuecomment-666988129
https://github.com/scverse/scanpy/issues/1347#issuecomment-666988129:372,Testability,log,logo,372,"Yeah, Alex came up with the name and I came up with the idea to make the logo a shrimp. I think it was the logo’s designer [Daniela Barreto from südakzente](http://suedakzente.de/) who came up with the body-segments-are-cells idea, but it might also have been one of us, as we were floating ideas about combining something techy with something biological in the logo. The logo page is here btw: https://scverse.org/scanpy/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1347#issuecomment-666988129
https://github.com/scverse/scanpy/issues/1347#issuecomment-2203839037:69,Testability,log,logo,69,thanks for the question and the answer. I have been interpreting the logo as an ant for 7 years.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1347#issuecomment-2203839037
https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969:388,Deployability,release,release,388,"Thank you. Updating and releasing a new scanpydoc version is very simple:. First you make and check out your changes in scanpydoc’s own documentation, like in any sphinx project:. ```console; $ $EDITOR scanpydoc/theme/static/css/scanpy.css; [hack away]; $ cd docs; $ make html; $ $BROWSER _build/html/index.html; [check if it looks right]; ```. Then you can very quickly commit, tag, and release:. ```console; $ git add scanpydoc/theme/static/css/scanpy.css; $ git commit -m 'Made layout even wider (o________o)'; $ git tag v0.5.1 # Don’t forget the “v”!; $ flit publish; ```. That’s literally all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969
https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969:66,Usability,simpl,simple,66,"Thank you. Updating and releasing a new scanpydoc version is very simple:. First you make and check out your changes in scanpydoc’s own documentation, like in any sphinx project:. ```console; $ $EDITOR scanpydoc/theme/static/css/scanpy.css; [hack away]; $ cd docs; $ make html; $ $BROWSER _build/html/index.html; [check if it looks right]; ```. Then you can very quickly commit, tag, and release:. ```console; $ git add scanpydoc/theme/static/css/scanpy.css; $ git commit -m 'Made layout even wider (o________o)'; $ git tag v0.5.1 # Don’t forget the “v”!; $ flit publish; ```. That’s literally all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1349#issuecomment-667892969
https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196:336,Availability,error,error,336,"That problem occurs within h5py (we just wrap the underlying OSError) and isn’t a consequence of how scanpy uses h5py. The relevant part of the traceback is:. ```pytb; OSError: Can't read data (file read failed:; time = Sat Aug 1 13:27:54 2020,; filename = '/path.../filtered_gene_bc_matrices.h5ad',; file descriptor = 47,; errno = 5,; error message = 'Input/output error',; buf = 0x55ec782e9031,; total read size = 7011,; bytes this sub-read = 7011,; bytes actually read = 18446744073709551615,; offset = 0); ```. The reported filename looks weird: `'/path.../filtered_gene_bc_matrices.h5ad'`. Is that file on some network share or colab or so? Because that’d explain wonky I/O. 18 exabytes (18 quintillion bytes!) read seems really off too!. I assume `self.group[""data""][...]` tries to read all the data for `.X` and some bug or connection problem tells h5py that there’s 18 exabytes. h5py then asks the OS to give them those 18 exabytes which the OS politely denies. See also:. - https://github.com/googlecolab/colabtools/issues/559; - https://forum.hdfgroup.org/t/errors-accessing-hdf5-over-cifs-and-or-nfs/6341",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196
https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196:366,Availability,error,error,366,"That problem occurs within h5py (we just wrap the underlying OSError) and isn’t a consequence of how scanpy uses h5py. The relevant part of the traceback is:. ```pytb; OSError: Can't read data (file read failed:; time = Sat Aug 1 13:27:54 2020,; filename = '/path.../filtered_gene_bc_matrices.h5ad',; file descriptor = 47,; errno = 5,; error message = 'Input/output error',; buf = 0x55ec782e9031,; total read size = 7011,; bytes this sub-read = 7011,; bytes actually read = 18446744073709551615,; offset = 0); ```. The reported filename looks weird: `'/path.../filtered_gene_bc_matrices.h5ad'`. Is that file on some network share or colab or so? Because that’d explain wonky I/O. 18 exabytes (18 quintillion bytes!) read seems really off too!. I assume `self.group[""data""][...]` tries to read all the data for `.X` and some bug or connection problem tells h5py that there’s 18 exabytes. h5py then asks the OS to give them those 18 exabytes which the OS politely denies. See also:. - https://github.com/googlecolab/colabtools/issues/559; - https://forum.hdfgroup.org/t/errors-accessing-hdf5-over-cifs-and-or-nfs/6341",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196
https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196:1068,Availability,error,errors-accessing-,1068,"That problem occurs within h5py (we just wrap the underlying OSError) and isn’t a consequence of how scanpy uses h5py. The relevant part of the traceback is:. ```pytb; OSError: Can't read data (file read failed:; time = Sat Aug 1 13:27:54 2020,; filename = '/path.../filtered_gene_bc_matrices.h5ad',; file descriptor = 47,; errno = 5,; error message = 'Input/output error',; buf = 0x55ec782e9031,; total read size = 7011,; bytes this sub-read = 7011,; bytes actually read = 18446744073709551615,; offset = 0); ```. The reported filename looks weird: `'/path.../filtered_gene_bc_matrices.h5ad'`. Is that file on some network share or colab or so? Because that’d explain wonky I/O. 18 exabytes (18 quintillion bytes!) read seems really off too!. I assume `self.group[""data""][...]` tries to read all the data for `.X` and some bug or connection problem tells h5py that there’s 18 exabytes. h5py then asks the OS to give them those 18 exabytes which the OS politely denies. See also:. - https://github.com/googlecolab/colabtools/issues/559; - https://forum.hdfgroup.org/t/errors-accessing-hdf5-over-cifs-and-or-nfs/6341",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196
https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196:41,Integrability,wrap,wrap,41,"That problem occurs within h5py (we just wrap the underlying OSError) and isn’t a consequence of how scanpy uses h5py. The relevant part of the traceback is:. ```pytb; OSError: Can't read data (file read failed:; time = Sat Aug 1 13:27:54 2020,; filename = '/path.../filtered_gene_bc_matrices.h5ad',; file descriptor = 47,; errno = 5,; error message = 'Input/output error',; buf = 0x55ec782e9031,; total read size = 7011,; bytes this sub-read = 7011,; bytes actually read = 18446744073709551615,; offset = 0); ```. The reported filename looks weird: `'/path.../filtered_gene_bc_matrices.h5ad'`. Is that file on some network share or colab or so? Because that’d explain wonky I/O. 18 exabytes (18 quintillion bytes!) read seems really off too!. I assume `self.group[""data""][...]` tries to read all the data for `.X` and some bug or connection problem tells h5py that there’s 18 exabytes. h5py then asks the OS to give them those 18 exabytes which the OS politely denies. See also:. - https://github.com/googlecolab/colabtools/issues/559; - https://forum.hdfgroup.org/t/errors-accessing-hdf5-over-cifs-and-or-nfs/6341",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196
https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196:342,Integrability,message,message,342,"That problem occurs within h5py (we just wrap the underlying OSError) and isn’t a consequence of how scanpy uses h5py. The relevant part of the traceback is:. ```pytb; OSError: Can't read data (file read failed:; time = Sat Aug 1 13:27:54 2020,; filename = '/path.../filtered_gene_bc_matrices.h5ad',; file descriptor = 47,; errno = 5,; error message = 'Input/output error',; buf = 0x55ec782e9031,; total read size = 7011,; bytes this sub-read = 7011,; bytes actually read = 18446744073709551615,; offset = 0); ```. The reported filename looks weird: `'/path.../filtered_gene_bc_matrices.h5ad'`. Is that file on some network share or colab or so? Because that’d explain wonky I/O. 18 exabytes (18 quintillion bytes!) read seems really off too!. I assume `self.group[""data""][...]` tries to read all the data for `.X` and some bug or connection problem tells h5py that there’s 18 exabytes. h5py then asks the OS to give them those 18 exabytes which the OS politely denies. See also:. - https://github.com/googlecolab/colabtools/issues/559; - https://forum.hdfgroup.org/t/errors-accessing-hdf5-over-cifs-and-or-nfs/6341",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196
https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196:1075,Security,access,accessing-,1075,"That problem occurs within h5py (we just wrap the underlying OSError) and isn’t a consequence of how scanpy uses h5py. The relevant part of the traceback is:. ```pytb; OSError: Can't read data (file read failed:; time = Sat Aug 1 13:27:54 2020,; filename = '/path.../filtered_gene_bc_matrices.h5ad',; file descriptor = 47,; errno = 5,; error message = 'Input/output error',; buf = 0x55ec782e9031,; total read size = 7011,; bytes this sub-read = 7011,; bytes actually read = 18446744073709551615,; offset = 0); ```. The reported filename looks weird: `'/path.../filtered_gene_bc_matrices.h5ad'`. Is that file on some network share or colab or so? Because that’d explain wonky I/O. 18 exabytes (18 quintillion bytes!) read seems really off too!. I assume `self.group[""data""][...]` tries to read all the data for `.X` and some bug or connection problem tells h5py that there’s 18 exabytes. h5py then asks the OS to give them those 18 exabytes which the OS politely denies. See also:. - https://github.com/googlecolab/colabtools/issues/559; - https://forum.hdfgroup.org/t/errors-accessing-hdf5-over-cifs-and-or-nfs/6341",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196
https://github.com/scverse/scanpy/issues/1351#issuecomment-667898329:23,Availability,error,error,23,"This is quite a common error on our internal servers @Hrovatin. I have been getting around it by reading from a different server, and then it just often works. It would be great if you can figure our what the issue might be.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667898329
https://github.com/scverse/scanpy/issues/1351#issuecomment-667921094:121,Availability,error,errors,121,Which server do you suggest? - I had tried a couple with no success. I am having a lot of trouble with it - I am getting errors when reading different parts of the file - even when trying to use just h5py.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667921094
https://github.com/scverse/scanpy/issues/1351#issuecomment-667944564:58,Performance,queue,queue,58,"I have been moving between interactive servers not on the queue. `icb-lisa`, `icb-sarah`, and `icb-mona`, and if none of those work, then the older servers `hias`, `sepp`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667944564
https://github.com/scverse/scanpy/issues/1351#issuecomment-668185146:57,Availability,error,errors,57,"Great to hear! Usually when there’s weird, site-specific errors, I say I can’t help because I don’t have SSH access and “my crystal ball is currently out of order”. Seems like my crystal ball worked just fine these days!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-668185146
https://github.com/scverse/scanpy/issues/1351#issuecomment-668185146:109,Security,access,access,109,"Great to hear! Usually when there’s weird, site-specific errors, I say I can’t help because I don’t have SSH access and “my crystal ball is currently out of order”. Seems like my crystal ball worked just fine these days!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-668185146
https://github.com/scverse/scanpy/issues/1351#issuecomment-668225385:63,Availability,error,error,63,@flying-sheep just wait until tomorrow... when the next random error occurs ;).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-668225385
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:15,Availability,error,error,15,"Found the same error in our internal workflows. Saved the data to h5py files, but could not open them anymore for some reason. Error:. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_group(group); 531 if encoding_type:; --> 532 EncodingVersions[encoding_type].check(; 533 group.name, group.attrs[""encoding-version""]. /opt/conda/lib/python3.7/enum.py in __getitem__(cls, name); 356 def __getitem__(cls, name):; --> 357 return cls._member_map_[name]; 358 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:127,Availability,Error,Error,127,"Found the same error in our internal workflows. Saved the data to h5py files, but could not open them anymore for some reason. Error:. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_group(group); 531 if encoding_type:; --> 532 EncodingVersions[encoding_type].check(; 533 group.name, group.attrs[""encoding-version""]. /opt/conda/lib/python3.7/enum.py in __getitem__(cls, name); 356 def __getitem__(cls, name):; --> 357 return cls._member_map_[name]; 358 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1734,Availability,error,error,1734,"m__(cls, name); 356 def __getitem__(cls, name):; --> 357 return cls._member_map_[name]; 358 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cyth",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1857,Availability,error,error,1857,"nDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:5580,Deployability,update,updated,5580," natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; python-dateutil 2.8.1; python-igraph 0.9.1; pytorch-lightning 1.5.10; pytz 2021.1; PyWavelets 1.3.0; PyYAML 6.0; pyzmq 22.0.3; requests 2.25.1; requests-oauthlib 1.3.1; rich 12.4.4; rpy2 3.4.2; rsa 4.8; ruamel-yaml-conda 0.15.80; ruamel.yaml 0.17.21; ruamel.yaml.clib 0.2.6; s3transfer 0.4.2; sagemaker 2.39.0.post0; scanpy 1.6.1; scikit-image 0.19.2; scikit-learn 0.24.2; scikit-misc 0.1.4; scipy 1.6.0; scrublet 0.2.3; scvi-tools 0.16.2; seaborn 0.11.1; Send2Trash 1.8.0; setuptools 59.5.0; setuptools-scm 6.0.1; sinfo 0.3.1; six 1.15.0; smdebug-rulesconfig 1.0.1; soupsieve 2.3.2.post1; spectra 0.0.11; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; tensorboard 2.9.0; tensorboard-data-server 0.6.1; tensorboard-plugin-wit 1.8.1; terminado 0.15.0; texttable 1.6.3; threadpoolctl 2.1.0; tifffile 2021.11.2; tinycss2 1.1.1; toolz 0.11.2; torch 1.11.0; torchmetrics 0.9.0; tornado 6.1; tqdm 4.60.0; traitlets 5.2.2.post1; typing-extensions 4.2.0; tzlocal 2.1; umap-learn 0.4.6; urllib3 1.26.4; wcwidth 0.2.5; webencodings 0.5.1; Werkzeug 2.1.2; wheel 0.36.2; widgetsnbextension 3.6.0; yarl 1.7.2; zipp 3.4.1; Note: you may need to restart the kernel to use updated packages."". </details>. Has anyone found any solution to work around this issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1367,Integrability,wrap,wrapper,1367,"*kwargs); 157 except Exception as e:. /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_group(group); 531 if encoding_type:; --> 532 EncodingVersions[encoding_type].check(; 533 group.name, group.attrs[""encoding-version""]. /opt/conda/lib/python3.7/enum.py in __getitem__(cls, name); 356 def __getitem__(cls, name):; --> 357 return cls._member_map_[name]; 358 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:3424,Integrability,wrap,wrap,3424,rotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg 0.8.4; llvmlite 0.35.0; loompy 3.0.7; louvain 0.7.0; Markdown 3.3.7; MarkupSafe 2.1.1; matplotlib 3.4.1; matplotlib-inline 0.1.2; mistune 0.8.4; msgpack 1.0.4; multidict 6.0.2; multipledispatch 0.6.0; multiprocess 0.70.11.1; natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; pyth,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1890,Modifiability,layers,layers,1890,"nDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:5138,Modifiability,plugin,plugin-wit,5138," natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; python-dateutil 2.8.1; python-igraph 0.9.1; pytorch-lightning 1.5.10; pytz 2021.1; PyWavelets 1.3.0; PyYAML 6.0; pyzmq 22.0.3; requests 2.25.1; requests-oauthlib 1.3.1; rich 12.4.4; rpy2 3.4.2; rsa 4.8; ruamel-yaml-conda 0.15.80; ruamel.yaml 0.17.21; ruamel.yaml.clib 0.2.6; s3transfer 0.4.2; sagemaker 2.39.0.post0; scanpy 1.6.1; scikit-image 0.19.2; scikit-learn 0.24.2; scikit-misc 0.1.4; scipy 1.6.0; scrublet 0.2.3; scvi-tools 0.16.2; seaborn 0.11.1; Send2Trash 1.8.0; setuptools 59.5.0; setuptools-scm 6.0.1; sinfo 0.3.1; six 1.15.0; smdebug-rulesconfig 1.0.1; soupsieve 2.3.2.post1; spectra 0.0.11; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; tensorboard 2.9.0; tensorboard-data-server 0.6.1; tensorboard-plugin-wit 1.8.1; terminado 0.15.0; texttable 1.6.3; threadpoolctl 2.1.0; tifffile 2021.11.2; tinycss2 1.1.1; toolz 0.11.2; torch 1.11.0; torchmetrics 0.9.0; tornado 6.1; tqdm 4.60.0; traitlets 5.2.2.post1; typing-extensions 4.2.0; tzlocal 2.1; umap-learn 0.4.6; urllib3 1.26.4; wcwidth 0.2.5; webencodings 0.5.1; Werkzeug 2.1.2; wheel 0.36.2; widgetsnbextension 3.6.0; yarl 1.7.2; zipp 3.4.1; Note: you may need to restart the kernel to use updated packages."". </details>. Has anyone found any solution to work around this issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2435,Performance,cache,cached-property,2435,"spatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2458,Performance,cache,cachetools,2458,"s__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg 0.8.4; llvmlite 0.3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2286,Safety,timeout,timeout,2286,"raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonsc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:4776,Usability,learn,learn,4776," natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; python-dateutil 2.8.1; python-igraph 0.9.1; pytorch-lightning 1.5.10; pytz 2021.1; PyWavelets 1.3.0; PyYAML 6.0; pyzmq 22.0.3; requests 2.25.1; requests-oauthlib 1.3.1; rich 12.4.4; rpy2 3.4.2; rsa 4.8; ruamel-yaml-conda 0.15.80; ruamel.yaml 0.17.21; ruamel.yaml.clib 0.2.6; s3transfer 0.4.2; sagemaker 2.39.0.post0; scanpy 1.6.1; scikit-image 0.19.2; scikit-learn 0.24.2; scikit-misc 0.1.4; scipy 1.6.0; scrublet 0.2.3; scvi-tools 0.16.2; seaborn 0.11.1; Send2Trash 1.8.0; setuptools 59.5.0; setuptools-scm 6.0.1; sinfo 0.3.1; six 1.15.0; smdebug-rulesconfig 1.0.1; soupsieve 2.3.2.post1; spectra 0.0.11; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; tensorboard 2.9.0; tensorboard-data-server 0.6.1; tensorboard-plugin-wit 1.8.1; terminado 0.15.0; texttable 1.6.3; threadpoolctl 2.1.0; tifffile 2021.11.2; tinycss2 1.1.1; toolz 0.11.2; torch 1.11.0; torchmetrics 0.9.0; tornado 6.1; tqdm 4.60.0; traitlets 5.2.2.post1; typing-extensions 4.2.0; tzlocal 2.1; umap-learn 0.4.6; urllib3 1.26.4; wcwidth 0.2.5; webencodings 0.5.1; Werkzeug 2.1.2; wheel 0.36.2; widgetsnbextension 3.6.0; yarl 1.7.2; zipp 3.4.1; Note: you may need to restart the kernel to use updated packages."". </details>. Has anyone found any solution to work around this issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:5388,Usability,learn,learn,5388," natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; python-dateutil 2.8.1; python-igraph 0.9.1; pytorch-lightning 1.5.10; pytz 2021.1; PyWavelets 1.3.0; PyYAML 6.0; pyzmq 22.0.3; requests 2.25.1; requests-oauthlib 1.3.1; rich 12.4.4; rpy2 3.4.2; rsa 4.8; ruamel-yaml-conda 0.15.80; ruamel.yaml 0.17.21; ruamel.yaml.clib 0.2.6; s3transfer 0.4.2; sagemaker 2.39.0.post0; scanpy 1.6.1; scikit-image 0.19.2; scikit-learn 0.24.2; scikit-misc 0.1.4; scipy 1.6.0; scrublet 0.2.3; scvi-tools 0.16.2; seaborn 0.11.1; Send2Trash 1.8.0; setuptools 59.5.0; setuptools-scm 6.0.1; sinfo 0.3.1; six 1.15.0; smdebug-rulesconfig 1.0.1; soupsieve 2.3.2.post1; spectra 0.0.11; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; tensorboard 2.9.0; tensorboard-data-server 0.6.1; tensorboard-plugin-wit 1.8.1; terminado 0.15.0; texttable 1.6.3; threadpoolctl 2.1.0; tifffile 2021.11.2; tinycss2 1.1.1; toolz 0.11.2; torch 1.11.0; torchmetrics 0.9.0; tornado 6.1; tqdm 4.60.0; traitlets 5.2.2.post1; typing-extensions 4.2.0; tzlocal 2.1; umap-learn 0.4.6; urllib3 1.26.4; wcwidth 0.2.5; webencodings 0.5.1; Werkzeug 2.1.2; wheel 0.36.2; widgetsnbextension 3.6.0; yarl 1.7.2; zipp 3.4.1; Note: you may need to restart the kernel to use updated packages."". </details>. Has anyone found any solution to work around this issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336
https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945:109,Availability,error,error,109,I'm pretty sure none of you are having the same issue as the original one reported here. Compare @abuchin 's error message of `KeyError: 'dict'` to the original poster's error of `OSError: Can't read data`. The thing you're seeing is a new one stemming from an update to anndata. You're trying to read in a `h5ad` file created with a newer version of the package with your older one. I think the cutoff point is 0.8.0 but I could be mistaken. Upgrade your anndata and you should be ok.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945
https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945:170,Availability,error,error,170,I'm pretty sure none of you are having the same issue as the original one reported here. Compare @abuchin 's error message of `KeyError: 'dict'` to the original poster's error of `OSError: Can't read data`. The thing you're seeing is a new one stemming from an update to anndata. You're trying to read in a `h5ad` file created with a newer version of the package with your older one. I think the cutoff point is 0.8.0 but I could be mistaken. Upgrade your anndata and you should be ok.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945
https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945:261,Deployability,update,update,261,I'm pretty sure none of you are having the same issue as the original one reported here. Compare @abuchin 's error message of `KeyError: 'dict'` to the original poster's error of `OSError: Can't read data`. The thing you're seeing is a new one stemming from an update to anndata. You're trying to read in a `h5ad` file created with a newer version of the package with your older one. I think the cutoff point is 0.8.0 but I could be mistaken. Upgrade your anndata and you should be ok.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945
https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945:443,Deployability,Upgrade,Upgrade,443,I'm pretty sure none of you are having the same issue as the original one reported here. Compare @abuchin 's error message of `KeyError: 'dict'` to the original poster's error of `OSError: Can't read data`. The thing you're seeing is a new one stemming from an update to anndata. You're trying to read in a `h5ad` file created with a newer version of the package with your older one. I think the cutoff point is 0.8.0 but I could be mistaken. Upgrade your anndata and you should be ok.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945
https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945:115,Integrability,message,message,115,I'm pretty sure none of you are having the same issue as the original one reported here. Compare @abuchin 's error message of `KeyError: 'dict'` to the original poster's error of `OSError: Can't read data`. The thing you're seeing is a new one stemming from an update to anndata. You're trying to read in a `h5ad` file created with a newer version of the package with your older one. I think the cutoff point is 0.8.0 but I could be mistaken. Upgrade your anndata and you should be ok.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945
https://github.com/scverse/scanpy/issues/1351#issuecomment-1378202005:87,Deployability,update,update,87,> I was facing this issue in 0.7.8. Upgrading to 0.8.0 solved the problem. how did you update? pip says that 0.7.8 is the latest version,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378202005
https://github.com/scverse/scanpy/issues/1351#issuecomment-1378424295:12,Deployability,install,install,12,For me `pip install anndata --upgrade` did the trick. You could also do `pip install anndata==0.8.0` if that's the specific version you want to have.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378424295
https://github.com/scverse/scanpy/issues/1351#issuecomment-1378424295:30,Deployability,upgrade,upgrade,30,For me `pip install anndata --upgrade` did the trick. You could also do `pip install anndata==0.8.0` if that's the specific version you want to have.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378424295
https://github.com/scverse/scanpy/issues/1351#issuecomment-1378424295:77,Deployability,install,install,77,For me `pip install anndata --upgrade` did the trick. You could also do `pip install anndata==0.8.0` if that's the specific version you want to have.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378424295
https://github.com/scverse/scanpy/issues/1351#issuecomment-1378452650:54,Performance,load,loaded,54,Then it's probably a case of having an old python3. I loaded up an environment where I have python 3.6.9 and the newest version it saw was 0.7.8.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378452650
https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220:4,Deployability,install,install,4,pip install anndata --upgrade works.; The issue occurs when you saved anndata from a new version and when you try to load with old version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220
https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220:22,Deployability,upgrade,upgrade,22,pip install anndata --upgrade works.; The issue occurs when you saved anndata from a new version and when you try to load with old version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220
https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220:117,Performance,load,load,117,pip install anndata --upgrade works.; The issue occurs when you saved anndata from a new version and when you try to load with old version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220
https://github.com/scverse/scanpy/issues/1351#issuecomment-1627431509:41,Deployability,install,install,41,"I am having the same problem,however pip install anndata --upgrade didn't work for me. pip said it is already the latest version: Requirement already satisfied: anndata in d:\python3.10.9\lib\site-packages (0.9.1), then I really don't know what to do. Could you guys help me with that? [crying]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1627431509
https://github.com/scverse/scanpy/issues/1351#issuecomment-1627431509:59,Deployability,upgrade,upgrade,59,"I am having the same problem,however pip install anndata --upgrade didn't work for me. pip said it is already the latest version: Requirement already satisfied: anndata in d:\python3.10.9\lib\site-packages (0.9.1), then I really don't know what to do. Could you guys help me with that? [crying]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1627431509
https://github.com/scverse/scanpy/issues/1351#issuecomment-1628515570:474,Availability,error,error,474,"> @Sunyiqing2003 we certainly don’t want you crying!; > ; > people here had problem reading with older anndata versions, but you seem to have the newest one, so it’s not the same issue. could you file a new issue?. Thank you for your time and attention , I really appreciate it. I have filed a new issue : [https://github.com/scverse/scanpy/issues/2551](url). I might know the reason why updating anndata didn't work. the main reason for me seems to be big array and memory error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1628515570
https://github.com/scverse/scanpy/issues/1355#issuecomment-668396560:640,Safety,safe,safely,640,"Thanks for all the great bug reports! This is really useful!. The way that we assign colors to each obs is basically:. ```; values = adata.obs[categorical_column]; color_vector = np.asarray(adata.uns[color_key])[values.codes]; ```. When there is a missing value in a categorical array, the code for that value is `-1` (which is why the color of the last category is being assigned). [Pandas does not allow you to make `NA` a category](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html#missing-data), so `NA` will always have a code of `-1`. I think handling of this could take some consideration. I'm not sure we can safely set a default color for missing categorical values since that color (or one that looks very similar) could already be used. That said, it could be much more complex to allow passing a color for missing values. It might be worth looking into how other plotting libraries deal with missing categorical values and color. . Right now we have a related concept with the `groups` argument. Any samples not in the passed groups will just be colored light gray. This could probably also handle missing values if we want to go with that. I've started an implementation of this in #1356.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1355#issuecomment-668396560
https://github.com/scverse/scanpy/issues/1355#issuecomment-668430991:101,Safety,safe,safer,101,"I think Seaborn would typically use `.dropna()` to just omit values that are `NA`, no? That would be safer than assigning a colour.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1355#issuecomment-668430991
https://github.com/scverse/scanpy/issues/1355#issuecomment-668515906:19,Availability,down,down,19,"I guess this comes down to what you interpret an `NA` value as. If I want to show another category, I give it a name and it's shown. Doing this is very easy. If I don't assign a category to a group of observations, I would interpret this as this group of observations not being relevant in the context.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1355#issuecomment-668515906
https://github.com/scverse/scanpy/issues/1355#issuecomment-668545163:136,Deployability,continuous,continuous,136,Then I guess light grays would need to be removed from the default colour maps? And the question remains for how to deal with this in a continuous covariate.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1355#issuecomment-668545163
https://github.com/scverse/scanpy/issues/1355#issuecomment-674744108:192,Deployability,continuous,continuous,192,"Currently matplotlibs colors maps are dealing with things, it's just their default ""bad color"" is often transparent. We currently don't do any handling of color maps, which makes dealing with continuous values a little more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1355#issuecomment-674744108
https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:47,Availability,mask,masked,47,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421
https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:348,Deployability,Continuous,Continuous,348,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421
https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:288,Testability,test,tests,288,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421
https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:321,Testability,test,tests,321,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421
https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421:236,Usability,simpl,simplification,236,"I've managed to fix this up a bit. Missing (or masked - for `groups`) values in categorical arrays are now always plotted on bottom and use a default color. For spatial plots this default color is transparent. This has led to some code simplification. Surprisingly, this didn't break any tests locally, so a bunch of new tests are probably needed. Continuous values are still a little weird. Right now the points don't show up on embedding plots, and mess up all the colors for spatial plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-674738421
https://github.com/scverse/scanpy/pull/1356#issuecomment-675273322:2800,Deployability,Continuous,Continuous,2800,"e169-11ea-86f9-597bb1d5da01.png). ### Master. ![output_6_0](https://user-images.githubusercontent.com/8238804/90475475-da17d780-e16a-11ea-9ae9-6fb61fd6357b.png). </details>. <details>; <summary> Embedding </summary>. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.umap(brain, color=[""leiden"", ""leiden_missing""]); ```. ### Current. ![output_8_0](https://user-images.githubusercontent.com/8238804/90474984-ec454600-e169-11ea-935c-cb094f520a1a.png). ### Master. ![output_8_0](https://user-images.githubusercontent.com/8238804/90475481-de43f500-e16a-11ea-80da-2636b1b6dddf.png). ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.umap(brain, color=[""leiden"", ""leiden_missing""], groups=[""0"", ""1""]); ```. ### Current. ![output_9_0](https://user-images.githubusercontent.com/8238804/90475011-fb2bf880-e169-11ea-90a6-17e13388672e.png). ### Master. ![output_9_0](https://user-images.githubusercontent.com/8238804/90475498-e69c3000-e16a-11ea-876c-747ec02fae6d.png). </details>. ## Continuous values. <details>; <summary> Spatial </summary>. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""Bc1"", ""Bc1_missing""]); ```. ### Current. ![output_12_0](https://user-images.githubusercontent.com/8238804/90475149-4219ee00-e16a-11ea-9b42-aea86a8883a4.png). ### Master. ![output_12_0](https://user-images.githubusercontent.com/8238804/90475522-eef46b00-e16a-11ea-8d88-4ede97e16252.png). ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""Bc1"", ""Bc1_missing""], vmin=""p10"", vmax=""p90""); ```. ### Current. ![output_13_0](https://user-images.githubusercontent.com/8238804/90475174-5100a080-e16a-11ea-91ae-d3c883829b9a.png). ### Master. ![output_13_0](https://user-images.githubusercontent.com/8238804/90475536-f3b91f00-e16a-11ea-8ceb-fc954a9601e6.png). </details>. <details>; <summary> Embedding </summary>. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.umap(brain, color=[""Bc1"", ""Bc1_missing""]); ```. ### Current. ![outpu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675273322
https://github.com/scverse/scanpy/pull/1356#issuecomment-675307040:212,Deployability,continuous,continuous,212,"I like the new behaviour. Maybe a parameter like na_colors could be used to specify a different na color if needed. ; Also, should the NaN and its colour be added to the legend (if categorical) or besides it (if continuous)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675307040
https://github.com/scverse/scanpy/pull/1356#issuecomment-675876471:352,Deployability,continuous,continuous,352,"> Maybe the only relevant issue would be to change the default color if this clashes with a color already assigned to a category. I'm not sure how to check if colors are similar. But I think @Hrovatin's suggestion could make this obvious enough to users:. > Also, should the NaN and its colour be added to the legend (if categorical) or besides it (if continuous)?. Any suggestions for how to handle this @fidelram?. > Now that you are looking into this, any chance that the missing_color can also be used as the default color when the color parameter is empty? The current default is light gray . Yes, but it took a little bit more work than I expected (https://github.com/matplotlib/matplotlib/issues/18294). Basically we can't just pass an array of nulls for the color values here, since matplotlib throws user visible warnings about this at plot time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675876471
https://github.com/scverse/scanpy/pull/1356#issuecomment-675924327:115,Deployability,continuous,continuous,115,"When would you want `na_as_category` off? Is it important enough to add a new parameter for?. Also, how should the continuous color bar show that there is a null value? (Suggestions with code very welcome)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675924327
https://github.com/scverse/scanpy/pull/1356#issuecomment-675941540:13,Deployability,continuous,continuous,13,When data is continuous and legend is a continuous color bar a second legend could be added below it showing the categorical NaN (like the usual categorical legend).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675941540
https://github.com/scverse/scanpy/pull/1356#issuecomment-675941540:40,Deployability,continuous,continuous,40,When data is continuous and legend is a continuous color bar a second legend could be added below it showing the categorical NaN (like the usual categorical legend).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-675941540
https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507:4,Deployability,continuous,continuous,4,"For continuous values I don't think we need to add anything to the color bar. If a dot color is not part of the colorbar then is assumed that is a NaN. I searched in matplotlib for a similar case in which a colorbar includes NaN values but could not find any example. If this feature is wanted, what we can do is to use the option for colorbar extension and use it for NaNs but we need to find a way to set the label for NaN. ```PYTHON; import numpy as np; import matplotlib.pyplot as plt. adata = sc.datasets.pbmc68k_reduced(); adata.obs['n_genes'].iloc[::4] = np.nan; cmap = plt.get_cmap('viridis'); cmap.set_under('lightgray'); cmap.set_bad('lightgray'). fig, ax = plt.subplots(); cax = ax.scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1], ; c=adata.obs['n_genes'], s=20, ; cmap=cmap, ; vmin=1000, ; vmax=2000, plotnonfinite=True); fig.colorbar(cax, extend='min', extendrect=True, extendfrac=0.1). plt.show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90750699-7b22a180-e2d5-11ea-9a67-1ad7feb8a6a4.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507
https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507:865,Modifiability,extend,extend,865,"For continuous values I don't think we need to add anything to the color bar. If a dot color is not part of the colorbar then is assumed that is a NaN. I searched in matplotlib for a similar case in which a colorbar includes NaN values but could not find any example. If this feature is wanted, what we can do is to use the option for colorbar extension and use it for NaNs but we need to find a way to set the label for NaN. ```PYTHON; import numpy as np; import matplotlib.pyplot as plt. adata = sc.datasets.pbmc68k_reduced(); adata.obs['n_genes'].iloc[::4] = np.nan; cmap = plt.get_cmap('viridis'); cmap.set_under('lightgray'); cmap.set_bad('lightgray'). fig, ax = plt.subplots(); cax = ax.scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1], ; c=adata.obs['n_genes'], s=20, ; cmap=cmap, ; vmin=1000, ; vmax=2000, plotnonfinite=True); fig.colorbar(cax, extend='min', extendrect=True, extendfrac=0.1). plt.show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90750699-7b22a180-e2d5-11ea-9a67-1ad7feb8a6a4.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507
https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507:879,Modifiability,extend,extendrect,879,"For continuous values I don't think we need to add anything to the color bar. If a dot color is not part of the colorbar then is assumed that is a NaN. I searched in matplotlib for a similar case in which a colorbar includes NaN values but could not find any example. If this feature is wanted, what we can do is to use the option for colorbar extension and use it for NaNs but we need to find a way to set the label for NaN. ```PYTHON; import numpy as np; import matplotlib.pyplot as plt. adata = sc.datasets.pbmc68k_reduced(); adata.obs['n_genes'].iloc[::4] = np.nan; cmap = plt.get_cmap('viridis'); cmap.set_under('lightgray'); cmap.set_bad('lightgray'). fig, ax = plt.subplots(); cax = ax.scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1], ; c=adata.obs['n_genes'], s=20, ; cmap=cmap, ; vmin=1000, ; vmax=2000, plotnonfinite=True); fig.colorbar(cax, extend='min', extendrect=True, extendfrac=0.1). plt.show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90750699-7b22a180-e2d5-11ea-9a67-1ad7feb8a6a4.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507
https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507:896,Modifiability,extend,extendfrac,896,"For continuous values I don't think we need to add anything to the color bar. If a dot color is not part of the colorbar then is assumed that is a NaN. I searched in matplotlib for a similar case in which a colorbar includes NaN values but could not find any example. If this feature is wanted, what we can do is to use the option for colorbar extension and use it for NaNs but we need to find a way to set the label for NaN. ```PYTHON; import numpy as np; import matplotlib.pyplot as plt. adata = sc.datasets.pbmc68k_reduced(); adata.obs['n_genes'].iloc[::4] = np.nan; cmap = plt.get_cmap('viridis'); cmap.set_under('lightgray'); cmap.set_bad('lightgray'). fig, ax = plt.subplots(); cax = ax.scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1], ; c=adata.obs['n_genes'], s=20, ; cmap=cmap, ; vmin=1000, ; vmax=2000, plotnonfinite=True); fig.colorbar(cax, extend='min', extendrect=True, extendfrac=0.1). plt.show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90750699-7b22a180-e2d5-11ea-9a67-1ad7feb8a6a4.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507
https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:100,Deployability,continuous,continuous,100,"I think this is now ready for review. . ## Legends. I've decided to leave showing the null value in continuous legends for another PR, since I don't have an obvious solution now. I have added an argument for specifying whether the na value should show up in the legend, `na_in_legend`. It defaults to `True`. Here's an example:. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""]); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855448-fd3cc400-e3c2-11ea-9e01-6e8266ab6d10.png); ![image](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238
https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:1951,Modifiability,parameteriz,parameterized,1951,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238
https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:1931,Testability,Test,Tests,1931,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238
https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:1976,Testability,test,test,1976,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238
https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:2007,Testability,test,test,2007,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238
https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:2039,Testability,Test,Test,2039,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238
https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:2242,Testability,test,test,2242,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238
https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:2411,Testability,test,testing,2411,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238
https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:2571,Usability,simpl,simple,2571,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238
https://github.com/scverse/scanpy/pull/1356#issuecomment-678099918:784,Energy Efficiency,power,power,784,"@ivirshup Looks great! I like the new spatial test image ;) well done!. I just give it a try and didn't find any problem. One little change: can you add to the legend of `na_color` that this is also the color used when the parameter for `color` is not given. . I noticed two parameters in the embedding that I think belong only to the spatial.. Those are `bw` and `alpha_img`. In embeddings they do nothing. . Other issue, that I don't expect to address at the moment, is the increase in parameters because is becoming difficult to go through the list of parameters when browsing through the documentation. To help on this we can start using alphabetical order for all optional parameters. Other suggestion is to add to the documentation in which version a parameter was added. Thus, power users can easily track changes and try new options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678099918
https://github.com/scverse/scanpy/pull/1356#issuecomment-678099918:46,Testability,test,test,46,"@ivirshup Looks great! I like the new spatial test image ;) well done!. I just give it a try and didn't find any problem. One little change: can you add to the legend of `na_color` that this is also the color used when the parameter for `color` is not given. . I noticed two parameters in the embedding that I think belong only to the spatial.. Those are `bw` and `alpha_img`. In embeddings they do nothing. . Other issue, that I don't expect to address at the moment, is the increase in parameters because is becoming difficult to go through the list of parameters when browsing through the documentation. To help on this we can start using alphabetical order for all optional parameters. Other suggestion is to add to the documentation in which version a parameter was added. Thus, power users can easily track changes and try new options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678099918
https://github.com/scverse/scanpy/pull/1356#issuecomment-678115710:759,Deployability,release,release,759,"@fidelram Thanks!. > I noticed two parameters in the embedding that I think belong only to the spatial.. Those are bw and alpha_img. In embeddings they do nothing. Yeah, those probably shouldn't get documented for functions like `sc.pl.umap`. > Other issue, that I don't expect to address at the moment, is the increase in parameters is becoming difficult to go through. I agree (related: #956). There are so many that I'm not sure alphabetical always makes sense? Perhaps they could be grouped into sections of related parameters? This would require some work on how the docs are generated. It would definitely be good to note when features were added. Related to this, I want to discuss versioning at the next meeting to figure out when this should go in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678115710
https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138:74,Deployability,update,updated,74,"On the point of the notebooks... some of the tutorials should probably be updated. The analysis steps that are performed in those are quite old and would not be considered as good practice anymore. Might be worth combining this effort... (see e.g., #1338 )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138
https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138:111,Performance,perform,performed,111,"On the point of the notebooks... some of the tutorials should probably be updated. The analysis steps that are performed in those are quite old and would not be considered as good practice anymore. Might be worth combining this effort... (see e.g., #1338 )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138
https://github.com/scverse/scanpy/issues/1357#issuecomment-669097668:248,Integrability,interface,interface,248,"@LuckyMD, I definitely agree. . I personally find it kinda hard to work with version management and updating of notebooks, as well as keeping them clean with useful prose. This makes updating tutorials a pain. A lot of this just has to do with the interface, as I find this much easier with `.Rmd`. . I'm thinking this could be alleviated a bit with better automation around tutorials. Namely:. * Running + rendering notebooks through CI; * Save tutorials in a more git friendly format, maybe through something like [jupytext](https://jupytext.readthedocs.io). It looks to me like @michalk8 has set up some more extensive CI for tutorials with notebooks. @michalk8 do you have any recommendations here? How are you finding running CI against notebooks?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-669097668
https://github.com/scverse/scanpy/issues/1357#issuecomment-670685026:91,Deployability,update,update,91,"@ivirshup I find it useful, since we're still changing our API a lot, so I don't forget to update the tutorials. Our main issue is just runtime of some functions.; I'm using https://github.com/chrisjsewell/pytest-notebook, which can compare expected output of certain cells (or completely) ignore them, but it's still a very small library. I haven't gotten around pushing the updates, since I figured if the notebooks run, it's fine if there are some small discrepencies in output (like images/printed stuff) - though maybe I will update this soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-670685026
https://github.com/scverse/scanpy/issues/1357#issuecomment-670685026:376,Deployability,update,updates,376,"@ivirshup I find it useful, since we're still changing our API a lot, so I don't forget to update the tutorials. Our main issue is just runtime of some functions.; I'm using https://github.com/chrisjsewell/pytest-notebook, which can compare expected output of certain cells (or completely) ignore them, but it's still a very small library. I haven't gotten around pushing the updates, since I figured if the notebooks run, it's fine if there are some small discrepencies in output (like images/printed stuff) - though maybe I will update this soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-670685026
https://github.com/scverse/scanpy/issues/1357#issuecomment-670685026:531,Deployability,update,update,531,"@ivirshup I find it useful, since we're still changing our API a lot, so I don't forget to update the tutorials. Our main issue is just runtime of some functions.; I'm using https://github.com/chrisjsewell/pytest-notebook, which can compare expected output of certain cells (or completely) ignore them, but it's still a very small library. I haven't gotten around pushing the updates, since I figured if the notebooks run, it's fine if there are some small discrepencies in output (like images/printed stuff) - though maybe I will update this soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-670685026
https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154:161,Deployability,integrat,integration,161,"Just my 2cents: ; I made really good experiences with Github actions.; * I find them easy to set-up and they run many (20-40?) jobs in parallel. ; * Really good integration with Github (e.g. upload to PyPI on release) ; * windows testing works well, but it is a pain to setup pycairo, see [here](https://github.com/icbi-lab/scirpy/blob/725664a22e6265643633d89a7f38ea3383ccab48/.github/workflows/test.yml#L34) and [here](https://github.com/pygobject/pycairo/issues/19#issuecomment-638716293). . Here's the [github actions script for scirpy](https://github.com/icbi-lab/scirpy/blob/master/.github/workflows/test.yml).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154
https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154:209,Deployability,release,release,209,"Just my 2cents: ; I made really good experiences with Github actions.; * I find them easy to set-up and they run many (20-40?) jobs in parallel. ; * Really good integration with Github (e.g. upload to PyPI on release) ; * windows testing works well, but it is a pain to setup pycairo, see [here](https://github.com/icbi-lab/scirpy/blob/725664a22e6265643633d89a7f38ea3383ccab48/.github/workflows/test.yml#L34) and [here](https://github.com/pygobject/pycairo/issues/19#issuecomment-638716293). . Here's the [github actions script for scirpy](https://github.com/icbi-lab/scirpy/blob/master/.github/workflows/test.yml).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154
https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154:161,Integrability,integrat,integration,161,"Just my 2cents: ; I made really good experiences with Github actions.; * I find them easy to set-up and they run many (20-40?) jobs in parallel. ; * Really good integration with Github (e.g. upload to PyPI on release) ; * windows testing works well, but it is a pain to setup pycairo, see [here](https://github.com/icbi-lab/scirpy/blob/725664a22e6265643633d89a7f38ea3383ccab48/.github/workflows/test.yml#L34) and [here](https://github.com/pygobject/pycairo/issues/19#issuecomment-638716293). . Here's the [github actions script for scirpy](https://github.com/icbi-lab/scirpy/blob/master/.github/workflows/test.yml).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154
https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154:230,Testability,test,testing,230,"Just my 2cents: ; I made really good experiences with Github actions.; * I find them easy to set-up and they run many (20-40?) jobs in parallel. ; * Really good integration with Github (e.g. upload to PyPI on release) ; * windows testing works well, but it is a pain to setup pycairo, see [here](https://github.com/icbi-lab/scirpy/blob/725664a22e6265643633d89a7f38ea3383ccab48/.github/workflows/test.yml#L34) and [here](https://github.com/pygobject/pycairo/issues/19#issuecomment-638716293). . Here's the [github actions script for scirpy](https://github.com/icbi-lab/scirpy/blob/master/.github/workflows/test.yml).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154
https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154:395,Testability,test,test,395,"Just my 2cents: ; I made really good experiences with Github actions.; * I find them easy to set-up and they run many (20-40?) jobs in parallel. ; * Really good integration with Github (e.g. upload to PyPI on release) ; * windows testing works well, but it is a pain to setup pycairo, see [here](https://github.com/icbi-lab/scirpy/blob/725664a22e6265643633d89a7f38ea3383ccab48/.github/workflows/test.yml#L34) and [here](https://github.com/pygobject/pycairo/issues/19#issuecomment-638716293). . Here's the [github actions script for scirpy](https://github.com/icbi-lab/scirpy/blob/master/.github/workflows/test.yml).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154
https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154:605,Testability,test,test,605,"Just my 2cents: ; I made really good experiences with Github actions.; * I find them easy to set-up and they run many (20-40?) jobs in parallel. ; * Really good integration with Github (e.g. upload to PyPI on release) ; * windows testing works well, but it is a pain to setup pycairo, see [here](https://github.com/icbi-lab/scirpy/blob/725664a22e6265643633d89a7f38ea3383ccab48/.github/workflows/test.yml#L34) and [here](https://github.com/pygobject/pycairo/issues/19#issuecomment-638716293). . Here's the [github actions script for scirpy](https://github.com/icbi-lab/scirpy/blob/master/.github/workflows/test.yml).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154
https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732:315,Performance,perform,performance,315,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732
https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732:340,Testability,log,logic,340,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732
https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732:396,Usability,simpl,simple,396,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732
https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616:1262,Testability,log,logfoldchanges,1262,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```; # compare expression levels of mel vs all other cell types in pairwise manner; sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-32-73add1f79f3a> in <module>; 1 # save as a data frame; 2 ; ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'); 4 ; 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols); 53 d = pd.DataFrame(); 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:; ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]; 56 if pval_cutoff is not None:; 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx); 517 ; 518 def __getitem__(self, indx):; --> 519 obj = super(recarray, self).__getitem__(indx); 520 ; 521 # copy behavior of getattr, except that here. ValueError: no field of name mel; ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616
https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616:67,Usability,clear,clear,67,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```; # compare expression levels of mel vs all other cell types in pairwise manner; sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-32-73add1f79f3a> in <module>; 1 # save as a data frame; 2 ; ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'); 4 ; 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols); 53 d = pd.DataFrame(); 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:; ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]; 56 if pval_cutoff is not None:; 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx); 517 ; 518 def __getitem__(self, indx):; --> 519 obj = super(recarray, self).__getitem__(indx); 520 ; 521 # copy behavior of getattr, except that here. ValueError: no field of name mel; ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-717575616
https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138:122,Testability,test,tests,122,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138
https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138:170,Usability,guid,guidelines,170,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360#issuecomment-719807138
https://github.com/scverse/scanpy/issues/1361#issuecomment-674912356:649,Integrability,depend,depending,649,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361#issuecomment-674912356
https://github.com/scverse/scanpy/pull/1362#issuecomment-671720357:181,Testability,test,tests,181,"I think this would be more appropriate in `anndata`. Since it's not an on disk format, maybe it the function could be called something like `from_starfish`?. * This would also need tests, so some kind of example data.; * Why the differences between this and [starfish's `save_anndata` method](https://spacetx-starfish.readthedocs.io/en/latest/_modules/starfish/core/expression_matrix/expression_matrix.html#ExpressionMatrix.save_anndata)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362#issuecomment-671720357
https://github.com/scverse/scanpy/pull/1362#issuecomment-671884211:273,Availability,mask,masks,273,"So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Also, my understanding with `save_anndata` method is that it doesn't export all the features we might want (e.g. area of segmentation masks or others, to be stored in obs). However, this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix.; What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362#issuecomment-671884211
https://github.com/scverse/scanpy/pull/1362#issuecomment-671896754:220,Modifiability,variab,variables,220,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362#issuecomment-671896754
https://github.com/scverse/scanpy/issues/1363#issuecomment-674658333:257,Testability,Test,Test,257,"> If you don’t specify a seed, nondeterminism is to be expected. In general, I thought we went for determinism with default arguments in scanpy. I believe pynndescent faster, if non-deterministic, but I get constant output from scanpy. <details>; <summary> Test case </summary>. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc3k(). sc.pp.filter_genes(pbmc, min_counts=1); sc.pp.normalize_total(pbmc); sc.pp.log1p(pbmc). adatas = [pbmc.copy() for _ in range(10)]; for adata in adatas:; sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.umap(adata). umap_coords = [adata.obsm[""X_umap""] for adata in adatas]; assert all([np.array_equal(umap_coords[i], umap_coords[i+1]) for i in range(9)]); ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-674658333
https://github.com/scverse/scanpy/issues/1363#issuecomment-674658333:632,Testability,assert,assert,632,"> If you don’t specify a seed, nondeterminism is to be expected. In general, I thought we went for determinism with default arguments in scanpy. I believe pynndescent faster, if non-deterministic, but I get constant output from scanpy. <details>; <summary> Test case </summary>. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc3k(). sc.pp.filter_genes(pbmc, min_counts=1); sc.pp.normalize_total(pbmc); sc.pp.log1p(pbmc). adatas = [pbmc.copy() for _ in range(10)]; for adata in adatas:; sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.umap(adata). umap_coords = [adata.obsm[""X_umap""] for adata in adatas]; assert all([np.array_equal(umap_coords[i], umap_coords[i+1]) for i in range(9)]); ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-674658333
https://github.com/scverse/scanpy/issues/1363#issuecomment-678040801:66,Performance,load,loading,66,At the moment I am saving the adata and I am generating the plots loading it. In this way the coordinates are always the same.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-678040801
https://github.com/scverse/scanpy/issues/1363#issuecomment-678129332:61,Deployability,update,updates,61,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-678129332
https://github.com/scverse/scanpy/issues/1363#issuecomment-678129332:125,Testability,test,tests,125,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-678129332
https://github.com/scverse/scanpy/issues/1363#issuecomment-678129332:146,Testability,test,tests,146,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-678129332
https://github.com/scverse/scanpy/issues/1363#issuecomment-678257370:44,Deployability,update,updates,44,I don't think i saw changes without version updates. The only thing I noticed there are the diffmap coordinates occasionally being mirrored.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-678257370
https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269:361,Modifiability,variab,variable,361,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:; ```py; # Load the PBMC 3k data; adata = sc.read_10x_mtx(; os.path.join(; save_path, ""filtered_gene_bc_matrices/hg19/""; ), # the directory with the `.mtx` file; var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index); ); adata.var_names_make_unique(). # Get counts; adata.obs[""n_counts""] = adata.X.sum(axis=1).A1; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1; sc.pp.log1p(adata); adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction; sc.tl.pca(adata, svd_solver=""arpack""); sc.pp.neighbors(adata); sc.tl.umap(adata); sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269
https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269:377,Modifiability,variab,variables-axis,377,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:; ```py; # Load the PBMC 3k data; adata = sc.read_10x_mtx(; os.path.join(; save_path, ""filtered_gene_bc_matrices/hg19/""; ), # the directory with the `.mtx` file; var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index); ); adata.var_names_make_unique(). # Get counts; adata.obs[""n_counts""] = adata.X.sum(axis=1).A1; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1; sc.pp.log1p(adata); adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction; sc.tl.pca(adata, svd_solver=""arpack""); sc.pp.neighbors(adata); sc.tl.umap(adata); sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269
https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269:157,Performance,Load,Load,157,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:; ```py; # Load the PBMC 3k data; adata = sc.read_10x_mtx(; os.path.join(; save_path, ""filtered_gene_bc_matrices/hg19/""; ), # the directory with the `.mtx` file; var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index); ); adata.var_names_make_unique(). # Get counts; adata.obs[""n_counts""] = adata.X.sum(axis=1).A1; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1; sc.pp.log1p(adata); adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction; sc.tl.pca(adata, svd_solver=""arpack""); sc.pp.neighbors(adata); sc.tl.umap(adata); sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:655,Availability,down,downstream,655,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:; 1. Do we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1296,Availability,down,downstream,1296,"g a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:; 1. Do we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1438,Deployability,integrat,integration,1438," methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:; 1. Do we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1438,Integrability,integrat,integration,1438," methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:; 1. Do we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1626,Performance,perform,performing,1626," we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:127,Testability,log,log,127,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:; 1. Do we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1385,Testability,test,tests,1385," methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:; 1. Do we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1460,Testability,Log,Log,1460,"47-7)), there are a couple of things to consider here:; 1. Do we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1649,Testability,log,log,1649," we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:2135,Testability,log,log-transformation,2135," we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643
https://github.com/scverse/scanpy/issues/1364#issuecomment-678816818:174,Testability,log,log,174,"Thank you very much @LuckyMD for those insightful comments, that gives me plenty to think about beyond my original question about the mean normalisation being 'distorted' by log transformation. I had not considered how tricky a problem normalisation is. This actually makes me feel that perhaps the long term solution will be mostly experimental rather than computational, through developing better spike-ins. Anyway, I guess this discussion is not a Scanpy issue, so I will close this, but I appreciate your thoughts.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678816818
https://github.com/scverse/scanpy/issues/1364#issuecomment-679132966:157,Modifiability,variab,variable,157,"Good point that spike-ins may not always behave like endogenous transcripts. However, since spike-ins can account for the later biases in the workflow (e.g. variable sequencing depth, capture efficiencies, amplification etc.), I do think that they are still an important part of the solution and a step closer to absolute quantification- would you disagree?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-679132966
https://github.com/scverse/scanpy/issues/1364#issuecomment-679164180:700,Safety,avoid,avoided,700,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-679164180
https://github.com/scverse/scanpy/issues/1366#issuecomment-673944797:16,Usability,clear,clear,16,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-673944797
https://github.com/scverse/scanpy/issues/1366#issuecomment-674659520:139,Usability,clear,clear,139,On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-674659520
https://github.com/scverse/scanpy/issues/1366#issuecomment-689002984:326,Deployability,patch,patches,326,One idea: ; 1) Cluster the graph with leiden; 2) Coarsen the graph (collapse cells in a single cluster into super nodes); 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color; 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-689002984
https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599:507,Energy Efficiency,reduce,reduced,507,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?. It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599
https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599:141,Usability,clear,clear,141,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?. It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277599
https://github.com/scverse/scanpy/issues/1366#issuecomment-698277804:346,Deployability,patch,patches,346,> One idea:; > ; > 1. Cluster the graph with leiden; > 2. Coarsen the graph (collapse cells in a single cluster into super nodes); > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color; > 4. Assign all cells in each cluster that cluster's color.; > ; > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-698277804
https://github.com/scverse/scanpy/issues/1366#issuecomment-761937197:11,Deployability,update,updated,11,"Just as an updated thought on this, I don't think using the connectivity graph is the most straightforward way to approach this. We don't really care about closeness of points on the manifold, we care about closeness of the points on the plot. I think you'd want to constrain color assignment by points on the plot. This has the side benefit of being more widely applicable, since it doesn't require the plot to be connected to some graph representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-761937197
https://github.com/scverse/scanpy/issues/1366#issuecomment-763156349:62,Usability,simpl,simplified,62,wow. I was thinking...maybe the PAGA graph can be used as the simplified graph for color assignment. But will try your solution here!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-763156349
https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345:5506,Integrability,depend,dependencies,5506,"es())))); tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated.; * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?); * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering?; * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity?. I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345
https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345:4482,Performance,optimiz,optimization,4482,"age](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>; <summary> code </summary>. ```python; from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))); tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated.; * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?); * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering?; * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity?. I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345
https://github.com/scverse/scanpy/issues/1366#issuecomment-770675360:188,Performance,queue,queue,188,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>; <summary> Example </summary>. ```; A; /; C - B; ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`; * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-770675360
https://github.com/scverse/scanpy/issues/1366#issuecomment-770675360:563,Performance,queue,queue,563,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>; <summary> Example </summary>. ```; A; /; C - B; ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`; * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-770675360
https://github.com/scverse/scanpy/issues/1367#issuecomment-673941590:163,Availability,error,error,163,Hi. I can’t copy/paste/run the above code sample. There’s nowhere you define `adata`. Please add some code that uses some builtin dataset or so and reproduces the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-673941590
https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537:218,Availability,error,error,218,"Hi, ; That was from my own datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_list; ```. ```; [View of AnnData object with n_obs × n_vars = 2267 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1976 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1663 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2356 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2422 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1773 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts']; ```. ```; import mnnpy; corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""); ```. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-35-7ad830fcd907> in <module>; 1 import mnnpy; ----> 2 corrected = mnnpy.mnn_correct(*a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537
https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537:575,Modifiability,layers,layers,575," datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_list; ```. ```; [View of AnnData object with n_obs × n_vars = 2267 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1976 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1663 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2356 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2422 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1773 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts']; ```. ```; import mnnpy; corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""); ```. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-35-7ad830fcd907> in <module>; 1 import mnnpy; ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537
https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537:792,Modifiability,layers,layers,792," datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_list; ```. ```; [View of AnnData object with n_obs × n_vars = 2267 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1976 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1663 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2356 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2422 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1773 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts']; ```. ```; import mnnpy; corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""); ```. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-35-7ad830fcd907> in <module>; 1 import mnnpy; ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537
https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537:1009,Modifiability,layers,layers,1009," datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_list; ```. ```; [View of AnnData object with n_obs × n_vars = 2267 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1976 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1663 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2356 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2422 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1773 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts']; ```. ```; import mnnpy; corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""); ```. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-35-7ad830fcd907> in <module>; 1 import mnnpy; ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537
https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537:1226,Modifiability,layers,layers,1226," datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_list; ```. ```; [View of AnnData object with n_obs × n_vars = 2267 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1976 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1663 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2356 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2422 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1773 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts']; ```. ```; import mnnpy; corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""); ```. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-35-7ad830fcd907> in <module>; 1 import mnnpy; ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537
https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537:1443,Modifiability,layers,layers,1443," datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_list; ```. ```; [View of AnnData object with n_obs × n_vars = 2267 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1976 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1663 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2356 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2422 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1773 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts']; ```. ```; import mnnpy; corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""); ```. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-35-7ad830fcd907> in <module>; 1 import mnnpy; ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537
https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537:1660,Modifiability,layers,layers,1660," datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_list; ```. ```; [View of AnnData object with n_obs × n_vars = 2267 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1976 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1663 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2356 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 2422 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs × n_vars = 1773 × 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts']; ```. ```; import mnnpy; corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""); ```. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-35-7ad830fcd907> in <module>; 1 import mnnpy; ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537
https://github.com/scverse/scanpy/issues/1368#issuecomment-674649625:76,Deployability,update,update,76,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368#issuecomment-674649625
https://github.com/scverse/scanpy/issues/1368#issuecomment-674649625:100,Deployability,install,installed,100,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368#issuecomment-674649625
https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048:0,Deployability,Install,Install,0,"Install it, it’s an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048
https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048:29,Integrability,depend,dependency,29,"Install it, it’s an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048
https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279:55,Deployability,integrat,integrated,55,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279
https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279:55,Integrability,integrat,integrated,55,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279
https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:41,Deployability,integrat,integrated,41,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446
https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:41,Integrability,integrat,integrated,41,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446
https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:297,Integrability,wrap,wrapper,297,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446
https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:436,Usability,learn,learn,436,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446
https://github.com/scverse/scanpy/issues/1371#issuecomment-677516048:10,Deployability,release,release,10,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']; ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371#issuecomment-677516048
https://github.com/scverse/scanpy/issues/1371#issuecomment-677516048:111,Security,access,access,111,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']; ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371#issuecomment-677516048
https://github.com/scverse/scanpy/issues/1371#issuecomment-736307368:102,Safety,detect,detection,102,"Hi,. great options with the styles... just one comment/question... to me, it seems that the automatic detection of the background for the color of the dot is not working:. dot_edge_color : str, Tuple[float, …], None (default: 'black'); Dot edge color. When color_on='dot' the default is no edge. When color_on='square', edge color is white for darker colors and black for lighter background square colors. in my case, all dots are black independent of the background color:. ![image](https://user-images.githubusercontent.com/59560120/100715263-3c61b480-33b7-11eb-8874-2a85364acea4.png). Stefan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371#issuecomment-736307368
https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242:13,Deployability,install,installs,13,"“Development installs” aren’t standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project; - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesn’t support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way – and the new way has no spec for a “dev install” yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242
https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242:422,Deployability,install,install,422,"“Development installs” aren’t standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project; - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesn’t support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way – and the new way has no spec for a “dev install” yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242
https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242:525,Deployability,install,install,525,"“Development installs” aren’t standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project; - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesn’t support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way – and the new way has no spec for a “dev install” yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242
https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242:612,Deployability,install,installs,612,"“Development installs” aren’t standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project; - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesn’t support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way – and the new way has no spec for a “dev install” yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242
https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242:739,Deployability,install,install,739,"“Development installs” aren’t standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project; - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesn’t support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way – and the new way has no spec for a “dev install” yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675404242
https://github.com/scverse/scanpy/pull/1377#issuecomment-675422847:6,Deployability,install,install,6,"`flit install --deps none -s` breaks `conda list` for me.; using `PYTHONPATH` for scanpy **and** anndata won't allow importing scanpy because `importlib_metadata.PackageNotFoundError: anndata`. This might be windows specific problems, but `pip setup.py develop` worked perfectly for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675422847
https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209:426,Availability,error,error,426,"That’s weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound: ; - loompy[version='>=3.0.5']; ```. But since loompy 3.x isn’t on conda-forge, that’s correct. Seems that resolving anndata’s dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you don’t want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209
https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209:355,Deployability,integrat,integrate,355,"That’s weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound: ; - loompy[version='>=3.0.5']; ```. But since loompy 3.x isn’t on conda-forge, that’s correct. Seems that resolving anndata’s dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you don’t want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209
https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209:756,Deployability,upgrade,upgraded,756,"That’s weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound: ; - loompy[version='>=3.0.5']; ```. But since loompy 3.x isn’t on conda-forge, that’s correct. Seems that resolving anndata’s dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you don’t want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209
https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209:355,Integrability,integrat,integrate,355,"That’s weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound: ; - loompy[version='>=3.0.5']; ```. But since loompy 3.x isn’t on conda-forge, that’s correct. Seems that resolving anndata’s dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you don’t want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209
https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209:669,Integrability,depend,dependencies,669,"That’s weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound: ; - loompy[version='>=3.0.5']; ```. But since loompy 3.x isn’t on conda-forge, that’s correct. Seems that resolving anndata’s dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you don’t want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209
https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069:314,Availability,error,error,314,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:; I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried; `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`.; It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069
https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069:184,Deployability,install,installations,184,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:; I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried; `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`.; It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069
https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069:226,Deployability,install,install,226,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:; I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried; `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`.; It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069
https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069:260,Deployability,install,install,260,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:; I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried; `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`.; It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069
https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069:494,Deployability,install,installed,494,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:; I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried; `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`.; It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675477069
https://github.com/scverse/scanpy/pull/1378#issuecomment-675523238:239,Security,secur,security,239,"Details: https://github.com/conda/conda/issues/9074#issuecomment-675552817. It won’t fix anything, but you should be able to give yourself symlink creation permissions on windows and just use `-s`: https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/create-symbolic-links",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675523238
https://github.com/scverse/scanpy/pull/1378#issuecomment-675523238:248,Security,threat,threat-protection,248,"Details: https://github.com/conda/conda/issues/9074#issuecomment-675552817. It won’t fix anything, but you should be able to give yourself symlink creation permissions on windows and just use `-s`: https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/create-symbolic-links",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675523238
https://github.com/scverse/scanpy/pull/1378#issuecomment-675523238:266,Security,secur,security-policy-settings,266,"Details: https://github.com/conda/conda/issues/9074#issuecomment-675552817. It won’t fix anything, but you should be able to give yourself symlink creation permissions on windows and just use `-s`: https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/create-symbolic-links",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378#issuecomment-675523238
https://github.com/scverse/scanpy/pull/1382#issuecomment-676542244:21,Availability,error,error,21,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**; ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**; ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**; ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382#issuecomment-676542244
https://github.com/scverse/scanpy/pull/1382#issuecomment-676542244:568,Deployability,update,update,568,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**; ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**; ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**; ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382#issuecomment-676542244
https://github.com/scverse/scanpy/pull/1382#issuecomment-676542244:99,Testability,test,tests,99,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**; ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**; ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**; ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382#issuecomment-676542244
https://github.com/scverse/scanpy/pull/1383#issuecomment-678387240:163,Security,expose,expose,163,"It looks great!; ![image](https://user-images.githubusercontent.com/25887487/90914608-7990d080-e3de-11ea-81ed-15fdf5c3be80.png). One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors (first 6 ring etc.). Btw, how do I push to this specific PR from my local repo? I managed to pull it but can't figure out a way to push here and not on my fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-678387240
https://github.com/scverse/scanpy/pull/1383#issuecomment-691844681:761,Availability,error,errors,761,"Oh, sorry, I had completely missed your comment here!. > It looks great!. Thanks! Can I ask why you used leiden clustering on this?. > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo?. This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh; gh pr checkout 1383; # whatever changes; git push; ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-691844681
https://github.com/scverse/scanpy/pull/1383#issuecomment-691844681:169,Security,expose,expose,169,"Oh, sorry, I had completely missed your comment here!. > It looks great!. Thanks! Can I ask why you used leiden clustering on this?. > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo?. This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh; gh pr checkout 1383; # whatever changes; git push; ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-691844681
https://github.com/scverse/scanpy/pull/1383#issuecomment-693389468:24,Deployability,update,updated,24,"@ivirshup @giovp i have updated the function, now it can process non-visium coordinates and have the number of rings option for visium.; https://github.com/theislab/spatial-tools/blob/graph/notebooks/build_spatial_graph.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-693389468
https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228:108,Deployability,integrat,integrate,108,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228
https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228:108,Integrability,integrat,integrate,108,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228
https://github.com/scverse/scanpy/pull/1383#issuecomment-701194608:96,Deployability,update,updates,96,"Hey! I was just getting back to this, and I'm not sure I agree with all the choices made in the updates. For example, I would probably rather have separate functions for different spatial neighbor strategies. Also this won't work if `coord_type` isn't `""visium""`. Should I be making changes to this PR, or are you relying on it?. # hexagonal connectivity. I would propose a separate function just for visium data, maybe called `visium_connectivity`. It works with the assumption of a hexagonal grid. This removes the `neigh`, `radius`, and `coord_type` arguments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python; def walk_nsteps(adj, n):; """"""Expand adjacency matrix adj by walking out n steps from each node.""""""; adj = adj.astype(bool); cur_step = adj; result = adj.copy(); for i in range(n):; cur_step = adj @ cur_step; cur_step.setdiag(False); result = result + cur_step; return result; ```. <details>; <summary> An example showing this works </summary>. ```python; import networkx as nx; from scipy import sparse; from matplotlib import pyplot as plt; import numpy as np. def walk_nsteps(adj, n):; """"""Expand adjacency matrix adj by walking out n steps from each node.""""""; adj = adj.astype(bool); cur_step = adj; result = adj.copy(); for i in range(n):; cur_step = adj @ cur_step; cur_step.setdiag(False); result = result + cur_step; return result. # Test data (path graph). G = nx.Graph(); G.add_nodes_from([0,1,2,3]); G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]); adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3); # Fixed circle layout; pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):; nx.draw(; nx.Graph(walk_nsteps(adj, n)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701194608
https://github.com/scverse/scanpy/pull/1383#issuecomment-701194608:1611,Testability,Test,Test,1611,"ments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python; def walk_nsteps(adj, n):; """"""Expand adjacency matrix adj by walking out n steps from each node.""""""; adj = adj.astype(bool); cur_step = adj; result = adj.copy(); for i in range(n):; cur_step = adj @ cur_step; cur_step.setdiag(False); result = result + cur_step; return result; ```. <details>; <summary> An example showing this works </summary>. ```python; import networkx as nx; from scipy import sparse; from matplotlib import pyplot as plt; import numpy as np. def walk_nsteps(adj, n):; """"""Expand adjacency matrix adj by walking out n steps from each node.""""""; adj = adj.astype(bool); cur_step = adj; result = adj.copy(); for i in range(n):; cur_step = adj @ cur_step; cur_step.setdiag(False); result = result + cur_step; return result. # Test data (path graph). G = nx.Graph(); G.add_nodes_from([0,1,2,3]); G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]); adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3); # Fixed circle layout; pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):; nx.draw(; nx.Graph(walk_nsteps(adj, n)),; pos=pos,; ax=ax,; ). plt.show(); ```; ![image](https://user-images.githubusercontent.com/8238804/94650946-f480cb80-033a-11eb-8d59-9716408d0bbd.png); </details>. # Radius/ n-neighbors coordinates. I'm a little less sure about this use case. Since cells come in all shapes and sizes, I'm not sure if representing each cell as a centroid with a fixed radius or number of neighbors is the way to go. That said, maybe it is. This could be a separate function `spatial_connectivity`? In this case, I think it would also be useful to return a distance matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701194608
https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872:115,Availability,redundant,redundant,115,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872
https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872:17,Energy Efficiency,power,powers,17,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872
https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872:286,Energy Efficiency,power,power,286,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872
https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872:115,Safety,redund,redundant,115,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701247872
https://github.com/scverse/scanpy/pull/1383#issuecomment-701296035:66,Availability,down,down,66,"@Koncopd here's an example of the current implementation breaking down at `n=3`. ```python; def walk_nsteps_current(adj, n=1):; adj = adj.copy(); if n > 1:; # get up to n_rings order connections; adj += adj ** n; adj.setdiag(0); adj.eliminate_zeros(); adj.data[:] = 1.0; return adj. fig, axes = plt.subplots(nrows=3); # Fixed circle layout; pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):; nx.draw(; nx.Graph(walk_nsteps_current(adj, n + 1)), # making sure we start at 1; pos=pos,; ax=ax,; ); ; plt.show(); ```. ![image](https://user-images.githubusercontent.com/8238804/94672185-3cfab200-0358-11eb-84d6-11abe07f7f1c.png). For `n=3` you're missing the neighbors at depth=2. Basically, you're just jumping to the 3rd step, instead of accumulating neighbors up to that step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701296035
https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140:117,Energy Efficiency,power,powers,117,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140
https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140:384,Energy Efficiency,power,powers,384,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140
https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140:194,Usability,simpl,simple,194,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701334140
https://github.com/scverse/scanpy/pull/1383#issuecomment-701344124:152,Energy Efficiency,power,powers,152,"@Koncopd yes, I believe that should cover everything (maybe test to make sure I'm not missing sth here). However, I still think taking adjacency matrix powers will not be as fast as a BFS/DFS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701344124
https://github.com/scverse/scanpy/pull/1383#issuecomment-701344124:60,Testability,test,test,60,"@Koncopd yes, I believe that should cover everything (maybe test to make sure I'm not missing sth here). However, I still think taking adjacency matrix powers will not be as fast as a BFS/DFS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-701344124
https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147:71,Energy Efficiency,efficient,efficient,71,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147
https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147:508,Safety,redund,redundancy,508,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147
https://github.com/scverse/scanpy/pull/1383#issuecomment-704130199:93,Testability,test,tests,93,"Btw, I've separated the visium and non-visium case (since they don't share much code), added tests for the visium case, and removed the warnings. I've rebased so I could re-use some test data from another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704130199
https://github.com/scverse/scanpy/pull/1383#issuecomment-704130199:182,Testability,test,test,182,"Btw, I've separated the visium and non-visium case (since they don't share much code), added tests for the visium case, and removed the warnings. I've rebased so I could re-use some test data from another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704130199
https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531:73,Energy Efficiency,efficient,efficient,73,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors?. You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531
https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531:208,Energy Efficiency,efficient,efficient,208,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors?. You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531
https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531:229,Performance,queue,queues,229,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors?. You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531
https://github.com/scverse/scanpy/pull/1383#issuecomment-705360617:700,Deployability,update,update,700,"> You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way. Thats a problem with this strategy. Since it's an undirected graph, a node is it's own second neighbor. Since it's a hexagonal grid my first neighbors are also my second neighbors as well as my nth neighbors (in most cases, if there are edges or missing cells this might not be the case). We would either have to do our own BFS which precludes back tracking (i.e. for each search from each node remove previously visited edges), or we could take the difference of the edge sets at each update. Taking the difference would probably be easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705360617
https://github.com/scverse/scanpy/pull/1383#issuecomment-705412306:158,Energy Efficiency,power,power,158,"With self-loops a node is also its first neighbor. That means if you binarize the adjacency matrix and you have a 1 in the adjacency matrix after taking some power, that means there is only 1 way to get to that neighbor and it must thus only be reachable in the N-th hope (N being the power you just multiplied with). You could therefore also just look for the 1s in the matrix after every multiplication.... that might be a bit easier than taking the difference.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705412306
https://github.com/scverse/scanpy/pull/1383#issuecomment-705412306:285,Energy Efficiency,power,power,285,"With self-loops a node is also its first neighbor. That means if you binarize the adjacency matrix and you have a 1 in the adjacency matrix after taking some power, that means there is only 1 way to get to that neighbor and it must thus only be reachable in the N-th hope (N being the power you just multiplied with). You could therefore also just look for the 1s in the matrix after every multiplication.... that might be a bit easier than taking the difference.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705412306
https://github.com/scverse/scanpy/pull/1383#issuecomment-705462614:2001,Energy Efficiency,reduce,reduce,2001,"check:. <details>; <summary> Code </summary>. ```python; import scanpy as sc; from sparse_wrapper._core.coordinate_ops import difference_indices; import numpy as np; from scipy import sparse. def find_steps_val(adj, n_steps):; """"""Finding positions with value of 1.""""""; cur = adj.astype(int); diffs = [adj.copy()]; for i in range(n_steps):; cur.data[:] = 1; cur = adj @ cur; diffs.append(cur == 1); return diffs. def find_steps(adj, n_steps):; """"""Finding differences in sparsity patterns.""""""; diffs = [adj.copy()]; prev = adj + sparse.eye(adj.shape[0]); for i in range(n_steps):; cur = prev @ adj; diffs.append(difference_indices(cur, prev)); prev = cur; return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""); sc.pp.visium_connectivity(adata); adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1); val_res = find_steps_val(adj, N-1). for i in range(N):; adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int); adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(; adata,; color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],; ncols=N; ); ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>; <summary> </summary>. ```python; from operator import add; from functools import reduce. cells = np.random.choice(adata.n_obs, 20, replace=False); adata.obs[""fireworks""] = (; reduce(add, (diff_res[i][cells] * (3 - i) for i in range(3))); .max(axis=0); .toarray(); .ravel(); ); sc.pl.spatial(adata, color=""fireworks"", cmap=""inferno""); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/95443265-1962e780-09a8-11eb-9683-92f28574f0f7.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705462614
https://github.com/scverse/scanpy/pull/1383#issuecomment-705462614:2095,Energy Efficiency,reduce,reduce,2095,"check:. <details>; <summary> Code </summary>. ```python; import scanpy as sc; from sparse_wrapper._core.coordinate_ops import difference_indices; import numpy as np; from scipy import sparse. def find_steps_val(adj, n_steps):; """"""Finding positions with value of 1.""""""; cur = adj.astype(int); diffs = [adj.copy()]; for i in range(n_steps):; cur.data[:] = 1; cur = adj @ cur; diffs.append(cur == 1); return diffs. def find_steps(adj, n_steps):; """"""Finding differences in sparsity patterns.""""""; diffs = [adj.copy()]; prev = adj + sparse.eye(adj.shape[0]); for i in range(n_steps):; cur = prev @ adj; diffs.append(difference_indices(cur, prev)); prev = cur; return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""); sc.pp.visium_connectivity(adata); adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1); val_res = find_steps_val(adj, N-1). for i in range(N):; adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int); adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(; adata,; color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],; ncols=N; ); ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>; <summary> </summary>. ```python; from operator import add; from functools import reduce. cells = np.random.choice(adata.n_obs, 20, replace=False); adata.obs[""fireworks""] = (; reduce(add, (diff_res[i][cells] * (3 - i) for i in range(3))); .max(axis=0); .toarray(); .ravel(); ); sc.pl.spatial(adata, color=""fireworks"", cmap=""inferno""); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/95443265-1962e780-09a8-11eb-9683-92f28574f0f7.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705462614
https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776:319,Integrability,depend,dependency,319,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776
https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776:303,Safety,avoid,avoid,303,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776
https://github.com/scverse/scanpy/pull/1383#issuecomment-707590491:243,Availability,Ping,Pinging,243,"quick practical comment on this very interesting discussion.; @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well.; Sorry to put pressure but we are on a tight schedule 😅 . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707590491
https://github.com/scverse/scanpy/pull/1383#issuecomment-707590491:314,Energy Efficiency,schedul,schedule,314,"quick practical comment on this very interesting discussion.; @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well.; Sorry to put pressure but we are on a tight schedule 😅 . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707590491
https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083:35,Testability,test,tests,35,"I'd like to add this function plus tests to scanpy. I think I'll leave out `n_rings` argument and the `radius_neighbors` functions until there are clear use-cases. I would recommend just having a copy of the code in spatial-tools, which can be deduplicated later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083
https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083:147,Usability,clear,clear,147,"I'd like to add this function plus tests to scanpy. I think I'll leave out `n_rings` argument and the `radius_neighbors` functions until there are clear use-cases. I would recommend just having a copy of the code in spatial-tools, which can be deduplicated later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707603083
https://github.com/scverse/scanpy/pull/1383#issuecomment-707604861:61,Usability,clear,clear,61,"ok that's great, thank you! The `radius_neighbors` have very clear applications in fish-like data, and we are assmebling a tutorial to show exactly that. The n-rings as well especially in the context of cell-cell communication (although did not check that systematically yet). So shall we add this functionality to spatial-tools ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707604861
https://github.com/scverse/scanpy/pull/1383#issuecomment-707610846:79,Testability,test,test,79,"I could probably be convinced to include `radius_neighbors`. I'd mainly need a test case. My initial opposition was that (1) it's a pretty trivial implementation and (2) without a real example I'm not sure if it's missing any obvious edge cases. For n-rings, I think that walking some steps from each node generalizes beyond graph construction, and might be reasonable to have as a separate method. I also haven't seen any recommendations about how to weigh the edges, which I think is pretty important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707610846
https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821:62,Testability,test,test,62,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821
https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821:84,Testability,test,tests,84,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821
https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821:41,Usability,clear,clear,41,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-707614821
https://github.com/scverse/scanpy/pull/1384#issuecomment-740500580:25,Testability,test,test-backports,25,@meeseeksdev backport to test-backports,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1384#issuecomment-740500580
https://github.com/scverse/scanpy/issues/1387#issuecomment-679870419:559,Deployability,install,installation,559,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight?. ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages?; * How does this impact users vs. developers?; * Is IO special, or should more parts go into sub-packages?; * What gets re-exported from ""main"" modules?; * Who manages the sub-packages?. A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-679870419
https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:945,Deployability,pipeline,pipelines,945,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365
https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:877,Integrability,depend,dependencies,877,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365
https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:110,Modifiability,refactor,refactor,110,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365
https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:332,Performance,load,loading,332,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365
https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:450,Performance,load,loading,450,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365
https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:214,Testability,log,logically,214,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365
https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:472,Usability,simpl,simple,472,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365
https://github.com/scverse/scanpy/issues/1387#issuecomment-683586582:69,Modifiability,refactor,refactor,69,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-683586582
https://github.com/scverse/scanpy/issues/1387#issuecomment-683586582:173,Testability,log,logically,173,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-683586582
https://github.com/scverse/scanpy/issues/1387#issuecomment-879682043:64,Modifiability,rewrite,rewrite,64,"pytables is starting to throw warnings, so it may be time for a rewrite and moving the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-879682043
https://github.com/scverse/scanpy/issues/1387#issuecomment-879693562:279,Performance,load,loading,279,I would love to see file I/O in Anndata. I imagine this would make things easier for episcanpy as well. That package can then focus more on setting up count tables where they are not nicely provided. Otherwise it becomes a bit difficult for the new user (me) to distinguish data loading and setting up new tables.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-879693562
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815:415,Integrability,depend,depend,415,"> Splitting off new modules; Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages?. method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers?. user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages?. it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules?. didn't get this sorry. > Who manages the sub-packages?. the IO subpackage? everyone 😅 . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815:900,Modifiability,refactor,refactoring,900,"> Splitting off new modules; Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages?. method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers?. user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages?. it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules?. didn't get this sorry. > Who manages the sub-packages?. the IO subpackage? everyone 😅 . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059349278:209,Modifiability,refactor,refactoring,209,"> it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). @giovp and I are synced on these ideas :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059349278
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650:777,Deployability,install,install,777,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers?; >; > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages?; >; > the IO subpackage? everyone 😅. 😅 indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650:692,Integrability,depend,dependency,692,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers?; >; > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages?; >; > the IO subpackage? everyone 😅. 😅 indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650:872,Integrability,depend,dependencies,872,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers?; >; > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages?; >; > the IO subpackage? everyone 😅. 😅 indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650:1037,Integrability,depend,dependencies,1037,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers?; >; > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages?; >; > the IO subpackage? everyone 😅. 😅 indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650:1168,Integrability,depend,dependencies,1168,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers?; >; > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages?; >; > the IO subpackage? everyone 😅. 😅 indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650:741,Safety,avoid,avoid,741,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers?; >; > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages?; >; > the IO subpackage? everyone 😅. 😅 indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:733,Deployability,release,releases,733,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:796,Integrability,depend,dependency,796,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:882,Integrability,depend,dependency,882,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1139,Integrability,synchroniz,synchronization,1139,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1257,Integrability,depend,dependencies,1257,"ium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1820,Integrability,depend,depend,1820,"round. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1159,Modifiability,enhance,enhanced,1159,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1469,Performance,load,loading,1469,"p us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:845,Safety,avoid,avoid,845,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1696,Security,access,access,1696,"round. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1168,Usability,user experience,user experience,1168,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:779,Availability,failure,failure,779,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:421,Deployability,release,releases,421,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:567,Deployability,release,releases,567,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:821,Deployability,install,install,821,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:939,Deployability,install,install,939,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:557,Energy Efficiency,charge,charge,557,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:916,Integrability,depend,dependencies,916,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:1076,Integrability,depend,depend,1076," about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_visium(path, schema=""v1""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261
https://github.com/scverse/scanpy/issues/1387#issuecomment-1059798725:575,Integrability,wrap,wrap,575,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation?. > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059798725
https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361:152,Performance,load,load,152,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361
https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361:195,Usability,clear,clear,195,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361
https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361:201,Usability,guid,guidance,201,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361
https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973:331,Security,access,accessibility,331,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973
https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973:466,Usability,clear,clear,466,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1112350973
https://github.com/scverse/scanpy/pull/1388#issuecomment-678829691:115,Usability,feedback,feedback,115,I renamed pts and pts_rest to `fraction_group` and `fraction_rest`. I'd like to merge this PR if there is no other feedback. We can think about how to provide aggregate statistics somewhere else. Then we can revisit this function and merge this info too.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-678829691
https://github.com/scverse/scanpy/pull/1388#issuecomment-706623744:3398,Testability,log,logistic,3398," other packages provide this value? If so, what do they call this column?. Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-706623744
https://github.com/scverse/scanpy/pull/1388#issuecomment-706623744:3432,Testability,log,logreg,3432," other packages provide this value? If so, what do they call this column?. Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-706623744
https://github.com/scverse/scanpy/pull/1388#issuecomment-706623744:3461,Testability,test,test,3461," other packages provide this value? If so, what do they call this column?. Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-706623744
https://github.com/scverse/scanpy/pull/1388#issuecomment-724859143:0,Availability,Ping,Ping,0,"Ping @ivirshup. I wanna merge this if possible, it'd be great if you can have a look at the reply above. Together with this PR and https://github.com/theislab/scanpy/pull/1488, it would be great to do gene-set enrichment of all cell types at once without loops \o/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-724859143
https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:2139,Performance,Perform,Performance,2139,"ups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > ; > ### Performance; > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results?. I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654
https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:2177,Performance,perform,performance,2177,"ups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > ; > ### Performance; > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results?. I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654
https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:1038,Testability,log,logFC,1038,"think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison.; > ; > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654
https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:742,Usability,clear,clear,742,"> ## `groups`; > I still don't think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison.; > ; > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-scor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654
https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:1611,Usability,clear,clear,1611,"ups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > ; > ### Performance; > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results?. I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654
https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:1756,Usability,clear,clear,1756,"ups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > ; > ### Performance; > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results?. I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654
https://github.com/scverse/scanpy/pull/1391#issuecomment-684930608:16,Testability,test,test,16,"Yes, i'll add a test for this. ; Yes, it uses the right formula for fold changes now, as we need to use the formula from `rank_genes_groups` which includes exponentiation of means, implying logarithmized data (we have this in the [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), for example).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391#issuecomment-684930608
https://github.com/scverse/scanpy/pull/1391#issuecomment-684930608:190,Testability,log,logarithmized,190,"Yes, i'll add a test for this. ; Yes, it uses the right formula for fold changes now, as we need to use the formula from `rank_genes_groups` which includes exponentiation of means, implying logarithmized data (we have this in the [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), for example).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391#issuecomment-684930608
https://github.com/scverse/scanpy/pull/1391#issuecomment-698869934:13,Deployability,update,update,13,@Koncopd Any update on this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391#issuecomment-698869934
https://github.com/scverse/scanpy/pull/1391#issuecomment-698883595:15,Deployability,update,update,15,@fidelram i'll update this soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391#issuecomment-698883595
https://github.com/scverse/scanpy/pull/1391#issuecomment-703691873:22,Deployability,update,updated,22,"@ivirshup @fidelram ; updated, this should be ready for merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391#issuecomment-703691873
https://github.com/scverse/scanpy/pull/1392#issuecomment-680962922:30,Usability,simpl,simply,30,Underlying wish: Why don't we simply ditch list-based coloring everywhere :D,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1392#issuecomment-680962922
https://github.com/scverse/scanpy/issues/1396#issuecomment-683633085:114,Availability,down,down,114,"AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-683633085
https://github.com/scverse/scanpy/issues/1396#issuecomment-684103610:116,Availability,down,down,116,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization?. FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:04:05); ```; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:07:41); ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected?; ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OMP_NUM_THREADS=1; ```; More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:00:23); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-684103610
https://github.com/scverse/scanpy/issues/1396#issuecomment-684103610:868,Modifiability,variab,variables,868,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization?. FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:04:05); ```; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:07:41); ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected?; ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OMP_NUM_THREADS=1; ```; More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:00:23); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-684103610
https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240:951,Deployability,update,update,951,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs?; * Do we only limit the number of threads if `n_jobs` is specified? ; * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`?. *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python; from joblib import Parallel, delayed; res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240
https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240:184,Energy Efficiency,schedul,scheduling,184,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs?; * Do we only limit the number of threads if `n_jobs` is specified? ; * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`?. *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python; from joblib import Parallel, delayed; res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240
https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240:1135,Safety,avoid,avoiding-over-subscription-of-cpu-resources,1135,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs?; * Do we only limit the number of threads if `n_jobs` is specified? ; * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`?. *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python; from joblib import Parallel, delayed; res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240
https://github.com/scverse/scanpy/issues/1396#issuecomment-720680171:199,Energy Efficiency,schedul,scheduling,199,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-720680171
https://github.com/scverse/scanpy/issues/1397#issuecomment-683807774:197,Availability,down,downgrading,197,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used; `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-683807774
https://github.com/scverse/scanpy/issues/1397#issuecomment-683807774:13,Testability,test,testing,13,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used; `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-683807774
https://github.com/scverse/scanpy/issues/1397#issuecomment-684930174:201,Availability,down,downgrading,201,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used; > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!. I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-684930174
https://github.com/scverse/scanpy/issues/1397#issuecomment-684930174:15,Testability,test,testing,15,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used; > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!. I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-684930174
https://github.com/scverse/scanpy/issues/1397#issuecomment-684933191:205,Availability,down,downgrading,205,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used; > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!; > ; > I encountered the same issue. Which version are you using to fix this?. nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-684933191
https://github.com/scverse/scanpy/issues/1397#issuecomment-684933191:342,Availability,down,downgrading,342,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used; > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!; > ; > I encountered the same issue. Which version are you using to fix this?. nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-684933191
https://github.com/scverse/scanpy/issues/1397#issuecomment-684933191:17,Testability,test,testing,17,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used; > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!; > ; > I encountered the same issue. Which version are you using to fix this?. nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-684933191
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:131,Availability,ERROR,ERROR,131,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:210,Availability,ERROR,ERROR,210,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:1275,Deployability,patch,patch,1275,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:107,Performance,load,loader,107,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:238,Performance,load,loader,238,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:456,Performance,load,loader,456,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:576,Performance,load,loader,576,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:70,Testability,test,test,70,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:361,Testability,test,test,361,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:1233,Testability,test,test,1233,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:1292,Testability,test,tests,1292,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952
https://github.com/scverse/scanpy/pull/1398#issuecomment-738554080:14,Deployability,Pipeline,Pipelines,14,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1398#issuecomment-738554080
https://github.com/scverse/scanpy/pull/1398#issuecomment-738554080:55,Deployability,pipeline,pipeline,55,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1398#issuecomment-738554080
https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280:269,Deployability,pipeline,pipelines,269,"<samp>; <b>Supported commands</b><br>; <ul type=""none""><li><b>help:</b></li><ul type=""none""><li>Get descriptions, examples and documentation about supported commands</li><li><b>Example: </b>help ""command_name""</li></ul><li><b>list:</b></li><ul type=""none""><li>List all pipelines for this repository using a comment.</li><li><b>Example: </b>""list""</li></ul><li><b>run:</b></li><ul type=""none""><li>Run all pipelines or specific pipelines for this repository using a comment. Use this command by itself to trigger all related pipelines, or specify specific pipelines to run.</li><li><b>Example: </b>""run"" or ""run pipeline_name, pipeline_name, pipeline_name""</li></ul><li><b>where:</b></li><ul type=""none""><li>Report back the Azure DevOps orgs that are related to this repository and org</li><li><b>Example: </b>""where""</li></ul></ul><br>; See <a href=""https://go.microsoft.com/fwlink/?linkid=2056279"">additional documentation.</a>; </samp>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280
https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280:404,Deployability,pipeline,pipelines,404,"<samp>; <b>Supported commands</b><br>; <ul type=""none""><li><b>help:</b></li><ul type=""none""><li>Get descriptions, examples and documentation about supported commands</li><li><b>Example: </b>help ""command_name""</li></ul><li><b>list:</b></li><ul type=""none""><li>List all pipelines for this repository using a comment.</li><li><b>Example: </b>""list""</li></ul><li><b>run:</b></li><ul type=""none""><li>Run all pipelines or specific pipelines for this repository using a comment. Use this command by itself to trigger all related pipelines, or specify specific pipelines to run.</li><li><b>Example: </b>""run"" or ""run pipeline_name, pipeline_name, pipeline_name""</li></ul><li><b>where:</b></li><ul type=""none""><li>Report back the Azure DevOps orgs that are related to this repository and org</li><li><b>Example: </b>""where""</li></ul></ul><br>; See <a href=""https://go.microsoft.com/fwlink/?linkid=2056279"">additional documentation.</a>; </samp>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280
https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280:426,Deployability,pipeline,pipelines,426,"<samp>; <b>Supported commands</b><br>; <ul type=""none""><li><b>help:</b></li><ul type=""none""><li>Get descriptions, examples and documentation about supported commands</li><li><b>Example: </b>help ""command_name""</li></ul><li><b>list:</b></li><ul type=""none""><li>List all pipelines for this repository using a comment.</li><li><b>Example: </b>""list""</li></ul><li><b>run:</b></li><ul type=""none""><li>Run all pipelines or specific pipelines for this repository using a comment. Use this command by itself to trigger all related pipelines, or specify specific pipelines to run.</li><li><b>Example: </b>""run"" or ""run pipeline_name, pipeline_name, pipeline_name""</li></ul><li><b>where:</b></li><ul type=""none""><li>Report back the Azure DevOps orgs that are related to this repository and org</li><li><b>Example: </b>""where""</li></ul></ul><br>; See <a href=""https://go.microsoft.com/fwlink/?linkid=2056279"">additional documentation.</a>; </samp>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280
https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280:523,Deployability,pipeline,pipelines,523,"<samp>; <b>Supported commands</b><br>; <ul type=""none""><li><b>help:</b></li><ul type=""none""><li>Get descriptions, examples and documentation about supported commands</li><li><b>Example: </b>help ""command_name""</li></ul><li><b>list:</b></li><ul type=""none""><li>List all pipelines for this repository using a comment.</li><li><b>Example: </b>""list""</li></ul><li><b>run:</b></li><ul type=""none""><li>Run all pipelines or specific pipelines for this repository using a comment. Use this command by itself to trigger all related pipelines, or specify specific pipelines to run.</li><li><b>Example: </b>""run"" or ""run pipeline_name, pipeline_name, pipeline_name""</li></ul><li><b>where:</b></li><ul type=""none""><li>Report back the Azure DevOps orgs that are related to this repository and org</li><li><b>Example: </b>""where""</li></ul></ul><br>; See <a href=""https://go.microsoft.com/fwlink/?linkid=2056279"">additional documentation.</a>; </samp>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280
https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280:554,Deployability,pipeline,pipelines,554,"<samp>; <b>Supported commands</b><br>; <ul type=""none""><li><b>help:</b></li><ul type=""none""><li>Get descriptions, examples and documentation about supported commands</li><li><b>Example: </b>help ""command_name""</li></ul><li><b>list:</b></li><ul type=""none""><li>List all pipelines for this repository using a comment.</li><li><b>Example: </b>""list""</li></ul><li><b>run:</b></li><ul type=""none""><li>Run all pipelines or specific pipelines for this repository using a comment. Use this command by itself to trigger all related pipelines, or specify specific pipelines to run.</li><li><b>Example: </b>""run"" or ""run pipeline_name, pipeline_name, pipeline_name""</li></ul><li><b>where:</b></li><ul type=""none""><li>Report back the Azure DevOps orgs that are related to this repository and org</li><li><b>Example: </b>""where""</li></ul></ul><br>; See <a href=""https://go.microsoft.com/fwlink/?linkid=2056279"">additional documentation.</a>; </samp>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1398#issuecomment-738558280
https://github.com/scverse/scanpy/issues/1399#issuecomment-685401364:117,Deployability,release,releases,117,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this?. Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399#issuecomment-685401364
https://github.com/scverse/scanpy/issues/1399#issuecomment-685401364:386,Deployability,continuous,continuously,386,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this?. Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399#issuecomment-685401364
https://github.com/scverse/scanpy/issues/1399#issuecomment-685401364:258,Security,access,access,258,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this?. Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399#issuecomment-685401364
https://github.com/scverse/scanpy/issues/1405#issuecomment-686782513:49,Availability,error,error,49,Thanks. Can you share the code that produces the error? Maybe also the image as well?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405#issuecomment-686782513
https://github.com/scverse/scanpy/issues/1405#issuecomment-686783736:41,Testability,test,test,41,"`sc.pl.umap(ad, color=['mt.count'],save='test.pdf') `. [umaptest.pdf](https://github.com/theislab/scanpy/files/5171583/umaptest.pdf)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405#issuecomment-686783736
https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652:2757,Availability,error,error,2757,">; ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1809 num_categories,; 1810 layer=layer,; -> 1811 gene_symbols=gene_symbols,; 1812 ); 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 2911 matrix = adata[:, var_names].layers[layer]; 2912 elif use_raw:; -> 2913 matrix = adata.raw[:, var_names].X; 2914 else:; 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index); 94 ; 95 def __getitem__(self, index):; ---> 96 oidx, vidx = self._normalize_indices(index); 97 ; 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index); 154 obs, var = unpack_index(packed_index); 155 obs = _normalize_index(obs, self._adata.obs_names); --> 156 var = _normalize_index(var, self.var_names); 157 return obs, var; 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index); 91 not_found = indexer[positions < 0]; 92 raise KeyError(; ---> 93 f""Values {list(not_found)}, from {list(indexer)}, ""; 94 ""are not valid obs/ var names or indices.""; 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error; sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652
https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652:1626,Modifiability,layers,layers,1626,"var_names[0:4], groupby='celltype', color_map = 'Reds'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-72-4fc81df5ca3f> in <module>; ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1809 num_categories,; 1810 layer=layer,; -> 1811 gene_symbols=gene_symbols,; 1812 ); 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 2911 matrix = adata[:, var_names].layers[layer]; 2912 elif use_raw:; -> 2913 matrix = adata.raw[:, var_names].X; 2914 else:; 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index); 94 ; 95 def __getitem__(self, index):; ---> 96 oidx, vidx = self._normalize_indices(index); 97 ; 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index); 154 obs, var = unpack_index(packed_index); 155 obs = _normalize_index(obs, self._adata.obs_names); --> 156 var = _normalize_index(var, self.var_names); 157 return obs, var; 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index); 91 not_found = indexer[positions < 0]; 92 raise KeyError(; ---> 93 f""Values {list(not_found)}, from {list(indexer)}, ""; 94 ""are not valid obs/ var names or indices.""; 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652
https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652:1089,Testability,log,log,1089," one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```; adata_1 = sc.read_h5ad(""path/to/file1.h5ad""); adata_2 = sc.read_h5ad(""path/to/file2.h5ad""); ```. ```; #this works; sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not; sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-72-4fc81df5ca3f> in <module>; ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1809 num_categories,; 1810 layer=layer,; -> 1811 gene_symbols=gene_symbols,; 1812 ); 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 2911 matrix = adata[:, var_names].layers[layer]; 2912 elif use_raw:; -> 2913 matrix = adata.raw[:, var_names].X; 2914 else:; 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index); 94 ; 95 def __getitem__(self, index):; ---> 96 oidx, vidx = self._normalize_indices(index); 97 ; 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index); 154 obs, var = unpack_index(packed_index); 155 obs = _normalize_index(obs, self._adata.obs_names); --> 156 var = _normalize_index(var, self.var_na",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652
https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652:1549,Testability,log,log,1549,"ames[0:4], groupby='clusters', color_map = 'Reds'). #these do not; sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-72-4fc81df5ca3f> in <module>; ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1809 num_categories,; 1810 layer=layer,; -> 1811 gene_symbols=gene_symbols,; 1812 ); 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 2911 matrix = adata[:, var_names].layers[layer]; 2912 elif use_raw:; -> 2913 matrix = adata.raw[:, var_names].X; 2914 else:; 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index); 94 ; 95 def __getitem__(self, index):; ---> 96 oidx, vidx = self._normalize_indices(index); 97 ; 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index); 154 obs, var = unpack_index(packed_index); 155 obs = _normalize_index(obs, self._adata.obs_names); --> 156 var = _normalize_index(var, self.var_names); 157 return obs, var; 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index); 91 not_found = indexer[positions < 0]; 92 raise KeyError(; ---> 93 f""Values {list(not_found)}, from {list(indexer)}, ""; 94 ""are not valid obs/ var names or indices.""; 95 ). KeyError: ""Values ['",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652
https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665:8,Testability,log,logging,8,"```; sc.logging.print_versions(); scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```. ```; adata_2.raw.shape; > (5558, 2000); adata_2.X.shape; > (5558, 2000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665
https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665:124,Usability,learn,learn,124,"```; sc.logging.print_versions(); scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```. ```; adata_2.raw.shape; > (5558, 2000); adata_2.X.shape; > (5558, 2000); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704310665
https://github.com/scverse/scanpy/issues/1406#issuecomment-704312034:87,Modifiability,refactor,refactoring,87,"ok, do you mind updating both scanpy and anndata and try again? There has been a major refactoring of plotting functions in 1.6.0 and it could address your issue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704312034
https://github.com/scverse/scanpy/issues/1406#issuecomment-704317129:41,Deployability,update,update,41,mmh can I help you with how to go wrt to update?. @bz520251 is this still an issue for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704317129
https://github.com/scverse/scanpy/issues/1406#issuecomment-708958791:206,Availability,error,error,206,"@Chenyanjuan1993 thank you for reporting this. I think it's some typing problem with the var_names index, and so possibly regarding your anndata. Can you share the list of commands that you run before that error? Like do you concat/merge? Also, could you run that with this anndata `adata = scanpy.datasets.pbmc3k()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-708958791
https://github.com/scverse/scanpy/issues/1406#issuecomment-767855186:33,Deployability,update,updated,33,"I am having this same problem. I updated:; anndata 0.7.5; scanpy 1.6.0. It did not fix the problem for me. . I can try with the pbmc data set, what should I use for the groupby?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-767855186
https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714:265,Availability,avail,available,265,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`?. If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714
https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714:473,Availability,error,error,473,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`?. If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714
https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714:491,Deployability,release,release,491,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`?. If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714
https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714:467,Usability,clear,clear,467,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`?. If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-768012714
https://github.com/scverse/scanpy/issues/1406#issuecomment-850250397:359,Deployability,update,updates,359,"> Hi, Just wanted to comment that I had this issue. Converted from Seurat to h5ad using SeuratDisk. `adata.raw.var_names` is different than `adata.var_names`. As a result, I couldn't plot since none of my features were found (keys). Using `use_raw=False` worked. Yep, I have the same problem, if you worked on a Seurat-converted h5ad adata.; Any solutions or updates on this? Or we have to use useRaw=False????",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-850250397
https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296:138,Energy Efficiency,reduce,reduced,138,"`adata.raw.var_names` will have a different set of variables than `adata.var_names`, see #2018. This is by design, to allow you to have a reduced set of features in a dense matrix in `adata.X`, but have the full dataset in `adata.raw`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296
https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296:51,Modifiability,variab,variables,51,"`adata.raw.var_names` will have a different set of variables than `adata.var_names`, see #2018. This is by design, to allow you to have a reduced set of features in a dense matrix in `adata.X`, but have the full dataset in `adata.raw`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296
https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575:82,Availability,error,error,82,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575
https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575:96,Usability,clear,clear,96,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1442407575
https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577:308,Modifiability,variab,variable,308,"Maybe this helps someone who encounters this problem as well. ; Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically.; ```python; # load the adata object, converted using SeuratDisk; adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found; adata.var.index = adata.var.features; adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work); adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577
https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577:208,Performance,load,load,208,"Maybe this helps someone who encounters this problem as well. ; Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically.; ```python; # load the adata object, converted using SeuratDisk; adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found; adata.var.index = adata.var.features; adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work); adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577
https://github.com/scverse/scanpy/issues/1407#issuecomment-690964260:68,Deployability,update,update,68,"Also since recently. Used to work like a charm previously, before I update some packages. I guess it has to do with the latter. Any solutions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-690964260
https://github.com/scverse/scanpy/issues/1407#issuecomment-690967045:53,Availability,Down,Downgrading,53,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-690967045
https://github.com/scverse/scanpy/issues/1407#issuecomment-690967045:30,Integrability,depend,dependency,30,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-690967045
https://github.com/scverse/scanpy/issues/1407#issuecomment-691088607:11,Availability,down,downgrading,11,"Same here, downgrading made everything work",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-691088607
https://github.com/scverse/scanpy/issues/1407#issuecomment-691137508:33,Integrability,depend,dependencies,33,1.4.2. 1.5.X seemed to break the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-691137508
https://github.com/scverse/scanpy/issues/1407#issuecomment-699501883:24,Availability,error,error,24,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-699501883
https://github.com/scverse/scanpy/issues/1407#issuecomment-699501883:52,Deployability,install,install,52,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-699501883
https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788:58,Availability,avail,available,58,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd .; Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788
https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788:24,Deployability,update,updated,24,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd .; Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788
https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788:45,Integrability,wrap,wrappers,45,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd .; Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788
https://github.com/scverse/scanpy/issues/1407#issuecomment-734388537:27,Deployability,install,installing,27,I solved the problem after installing with this command:. pip install scipy==1.4.1 --use-feature=2020-resolver,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-734388537
https://github.com/scverse/scanpy/issues/1407#issuecomment-734388537:62,Deployability,install,install,62,I solved the problem after installing with this command:. pip install scipy==1.4.1 --use-feature=2020-resolver,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-734388537
https://github.com/scverse/scanpy/issues/1407#issuecomment-795204132:92,Availability,error,error,92,Still have to use scipy 1.4.1 or 1.4.2. There's now 1.6.1 but this also results in the same error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-795204132
https://github.com/scverse/scanpy/issues/1407#issuecomment-932815914:35,Deployability,install,install,35,I had the same issue using ; `pip3 install git+https://github.com/jacoblevine/phenograph.git` that gave version 1.5.2; but it worked using ; `pip install Phenograph==1.5.7`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-932815914
https://github.com/scverse/scanpy/issues/1407#issuecomment-932815914:146,Deployability,install,install,146,I had the same issue using ; `pip3 install git+https://github.com/jacoblevine/phenograph.git` that gave version 1.5.2; but it worked using ; `pip install Phenograph==1.5.7`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-932815914
https://github.com/scverse/scanpy/issues/1408#issuecomment-689676689:215,Integrability,rout,routinely,215,"No worries. Arguably scanpy should check for common compression formats with V2 (and no compression for V3) while reading the input files, or at least provide a more informative traceback, especially if data on GEO routinely includes these formats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408#issuecomment-689676689
https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1851,Availability,error,error,1851,"workx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 1698 out = concat(; 1699 all_adatas,; 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join; 800 ); --> 801 reindexers = [; 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875
https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:3749,Availability,toler,tolerance,3749,"l last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 1698 out = concat(; 1699 all_adatas,; 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join; 800 ); --> 801 reindexers = [; 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas; 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0); 800 ); 801 reindexers = [; --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas; 803 ]; 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var); 393 [1., 0., 0.]], dtype=float32); 394 """"""; --> 395 return Reindexer(cur_var, new_var); 396 ; 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx); 265 self.no_change = new_idx.equals(old_idx); 266 ; --> 267 new_pos = new_idx.get_indexer(old_idx); 268 old_pos = np.arange(len(new_pos)); 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 2731 ; 2732 if not self.is_unique:; -> 2733 raise InvalidIndexError(; 2734 ""Reindexing only valid with uniquely valued Index objects""; 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875
https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1805,Deployability,update,updated,1805,"ib 3.3.2; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.7; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 1698 out = concat(; 1699 all_adatas,; 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join; 800 ); --> 801 reindexer",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875
https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1464,Integrability,wrap,wrapt,1464,"0.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; easydev 0.10.1; get_version 2.1; gseapy 0.10.1; h5py 2.10.0; idna 2.10; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.15.2; jinja2 2.11.2; joblib 0.16.0; jsonschema 3.2.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.34.0; lxml 4.5.2; markupsafe 1.1.1; matplotlib 3.3.2; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.7; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 169",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875
https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1857,Integrability,message,message,1857,"workx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 1698 out = concat(; 1699 all_adatas,; 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join; 800 ); --> 801 reindexers = [; 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875
https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1751,Testability,log,logical,1751,"ib 3.3.2; mpl_toolkits NA; natsort 7.0.1; nbformat 5.0.7; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 1698 out = concat(; 1699 all_adatas,; 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join; 800 ); --> 801 reindexer",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875
https://github.com/scverse/scanpy/issues/1409#issuecomment-694668315:25,Availability,error,error,25,"To make you reproduce my error, here is what I did with a Docker image:. 1. `singularity -d build sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img docker://xie186/scrna_tutorial:0.1.0`. 2. sudo ufw allow 8689. 3. `singularity run --nv sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img jupyter-lab --ip=<your_ip> --port=8689 --no-browser `. Then in browser, open '<your_ip>:8689' and run the code below in jupyterlab. . The data and code I ran is shown as below:. * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file.; Uncompress the files:. ```; tar xvf GSE132188_RAW.tar; ```; You will get the following files.; ```; GSM3852752_E12_5_counts.tar.gz; GSM3852753_E13_5_counts.tar.gz; GSM3852754_E14_5_counts.tar.gz; GSM3852755_E15_5_counts.tar.gz; ```; Uncompress: ; ```; mkdir -p E12_5_counts/; tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/; mkdir -p E13_5_counts/; tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/; mkdir -p E14_5_counts/; tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/; mkdir -p E15_5_counts/; tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/; ```. Code:. ```; import numpy as np; import matplotlib.pyplot as pl; import numpy as np; import scanpy as sc; import scanpy.external as sce; import pandas as pd; from anndata import AnnData; import seaborn as sns; from scipy.sparse import csr_matrix; import networkx as nx; import xlsxwriter; from matplotlib import rcParams; import seaborn as sns; import scipy as sci; #GSEApy: Gene Set Enrichment Analysis in Python.; #import gseapy as gp; sc.settings.verbosity = 3; sc.logging.print_versions(). # Read cellranger files for all four samples; filename = './E12_5_counts/mm10/matrix.mtx'; filename_genes = './E12_5_counts/mm10/genes.tsv'; filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(); e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; e125",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694668315
https://github.com/scverse/scanpy/issues/1409#issuecomment-694668315:563,Availability,Down,Download,563,"To make you reproduce my error, here is what I did with a Docker image:. 1. `singularity -d build sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img docker://xie186/scrna_tutorial:0.1.0`. 2. sudo ufw allow 8689. 3. `singularity run --nv sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img jupyter-lab --ip=<your_ip> --port=8689 --no-browser `. Then in browser, open '<your_ip>:8689' and run the code below in jupyterlab. . The data and code I ran is shown as below:. * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file.; Uncompress the files:. ```; tar xvf GSE132188_RAW.tar; ```; You will get the following files.; ```; GSM3852752_E12_5_counts.tar.gz; GSM3852753_E13_5_counts.tar.gz; GSM3852754_E14_5_counts.tar.gz; GSM3852755_E15_5_counts.tar.gz; ```; Uncompress: ; ```; mkdir -p E12_5_counts/; tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/; mkdir -p E13_5_counts/; tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/; mkdir -p E14_5_counts/; tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/; mkdir -p E15_5_counts/; tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/; ```. Code:. ```; import numpy as np; import matplotlib.pyplot as pl; import numpy as np; import scanpy as sc; import scanpy.external as sce; import pandas as pd; from anndata import AnnData; import seaborn as sns; from scipy.sparse import csr_matrix; import networkx as nx; import xlsxwriter; from matplotlib import rcParams; import seaborn as sns; import scipy as sci; #GSEApy: Gene Set Enrichment Analysis in Python.; #import gseapy as gp; sc.settings.verbosity = 3; sc.logging.print_versions(). # Read cellranger files for all four samples; filename = './E12_5_counts/mm10/matrix.mtx'; filename_genes = './E12_5_counts/mm10/genes.tsv'; filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(); e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; e125",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694668315
https://github.com/scverse/scanpy/issues/1409#issuecomment-694668315:1671,Testability,log,logging,1671,".; ```; GSM3852752_E12_5_counts.tar.gz; GSM3852753_E13_5_counts.tar.gz; GSM3852754_E14_5_counts.tar.gz; GSM3852755_E15_5_counts.tar.gz; ```; Uncompress: ; ```; mkdir -p E12_5_counts/; tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/; mkdir -p E13_5_counts/; tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/; mkdir -p E14_5_counts/; tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/; mkdir -p E15_5_counts/; tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/; ```. Code:. ```; import numpy as np; import matplotlib.pyplot as pl; import numpy as np; import scanpy as sc; import scanpy.external as sce; import pandas as pd; from anndata import AnnData; import seaborn as sns; from scipy.sparse import csr_matrix; import networkx as nx; import xlsxwriter; from matplotlib import rcParams; import seaborn as sns; import scipy as sci; #GSEApy: Gene Set Enrichment Analysis in Python.; #import gseapy as gp; sc.settings.verbosity = 3; sc.logging.print_versions(). # Read cellranger files for all four samples; filename = './E12_5_counts/mm10/matrix.mtx'; filename_genes = './E12_5_counts/mm10/genes.tsv'; filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(); e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; e125.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E13_5_counts/mm10/matrix.mtx'; filename_genes = './E13_5_counts/mm10/genes.tsv'; filename_barcodes = './E13_5_counts/mm10/barcodes.tsv'. e135 = sc.read(filename).transpose(); e135.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; e135.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E14_5_counts/mm10/matrix.mtx'; filename_genes = './E14_5_counts/mm10/genes.tsv'; filename_barcodes = './E14_5_counts/mm10/barcodes.tsv'. e145 = sc.read(filename).transpose(); e145.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; e145.obs_names = np.genfromtxt(filename_barcod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694668315
https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:157,Availability,error,error,157,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604
https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:584,Availability,error,error,584,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604
https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:17,Modifiability,variab,variable,17,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604
https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:427,Modifiability,variab,variables,427,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604
https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:461,Modifiability,variab,variable,461,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604
https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:123,Usability,simpl,simple,123,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604
https://github.com/scverse/scanpy/issues/1410#issuecomment-689475876:221,Safety,avoid,avoid,221,"Hi @cartal,. Could it be that you are using a subsetted anndata object on which you are running clustering? If you run an `adata_subset = adata_subset.copy()` You turn that from a view into an anndata object, which might avoid this. I'm however not sure why using a view would make the clustering results end up in `adata.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689475876
https://github.com/scverse/scanpy/issues/1410#issuecomment-689498294:149,Safety,detect,detectedGenesPerCell,149,"I see. . The output for `adata` is:; ```; AnnData object with n_obs × n_vars = 106774 × 33538; obs: 'Cell', 'sampleID', 'sample_name', 'UMI_count', 'detectedGenesPerCell', 'percent_mito', 'data_set', 'IsPassed', 'batch', 'status', 'n_genes', 'n_counts', 'leiden', 'leiden_scVI'; var: 'name'; uns: 'log1p'; ```. I can see that my attempts to `leiden`ing have ended up here in `adata.obs` ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689498294
https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466:45,Deployability,pipeline,pipeline,45,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466
https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466:91,Deployability,install,installed,91,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466
https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466:2,Testability,test,tested,2,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466
https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466:162,Usability,feedback,feedback,162,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410#issuecomment-689637466
https://github.com/scverse/scanpy/issues/1411#issuecomment-695065780:108,Usability,learn,learn,108,Same issue here. . scanpy==1.6.0 anndata==0.7.4 umap==0.4.4 numpy==1.19.0 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.8.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1411#issuecomment-695065780
https://github.com/scverse/scanpy/issues/1412#issuecomment-697923746:7,Deployability,update,update,7,Please update to the latest scanpy release (1.6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412#issuecomment-697923746
https://github.com/scverse/scanpy/issues/1412#issuecomment-697923746:35,Deployability,release,release,35,Please update to the latest scanpy release (1.6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412#issuecomment-697923746
https://github.com/scverse/scanpy/pull/1413#issuecomment-696949167:7,Testability,test,tests,7,The CI tests failed due to the image matching problems (in test_violin and test_pbmc3k) fixed by PR #1422 which is now merged into master. Should I merge master into my fix branch?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-696949167
https://github.com/scverse/scanpy/pull/1413#issuecomment-697932813:272,Integrability,depend,dependent,272,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-697932813
https://github.com/scverse/scanpy/pull/1413#issuecomment-697932813:338,Integrability,depend,dependency,338,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-697932813
https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808:709,Safety,detect,detective,709,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:; 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions.; 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808
https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808:81,Testability,test,tests,81,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:; 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions.; 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808
https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808:15,Usability,feedback,feedback,15,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:; 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions.; 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808
https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808:1144,Usability,learn,learn,1144,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:; 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions.; 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-698903808
https://github.com/scverse/scanpy/pull/1413#issuecomment-822439809:541,Safety,safe,safer,541,"Hi @dwnorton, . Sorry for the delay. I can see now that there are a few changes about this on the [UMAP side](https://github.com/lmcinnes/umap/blob/master/umap/spectral.py#L98):. <img width=""617"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/115237075-1f214e80-a0ea-11eb-8257-8352f16ac2ad.png"">. Few questions:. - Can we improve the sparse array handling (e.g. mimicking UMAP e.g. using `SPARSE_SPECIAL_METRICS`) ; - Can we also use the `SKLEARN_PAIRWISE_VALID_METRICS` instead of catching the ValueError? Would that be safer?; - If we start using these keywords, do we need to change any of the requirements of scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-822439809
https://github.com/scverse/scanpy/pull/1413#issuecomment-846628296:240,Testability,test,tests,240,Hello. Sorry I have been silent but I haven't used scanpy for a few months and thought there was nothing more for me to do with this pull request. @Koncopd: the commit I made in September last year (e4483e9) triggered the CI and passed the tests. Is there something further you need me to do?. Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846628296
https://github.com/scverse/scanpy/pull/1413#issuecomment-846959398:71,Usability,simpl,simple,71,"Long term, I'd prefer to just use pynndescent since it would be a more simple implementation. That could change some results. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846959398
https://github.com/scverse/scanpy/pull/1413#issuecomment-846959840:14,Deployability,Pipeline,Pipelines,14,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846959840
https://github.com/scverse/scanpy/pull/1413#issuecomment-846959840:55,Deployability,pipeline,pipeline,55,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846959840
https://github.com/scverse/scanpy/pull/1413#issuecomment-846966462:329,Availability,error,error-reference,329,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1413?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`master@62bb643`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `74.48%`. ```diff; @@ Coverage Diff @@; ## master #1413 +/- ##; =========================================; Coverage ? 71.34% ; =========================================; Files ? 92 ; Lines ? 11186 ; Branches ? 0 ; =========================================; Hits ? 7981 ; Misses ? 3205 ; Partials ? 0 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1413?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/\_\_main\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L19fbWFpbl9fLnB5) | `0.00% <0.00%> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `70.62% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846966462
https://github.com/scverse/scanpy/pull/1413#issuecomment-846966462:277,Usability,learn,learn,277,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1413?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`master@62bb643`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `74.48%`. ```diff; @@ Coverage Diff @@; ## master #1413 +/- ##; =========================================; Coverage ? 71.34% ; =========================================; Files ? 92 ; Lines ? 11186 ; Branches ? 0 ; =========================================; Hits ? 7981 ; Misses ? 3205 ; Partials ? 0 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1413?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/\_\_main\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L19fbWFpbl9fLnB5) | `0.00% <0.00%> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `70.62% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1413/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_c,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-846966462
https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191:115,Testability,log,logic,115,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances; * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191
https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191:202,Usability,learn,learn,202,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances; * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-861371191
https://github.com/scverse/scanpy/issues/1414#issuecomment-692446575:7,Safety,detect,detect,7,"when I detect the with adata.obs[""seurat_clusters""].dtype.name != 'category'. the resault shows Ture",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414#issuecomment-692446575
https://github.com/scverse/scanpy/issues/1414#issuecomment-692506550:25,Availability,error,error,25,"Hi @john-jiangyong,. The error is that your covariate you group by should be a categorical, while it is not at the moment. You could run:; `adata.obs[‘seurat_clusters’] = adata.obs[‘seurat_clusters’].astype(‘category’)` To turn the covariate into a categorical. Note the code above might not be 100% correct as i’m typing from my phone and haven’t verified.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414#issuecomment-692506550
https://github.com/scverse/scanpy/issues/1415#issuecomment-694688826:203,Testability,assert,assert,203,"I'm a little confused about the question. `normalize_total` does try to modify the data inplace:. ```python; adata = sc.AnnData(np.arange(16).reshape((4, 4))); a = adata.X; sc.pp.normalize_total(adata); assert a is adata.X; ```. In general, we try to make most operations in place for efficiency. This should allow people to work with datasets that fit uncomfortably in memory, which might not be an option if a copy was made. Your screenshot does show some weirdness in the anndata constructor where a copy get's made, which there are more details on here: https://github.com/theislab/anndata/issues/129. Basically there's a line in the constructor where:. ```python; adata.X = X.astype(dtype=dtype); ```. where `dtype` defaults to float32. This is something we'd like to remove, but it was troublesome when last attempted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415#issuecomment-694688826
https://github.com/scverse/scanpy/issues/1415#issuecomment-694916916:103,Usability,clear,clearly,103,"I must've mixed up `normalize_total` and `normalize_per_cell`, which I know is deprecated. I know it's clearly stated in the anndata docs regarding the float32 copy issue, but it's really quite confusing!. ```python; In [1]: import scanpy as sc. In [2]: import numpy as np. In [3]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [4]: adata = sc.AnnData(a). In [5]: sc.pp.normalize_total(adata). In [6]: adata.X; Out[6]: ; array([[ 0. , 5. , 10. , 15. ],; [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],; [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],; [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [7]: a; Out[7]: ; array([[ 0. , 5. , 10. , 15. ],; [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],; [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],; [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [9]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [10]: adata = sc.AnnData(a). In [11]: sc.pp.normalize_per_cell(adata). In [12]: adata.X; Out[12]: ; array([[ 0. , 5. , 10. , 15. ],; [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],; [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],; [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [13]: a; Out[13]: ; array([[ 0., 1., 2., 3.],; [ 4., 5., 6., 7.],; [ 8., 9., 10., 11.],; [12., 13., 14., 15.]], dtype=float32); ```. Edit: So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415#issuecomment-694916916
https://github.com/scverse/scanpy/issues/1415#issuecomment-696580713:208,Availability,failure,failures,208,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon?. I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415#issuecomment-696580713
https://github.com/scverse/scanpy/issues/1415#issuecomment-696580713:203,Testability,test,test,203,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon?. I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415#issuecomment-696580713
https://github.com/scverse/scanpy/issues/1415#issuecomment-697187069:131,Safety,avoid,avoid,131,"We will always try to make `.X` a reference to the passed array. There may be cases where we need to make a copy, but we'll try to avoid that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415#issuecomment-697187069
https://github.com/scverse/scanpy/pull/1417#issuecomment-693318284:58,Deployability,update,update,58,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693318284
https://github.com/scverse/scanpy/pull/1417#issuecomment-693318284:0,Testability,Test,Tests,0,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693318284
https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949:60,Deployability,update,update,60,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949
https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949:234,Deployability,update,updated,234,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949
https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949:2,Testability,Test,Tests,2,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949
https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949:166,Testability,test,tests,166,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693331949
https://github.com/scverse/scanpy/pull/1417#issuecomment-693339957:84,Testability,test,tests,84,Thanks. I just confirmed that the newer versions for both packages were used in the tests. I am checking now what is the difference.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693339957
https://github.com/scverse/scanpy/pull/1417#issuecomment-693346995:17,Testability,test,tests,17,"When running the tests locally with Python 3.7, `seaborn` seems to be the problem, as the tests pass for `matplotlib==3.3.2` and `seaborn==0.10.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693346995
https://github.com/scverse/scanpy/pull/1417#issuecomment-693346995:90,Testability,test,tests,90,"When running the tests locally with Python 3.7, `seaborn` seems to be the problem, as the tests pass for `matplotlib==3.3.2` and `seaborn==0.10.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693346995
https://github.com/scverse/scanpy/pull/1417#issuecomment-693574261:420,Testability,test,tests,420,The problem seems to be [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L745) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L747) as `seaborn` throws the warning. ```bash; UserWarning: Vertical orientation ignored with only `x` specified.; ```. in the stack trace of the failed tests. I'll open a separate issue as it is unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-693574261
https://github.com/scverse/scanpy/pull/1417#issuecomment-696939838:40,Testability,test,tests,40,"@fidelram, I merged with master and all tests pass. Should be ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417#issuecomment-696939838
https://github.com/scverse/scanpy/issues/1418#issuecomment-698870987:95,Availability,error,errors,95,Did you check if the expected and actual images were significantly different. Some times I get errors locally because is difficult to have an identical environment as the one used by Travis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418#issuecomment-698870987
https://github.com/scverse/scanpy/issues/1419#issuecomment-694244682:39,Availability,avail,available,39,"Realised this functionality is already available via `pip install "".[dev]""`. May be good to mention somewhere, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-694244682
https://github.com/scverse/scanpy/issues/1419#issuecomment-694244682:58,Deployability,install,install,58,"Realised this functionality is already available via `pip install "".[dev]""`. May be good to mention somewhere, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-694244682
https://github.com/scverse/scanpy/issues/1419#issuecomment-698872047:129,Deployability,install,installation,129,This is indeed very valuable information. . Do you mind adding a PR updating https://github.com/theislab/scanpy/blob/master/docs/installation.rst ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-698872047
https://github.com/scverse/scanpy/issues/1419#issuecomment-699829357:22,Deployability,update,update,22,"Yes, no problem. I'll update it in a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-699829357
https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876:146,Deployability,install,install,146,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash; pip install -e .; pip install "".[dev]""; ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash; pip install -e .; pip install -r requirements-dev.txt; ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876
https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876:164,Deployability,install,install,164,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash; pip install -e .; pip install "".[dev]""; ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash; pip install -e .; pip install -r requirements-dev.txt; ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876
https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876:200,Deployability,install,install,200,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash; pip install -e .; pip install "".[dev]""; ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash; pip install -e .; pip install -r requirements-dev.txt; ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876
https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876:334,Deployability,install,installed,334,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash; pip install -e .; pip install "".[dev]""; ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash; pip install -e .; pip install -r requirements-dev.txt; ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876
https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876:571,Deployability,install,install,571,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash; pip install -e .; pip install "".[dev]""; ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash; pip install -e .; pip install -r requirements-dev.txt; ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876
https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876:589,Deployability,install,install,589,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash; pip install -e .; pip install "".[dev]""; ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash; pip install -e .; pip install -r requirements-dev.txt; ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876
https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876:295,Testability,test,testing,295,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash; pip install -e .; pip install "".[dev]""; ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash; pip install -e .; pip install -r requirements-dev.txt; ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-703124876
https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243:222,Deployability,install,installation,222,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243
https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243:248,Deployability,install,installing,248,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243
https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243:354,Deployability,install,installing,354,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243
https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243:378,Deployability,install,install,378,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243
https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243:308,Testability,test,tests,308,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-776896243
https://github.com/scverse/scanpy/issues/1419#issuecomment-777252906:42,Deployability,install,install,42,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-777252906
https://github.com/scverse/scanpy/issues/1419#issuecomment-777252906:72,Deployability,install,installation,72,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-777252906
https://github.com/scverse/scanpy/issues/1419#issuecomment-777252906:126,Testability,test,tests,126,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-777252906
https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947:6,Deployability,install,install,6,"`flit install -s` by default would install everything, passing `--deps=develop` actually leads to fewer things being installed. I think this is weird behavior, but it's the way flit works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947
https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947:35,Deployability,install,install,35,"`flit install -s` by default would install everything, passing `--deps=develop` actually leads to fewer things being installed. I think this is weird behavior, but it's the way flit works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947
https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947:117,Deployability,install,installed,117,"`flit install -s` by default would install everything, passing `--deps=develop` actually leads to fewer things being installed. I think this is weird behavior, but it's the way flit works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419#issuecomment-777262947
https://github.com/scverse/scanpy/issues/1420#issuecomment-694079984:131,Availability,error,error,131,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694079984
https://github.com/scverse/scanpy/issues/1420#issuecomment-694079984:137,Integrability,message,message,137,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694079984
https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139:497,Energy Efficiency,adapt,adapting,497,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python; # ...; kwds.setdefault('cut', 0); kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]; g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`; g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds); ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); if stripplot:; sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""); ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139
https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139:497,Modifiability,adapt,adapting,497,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python; # ...; kwds.setdefault('cut', 0); kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]; g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`; g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds); ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); if stripplot:; sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""); ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139
https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891:239,Energy Efficiency,adapt,adapting,239,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python; if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:; grouped_df = obs_tidy.groupby(x); for ax_id, key in zip(range(g.axes.shape[1]), keys):; sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds); ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891
https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891:239,Modifiability,adapt,adapting,239,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python; if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:; grouped_df = obs_tidy.groupby(x); for ax_id, key in zip(range(g.axes.shape[1]), keys):; sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds); ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891
https://github.com/scverse/scanpy/issues/1420#issuecomment-694327799:157,Testability,test,tests,157,@WeilerP probably there is a more direct way to overlay the stripplot but I don't think that it makes any big difference. . Can you make a PR to see how the tests work?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694327799
https://github.com/scverse/scanpy/pull/1421#issuecomment-697317906:21,Deployability,update,update,21,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421#issuecomment-697317906
https://github.com/scverse/scanpy/pull/1421#issuecomment-697317906:68,Testability,test,tests,68,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421#issuecomment-697317906
https://github.com/scverse/scanpy/pull/1421#issuecomment-697317906:139,Testability,test,tests,139,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421#issuecomment-697317906
https://github.com/scverse/scanpy/pull/1422#issuecomment-694389007:102,Availability,failure,failures,102,"@fidelram, from `seaborn==0.12` on, the only valid positional argument will be `data` which may cause failures or unexpected behaviour. This is relevant at least [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L774) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L785). Should we add / specify the keyword argument `x` in this PR or open a separate issue and PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-694389007
https://github.com/scverse/scanpy/pull/1422#issuecomment-694832160:29,Testability,test,tests,29,"I noticed, however, that the tests pass even when removing the `if stripplot:` part. Any idea on why this is happening and how to prevent it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-694832160
https://github.com/scverse/scanpy/pull/1422#issuecomment-694859994:17,Availability,toler,tolerance,17,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-694859994
https://github.com/scverse/scanpy/pull/1422#issuecomment-694859994:4,Testability,test,tests,4,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-694859994
https://github.com/scverse/scanpy/pull/1422#issuecomment-696687023:17,Testability,test,tests,17,I wonder why the tests are not working now?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-696687023
https://github.com/scverse/scanpy/pull/1422#issuecomment-696705252:66,Deployability,update,update,66,"> I wonder why the tests are not working now?. Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-696705252
https://github.com/scverse/scanpy/pull/1422#issuecomment-696705252:19,Testability,test,tests,19,"> I wonder why the tests are not working now?. Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-696705252
https://github.com/scverse/scanpy/pull/1422#issuecomment-696705252:146,Testability,test,tests,146,"> I wonder why the tests are not working now?. Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-696705252
https://github.com/scverse/scanpy/pull/1422#issuecomment-696710488:19,Availability,toler,tolerance,19,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-696710488
https://github.com/scverse/scanpy/pull/1422#issuecomment-696710488:6,Testability,test,tests,6,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-696710488
https://github.com/scverse/scanpy/pull/1422#issuecomment-733442033:208,Deployability,install,install,208,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-733442033
https://github.com/scverse/scanpy/pull/1422#issuecomment-733442033:227,Deployability,update,update,227,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-733442033
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:353,Availability,avail,available,353,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:41,Deployability,release,release,41,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:91,Deployability,release,released,91,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:217,Deployability,install,installation,217,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:277,Deployability,install,installation,277,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:342,Deployability,release,release,342,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:381,Deployability,install,install,381,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:401,Deployability,install,install,401,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:411,Deployability,upgrade,upgrade,411,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:681,Deployability,update,updated,681,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:634,Integrability,depend,depend,634,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:451,Safety,avoid,avoid,451,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:484,Usability,simpl,simply,484,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539
https://github.com/scverse/scanpy/issues/1426#issuecomment-706535883:16,Availability,error,error,16,"Hi,. First, the error that you are reporting has to do with series types of the dataframes. Howerver, it's very difficult to provide inputs, because it's unclear what `database` and `groupA` are. Can you report a reproducible example? Also, can you update to scanpy 1.6?. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1426#issuecomment-706535883
https://github.com/scverse/scanpy/issues/1426#issuecomment-706535883:249,Deployability,update,update,249,"Hi,. First, the error that you are reporting has to do with series types of the dataframes. Howerver, it's very difficult to provide inputs, because it's unclear what `database` and `groupA` are. Can you report a reproducible example? Also, can you update to scanpy 1.6?. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1426#issuecomment-706535883
https://github.com/scverse/scanpy/issues/1429#issuecomment-1759763329:91,Availability,avail,available,91,"Just came across this - still think its a good idea, other `make_blobs` arguments are also available and setting different random states might come in handy. Will make a small PR... If you think its not worth it we can also close the issue :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1429#issuecomment-1759763329
https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1038,Deployability,integrat,integration,1038,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414
https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1142,Deployability,integrat,integration,1142,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414
https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1038,Integrability,integrat,integration,1038,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414
https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1142,Integrability,integrat,integration,1142,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414
https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:656,Performance,optimiz,optimize,656,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414
https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1122,Performance,scalab,scalability,1122,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414
https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1370,Performance,optimiz,optimization,1370,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414
https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1115,Testability,test,tested,1115,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414
https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:783,Usability,usab,usable,783,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414
https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1053,Deployability,integrat,integration,1053,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229
https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1157,Deployability,integrat,integration,1157,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229
https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1053,Integrability,integrat,integration,1053,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229
https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1157,Integrability,integrat,integration,1157,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229
https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:671,Performance,optimiz,optimize,671,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229
https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1137,Performance,scalab,scalability,1137,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229
https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1385,Performance,optimiz,optimization,1385,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229
https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1130,Testability,test,tested,1130,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229
https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:798,Usability,usab,usable,798,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229
https://github.com/scverse/scanpy/pull/1432#issuecomment-698851156:209,Availability,mainten,maintenance,209,"Is this something for scanpy external or for scanpy core? It seems like a core-type functionality, but given external development easier to credit correctly in scanpy external? and there's ofc the question of maintenance if it's put into core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-698851156
https://github.com/scverse/scanpy/pull/1432#issuecomment-698884956:113,Availability,mainten,maintenance,113,@LuckyMD Its up to you all where'd you like it. I thought it was a pretty core tool. What is the expectation for maintenance in core vs external?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-698884956
https://github.com/scverse/scanpy/pull/1432#issuecomment-698886893:12,Availability,mainten,maintenance,12,"Scanpy core maintenance is done by the core team, while in external the maintenance is expected by the external contributor. Of course the scanpy core team grows as well... so i think this is a organization question maybe for @ivirshup and @flying-sheep.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-698886893
https://github.com/scverse/scanpy/pull/1432#issuecomment-698886893:72,Availability,mainten,maintenance,72,"Scanpy core maintenance is done by the core team, while in external the maintenance is expected by the external contributor. Of course the scanpy core team grows as well... so i think this is a organization question maybe for @ivirshup and @flying-sheep.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-698886893
https://github.com/scverse/scanpy/pull/1432#issuecomment-699862845:99,Availability,mainten,maintenance,99,Thanks for the contribution. This is great. I tried hash solo. This tool should be external as the maintenance and code falls outside the core development. The documentation should also point to the main developer of the functionality and give the due credits.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-699862845
https://github.com/scverse/scanpy/pull/1432#issuecomment-699862845:52,Security,hash,hash,52,Thanks for the contribution. This is great. I tried hash solo. This tool should be external as the maintenance and code falls outside the core development. The documentation should also point to the main developer of the functionality and give the due credits.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-699862845
https://github.com/scverse/scanpy/pull/1432#issuecomment-700582572:80,Testability,test,test,80,Can you run black (https://black.readthedocs.io/en/stable/) on the new files. A test is failing because of this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-700582572
https://github.com/scverse/scanpy/pull/1432#issuecomment-700952241:15,Testability,test,tests,15,Looks like the tests passed! @fidelram,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-700952241
https://github.com/scverse/scanpy/pull/1432#issuecomment-788389902:46,Safety,detect,detection,46,"Is there any interest in regular solo doublet detection to scanpy external? I have used scrublet for some time, but solo claims to outperform it and I am not sure that scrublet is actively maintained",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-788389902
https://github.com/scverse/scanpy/pull/1432#issuecomment-788402755:157,Usability,simpl,simple,157,"Hi @chris-rands I hope so. . I'm one of the authors of `solo`. I'm currently working on getting `solo` into `scvi-tools`. After that it should be relatively simple to add to scanpy. Best,; Nick",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1432#issuecomment-788402755
https://github.com/scverse/scanpy/issues/1433#issuecomment-704248522:102,Availability,error,error,102,"From the traceback, it'ss look like it's scvelo https://github.com/theislab/scvelo; Can you post this error with a reproducible example in scvelo repo ?. Will close this, feel free to reopen if problem persist",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1433#issuecomment-704248522
https://github.com/scverse/scanpy/issues/1433#issuecomment-704255621:52,Deployability,install,installed,52,@giovp . I solved the problem its scvelo and I have installed the package and now it works fine. Thank you,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1433#issuecomment-704255621
https://github.com/scverse/scanpy/issues/1434#issuecomment-783193441:30,Usability,feedback,feedback,30,"@havardtl, sorry for the late feedback, but this would be a great first PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434#issuecomment-783193441
https://github.com/scverse/scanpy/issues/1434#issuecomment-2029488023:11,Usability,clear,clear,11,"Just to be clear, is **n_genes** = `nFeature_RNA` and **n_genes_by_counts** = `nCounts_RNA `?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434#issuecomment-2029488023
https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732:76,Deployability,pipeline,pipeline,76,"Hi @geovp!. Yes, I mean the original image that was supplied to SpaceRanger pipeline.; It doesn't have to be a TIFF image - in my experience slide scanners save; JPEG images internally, so there is no value in converting that to TIFF.; Also, it would be cool to use sc.pl.spatial for other technologies - say to; overlay single cell spatial over the microscopy image image. Nice, I was using this hacky way before (if I remember correctly I also; changed spot size in the respective slot) - so it does work. I am wondering if you could add support for a fullres slot with size factor; 1 and explain which variables need to be set for it to work in the tutorial. On Thu, Oct 1, 2020 at 8:32 PM giovp <notifications@github.com> wrote:. > Hi @vitkl <https://github.com/vitkl> ,; > by fullres you mean the tiff image yes? This is not supported for now; > unfortunately, but we are working toward some extensions to make this; > possible (cc @hspitzer <https://github.com/hspitzer> ).; > One hacky way to go about this for now could be to:; >; > - assign the tiff to the hires slot in; > adata.uns['spatial]['library_id']['images']['hires']; > - change the hires scalefactor value to 1 in the respective slot; > This should work. also for plotting the spots in the right size. Of; > course, this is also possible if you replace the ""lowres"" instead.; > Let me know what you think about it and if it works.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1436#issuecomment-702351783>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTV5FBT2DB4GKUIZUVNTSITKMBANCNFSM4R5XDYSQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732
https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732:605,Modifiability,variab,variables,605,"Hi @geovp!. Yes, I mean the original image that was supplied to SpaceRanger pipeline.; It doesn't have to be a TIFF image - in my experience slide scanners save; JPEG images internally, so there is no value in converting that to TIFF.; Also, it would be cool to use sc.pl.spatial for other technologies - say to; overlay single cell spatial over the microscopy image image. Nice, I was using this hacky way before (if I remember correctly I also; changed spot size in the respective slot) - so it does work. I am wondering if you could add support for a fullres slot with size factor; 1 and explain which variables need to be set for it to work in the tutorial. On Thu, Oct 1, 2020 at 8:32 PM giovp <notifications@github.com> wrote:. > Hi @vitkl <https://github.com/vitkl> ,; > by fullres you mean the tiff image yes? This is not supported for now; > unfortunately, but we are working toward some extensions to make this; > possible (cc @hspitzer <https://github.com/hspitzer> ).; > One hacky way to go about this for now could be to:; >; > - assign the tiff to the hires slot in; > adata.uns['spatial]['library_id']['images']['hires']; > - change the hires scalefactor value to 1 in the respective slot; > This should work. also for plotting the spots in the right size. Of; > course, this is also possible if you replace the ""lowres"" instead.; > Let me know what you think about it and if it works.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1436#issuecomment-702351783>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTV5FBT2DB4GKUIZUVNTSITKMBANCNFSM4R5XDYSQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732
https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980:501,Modifiability,variab,variables,501,"> It doesn't have to be a TIFF image - in my experience slide scanners save JPEG images internally, so there is no value in converting that to TIFF. . This is interesting to know. > Also, it would be cool to use sc.pl.spatial for other technologies - say to overlay single cell spatial over the microscopy image image. . Can you elaborate on this? What tipe of technology and plot do you have in mind?. > I am wondering if you could add support for a fullres slot with size factor 1 and explain which variables need to be set for it to work in the tutorial. Mmh, I still think that the added value for looking at the fullres instead of the png in the context of overlaying spots to image is very little. In the hires png, even when cropping, the underlying resulting image is still quite good. Maybe not enough for analysis purpose, but for visualization should do the job no? I'm interested to hear your thoughts on this. The reason for not supporting it in the same way it's done now is that the image can be quite big (several GBs for fluorescent visium for instance) and so maybe it's not a good idea to load it in the anndata. I'll think about including a small section on this in the tutorial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980
https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980:1108,Performance,load,load,1108,"> It doesn't have to be a TIFF image - in my experience slide scanners save JPEG images internally, so there is no value in converting that to TIFF. . This is interesting to know. > Also, it would be cool to use sc.pl.spatial for other technologies - say to overlay single cell spatial over the microscopy image image. . Can you elaborate on this? What tipe of technology and plot do you have in mind?. > I am wondering if you could add support for a fullres slot with size factor 1 and explain which variables need to be set for it to work in the tutorial. Mmh, I still think that the added value for looking at the fullres instead of the png in the context of overlaying spots to image is very little. In the hires png, even when cropping, the underlying resulting image is still quite good. Maybe not enough for analysis purpose, but for visualization should do the job no? I'm interested to hear your thoughts on this. The reason for not supporting it in the same way it's done now is that the image can be quite big (several GBs for fluorescent visium for instance) and so maybe it's not a good idea to load it in the anndata. I'll think about including a small section on this in the tutorial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980
https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:312,Availability,down,download-,312,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276
https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:1124,Availability,down,download-,1124,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276
https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:179,Deployability,release,released,179,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276
https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:633,Integrability,depend,depends,633,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276
https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:512,Performance,load,load,512,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276
https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:577,Performance,load,load,577,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276
https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782:100,Availability,avail,available,100,"Hi @vitkl . thanks a lot for the feedback, all noted, we'll work toward enabling large tissue image available for storing+plotting. Will keep this open for reference!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782
https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782:33,Usability,feedback,feedback,33,"Hi @vitkl . thanks a lot for the feedback, all noted, we'll work toward enabling large tissue image available for storing+plotting. Will keep this open for reference!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-706060782
https://github.com/scverse/scanpy/issues/1438#issuecomment-1626976895:125,Availability,error,error,125,"Still happens in 2023. If I use `palette=sc.pl.palettes.zeileis_28` it works, but when I use `palette='Set2'` I got the same error",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1626976895
https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040:398,Deployability,pipeline,pipeline,398,"> Hi @JackieMium, I remember you said something similar in another issue.; > ; > If there’s things bugging you, how about making a PR that fixes it?. Not sure what you're referring to but I don't think I ever reported color pallette issue before. ; I hope I could help fix things but I am familiar with R/Seurat and Python/scanpy is a whole new universe to me. I am starting to learning the scanpy pipeline. How things work under the hood with scanpy or basically Python plotting are really beyond my capabilities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040
https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040:378,Usability,learn,learning,378,"> Hi @JackieMium, I remember you said something similar in another issue.; > ; > If there’s things bugging you, how about making a PR that fixes it?. Not sure what you're referring to but I don't think I ever reported color pallette issue before. ; I hope I could help fix things but I am familiar with R/Seurat and Python/scanpy is a whole new universe to me. I am starting to learning the scanpy pipeline. How things work under the hood with scanpy or basically Python plotting are really beyond my capabilities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640521040
https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114:677,Testability,log,logic,677,"Yeah, plotting in python is difficult, and our plotting code doesn’t make things simpler. This is the relevant part:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_anndata.py#L273-L284. The code that behaves like advertised in the docs is in here, but that function does more things after that:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_utils.py#L364. There’s also this:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_tools/scatterplots.py#L1181. I think things should be unified so they use the same palette selection logic. But I understand that that’s a pretty complex part of our code base. I meant this comment: https://github.com/scverse/scanpy/issues/1258#issuecomment-1626690231",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114
https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114:81,Usability,simpl,simpler,81,"Yeah, plotting in python is difficult, and our plotting code doesn’t make things simpler. This is the relevant part:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_anndata.py#L273-L284. The code that behaves like advertised in the docs is in here, but that function does more things after that:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_utils.py#L364. There’s also this:. https://github.com/scverse/scanpy/blob/0594b7f03917f8c5166d5bb2752031e1665065de/scanpy/plotting/_tools/scatterplots.py#L1181. I think things should be unified so they use the same palette selection logic. But I understand that that’s a pretty complex part of our code base. I meant this comment: https://github.com/scverse/scanpy/issues/1258#issuecomment-1626690231",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1640638114
https://github.com/scverse/scanpy/issues/1438#issuecomment-1641632885:318,Availability,error,error,318,"I don't know how to fix this in the code, but I do find a workaround to specify a color palette. Turns out both `sc.pl.umap()` and `sc.pl.scatter()` accept a color palette if it's from `seaborn` color palette:. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette='Set3'); ```. throws the above error message. But if I use. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=sns.color_palette('Set3')); ```. if works as expected. Also you can manually assign a color list as :. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=[; '#000000', '#575757', '#AD2323', '#2A4BD7', '#1D6914', ; '#814A19', '#8126C0', '#A0A0A0', '#81C57A', '#9DAFFF', ; '#29D0D0', '#FF9233', '#FFEE33', '#E9DEBB', '#FFCDF3', ; '#F2F3F4'; ]); ```. `palette=sc.pl.palettes.zeileis_28` works because `sc.pl.palettes.zeileis_28` is already a list of color. This also works for the `palete` argument in`sc.pl.umap()`, but it changes the `adata.uns['louvain_colors']` column values and will change other plots when using this column for plotting. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1641632885
https://github.com/scverse/scanpy/issues/1438#issuecomment-1641632885:324,Integrability,message,message,324,"I don't know how to fix this in the code, but I do find a workaround to specify a color palette. Turns out both `sc.pl.umap()` and `sc.pl.scatter()` accept a color palette if it's from `seaborn` color palette:. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette='Set3'); ```. throws the above error message. But if I use. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=sns.color_palette('Set3')); ```. if works as expected. Also you can manually assign a color list as :. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=[; '#000000', '#575757', '#AD2323', '#2A4BD7', '#1D6914', ; '#814A19', '#8126C0', '#A0A0A0', '#81C57A', '#9DAFFF', ; '#29D0D0', '#FF9233', '#FFEE33', '#E9DEBB', '#FFCDF3', ; '#F2F3F4'; ]); ```. `palette=sc.pl.palettes.zeileis_28` works because `sc.pl.palettes.zeileis_28` is already a list of color. This also works for the `palete` argument in`sc.pl.umap()`, but it changes the `adata.uns['louvain_colors']` column values and will change other plots when using this column for plotting. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1641632885
https://github.com/scverse/scanpy/issues/1438#issuecomment-1646498818:365,Usability,clear,clear,365,"> Or just the standard matplotlib palettes, yes:; > ; > ```python; > from matplotlib import cm; > ; > sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=cm.get_cmap('Set3')); > ```; > ; > The bug is that you can’t pass the colormap name. Passing a colormap directly works. Is it then, a good idea to state this the documents for the time being to clear the confusion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1646498818
https://github.com/scverse/scanpy/issues/1439#issuecomment-703157510:114,Deployability,install,install,114,"Hej,. I stumbled upon your issue. Test for my PR #1440:. ```; python3 -m venv venv; source venv/bin/activate; pip install -e . ; pip install ""anndata<=0.7.3""; python3 -c ""import scanpy as sc""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1439#issuecomment-703157510
https://github.com/scverse/scanpy/issues/1439#issuecomment-703157510:133,Deployability,install,install,133,"Hej,. I stumbled upon your issue. Test for my PR #1440:. ```; python3 -m venv venv; source venv/bin/activate; pip install -e . ; pip install ""anndata<=0.7.3""; python3 -c ""import scanpy as sc""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1439#issuecomment-703157510
https://github.com/scverse/scanpy/issues/1439#issuecomment-703157510:34,Testability,Test,Test,34,"Hej,. I stumbled upon your issue. Test for my PR #1440:. ```; python3 -m venv venv; source venv/bin/activate; pip install -e . ; pip install ""anndata<=0.7.3""; python3 -c ""import scanpy as sc""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1439#issuecomment-703157510
https://github.com/scverse/scanpy/issues/1439#issuecomment-835827799:279,Deployability,install,install,279,"I had a false alarm related to this just now (I thought I was on recent anndata+scanpy versions, but a defunct older clone of anndata was sneaking onto my `$PYTHONPATH`). Confirming it's fixed:. **Dockerfile:**; ```Dockerfile; FROM python:3.8.5; ARG anndata; ARG scanpy; RUN pip install anndata==${anndata} scanpy==${scanpy}; ENTRYPOINT [""python"",""-c"",""import scanpy""]; ```. **Works:**; ```bash; docker build --build-arg anndata=0.7.5 --build-arg scanpy=1.7.1 -t scanpy . && docker run scanpy # ✅; ```; **Fails:**; ```bash; docker build --build-arg anndata=0.7.3 --build-arg scanpy=1.6.1 -t scanpy . && docker run scanpy # ❌; # Traceback (most recent call last):; # File ""<string>"", line 1, in <module>; # File ""/usr/local/lib/python3.8/site-packages/scanpy/__init__.py"", line 41, in <module>; # from anndata import AnnData, concat; # ImportError: cannot import name 'concat' from 'anndata' (/usr/local/lib/python3.8/site-packages/anndata/__init__.py); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1439#issuecomment-835827799
https://github.com/scverse/scanpy/pull/1440#issuecomment-703224351:84,Deployability,update,update,84,"Thanks! This will, however, only work for `anndata>=0.7.2a1`. So you'd also have to update `scanpy/requirements.txt`. I left some more comments in #1439.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1440#issuecomment-703224351
https://github.com/scverse/scanpy/issues/1441#issuecomment-703314166:25,Testability,test,test,25,I would suggest `scanpy\[test\]` . Works with BASH and ZSH.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1441#issuecomment-703314166
https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008:129,Deployability,install,install,129,"Noglob turns off all globbing though. Would be great if one could turn off just Extended globbing for a command. After all, `pip install *.whl` could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008
https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008:80,Modifiability,Extend,Extended,80,"Noglob turns off all globbing though. Would be great if one could turn off just Extended globbing for a command. After all, `pip install *.whl` could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008
https://github.com/scverse/scanpy/issues/1443#issuecomment-703461716:135,Integrability,message,message,135,"Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to `scvi-tools`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703461716
https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988:443,Deployability,integrat,integrated,443,"With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users. . Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988
https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988:443,Integrability,integrat,integrated,443,"With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users. . Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988
https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133:220,Deployability,update,update,220,"Thanks for the responses!. > Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to scvi-tools?. I already made a PR to update the docs (https://scanpy.readthedocs.io/en/latest/ecosystem.html). I can also do the latter. > With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users.; >; > Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?. Is this different than the ecosystem page? Sounds reasonable though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133
https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133:769,Deployability,integrat,integrated,769,"Thanks for the responses!. > Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to scvi-tools?. I already made a PR to update the docs (https://scanpy.readthedocs.io/en/latest/ecosystem.html). I can also do the latter. > With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users.; >; > Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?. Is this different than the ecosystem page? Sounds reasonable though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133
https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133:164,Integrability,message,message,164,"Thanks for the responses!. > Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to scvi-tools?. I already made a PR to update the docs (https://scanpy.readthedocs.io/en/latest/ecosystem.html). I can also do the latter. > With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users.; >; > Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?. Is this different than the ecosystem page? Sounds reasonable though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133
https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133:769,Integrability,integrat,integrated,769,"Thanks for the responses!. > Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to scvi-tools?. I already made a PR to update the docs (https://scanpy.readthedocs.io/en/latest/ecosystem.html). I can also do the latter. > With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users.; >; > Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?. Is this different than the ecosystem page? Sounds reasonable though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133
https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:141,Deployability,integrat,integration,141,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683
https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:141,Integrability,integrat,integration,141,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683
https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:126,Usability,clear,clear,126,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683
https://github.com/scverse/scanpy/issues/1446#issuecomment-782180615:11,Availability,error,error,11,I saw this error in my analysis as welll.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1446#issuecomment-782180615
https://github.com/scverse/scanpy/issues/1446#issuecomment-1566468673:11,Availability,error,error,11,"Exact same error also happened to me in my analysis, need a fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1446#issuecomment-1566468673
https://github.com/scverse/scanpy/issues/1446#issuecomment-1759598407:97,Testability,log,logFC,97,"In the main code, they have just filtered the gene names and they haven't done anything with the logFC, thus they get left out. I have written the code which can take care of this, please do let me know if I can push this or not.; Besides this, the logFC can be negative but still they are equally significant as that of positive, so can we use **abs** so that the genes for which logFC < -threshold, also holds??",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1446#issuecomment-1759598407
https://github.com/scverse/scanpy/issues/1446#issuecomment-1759598407:249,Testability,log,logFC,249,"In the main code, they have just filtered the gene names and they haven't done anything with the logFC, thus they get left out. I have written the code which can take care of this, please do let me know if I can push this or not.; Besides this, the logFC can be negative but still they are equally significant as that of positive, so can we use **abs** so that the genes for which logFC < -threshold, also holds??",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1446#issuecomment-1759598407
https://github.com/scverse/scanpy/issues/1446#issuecomment-1759598407:381,Testability,log,logFC,381,"In the main code, they have just filtered the gene names and they haven't done anything with the logFC, thus they get left out. I have written the code which can take care of this, please do let me know if I can push this or not.; Besides this, the logFC can be negative but still they are equally significant as that of positive, so can we use **abs** so that the genes for which logFC < -threshold, also holds??",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1446#issuecomment-1759598407
https://github.com/scverse/scanpy/issues/1446#issuecomment-2211244159:191,Usability,simpl,simply,191,"@giovp this issue can be closed since the documentation already states that ""To preserve the original structure of adata.uns[‘rank_genes_groups’], filtered genes are set to NaN."" . Users can simply drop the NANs for each cluster column in the adata.uns[‘rank_genes_groups_filtered’] dataframe.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1446#issuecomment-2211244159
https://github.com/scverse/scanpy/issues/1448#issuecomment-706060383:73,Deployability,update,updated,73,> Can you elaborate? You can use `sc.pl.violin` independently. OK I just updated my question and attached a sample image. What I want to do is split-violinplot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-706060383
https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:15,Deployability,update,update,15,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626
https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:296,Modifiability,variab,variable,296,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626
https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:30,Usability,clear,clear,30,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626
https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:125,Usability,simpl,simply,125,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626
https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:17,Deployability,update,update,17,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433
https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:312,Modifiability,variab,variable,312,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433
https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:32,Usability,clear,clear,32,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433
https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:134,Usability,simpl,simply,134,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433
https://github.com/scverse/scanpy/issues/1450#issuecomment-707544193:54,Modifiability,enhance,enhance,54,"The latest version of scanpy added the possibility to enhance the plots in multiple ways. For this you need to use the new classes documented here https://scanpy.readthedocs.io/en/stable/api/scanpy.plotting.html#classes. In particular you want to check the function `add_totals()`: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.StackedViolin.add_totals.html#scanpy.pl.StackedViolin.add_totals. For your case you can do:; ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; sc.pl.StackedViolin(adata, markers, groupby='bulk_labels').add_totals().show(); ```. Other options using the function that you are familiar with is:. ```PYTHON; # here return_fig=True is used; plot = sc.pl.stacked_violin(adata, markers, groupby='bulk_labels', return_fig=True); plot.add_totals().show(); ```. You can find further info here: https://github.com/theislab/scanpy/pull/1210",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1450#issuecomment-707544193
https://github.com/scverse/scanpy/issues/1452#issuecomment-707656885:381,Availability,error,error,381,"Hi @saiteja-danda,. I cannot reproduce your code above as I don't have your data. Could you try to generate a minimal reproducible example with data from e.g., `sc.datasets.pbmc68k_reduced()`?. In general, could you check the output of `adata.obs['km'].value_counts()` to check whether the covariate you added was correctly stored? What output are you getting? You didn't share an error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1452#issuecomment-707656885
https://github.com/scverse/scanpy/issues/1452#issuecomment-707656885:387,Integrability,message,message,387,"Hi @saiteja-danda,. I cannot reproduce your code above as I don't have your data. Could you try to generate a minimal reproducible example with data from e.g., `sc.datasets.pbmc68k_reduced()`?. In general, could you check the output of `adata.obs['km'].value_counts()` to check whether the covariate you added was correctly stored? What output are you getting? You didn't share an error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1452#issuecomment-707656885
https://github.com/scverse/scanpy/issues/1454#issuecomment-707400680:217,Modifiability,flexible,flexible,217,"I think that's a good idea. A general solution would be to move the part of rank_genes_groups where some statistics are calculated (e.g. log2fc, fractions, mean expression per group) to a different function with more flexible features. For example, users run regress_out or combat sometimes and then run rank_genes_groups on these corrected values, but they wanna calculate log2fc and other summary stats on the ""raw"" logTP10k values. Having another function with a layer argument would solve this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1454#issuecomment-707400680
https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407:765,Energy Efficiency,efficient,efficient,765,"How can one get a DEG table with a pts column for each cluster? So that for each group there would be 4 columns: 'names', 'logfoldchanges', 'pvals_adj' and 'pts'?. Manual sorting from 2 files is not quite optimal:; ```; sc.tl.rank_genes_groups(adata, 'cell_types', method='wilcoxon', pts=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; degs_by_cluster = pd.DataFrame({group + '_' + key[:14]: result[key][group]; for group in groups for key in ['names', 'logfoldchanges', 'pvals_adj']}); degs_by_cluster.to_csv(""DEG_adata_cell_types_pct_to_sort.csv""); pts=pd.DataFrame(adata.uns['rank_genes_groups']['pts']); pts.to_csv(""pts_adata.csv""); ```. Could you help with a more efficient way to do that? ; @fidelram @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407
https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407:123,Testability,log,logfoldchanges,123,"How can one get a DEG table with a pts column for each cluster? So that for each group there would be 4 columns: 'names', 'logfoldchanges', 'pvals_adj' and 'pts'?. Manual sorting from 2 files is not quite optimal:; ```; sc.tl.rank_genes_groups(adata, 'cell_types', method='wilcoxon', pts=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; degs_by_cluster = pd.DataFrame({group + '_' + key[:14]: result[key][group]; for group in groups for key in ['names', 'logfoldchanges', 'pvals_adj']}); degs_by_cluster.to_csv(""DEG_adata_cell_types_pct_to_sort.csv""); pts=pd.DataFrame(adata.uns['rank_genes_groups']['pts']); pts.to_csv(""pts_adata.csv""); ```. Could you help with a more efficient way to do that? ; @fidelram @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407
https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407:550,Testability,log,logfoldchanges,550,"How can one get a DEG table with a pts column for each cluster? So that for each group there would be 4 columns: 'names', 'logfoldchanges', 'pvals_adj' and 'pts'?. Manual sorting from 2 files is not quite optimal:; ```; sc.tl.rank_genes_groups(adata, 'cell_types', method='wilcoxon', pts=True); sc.pl.rank_genes_groups(adata, n_genes=25, sharey=False); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; degs_by_cluster = pd.DataFrame({group + '_' + key[:14]: result[key][group]; for group in groups for key in ['names', 'logfoldchanges', 'pvals_adj']}); degs_by_cluster.to_csv(""DEG_adata_cell_types_pct_to_sort.csv""); pts=pd.DataFrame(adata.uns['rank_genes_groups']['pts']); pts.to_csv(""pts_adata.csv""); ```. Could you help with a more efficient way to do that? ; @fidelram @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1066545407
https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375:601,Deployability,integrat,integration,601,"Hello ; I am also facing the same problem.; I would like to get gene name, log fold change, pval_adj, pts.pts_rest in a single output CSV file but i couldn't able to do that; ` ; sc.tl.rank_genes_groups(adata,""leiden_0.6"", method='t-test',pts=True,corr_method='benjamini-hochberg'); pd.DataFrame(adata.uns['rank_genes_groups']['names']); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; df= pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names','logfoldchanges','pts','pts_rest','pvals','pvals_adj']}); df.to_csv(""/home/Akila/integration/harmony/subset/celltype/find_markergenes.csv"")`; ; Any idea how to get in the single file along with pts??; ; Thanks; Akila",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375
https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375:601,Integrability,integrat,integration,601,"Hello ; I am also facing the same problem.; I would like to get gene name, log fold change, pval_adj, pts.pts_rest in a single output CSV file but i couldn't able to do that; ` ; sc.tl.rank_genes_groups(adata,""leiden_0.6"", method='t-test',pts=True,corr_method='benjamini-hochberg'); pd.DataFrame(adata.uns['rank_genes_groups']['names']); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; df= pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names','logfoldchanges','pts','pts_rest','pvals','pvals_adj']}); df.to_csv(""/home/Akila/integration/harmony/subset/celltype/find_markergenes.csv"")`; ; Any idea how to get in the single file along with pts??; ; Thanks; Akila",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375
https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375:75,Testability,log,log,75,"Hello ; I am also facing the same problem.; I would like to get gene name, log fold change, pval_adj, pts.pts_rest in a single output CSV file but i couldn't able to do that; ` ; sc.tl.rank_genes_groups(adata,""leiden_0.6"", method='t-test',pts=True,corr_method='benjamini-hochberg'); pd.DataFrame(adata.uns['rank_genes_groups']['names']); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; df= pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names','logfoldchanges','pts','pts_rest','pvals','pvals_adj']}); df.to_csv(""/home/Akila/integration/harmony/subset/celltype/find_markergenes.csv"")`; ; Any idea how to get in the single file along with pts??; ; Thanks; Akila",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375
https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375:233,Testability,test,test,233,"Hello ; I am also facing the same problem.; I would like to get gene name, log fold change, pval_adj, pts.pts_rest in a single output CSV file but i couldn't able to do that; ` ; sc.tl.rank_genes_groups(adata,""leiden_0.6"", method='t-test',pts=True,corr_method='benjamini-hochberg'); pd.DataFrame(adata.uns['rank_genes_groups']['names']); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; df= pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names','logfoldchanges','pts','pts_rest','pvals','pvals_adj']}); df.to_csv(""/home/Akila/integration/harmony/subset/celltype/find_markergenes.csv"")`; ; Any idea how to get in the single file along with pts??; ; Thanks; Akila",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375
https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375:521,Testability,log,logfoldchanges,521,"Hello ; I am also facing the same problem.; I would like to get gene name, log fold change, pval_adj, pts.pts_rest in a single output CSV file but i couldn't able to do that; ` ; sc.tl.rank_genes_groups(adata,""leiden_0.6"", method='t-test',pts=True,corr_method='benjamini-hochberg'); pd.DataFrame(adata.uns['rank_genes_groups']['names']); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; df= pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names','logfoldchanges','pts','pts_rest','pvals','pvals_adj']}); df.to_csv(""/home/Akila/integration/harmony/subset/celltype/find_markergenes.csv"")`; ; Any idea how to get in the single file along with pts??; ; Thanks; Akila",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375
https://github.com/scverse/scanpy/issues/1455#issuecomment-1225360304:253,Testability,log,logfoldchanges,253,"Try the following code:; # Differential expression and marker genes; result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; df1 = pd.DataFrame({group+'_' + key:result[key][group] for group in groups for key in ['names','scores','logfoldchanges','pvals','pvals_adj']}); df2 = pd.DataFrame({group+'_' + key:result[key][group] for group in groups for key in ['pts','pts_rest']}); pd.concat([df1[[group+'_names',group+'_scores',group+'_logfoldchanges',group+'_pvals',group+'_pvals_adj']].merge(df2[[group+""_pts"",group+""_pts_rest""]],how=""left"",left_on=group+""_names"",right_index=True) for group in groups],axis=1).to_csv(""markers.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1225360304
https://github.com/scverse/scanpy/pull/1456#issuecomment-709197166:0,Usability,Simpl,Simply,0,"Simply leaving this out, solves it and guarantees exactness. Imho, there's no reason to do this type conversion. It's, the threshold value, thus just a single value that gets converted to float32, which won't yield any considerable speedup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1456#issuecomment-709197166
https://github.com/scverse/scanpy/issues/1457#issuecomment-709382258:398,Testability,log,logg,398,"in the source file(highly_variable_genes.py); ```; dispersion_norm = df['dispersion_norm'].values.astype('float32'); if n_top_genes is not None:; dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]; dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower; disp_cut_off = dispersion_norm[n_top_genes-1]; gene_subset = df['dispersion_norm'].values >= disp_cut_off; logg.debug(; f'the {n_top_genes} top genes correspond to a '; f'normalized dispersion cutoff of {disp_cut_off}'; ); ``` ; the ` df['dispersion_norm']` is the type of `np.float64`,but the `dispersion_norm` is `np.float32`,so the `disp_cut_off` is `np.float32`,but in line ; `gene_subset = df['dispersion_norm'].values >= disp_cut_off` , ; It means the array of `float64` will compare with `float32`, it is not accurate sometimes.; in my simulation code, the dubug result is follows; ```; disp_cut_off; 0.5938217. type(disp_cut_off); <class 'numpy.float32'>. df['dispersion_norm'][44]; 0.593821696856851. type(df['dispersion_norm'][44]); <class 'numpy.float64'>. df['dispersion_norm'][44]>=disp_cut_off; False; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1457#issuecomment-709382258
https://github.com/scverse/scanpy/issues/1460#issuecomment-718760332:298,Availability,error,error,298,"Hi @SNRNS, . `sc.pl.umap` looks for the UMAP coordinates that should be stored in `adata.obsm`, specifically it looks for coordinates in `adata.obsm['X_umap']`. These coordinates are computed and stored by `sc.tl.umap`. If `adata.obsm['X_umap']` does not exist the plotting function does return an error which I think is pretty self-explanatory:. `KeyError: ""Could not find entry in 'obsm' for 'umap'.\nAvailable keys are: ['X_pca'].""`. It must be that you have some coordinates already stored in `adata.obsm['X_umap']`, this could happen because you have subset the current adata from a bigger one, for which you had computed the UMAP, or maybe you got the data already preprocessed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1460#issuecomment-718760332
https://github.com/scverse/scanpy/pull/1464#issuecomment-717201928:44,Deployability,release,release,44,@LuckyMD Could you also add this fix to the release notes?; https://github.com/theislab/scanpy/blob/master/docs/release-latest.rst,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1464#issuecomment-717201928
https://github.com/scverse/scanpy/pull/1464#issuecomment-717201928:112,Deployability,release,release-latest,112,@LuckyMD Could you also add this fix to the release notes?; https://github.com/theislab/scanpy/blob/master/docs/release-latest.rst,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1464#issuecomment-717201928
https://github.com/scverse/scanpy/issues/1467#issuecomment-715475234:308,Availability,error,error,308,Please check this issue: #456 . Your data in `adata.raw` are probably `np.matrix`. You can either format to `np.ndarray` or to `scipy.sparse.csr_matrix()` to solve this. Note you are using `adata.raw.X` and not `adata.X` in `rank_genes_groups()` by default. So your proposed line of code will not solve your error. Please instead use for example:. `adata.raw.X = scipy.sparse.csr_matrix(adata.raw.X)`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1467#issuecomment-715475234
https://github.com/scverse/scanpy/issues/1468#issuecomment-716156261:33,Deployability,install,install,33,"Hi, if you use conda, try `conda install pytables`.; If you don't, try installing from the corresponding wheel here https://www.lfd.uci.edu/~gohlke/pythonlibs/#pytables",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716156261
https://github.com/scverse/scanpy/issues/1468#issuecomment-716156261:71,Deployability,install,installing,71,"Hi, if you use conda, try `conda install pytables`.; If you don't, try installing from the corresponding wheel here https://www.lfd.uci.edu/~gohlke/pythonlibs/#pytables",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716156261
https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232:134,Deployability,install,installing,134,"@Koncopd Hi, Thank you for the pointer. It seems to be a problem caused by pytables package. But I still couldn't import tables after installing and uninstalling pytables packages for many times. And I'm in Windows system.; (base) C:\Users\yuhong>python; ```; (base) C:\Users\yuhong>conda list | grep pytables; pytables 3.6.1 py37h14417ae_3 conda-forge ; Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)] on win32; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import tables; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\tables\__init__.py"", line 99, in <module>; from .utilsextension import (; ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232
https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232:790,Performance,load,load,790,"@Koncopd Hi, Thank you for the pointer. It seems to be a problem caused by pytables package. But I still couldn't import tables after installing and uninstalling pytables packages for many times. And I'm in Windows system.; (base) C:\Users\yuhong>python; ```; (base) C:\Users\yuhong>conda list | grep pytables; pytables 3.6.1 py37h14417ae_3 conda-forge ; Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)] on win32; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import tables; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\tables\__init__.py"", line 99, in <module>; from .utilsextension import (; ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232
https://github.com/scverse/scanpy/issues/1468#issuecomment-716540173:54,Deployability,install,installing,54,You can also try creating a new conda environment and installing pytables there at first and try to import it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716540173
https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:103,Deployability,install,install,103,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584
https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:164,Deployability,install,installation,164,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584
https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:224,Deployability,install,installation,224,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584
https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:256,Deployability,install,installing,256,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584
https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:280,Deployability,install,install,280,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584
https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:429,Integrability,message,message,429,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584
https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:177,Usability,guid,guide,177,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584
https://github.com/scverse/scanpy/issues/1469#issuecomment-725916572:566,Testability,log,log,566,"It's equivalent to the CLR transform. However, I'm not sure if this is the right way to go. CLR was used in a formative paper on the topic (doi:10.1038/nmeth.4380), and it's been brought forward from there. Here's that explanation for it's use:. <img width=""559"" alt=""CBMC antibody-derived tag normalization and clustering. Since each ADT count for a given cell can be interpreted as part of a whole (all ADT counts assigned to that cell), and there are only 13 components in this experiment, we treated this data type as compositional data and applied the centered log ratio (CLR) transformation"" src=""https://user-images.githubusercontent.com/8238804/98912624-75c2a500-251a-11eb-92e0-6d1d9004e3e2.png"">. I'm not convinced the UMIs from antibodies are more compositional than UMIs from mRNA, especially considering they are captured and amplified together.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1469#issuecomment-725916572
https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290:436,Testability,log,log,436,"I'm also a bit confused about how the CLR is applied. In the CITE-seq paper, I think it was done within a cell (over proteins), then I think they had switched to within a protein (over cells), and now in Seurat v4 it appears to be back to within a cell. Any per cell normalization is a bit tricky because the panels will differ between datasets as well as the titration of antibodies used. The simplest thing to me seems to be a simple log transformation combined with per protein scaling, as values between proteins are not comparable to begin with. We have some additional thoughts in the appendix of our totalVI paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290
https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290:394,Usability,simpl,simplest,394,"I'm also a bit confused about how the CLR is applied. In the CITE-seq paper, I think it was done within a cell (over proteins), then I think they had switched to within a protein (over cells), and now in Seurat v4 it appears to be back to within a cell. Any per cell normalization is a bit tricky because the panels will differ between datasets as well as the titration of antibodies used. The simplest thing to me seems to be a simple log transformation combined with per protein scaling, as values between proteins are not comparable to begin with. We have some additional thoughts in the appendix of our totalVI paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290
https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290:429,Usability,simpl,simple,429,"I'm also a bit confused about how the CLR is applied. In the CITE-seq paper, I think it was done within a cell (over proteins), then I think they had switched to within a protein (over cells), and now in Seurat v4 it appears to be back to within a cell. Any per cell normalization is a bit tricky because the panels will differ between datasets as well as the titration of antibodies used. The simplest thing to me seems to be a simple log transformation combined with per protein scaling, as values between proteins are not comparable to begin with. We have some additional thoughts in the appendix of our totalVI paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1469#issuecomment-729339290
https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596:297,Deployability,install,install,297,"Thanks a lot for submitting a PR!!!; Will add as reviewer core dev @fidelram to see what he thinks. It's a bit an ad-hoc since it's an additional argument just for this function. Meanwhile, for failing tests, can you reformat with black? That seems the reason why tests are failing.; ```bash; pip install black #if you don't have it installed; black scanpy/plotting/_tools/scatterplots.py; ```; or you can do the same from within whatever IDE you work with. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596
https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596:333,Deployability,install,installed,333,"Thanks a lot for submitting a PR!!!; Will add as reviewer core dev @fidelram to see what he thinks. It's a bit an ad-hoc since it's an additional argument just for this function. Meanwhile, for failing tests, can you reformat with black? That seems the reason why tests are failing.; ```bash; pip install black #if you don't have it installed; black scanpy/plotting/_tools/scatterplots.py; ```; or you can do the same from within whatever IDE you work with. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596
https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596:202,Testability,test,tests,202,"Thanks a lot for submitting a PR!!!; Will add as reviewer core dev @fidelram to see what he thinks. It's a bit an ad-hoc since it's an additional argument just for this function. Meanwhile, for failing tests, can you reformat with black? That seems the reason why tests are failing.; ```bash; pip install black #if you don't have it installed; black scanpy/plotting/_tools/scatterplots.py; ```; or you can do the same from within whatever IDE you work with. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596
https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596:264,Testability,test,tests,264,"Thanks a lot for submitting a PR!!!; Will add as reviewer core dev @fidelram to see what he thinks. It's a bit an ad-hoc since it's an additional argument just for this function. Meanwhile, for failing tests, can you reformat with black? That seems the reason why tests are failing.; ```bash; pip install black #if you don't have it installed; black scanpy/plotting/_tools/scatterplots.py; ```; or you can do the same from within whatever IDE you work with. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1470#issuecomment-718970596
https://github.com/scverse/scanpy/pull/1470#issuecomment-724573518:286,Availability,avail,available,286,"Two ideas: ; 1. pass to `embedding` `show=False, save=False` and then call `save_fig_or_show` within `pca` with the user values for `show` and `save`.; 2. allow `embedding` to rename any axis using a parameter like `component_labels`. Thus, this feature is not specific to PCA but also available to any other embedding.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1470#issuecomment-724573518
https://github.com/scverse/scanpy/issues/1471#issuecomment-776889361:86,Integrability,message,message,86,"closed with #1472 , next time @danielStrobl you could just do `git commit -m ""<commit-message> #1471""` and it would be automatically linked (and then closed if PR is merged)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1471#issuecomment-776889361
https://github.com/scverse/scanpy/pull/1472#issuecomment-719667764:46,Deployability,release,release,46,I'm guessing this should also be added to the release notes @Koncopd?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-719667764
https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:34,Availability,error,error,34,"Even I changed it manually, still error with CERTIFICATE_VERIFY_FAILED:. ```; >>> import scanpy as sc; >>> adata_ref = sc.datasets.pbmc3k_processed(); pbmc3k_processed.h5ad: 0.00B [00:00, ?B/s]; Traceback (most recent call last):; File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1317, in do_open; encode_chunked=req.has_header('Transfer-encoding')); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1229, in request; self._send_request(method, url, body, headers, encode_chunked); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1275, in _send_request; self.endheaders(body, encode_chunked=encode_chunked); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1224, in endheaders; self._send_output(message_body, encode_chunked=encode_chunked); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1016, in _send_output; self.send(msg); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 956, in send; self.connect(); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1392, in connect; server_hostname=server_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 412, in wrap_socket; session=session; File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 853, in _create; self.do_handshake(); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 1117, in do_handshake; self._sslobj.do_handshake(); ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665
https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:3520,Availability,error,error,3520,"ception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile_present_and_download; _download(backup_url, path); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 936, in _download; urlopen(Request(url, headers={""User-agent"": ""scanpy-user""})) as resp:; File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 222, in urlopen; return opener.open(url, data, timeout); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 525, in open; response = self._open(req, data); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 543, in _open; '_open', req); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 503, in _call_chain; result = func(*args); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1360, in https_open; context=self._context, check_hostname=self._check_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1319, in do_open; raise URLError(err); urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665
https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:3545,Availability,error,error,3545,"ception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile_present_and_download; _download(backup_url, path); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 936, in _download; urlopen(Request(url, headers={""User-agent"": ""scanpy-user""})) as resp:; File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 222, in urlopen; return opener.open(url, data, timeout); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 525, in open; response = self._open(req, data); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 543, in _open; '_open', req); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 503, in _call_chain; result = func(*args); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1360, in https_open; context=self._context, check_hostname=self._check_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1319, in do_open; raise URLError(err); urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665
https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:1863,Integrability,wrap,wrapper,1863,"line 1016, in _send_output; self.send(msg); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 956, in send; self.connect(); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1392, in connect; server_hostname=server_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 412, in wrap_socket; session=session; File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 853, in _create; self.do_handshake(); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 1117, in do_handshake; self._sslobj.do_handshake(); ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile_present_and_download; _download(backup_url, path); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 936, in _download; urlopen(Request(url, headers={""User-agent"": ""scanpy-user""})) as resp:; File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 222, in urlopen; return opener.open(url, data, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665
https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:2896,Safety,timeout,timeout,2896,"ception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile_present_and_download; _download(backup_url, path); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 936, in _download; urlopen(Request(url, headers={""User-agent"": ""scanpy-user""})) as resp:; File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 222, in urlopen; return opener.open(url, data, timeout); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 525, in open; response = self._open(req, data); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 543, in _open; '_open', req); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 503, in _call_chain; result = func(*args); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1360, in https_open; context=self._context, check_hostname=self._check_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1319, in do_open; raise URLError(err); urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665
https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:1538,Security,certificate,certificate,1538,"envs/scIB-python/lib/python3.7/http/client.py"", line 1275, in _send_request; self.endheaders(body, encode_chunked=encode_chunked); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1224, in endheaders; self._send_output(message_body, encode_chunked=encode_chunked); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1016, in _send_output; self.send(msg); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 956, in send; self.connect(); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1392, in connect; server_hostname=server_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 412, in wrap_socket; session=session; File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 853, in _create; self.do_handshake(); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 1117, in do_handshake; self._sslobj.do_handshake(); ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665
https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:1592,Security,certificate,certificate,1592,"envs/scIB-python/lib/python3.7/http/client.py"", line 1275, in _send_request; self.endheaders(body, encode_chunked=encode_chunked); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1224, in endheaders; self._send_output(message_body, encode_chunked=encode_chunked); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1016, in _send_output; self.send(msg); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 956, in send; self.connect(); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1392, in connect; server_hostname=server_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 412, in wrap_socket; session=session; File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 853, in _create; self.do_handshake(); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 1117, in do_handshake; self._sslobj.do_handshake(); ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665
https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:3584,Security,certificate,certificate,3584,"ception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile_present_and_download; _download(backup_url, path); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 936, in _download; urlopen(Request(url, headers={""User-agent"": ""scanpy-user""})) as resp:; File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 222, in urlopen; return opener.open(url, data, timeout); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 525, in open; response = self._open(req, data); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 543, in _open; '_open', req); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 503, in _call_chain; result = func(*args); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1360, in https_open; context=self._context, check_hostname=self._check_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1319, in do_open; raise URLError(err); urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665
https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:3638,Security,certificate,certificate,3638,"ception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile_present_and_download; _download(backup_url, path); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 936, in _download; urlopen(Request(url, headers={""User-agent"": ""scanpy-user""})) as resp:; File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 222, in urlopen; return opener.open(url, data, timeout); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 525, in open; response = self._open(req, data); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 543, in _open; '_open', req); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 503, in _call_chain; result = func(*args); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1360, in https_open; context=self._context, check_hostname=self._check_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 1319, in do_open; raise URLError(err); urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056)>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665
https://github.com/scverse/scanpy/pull/1474#issuecomment-722610470:0,Availability,Ping,Ping,0,Ping. This is a needed in the https://github.com/clara-parabricks/rapids-single-cell-examples so that we can make our notebooks reproducible. It's a trivial addition to propagate the `random_state` argument to the RAPIDS UMAP estimator. Any chance this can be considered for review?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1474#issuecomment-722610470
https://github.com/scverse/scanpy/pull/1476#issuecomment-723092789:77,Availability,error,error,77,@Koncopd @ivirshup can you have a look at this? CI fails for some weird docs error that we can't nail down.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-723092789
https://github.com/scverse/scanpy/pull/1476#issuecomment-723092789:102,Availability,down,down,102,@Koncopd @ivirshup can you have a look at this? CI fails for some weird docs error that we can't nail down.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-723092789
https://github.com/scverse/scanpy/pull/1476#issuecomment-726918992:24,Usability,feedback,feedback,24,Thanks for all the good feedback @ivirshup - I'll work on it and re-request review when I'm done.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-726918992
https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553:304,Availability,avail,available,304,"Okay @ivirshup , think I've addressed your comments:. - old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). New sce.pp.scrublet now the main exposed function, with scrublet_simulate_doublets() function available for advanced users.; - plot function moved to scanpy/external/pl.py as scrublet_score_distribution().; - functions linked via 'See also' sections.; - tests added for 'scrublet()' and scrublet_simlulate_doublets().",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553
https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553:88,Security,expose,exposed,88,"Okay @ivirshup , think I've addressed your comments:. - old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). New sce.pp.scrublet now the main exposed function, with scrublet_simulate_doublets() function available for advanced users.; - plot function moved to scanpy/external/pl.py as scrublet_score_distribution().; - functions linked via 'See also' sections.; - tests added for 'scrublet()' and scrublet_simlulate_doublets().",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553
https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553:243,Security,expose,exposed,243,"Okay @ivirshup , think I've addressed your comments:. - old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). New sce.pp.scrublet now the main exposed function, with scrublet_simulate_doublets() function available for advanced users.; - plot function moved to scanpy/external/pl.py as scrublet_score_distribution().; - functions linked via 'See also' sections.; - tests added for 'scrublet()' and scrublet_simlulate_doublets().",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553
https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553:187,Testability,log,logic,187,"Okay @ivirshup , think I've addressed your comments:. - old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). New sce.pp.scrublet now the main exposed function, with scrublet_simulate_doublets() function available for advanced users.; - plot function moved to scanpy/external/pl.py as scrublet_score_distribution().; - functions linked via 'See also' sections.; - tests added for 'scrublet()' and scrublet_simlulate_doublets().",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553
https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553:464,Testability,test,tests,464,"Okay @ivirshup , think I've addressed your comments:. - old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). New sce.pp.scrublet now the main exposed function, with scrublet_simulate_doublets() function available for advanced users.; - plot function moved to scanpy/external/pl.py as scrublet_score_distribution().; - functions linked via 'See also' sections.; - tests added for 'scrublet()' and scrublet_simlulate_doublets().",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-727953553
https://github.com/scverse/scanpy/pull/1476#issuecomment-728104293:18,Testability,test,tests,18,"I checked the new tests locally, can't work out why they're not running in the CI here- hopefully it's obvious to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-728104293
https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:1025,Availability,down,down,1025,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013
https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:34,Security,expose,exposed,34,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013
https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:133,Testability,log,logic,133,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013
https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:177,Usability,clear,clear,177,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013
https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013:742,Usability,simpl,simple,742,"> old 'scrublet' function now not exposed, has become an internal _scrublet_call_doublets (I like it still being separate, makes the logic easier to read). Oh, I think I wasn't clear here. I was thinking that there would be three doublet calling functions:. 1. Simulate doublets. Receives count anndata, returns simulated doublet count anndata.; 2. Given two anndata objects, one source data, one simulated, call doublets in the source data. It's assumed both objects have already been normalized.; 3. The full workflow. Takes an AnnData object with count data, simulates doublets, runs normalization on both, and then calls doublets on the source object. Uses the previous two functions as well as the normalization workflow internally. The simple use case is just to call function 3. The advanced use case is to use function 2, potentially with data from function 1, or generated some other way. The advanced use case also allows you to use your own normalization. By not giving function 2 the ability to normalize, we cut down on arguments, and have more modular functions. What do you think of that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730939013
https://github.com/scverse/scanpy/pull/1476#issuecomment-730987904:88,Testability,log,logic,88,"> What do you think of that?. I actually prefer my current layout- it accomplishes your logic without duplication of arguments etc, and from a user perspective I prefer having one main function I can use in the two ways (one anndata you get 3., Two anndata you get 2). But I'm also very happy for you to tweak things to match your vision!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-730987904
https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:75,Deployability,install,installed,75,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707
https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:198,Deployability,install,install,198,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707
https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:323,Deployability,Install,Installing,323,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707
https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:340,Integrability,depend,dependencies,340,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707
https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:18,Testability,test,tests,18,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707
https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:177,Testability,log,logs,177,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707
https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:212,Testability,test,test,212,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707
https://github.com/scverse/scanpy/pull/1476#issuecomment-734715153:77,Deployability,install,installed,77,"> @pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:; > You'll need to add a scrublet entry to `extras_require` here:. Ahh, I see- thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734715153
https://github.com/scverse/scanpy/pull/1476#issuecomment-734715153:20,Testability,test,tests,20,"> @pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:; > You'll need to add a scrublet entry to `extras_require` here:. Ahh, I see- thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734715153
https://github.com/scverse/scanpy/pull/1476#issuecomment-734715153:179,Testability,log,logs,179,"> @pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:; > You'll need to add a scrublet entry to `extras_require` here:. Ahh, I see- thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734715153
https://github.com/scverse/scanpy/pull/1476#issuecomment-734758379:0,Testability,Test,Tests,0,Tests passing! With exception of silly Black formatting issue I've just corrected.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734758379
https://github.com/scverse/scanpy/issues/1477#issuecomment-834394495:210,Modifiability,config,config,210,"Hey @ivirshup, . `hasattr(__builtins__, ""__IPYTHON__"")` now seems to always return False, even if it's True when run from within the notebook. That causes figures to end up very blurry due to the missing png2x config. Am I missing anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477#issuecomment-834394495
https://github.com/scverse/scanpy/issues/1477#issuecomment-835965024:498,Deployability,update,updated,498,"> `hasattr(__builtins__, ""__IPYTHON__"")` now seems to always return False. I'm not seeing this behaviour, could you check what versions you're using?. <details>; <summary> My versions </summary>. ```; -----; sinfo 0.3.1; -----; IPython 7.23.1; jupyter_client 6.1.11; jupyter_core 4.7.0; jupyterlab 2.2.9; notebook 6.3.0; -----; Python 3.8.9 (default, Apr 3 2021, 01:50:09) [Clang 12.0.0 (clang-1200.0.32.29)]; macOS-10.15.7-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2021-05-10 10:13; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477#issuecomment-835965024
https://github.com/scverse/scanpy/issues/1477#issuecomment-835965024:446,Testability,log,logical,446,"> `hasattr(__builtins__, ""__IPYTHON__"")` now seems to always return False. I'm not seeing this behaviour, could you check what versions you're using?. <details>; <summary> My versions </summary>. ```; -----; sinfo 0.3.1; -----; IPython 7.23.1; jupyter_client 6.1.11; jupyter_core 4.7.0; jupyterlab 2.2.9; notebook 6.3.0; -----; Python 3.8.9 (default, Apr 3 2021, 01:50:09) [Clang 12.0.0 (clang-1200.0.32.29)]; macOS-10.15.7-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2021-05-10 10:13; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477#issuecomment-835965024
https://github.com/scverse/scanpy/issues/1477#issuecomment-839715367:484,Testability,log,logical,484,"```; from scanpy._settings import ScanpyConfig; ScanpyConfig._is_run_from_ipython(); ```; returns `False` when running from within a notebook (confirmed by 3 other colleagues), and causes non-vectorized figures. <details><summary> Versions </summary>. ```; sinfo 0.3.1; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.15; notebook 6.3.0; -----; Python 3.8.8 (default, Feb 24 2021, 21:46:12) [GCC 7.3.0]; Linux-5.4.0-1038-aws-x86_64-with-glibc2.10; 16 logical CPU cores, x86_64; ```; </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477#issuecomment-839715367
https://github.com/scverse/scanpy/issues/1478#issuecomment-735364926:172,Deployability,install,installing,172,"I too have the exact same issue with scanpy.tl.rank_genes_groups(). Is this something to do with anndata 0.7.5 requires pandas version >=1.0, yet, pandas coming along with installing scanpy with 'pip install scanpy' is in version 0.23.4?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1478#issuecomment-735364926
https://github.com/scverse/scanpy/issues/1478#issuecomment-735364926:200,Deployability,install,install,200,"I too have the exact same issue with scanpy.tl.rank_genes_groups(). Is this something to do with anndata 0.7.5 requires pandas version >=1.0, yet, pandas coming along with installing scanpy with 'pip install scanpy' is in version 0.23.4?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1478#issuecomment-735364926
https://github.com/scverse/scanpy/issues/1478#issuecomment-1370868127:7,Deployability,install,install,7,Please install the latest scanpy in a clean environment. Feel free to reopen if this issue persists.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1478#issuecomment-1370868127
https://github.com/scverse/scanpy/issues/1479#issuecomment-723037276:474,Usability,clear,clear,474,"Thank you, @giovp . I see. So when using heatmap, the mapping of marker genes must represent all clusters.; It won't work if markers are supplied for only a subset of the clusters. As far as I can tell, though, this is not apparent from the documentation of the function `help(sc.pl.heatmap)`.; Also, in the plotting tutorial, which I referred to in the beginning, only a subset of markers are supplied.; So, either the documentation and tutorial should be adjusted to make clear that markers must be supplied for all clusters. Or, which I would find great, heatmap could be adjusted to be able to display markers for a subset of the clusters, which I had assumed to be the case in the beginning. Btw. from looking at the tutorial, I believe the same issue may apply to some other plotting functions, e.g. for [Tracksplot](https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html#Tracksplot)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723037276
https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024:1700,Availability,error,error,1700,"Mmh no, it does work as you expect, just need to turn the dendrogram off.; ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_dict = {; ""1"": [""GNLY"", ""NKG7""],; ""0"": [""CD3D""],; ""2"": [""CD79A"", ""MS4A1""],; ""4"": [""CD79A"", ""MS4A1""],; ""3"": [""FCER1A""],; }. sc.pl.heatmap(; pbmc,; marker_genes_dict,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=False,; swap_axes=True,; ); ```. or just pass the list of markers (list, not mapping); ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_list = [""GNLY"", ""NKG7""]. sc.pl.heatmap(; pbmc,; marker_genes_list,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=True,; swap_axes=True,; ); ```. If you pass a dict with incomplete annotation and request dendrogram, then it fails, and the warning says this clearly:; ```; WARNING: Groups are not reordered because the `groupby` categories and the `var_group_labels` are different.; categories: 0, 1, 2, etc.; var_group_labels: 1, 0, 2, etc.; ```; and it still produces a plot (yet unordered). The `var_group_labels` is also described in `help(sc.pl.heatmap)` as you pointed out. I think the misunderstanding is that passing a mapping or a list the behaviour is different, although potentially expected since a mapping and a list are different things. This could probably be explained clearer in the `var_names` argument yes. Just to go back to your original problem, in your case you were using as mapping categories that were not present in your `groupby` key altogether. This is a different issue, and probably the function should have thrown an error saying `var_group_labels` are not present in `categories`. . If you feel like opening a PR for this, we would really appreciate!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024
https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024:909,Usability,clear,clearly,909,"Mmh no, it does work as you expect, just need to turn the dendrogram off.; ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_dict = {; ""1"": [""GNLY"", ""NKG7""],; ""0"": [""CD3D""],; ""2"": [""CD79A"", ""MS4A1""],; ""4"": [""CD79A"", ""MS4A1""],; ""3"": [""FCER1A""],; }. sc.pl.heatmap(; pbmc,; marker_genes_dict,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=False,; swap_axes=True,; ); ```. or just pass the list of markers (list, not mapping); ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_list = [""GNLY"", ""NKG7""]. sc.pl.heatmap(; pbmc,; marker_genes_list,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=True,; swap_axes=True,; ); ```. If you pass a dict with incomplete annotation and request dendrogram, then it fails, and the warning says this clearly:; ```; WARNING: Groups are not reordered because the `groupby` categories and the `var_group_labels` are different.; categories: 0, 1, 2, etc.; var_group_labels: 1, 0, 2, etc.; ```; and it still produces a plot (yet unordered). The `var_group_labels` is also described in `help(sc.pl.heatmap)` as you pointed out. I think the misunderstanding is that passing a mapping or a list the behaviour is different, although potentially expected since a mapping and a list are different things. This could probably be explained clearer in the `var_names` argument yes. Just to go back to your original problem, in your case you were using as mapping categories that were not present in your `groupby` key altogether. This is a different issue, and probably the function should have thrown an error saying `var_group_labels` are not present in `categories`. . If you feel like opening a PR for this, we would really appreciate!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024
https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024:1436,Usability,clear,clearer,1436,"Mmh no, it does work as you expect, just need to turn the dendrogram off.; ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_dict = {; ""1"": [""GNLY"", ""NKG7""],; ""0"": [""CD3D""],; ""2"": [""CD79A"", ""MS4A1""],; ""4"": [""CD79A"", ""MS4A1""],; ""3"": [""FCER1A""],; }. sc.pl.heatmap(; pbmc,; marker_genes_dict,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=False,; swap_axes=True,; ); ```. or just pass the list of markers (list, not mapping); ```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=1). marker_genes_list = [""GNLY"", ""NKG7""]. sc.pl.heatmap(; pbmc,; marker_genes_list,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=True,; swap_axes=True,; ); ```. If you pass a dict with incomplete annotation and request dendrogram, then it fails, and the warning says this clearly:; ```; WARNING: Groups are not reordered because the `groupby` categories and the `var_group_labels` are different.; categories: 0, 1, 2, etc.; var_group_labels: 1, 0, 2, etc.; ```; and it still produces a plot (yet unordered). The `var_group_labels` is also described in `help(sc.pl.heatmap)` as you pointed out. I think the misunderstanding is that passing a mapping or a list the behaviour is different, although potentially expected since a mapping and a list are different things. This could probably be explained clearer in the `var_names` argument yes. Just to go back to your original problem, in your case you were using as mapping categories that were not present in your `groupby` key altogether. This is a different issue, and probably the function should have thrown an error saying `var_group_labels` are not present in `categories`. . If you feel like opening a PR for this, we would really appreciate!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723051024
https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:223,Availability,error,error,223,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522
https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:740,Availability,error,error,740,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522
https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:1009,Availability,error,error,1009,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522
https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:1212,Deployability,pipeline,pipeline,1212,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522
https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:1022,Integrability,message,message,1022,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522
https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:1455,Safety,avoid,avoided,1455,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522
https://github.com/scverse/scanpy/issues/1479#issuecomment-723090290:97,Availability,error,error,97,"yes very good point, the tutorial needs to be fixed for sure because now it would fail.; trowing error in the heatmap is also probably the best way to go.; Thank you for reporting this! I'll have a look as soon as I have time",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723090290
https://github.com/scverse/scanpy/pull/1483#issuecomment-722025417:32,Security,hash,hashsolo,32,"@njbernstein I had also noticed hashsolo wasn't added to the documentation, which you can do by adding `pp.hashsolo` [here](https://github.com/theislab/scanpy/blob/master/scanpy/external/__init__.py). Though I'm unsure which heading it belongs under.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-722025417
https://github.com/scverse/scanpy/pull/1483#issuecomment-722025417:107,Security,hash,hashsolo,107,"@njbernstein I had also noticed hashsolo wasn't added to the documentation, which you can do by adding `pp.hashsolo` [here](https://github.com/theislab/scanpy/blob/master/scanpy/external/__init__.py). Though I'm unsure which heading it belongs under.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-722025417
https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260:86,Deployability,integrat,integration,86,@adamgayoso I don't think it fits under the other preprocessing tool headings of Data integration or Imputation. Maybe add a new one called Call hashing or Sample demultiplexing. @fidelram thoughts? Not sure who else to tag,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260
https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260:86,Integrability,integrat,integration,86,@adamgayoso I don't think it fits under the other preprocessing tool headings of Data integration or Imputation. Maybe add a new one called Call hashing or Sample demultiplexing. @fidelram thoughts? Not sure who else to tag,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260
https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260:145,Security,hash,hashing,145,@adamgayoso I don't think it fits under the other preprocessing tool headings of Data integration or Imputation. Maybe add a new one called Call hashing or Sample demultiplexing. @fidelram thoughts? Not sure who else to tag,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260
https://github.com/scverse/scanpy/pull/1483#issuecomment-723850755:54,Safety,detect,detection,54,"I think a new section would good here, maybe ""Doublet detection""?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-723850755
https://github.com/scverse/scanpy/pull/1483#issuecomment-724181103:26,Safety,detect,detection,26,"@ivirshup I think doublet detection is a little too narrow in that we can also associate cells to the original sample they came from. Although many folks use cell hashing solely for the purpose of doublet detection. I'd prefer ""Sample demultiplexing"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-724181103
https://github.com/scverse/scanpy/pull/1483#issuecomment-724181103:205,Safety,detect,detection,205,"@ivirshup I think doublet detection is a little too narrow in that we can also associate cells to the original sample they came from. Although many folks use cell hashing solely for the purpose of doublet detection. I'd prefer ""Sample demultiplexing"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-724181103
https://github.com/scverse/scanpy/pull/1483#issuecomment-724181103:163,Security,hash,hashing,163,"@ivirshup I think doublet detection is a little too narrow in that we can also associate cells to the original sample they came from. Although many folks use cell hashing solely for the purpose of doublet detection. I'd prefer ""Sample demultiplexing"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-724181103
https://github.com/scverse/scanpy/pull/1483#issuecomment-726001024:153,Deployability,release,release,153,"I'd be happy with whatever, but it needs to go somewhere!. How about ""Demultiplexing/ Doublet detection"", so it can go with scrublet (#1476) in the next release, and potentially get split out later if more demultiplexing methods are added?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-726001024
https://github.com/scverse/scanpy/pull/1483#issuecomment-726001024:94,Safety,detect,detection,94,"I'd be happy with whatever, but it needs to go somewhere!. How about ""Demultiplexing/ Doublet detection"", so it can go with scrublet (#1476) in the next release, and potentially get split out later if more demultiplexing methods are added?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-726001024
https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:91,Availability,error,error,91,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623
https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:316,Availability,error,error,316,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623
https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:456,Availability,error,error,456,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623
https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:569,Availability,failure,failure,569,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623
https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:103,Integrability,message,message,103,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623
https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:462,Integrability,message,message,462,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623
https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:161,Testability,test,test,161,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623
https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:284,Testability,test,tests,284,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623
https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:631,Testability,test,test,631,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623
https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155:12,Modifiability,flexible,flexible,12,"If a linter flexible enough to enforce this existed, it would be great. The test should definitely exist, something in our code requires the docstrings to have that format, I just forgot which part. (But in any case it guarantees consistent formatting so that’s nice). #1492 should fix that test to ignore blank lines for the time being. Also isn’t it cool that it points exactly to the problematic line?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155
https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155:76,Testability,test,test,76,"If a linter flexible enough to enforce this existed, it would be great. The test should definitely exist, something in our code requires the docstrings to have that format, I just forgot which part. (But in any case it guarantees consistent formatting so that’s nice). #1492 should fix that test to ignore blank lines for the time being. Also isn’t it cool that it points exactly to the problematic line?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155
https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155:291,Testability,test,test,291,"If a linter flexible enough to enforce this existed, it would be great. The test should definitely exist, something in our code requires the docstrings to have that format, I just forgot which part. (But in any case it guarantees consistent formatting so that’s nice). #1492 should fix that test to ignore blank lines for the time being. Also isn’t it cool that it points exactly to the problematic line?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155
https://github.com/scverse/scanpy/issues/1485#issuecomment-722482475:189,Testability,test,tests,189,This issue has been mentioned on **Scanpy**. There might be relevant details there:. https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328/3,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1485#issuecomment-722482475
https://github.com/scverse/scanpy/issues/1485#issuecomment-722483698:56,Usability,feedback,feedback,56,"Hi @ivirshup,. Yes, both of them are me. I incorporated feedback from the help forum which suggested that this function has a bug, hence, the bug report.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1485#issuecomment-722483698
https://github.com/scverse/scanpy/issues/1485#issuecomment-722581382:56,Deployability,update,update,56,@BrianLohman ; I can't reproduce this. Could you please update scanpy and check?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1485#issuecomment-722581382
https://github.com/scverse/scanpy/issues/1486#issuecomment-723136057:46,Deployability,update,updated,46,"> Hi @rbf22 ,; > what's the issue here?. I’ve updated the issue. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1486#issuecomment-723136057
https://github.com/scverse/scanpy/issues/1487#issuecomment-726776364:153,Availability,error,error,153,"Hi Brian,; Did you do any subsetting of your adata between running `rank_genes_groups` and `filter_rank_genes_groups`? The only way I can reproduce your error is by removing genes from my adata in between the two commands.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1487#issuecomment-726776364
https://github.com/scverse/scanpy/issues/1487#issuecomment-726806853:196,Availability,error,error,196,"Okay, when I run this:; adata = sc.datasets.pbmc68k_reduced(); sc.tl.rank_genes_groups(adata, ""bulk_labels"", method=""wilcoxon""); sc.tl.filter_rank_genes_groups(adata, min_fold_change=3); I get no error. ; Can you find a reproducible example of your error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1487#issuecomment-726806853
https://github.com/scverse/scanpy/issues/1487#issuecomment-726806853:249,Availability,error,error,249,"Okay, when I run this:; adata = sc.datasets.pbmc68k_reduced(); sc.tl.rank_genes_groups(adata, ""bulk_labels"", method=""wilcoxon""); sc.tl.filter_rank_genes_groups(adata, min_fold_change=3); I get no error. ; Can you find a reproducible example of your error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1487#issuecomment-726806853
https://github.com/scverse/scanpy/issues/1487#issuecomment-848420774:69,Availability,error,error,69,@brianpenghe Did you find a solution for this? I am getting the same error in scanpy 1.7.0.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1487#issuecomment-848420774
https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186:663,Testability,test,test,663,"The biggest advantage would be the possibility to create such panels as shown in the example, where e.g. quality metrics have different cmaps as gene expression.; Another advantage would be that cmaps could be defined globally for each parameter, resulting in simpler plotting calls. ```; adata = sc.datasets.paul15(); adata.X = adata.X.astype('float64'); sc.pp.filter_cells(adata, min_genes=100); sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); adata.uns['n_counts_all_cmap'] = 'copper'; adata.uns['n_genes_cmap'] = 'copper'; sc.pl.pca(adata, color=['paul15_clusters', 'n_counts_all', 'n_genes', 'Zyx', 'calp80', 'slc43a2'], ncols=3); ```; ![test](https://user-images.githubusercontent.com/23263654/99387978-28a55100-28d5-11eb-975d-f91211370c16.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186
https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186:260,Usability,simpl,simpler,260,"The biggest advantage would be the possibility to create such panels as shown in the example, where e.g. quality metrics have different cmaps as gene expression.; Another advantage would be that cmaps could be defined globally for each parameter, resulting in simpler plotting calls. ```; adata = sc.datasets.paul15(); adata.X = adata.X.astype('float64'); sc.pp.filter_cells(adata, min_genes=100); sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); adata.uns['n_counts_all_cmap'] = 'copper'; adata.uns['n_genes_cmap'] = 'copper'; sc.pl.pca(adata, color=['paul15_clusters', 'n_counts_all', 'n_genes', 'Zyx', 'calp80', 'slc43a2'], ncols=3); ```; ![test](https://user-images.githubusercontent.com/23263654/99387978-28a55100-28d5-11eb-975d-f91211370c16.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-728884186
https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302:213,Deployability,continuous,continuous,213,"For some context, this has come up in discussion with cellxgene before: (https://github.com/chanzuckerberg/cellxgene/issues/1152#issuecomment-604286306). I think I still feel the same way about this. Basically, a continuous colormap is defined by more than just the name of the colorspace. There are parameters like maximum value, minimum value, middle value (for divergent colormaps), scale, and binning. I'm not sure how useful it is to keep just the color scheme without any of these other values. Why this parameter, and not others?. I'm not sure it's the right solution for the use case. I think that use case would be better fit by being able to generate all the plots individually, then collect them into a figure. This way you would have complete control over how the colormaps were applied to each of the continuous variables separately. Unfortunately, this isn't particularly ergonomic to do with matplotlib since individuals plots have to know about the `Figure` when constructed. Side issue: We probably don't want to save separate color palettes for each gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302
https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302:814,Deployability,continuous,continuous,814,"For some context, this has come up in discussion with cellxgene before: (https://github.com/chanzuckerberg/cellxgene/issues/1152#issuecomment-604286306). I think I still feel the same way about this. Basically, a continuous colormap is defined by more than just the name of the colorspace. There are parameters like maximum value, minimum value, middle value (for divergent colormaps), scale, and binning. I'm not sure how useful it is to keep just the color scheme without any of these other values. Why this parameter, and not others?. I'm not sure it's the right solution for the use case. I think that use case would be better fit by being able to generate all the plots individually, then collect them into a figure. This way you would have complete control over how the colormaps were applied to each of the continuous variables separately. Unfortunately, this isn't particularly ergonomic to do with matplotlib since individuals plots have to know about the `Figure` when constructed. Side issue: We probably don't want to save separate color palettes for each gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302
https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302:825,Modifiability,variab,variables,825,"For some context, this has come up in discussion with cellxgene before: (https://github.com/chanzuckerberg/cellxgene/issues/1152#issuecomment-604286306). I think I still feel the same way about this. Basically, a continuous colormap is defined by more than just the name of the colorspace. There are parameters like maximum value, minimum value, middle value (for divergent colormaps), scale, and binning. I'm not sure how useful it is to keep just the color scheme without any of these other values. Why this parameter, and not others?. I'm not sure it's the right solution for the use case. I think that use case would be better fit by being able to generate all the plots individually, then collect them into a figure. This way you would have complete control over how the colormaps were applied to each of the continuous variables separately. Unfortunately, this isn't particularly ergonomic to do with matplotlib since individuals plots have to know about the `Figure` when constructed. Side issue: We probably don't want to save separate color palettes for each gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302
https://github.com/scverse/scanpy/pull/1489#issuecomment-729656081:93,Modifiability,variab,variable,93,"Ok, makes sense.; What if I implement that `cmap `in `embedding` also accepts a `dict ` of `{variable: colormap}`?. `{'n_counts_all': 'copper', 'n_genes_cmap': matplotlib.colors.Colormap}`. It maps variable names to `str ` or `Colormap`. Therefore, the colormap can be processed before and is not stored in AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729656081
https://github.com/scverse/scanpy/pull/1489#issuecomment-729656081:198,Modifiability,variab,variable,198,"Ok, makes sense.; What if I implement that `cmap `in `embedding` also accepts a `dict ` of `{variable: colormap}`?. `{'n_counts_all': 'copper', 'n_genes_cmap': matplotlib.colors.Colormap}`. It maps variable names to `str ` or `Colormap`. Therefore, the colormap can be processed before and is not stored in AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729656081
https://github.com/scverse/scanpy/pull/1489#issuecomment-730146471:184,Deployability,continuous,continuous,184,"Using a dict is an interesting idea. Right now I'd prefer that it matches with other ""vectorized"" arguments (like `vmin`, `vmax`) which take a list. I think for categorical values the continuous arguments are ignored (right @fidelram?). Using a `dict` would get rid of the ""sometimes arguments are ignored"" part of this, but I think consistency is more important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-730146471
https://github.com/scverse/scanpy/pull/1490#issuecomment-725881193:51,Availability,error,error,51,"My preference would be to throw a more informative error. Something like: ""Could not calculate statistics for group {group_name} since it only contains one sample."". I don't like that groups would be implicitly excluded from the results. In general I would expect each category of `adata.obs[""cat""]` to be in the results of `sc.tl.rank_genes_groups(adata, ""cat"")`. If they can implicitly be excluded, I think that will lead to more confusing downstream behavior. Does that sound reasonable @pinin4fjords, @Koncopd?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-725881193
https://github.com/scverse/scanpy/pull/1490#issuecomment-725881193:442,Availability,down,downstream,442,"My preference would be to throw a more informative error. Something like: ""Could not calculate statistics for group {group_name} since it only contains one sample."". I don't like that groups would be implicitly excluded from the results. In general I would expect each category of `adata.obs[""cat""]` to be in the results of `sc.tl.rank_genes_groups(adata, ""cat"")`. If they can implicitly be excluded, I think that will lead to more confusing downstream behavior. Does that sound reasonable @pinin4fjords, @Koncopd?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-725881193
https://github.com/scverse/scanpy/pull/1490#issuecomment-725930096:79,Safety,detect,detection,79,"So for our workflows, where we pass the clustering step directly to the marker detection, it would be good to be able to get markers for non-singlet groups when we specify 'all', even where there are singlet groups. We can add a workaround to our CLI layer and in fact I'm doing that anyway since we need a fix quickly https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/88, but there I'm having to transiently edit .obs to remove the singlet clusters which seems messier (and we're not sure if that will have unintended consequences).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-725930096
https://github.com/scverse/scanpy/pull/1490#issuecomment-726016542:659,Deployability,release,released,659,"We should probably let users provide a set of clusters for the reference. . Until then, could you do:. ```python; groups_to_test = (; adata.obs[""clusters""]; .value_counts(); .loc[lambda x: x > 1]; .index; ); subset_adata = adata[adata.obs[""clusters""].isin(groups_to_test)].copy(); sc.tl.rank_genes_groups(subset_adata, ...); adata.uns[""rank_genes_groups""] = subset_adata.uns[""rank_genes_groups""]; ```. Or, if you want the singlets in the reference:. ```python; sc.tl.rank_genes_groups(adata, groups=list(groups_to_test), ...); ```. BTW, about the h5py bytes thing, things written as strings are read as strings with h5py 3 as of anndata 0.7.5, which just got released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726016542
https://github.com/scverse/scanpy/pull/1490#issuecomment-726165481:113,Availability,avail,available,113,"Thanks @ivirshup that worked nicely, I should have thought of it. I'll remove our h5py pin once anndata 0.7.5 is available on Conda (have you seen that it's failing right now? https://github.com/conda-forge/anndata-feedstock/pull/13)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726165481
https://github.com/scverse/scanpy/pull/1490#issuecomment-726498381:198,Availability,error,error,198,"Great!. I'm not sure what's going on with that conda build, but hopefully one of the maintainers there will know what to do. On the topic of this PR, I do think this case should give a more helpful error message. Would you like to do that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726498381
https://github.com/scverse/scanpy/pull/1490#issuecomment-726498381:204,Integrability,message,message,204,"Great!. I'm not sure what's going on with that conda build, but hopefully one of the maintainers there will know what to do. On the topic of this PR, I do think this case should give a more helpful error message. Would you like to do that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726498381
https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017:213,Availability,error,error,213,"> Great!; > ; > I'm not sure what's going on with that conda build, but hopefully one of the maintainers there will know what to do.; > ; > On the topic of this PR, I do think this case should give a more helpful error message. Would you like to do that?. Sure- there you go. I reverted the above and just raised an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017
https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017:316,Availability,error,error,316,"> Great!; > ; > I'm not sure what's going on with that conda build, but hopefully one of the maintainers there will know what to do.; > ; > On the topic of this PR, I do think this case should give a more helpful error message. Would you like to do that?. Sure- there you go. I reverted the above and just raised an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017
https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017:219,Integrability,message,message,219,"> Great!; > ; > I'm not sure what's going on with that conda build, but hopefully one of the maintainers there will know what to do.; > ; > On the topic of this PR, I do think this case should give a more helpful error message. Would you like to do that?. Sure- there you go. I reverted the above and just raised an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017
https://github.com/scverse/scanpy/pull/1490#issuecomment-727717155:50,Availability,error,error,50,Great! Could you also add a test to make sure the error is being thrown? And does this handle cases where there are categories with no entries?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-727717155
https://github.com/scverse/scanpy/pull/1490#issuecomment-727717155:28,Testability,test,test,28,Great! Could you also add a test to make sure the error is being thrown? And does this handle cases where there are categories with no entries?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-727717155
https://github.com/scverse/scanpy/pull/1490#issuecomment-727864463:52,Availability,error,error,52,"> Great! Could you also add a test to make sure the error is being thrown? And does this handle cases where there are categories with no entries?. Sorry yep tests added. And yep, the value_counts() will also catch empty categories (though added a test for that too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-727864463
https://github.com/scverse/scanpy/pull/1490#issuecomment-727864463:30,Testability,test,test,30,"> Great! Could you also add a test to make sure the error is being thrown? And does this handle cases where there are categories with no entries?. Sorry yep tests added. And yep, the value_counts() will also catch empty categories (though added a test for that too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-727864463
https://github.com/scverse/scanpy/pull/1490#issuecomment-727864463:157,Testability,test,tests,157,"> Great! Could you also add a test to make sure the error is being thrown? And does this handle cases where there are categories with no entries?. Sorry yep tests added. And yep, the value_counts() will also catch empty categories (though added a test for that too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-727864463
https://github.com/scverse/scanpy/pull/1490#issuecomment-727864463:247,Testability,test,test,247,"> Great! Could you also add a test to make sure the error is being thrown? And does this handle cases where there are categories with no entries?. Sorry yep tests added. And yep, the value_counts() will also catch empty categories (though added a test for that too).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-727864463
https://github.com/scverse/scanpy/pull/1490#issuecomment-738572114:14,Deployability,Pipeline,Pipelines,14,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-738572114
https://github.com/scverse/scanpy/pull/1490#issuecomment-738572114:55,Deployability,pipeline,pipeline,55,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-738572114
https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519:332,Availability,error,error,332,"> Also isn’t it cool that it points exactly to the problematic line?. Currently, I think the line number reported is the number of lines past the `:` in the function definition. It'd be really nice if it could tell you which line number in the file it was (which might be difficult for manipulated doc-strings). Also, from what the error message says, isn't the `any(broken)` check testing the same thing as assert lines[0], `f""{name} needs a single-line summary""`? Isn't the first one sufficient?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519
https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519:338,Integrability,message,message,338,"> Also isn’t it cool that it points exactly to the problematic line?. Currently, I think the line number reported is the number of lines past the `:` in the function definition. It'd be really nice if it could tell you which line number in the file it was (which might be difficult for manipulated doc-strings). Also, from what the error message says, isn't the `any(broken)` check testing the same thing as assert lines[0], `f""{name} needs a single-line summary""`? Isn't the first one sufficient?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519
https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519:382,Testability,test,testing,382,"> Also isn’t it cool that it points exactly to the problematic line?. Currently, I think the line number reported is the number of lines past the `:` in the function definition. It'd be really nice if it could tell you which line number in the file it was (which might be difficult for manipulated doc-strings). Also, from what the error message says, isn't the `any(broken)` check testing the same thing as assert lines[0], `f""{name} needs a single-line summary""`? Isn't the first one sufficient?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519
https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519:408,Testability,assert,assert,408,"> Also isn’t it cool that it points exactly to the problematic line?. Currently, I think the line number reported is the number of lines past the `:` in the function definition. It'd be really nice if it could tell you which line number in the file it was (which might be difficult for manipulated doc-strings). Also, from what the error message says, isn't the `any(broken)` check testing the same thing as assert lines[0], `f""{name} needs a single-line summary""`? Isn't the first one sufficient?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519
https://github.com/scverse/scanpy/pull/1492#issuecomment-726004959:58,Testability,assert,assert,58,It’s using `__orig_doc__` so the line should be correct. `assert lines[0]` just asserts that the first line is non-empty. `any(broken)` checks if there’s under-indented lines.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726004959
https://github.com/scverse/scanpy/pull/1492#issuecomment-726004959:80,Testability,assert,asserts,80,It’s using `__orig_doc__` so the line should be correct. `assert lines[0]` just asserts that the first line is non-empty. `any(broken)` checks if there’s under-indented lines.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726004959
https://github.com/scverse/scanpy/pull/1492#issuecomment-726019005:68,Availability,error,error,68,"> any(broken) checks if there’s under-indented lines. Sure, but the error it throws is: ```""Header of function `{name}`’s docstring should start with one-line description:""```, which suggests that wasn't the intent of the check. If it's a check for white space, it should throw an error about white space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726019005
https://github.com/scverse/scanpy/pull/1492#issuecomment-726019005:281,Availability,error,error,281,"> any(broken) checks if there’s under-indented lines. Sure, but the error it throws is: ```""Header of function `{name}`’s docstring should start with one-line description:""```, which suggests that wasn't the intent of the check. If it's a check for white space, it should throw an error about white space.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726019005
https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728:109,Integrability,message,message,109,"Same thing: If the line is under-indented, the first line summary can’t be properly extracted. I’ll make the message more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728
https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728:122,Usability,clear,clear,122,"Same thing: If the line is under-indented, the first line summary can’t be properly extracted. I’ll make the message more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728
https://github.com/scverse/scanpy/issues/1496#issuecomment-729533125:119,Deployability,install,installed,119,"@TheAustinator, can you replicate this in a fresh environment (e.g. conda)? It could help to limit the number of other installed packages. @flying-sheep, any ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-729533125
https://github.com/scverse/scanpy/issues/1496#issuecomment-730805437:60,Deployability,install,install,60,"Hey guys, thanks for getting back to me. @flying-sheep `pip install -e` isn't working on the flit branch because it doesn't have a `setup.py` -- is there another way to install?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-730805437
https://github.com/scverse/scanpy/issues/1496#issuecomment-730805437:169,Deployability,install,install,169,"Hey guys, thanks for getting back to me. @flying-sheep `pip install -e` isn't working on the flit branch because it doesn't have a `setup.py` -- is there another way to install?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-730805437
https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274:56,Deployability,install,install,56,"I merged it into master now. You can try a regular `pip install .` or `flit install -s` for an editable install (after installing flit) as described here: https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Pip says that btw if you try `pip install -e .`. > A ""pyproject.toml"" file was found, but editable mode currently requires a setup.py based build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274
https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274:76,Deployability,install,install,76,"I merged it into master now. You can try a regular `pip install .` or `flit install -s` for an editable install (after installing flit) as described here: https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Pip says that btw if you try `pip install -e .`. > A ""pyproject.toml"" file was found, but editable mode currently requires a setup.py based build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274
https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274:104,Deployability,install,install,104,"I merged it into master now. You can try a regular `pip install .` or `flit install -s` for an editable install (after installing flit) as described here: https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Pip says that btw if you try `pip install -e .`. > A ""pyproject.toml"" file was found, but editable mode currently requires a setup.py based build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274
https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274:119,Deployability,install,installing,119,"I merged it into master now. You can try a regular `pip install .` or `flit install -s` for an editable install (after installing flit) as described here: https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Pip says that btw if you try `pip install -e .`. > A ""pyproject.toml"" file was found, but editable mode currently requires a setup.py based build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274
https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274:195,Deployability,install,installation,195,"I merged it into master now. You can try a regular `pip install .` or `flit install -s` for an editable install (after installing flit) as described here: https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Pip says that btw if you try `pip install -e .`. > A ""pyproject.toml"" file was found, but editable mode currently requires a setup.py based build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274
https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274:268,Deployability,install,install,268,"I merged it into master now. You can try a regular `pip install .` or `flit install -s` for an editable install (after installing flit) as described here: https://scanpy.readthedocs.io/en/latest/installation.html#development-version. Pip says that btw if you try `pip install -e .`. > A ""pyproject.toml"" file was found, but editable mode currently requires a setup.py based build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496#issuecomment-737558274
https://github.com/scverse/scanpy/issues/1497#issuecomment-729598041:954,Modifiability,layers,layers,954,"我同样面临着这个 bug; 我的代码是; ```python; #genes_to_plot = ['Blvrb','Klf1','Serpina3f','Coro1a','Napsa','Ly6c2']; genes_to_plot = ['Blvrb',#MEP marker; 'Klf1',#MEP marker; 'Serpina3f']#CMP marker; sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); genes_to_plot = ['Coro1a',#CMP and GMP marker; 'Napsa',#GMP marker; 'Ly6c2']#GMP marker; sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); ```; 错误信息：; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-13-61edd8063c25> in <module>(); 3 'Klf1',#MEP marker; 4 'Serpina3f']#CMP marker; ----> 5 sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); 6 genes_to_plot = ['Coro1a',#CMP and GMP marker; 7 'Napsa',#GMP marker. ~/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 124 (x in adata.obs.keys() or x in adata.var.index); 125 and (y in adata.obs.keys() or y in adata.var.index); --> 126 and (color is None or color in adata.obs.keys() or color in adata.var.index); 127 ):; 128 return _scatter_obs(**args). ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in __contains__(self, key); 4069 False; 4070 """"""; -> 4071 hash(key); 4072 try:; 4073 return key in self._engine. TypeError: unhashable type: 'list'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1497#issuecomment-729598041
https://github.com/scverse/scanpy/issues/1497#issuecomment-729598041:1537,Security,hash,hash,1537,"我同样面临着这个 bug; 我的代码是; ```python; #genes_to_plot = ['Blvrb','Klf1','Serpina3f','Coro1a','Napsa','Ly6c2']; genes_to_plot = ['Blvrb',#MEP marker; 'Klf1',#MEP marker; 'Serpina3f']#CMP marker; sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); genes_to_plot = ['Coro1a',#CMP and GMP marker; 'Napsa',#GMP marker; 'Ly6c2']#GMP marker; sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); ```; 错误信息：; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-13-61edd8063c25> in <module>(); 3 'Klf1',#MEP marker; 4 'Serpina3f']#CMP marker; ----> 5 sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); 6 genes_to_plot = ['Coro1a',#CMP and GMP marker; 7 'Napsa',#GMP marker. ~/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 124 (x in adata.obs.keys() or x in adata.var.index); 125 and (y in adata.obs.keys() or y in adata.var.index); --> 126 and (color is None or color in adata.obs.keys() or color in adata.var.index); 127 ):; 128 return _scatter_obs(**args). ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in __contains__(self, key); 4069 False; 4070 """"""; -> 4071 hash(key); 4072 try:; 4073 return key in self._engine. TypeError: unhashable type: 'list'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1497#issuecomment-729598041
https://github.com/scverse/scanpy/pull/1499#issuecomment-730348174:165,Modifiability,variab,variables,165,"About `use_raw` with `sc.get.var_df`: I didn't include this because the semantics differ significantly from `sc.get.obs_df`, as `raw` can have a different number of variables. I think it makes more sense for a user to call `sc.get.var_df(adata.raw, ...)`, since it's much more explicit that `adata.raw.var` and `adata.raw.varm` will be used.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-730348174
https://github.com/scverse/scanpy/pull/1499#issuecomment-731030337:528,Modifiability,variab,variable,528,"It's not just that the length is different, is that `sc.get.obs_df(adata, [""col""], use_raw=x)[""col""]` is the same regardless of the value of `x`, but it's different for `var_df`. I think it's easier to build code around functions with more orthogonal arguments. > However, I consider that since this option is everywhere it should be here as well. . Could we add an example of `sc.get.var_df(adata.raw, ...)`, leave out `use_raw` for now, and see if anyone complains?. I've been trying to leave out `use_raw` on functions where variable length matters anyways. For example: `adata.var_vector`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-731030337
https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048:13,Testability,test,test,13,"The previous test failed but is not clear to me why, as it passes the local tests (anndata 0.7.5). It seems that on travis server, backed slicing requires integer indices and will not work with a boolean vector. I changed to sorted integers hoping that this will solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048
https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048:76,Testability,test,tests,76,"The previous test failed but is not clear to me why, as it passes the local tests (anndata 0.7.5). It seems that on travis server, backed slicing requires integer indices and will not work with a boolean vector. I changed to sorted integers hoping that this will solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048
https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048:36,Usability,clear,clear,36,"The previous test failed but is not clear to me why, as it passes the local tests (anndata 0.7.5). It seems that on travis server, backed slicing requires integer indices and will not work with a boolean vector. I changed to sorted integers hoping that this will solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-733745048
https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047:232,Availability,mask,mask,232,"Should the reference object where you learn the transformation (currently `adata`) always be a subset of the data you're going to apply the transformation to (`adata2`)? If so, instead of passing a separate object, could there be a mask of which samples to train on?. If not, what do you think about making this a separate function? Maybe `combat_by_reference`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047
https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047:38,Usability,learn,learn,38,"Should the reference object where you learn the transformation (currently `adata`) always be a subset of the data you're going to apply the transformation to (`adata2`)? If so, instead of passing a separate object, could there be a mask of which samples to train on?. If not, what do you think about making this a separate function? Maybe `combat_by_reference`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730233047
https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703:203,Availability,mask,mask,203,"> Should the reference object where you learn the transformation always be a subset of the data you're going to apply the transformation to? If so, instead of passing a separate object, could there be a mask of which samples to train on?; > ; > If not, what do you think about making this a separate function? Maybe `combat_by_reference`?. Thank you for your great suggestions. I think it's easier to add a mask for train/evaluate instead of splitting into 2 objects. ; I don't think it should be a separate `combat_by_reference` function, though, because the chance in the function is small and I preserved the original functionality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703
https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703:407,Availability,mask,mask,407,"> Should the reference object where you learn the transformation always be a subset of the data you're going to apply the transformation to? If so, instead of passing a separate object, could there be a mask of which samples to train on?; > ; > If not, what do you think about making this a separate function? Maybe `combat_by_reference`?. Thank you for your great suggestions. I think it's easier to add a mask for train/evaluate instead of splitting into 2 objects. ; I don't think it should be a separate `combat_by_reference` function, though, because the chance in the function is small and I preserved the original functionality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703
https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703:40,Usability,learn,learn,40,"> Should the reference object where you learn the transformation always be a subset of the data you're going to apply the transformation to? If so, instead of passing a separate object, could there be a mask of which samples to train on?; > ; > If not, what do you think about making this a separate function? Maybe `combat_by_reference`?. Thank you for your great suggestions. I think it's easier to add a mask for train/evaluate instead of splitting into 2 objects. ; I don't think it should be a separate `combat_by_reference` function, though, because the chance in the function is small and I preserved the original functionality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1501#issuecomment-730248703
https://github.com/scverse/scanpy/issues/1502#issuecomment-730259879:113,Usability,clear,clear,113,"You can pass `legend_loc=None`. Thanks for noting this. Looking at the docs, I can definitely see how this isn't clear. ```; legend_loc : str, optional (default: 'right margin'); Location of legend, either `'on data'`, `'right margin'` or a valid keyword; for the `loc` parameter of :class:`~matplotlib.legend.Legend`.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-730259879
https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768:13,Deployability,update,update,13,"So one small update here -- it works like a charm for categorical variables, but not for continuous variables.; e.g.; > sc.pl.umap(testData, save = fileName, color='CCL5',s=50,frameon=False,legend_loc = None). Still gives something like a legend:; ![image](https://user-images.githubusercontent.com/10536275/99786010-40234a80-2b1e-11eb-83ab-77c9341dab05.png). Presumably this is because the color strip on the right is not actually a legend in the underlying matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768
https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768:89,Deployability,continuous,continuous,89,"So one small update here -- it works like a charm for categorical variables, but not for continuous variables.; e.g.; > sc.pl.umap(testData, save = fileName, color='CCL5',s=50,frameon=False,legend_loc = None). Still gives something like a legend:; ![image](https://user-images.githubusercontent.com/10536275/99786010-40234a80-2b1e-11eb-83ab-77c9341dab05.png). Presumably this is because the color strip on the right is not actually a legend in the underlying matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768
https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768:66,Modifiability,variab,variables,66,"So one small update here -- it works like a charm for categorical variables, but not for continuous variables.; e.g.; > sc.pl.umap(testData, save = fileName, color='CCL5',s=50,frameon=False,legend_loc = None). Still gives something like a legend:; ![image](https://user-images.githubusercontent.com/10536275/99786010-40234a80-2b1e-11eb-83ab-77c9341dab05.png). Presumably this is because the color strip on the right is not actually a legend in the underlying matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768
https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768:100,Modifiability,variab,variables,100,"So one small update here -- it works like a charm for categorical variables, but not for continuous variables.; e.g.; > sc.pl.umap(testData, save = fileName, color='CCL5',s=50,frameon=False,legend_loc = None). Still gives something like a legend:; ![image](https://user-images.githubusercontent.com/10536275/99786010-40234a80-2b1e-11eb-83ab-77c9341dab05.png). Presumably this is because the color strip on the right is not actually a legend in the underlying matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768
https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768:131,Testability,test,testData,131,"So one small update here -- it works like a charm for categorical variables, but not for continuous variables.; e.g.; > sc.pl.umap(testData, save = fileName, color='CCL5',s=50,frameon=False,legend_loc = None). Still gives something like a legend:; ![image](https://user-images.githubusercontent.com/10536275/99786010-40234a80-2b1e-11eb-83ab-77c9341dab05.png). Presumably this is because the color strip on the right is not actually a legend in the underlying matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768
https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704:23,Availability,error,error,23,Could you suggest some error handling behavior here? I think there could definitely be a more helpful error message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704
https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704:102,Availability,error,error,102,Could you suggest some error handling behavior here? I think there could definitely be a more helpful error message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704
https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704:108,Integrability,message,message,108,Could you suggest some error handling behavior here? I think there could definitely be a more helpful error message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704
https://github.com/scverse/scanpy/issues/1504#issuecomment-748161925:143,Availability,error,error,143,"I also experienced this a few times, and took me some time to understand what is going on. I fully agree with @ivirshup, we should improve the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-748161925
https://github.com/scverse/scanpy/issues/1504#issuecomment-748161925:149,Integrability,message,message,149,"I also experienced this a few times, and took me some time to understand what is going on. I fully agree with @ivirshup, we should improve the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-748161925
https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060:139,Availability,error,error,139,"In my experience, this happens if batch key is not None and one or more batches have low number of cells. Does it make sense to catch this error and simply skip the problematic batch or inform the user that batch doesn't have enough cells?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060
https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060:149,Usability,simpl,simply,149,"In my experience, this happens if batch key is not None and one or more batches have low number of cells. Does it make sense to catch this error and simply skip the problematic batch or inform the user that batch doesn't have enough cells?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-748548060
https://github.com/scverse/scanpy/issues/1504#issuecomment-1521864773:182,Availability,error,error,182,"I ran into this recently - the problem can occur when batch key has many cells in each batch (see plot). Increasing the span from the default of 0.3 to 0.5 seems to have ""fixed"" the error. Increasing the filtering stringency for lowly expressed genes (to min_gene=500, min_cells=10) also gets rid of the error. ```; sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); ```; ```; sc.pp.highly_variable_genes(; adata,; layer=""counts"",; flavor=""seurat_v3"",; n_top_genes=num_hvgs,; batch_key='sex_cell_subtype',; span=0.5; ); ```; <img width=""580"" alt=""image"" src=""https://user-images.githubusercontent.com/4561831/234303299-74bee98d-94a8-40a8-b0dd-cc10eac1acec.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-1521864773
https://github.com/scverse/scanpy/issues/1504#issuecomment-1521864773:304,Availability,error,error,304,"I ran into this recently - the problem can occur when batch key has many cells in each batch (see plot). Increasing the span from the default of 0.3 to 0.5 seems to have ""fixed"" the error. Increasing the filtering stringency for lowly expressed genes (to min_gene=500, min_cells=10) also gets rid of the error. ```; sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); ```; ```; sc.pp.highly_variable_genes(; adata,; layer=""counts"",; flavor=""seurat_v3"",; n_top_genes=num_hvgs,; batch_key='sex_cell_subtype',; span=0.5; ); ```; <img width=""580"" alt=""image"" src=""https://user-images.githubusercontent.com/4561831/234303299-74bee98d-94a8-40a8-b0dd-cc10eac1acec.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-1521864773
https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:5,Deployability,update,updated,5,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592
https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:341,Deployability,update,update,341,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592
https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:299,Integrability,depend,dependency,299,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592
https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:106,Testability,test,test,106,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592
https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:171,Testability,test,test,171,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592
https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:294,Testability,test,test,294,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592
https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:374,Testability,test,test,374,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592
https://github.com/scverse/scanpy/pull/1506#issuecomment-733618049:121,Availability,down,downloaded,121,"> with some name like key=image_name and value=path ? or something like that. sure! What do you mean with image_name? As downloaded from the 10x website, the tiff image always has the same filename (""image.tif""). Or do you mean the string ""image_name"" as key?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733618049
https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124:399,Availability,down,downloading,399,"Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?). A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124
https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124:61,Integrability,depend,dependency,61,"Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?). A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124
https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124:98,Integrability,depend,dependencies,98,"Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?). A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124
https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124:56,Testability,test,test,56,"Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?). A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124
https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462:407,Availability,down,downloading,407,"> Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?); > ; > A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading. I was not aware of `file`. I think it might be a good solution! `readwrite._download` should make sure that downloads are not incomplete, so just reading the header might be enough",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462
https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462:528,Availability,down,downloads,528,"> Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?); > ; > A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading. I was not aware of `file`. I think it might be a good solution! `readwrite._download` should make sure that downloads are not incomplete, so just reading the header might be enough",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462
https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462:63,Integrability,depend,dependency,63,"> Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?); > ; > A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading. I was not aware of `file`. I think it might be a good solution! `readwrite._download` should make sure that downloads are not incomplete, so just reading the header might be enough",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462
https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462:100,Integrability,depend,dependencies,100,"> Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?); > ; > A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading. I was not aware of `file`. I think it might be a good solution! `readwrite._download` should make sure that downloads are not incomplete, so just reading the header might be enough",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462
https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462:58,Testability,test,test,58,"> Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?); > ; > A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading. I was not aware of `file`. I think it might be a good solution! `readwrite._download` should make sure that downloads are not incomplete, so just reading the header might be enough",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462
https://github.com/scverse/scanpy/pull/1506#issuecomment-733750814:662,Integrability,depend,depending,662,"> > > Should this argument be added to `sc.read_visium` as well? That is, add a path to the tiff if it would be in a standardized place?; > > ; > > ; > > yes that's a very good point actually. It could even make it easier for us to initialize the image container maybe?; > ; > I thought about that already - standardised places for the tiff would be `sample_id""_image.tif` and `image.tif`. I don't think we need to add an extra argument to `sc.read_visium`, couldn't it automatically add the path if it exists?. good point, agree probably best solution, however one thing it's not given is the format of the file (could be tif or jpg, but potentially some else, depending on the lab). But an automatic search would be best, just not sure how extensive we can make it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750814
https://github.com/scverse/scanpy/pull/1506#issuecomment-734306503:686,Integrability,depend,depending,686,"> > > > Should this argument be added to `sc.read_visium` as well? That is, add a path to the tiff if it would be in a standardized place?; > > > ; > > > ; > > > yes that's a very good point actually. It could even make it easier for us to initialize the image container maybe?; > > ; > > ; > > I thought about that already - standardised places for the tiff would be `sample_id""_image.tif` and `image.tif`. I don't think we need to add an extra argument to `sc.read_visium`, couldn't it automatically add the path if it exists?; > ; > good point, agree probably best solution, however one thing it's not given is the format of the file (could be tif or jpg, but potentially some else, depending on the lab). But an automatic search would be best, just not sure how extensive we can make it. do you have an example where the image is a jpg? I can account for both tif and jpg, but I'm wondering if its worth it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734306503
https://github.com/scverse/scanpy/pull/1506#issuecomment-734307032:710,Integrability,depend,depending,710,"> > > > > Should this argument be added to `sc.read_visium` as well? That is, add a path to the tiff if it would be in a standardized place?; > > > > ; > > > > ; > > > > yes that's a very good point actually. It could even make it easier for us to initialize the image container maybe?; > > > ; > > > ; > > > I thought about that already - standardised places for the tiff would be `sample_id""_image.tif` and `image.tif`. I don't think we need to add an extra argument to `sc.read_visium`, couldn't it automatically add the path if it exists?; > > ; > > ; > > good point, agree probably best solution, however one thing it's not given is the format of the file (could be tif or jpg, but potentially some else, depending on the lab). But an automatic search would be best, just not sure how extensive we can make it; > ; > do you have an example where the image is a jpg? I can account for both tif and jpg, but I'm wondering if its worth it. collaborators that generated visium data sent me jpg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734307032
https://github.com/scverse/scanpy/pull/1506#issuecomment-734405082:5,Deployability,update,update,5,"I've update the code to ; - test that the file is actually a tiff image; - automatically add the path to the image to `adata.uns['spatial'][library_id]['metadata']['tissue_image_path']`. It's looking for a tiff or jpeg file with the name `""image""` or `library_id""_image""`. This should cover most cases hopefully?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734405082
https://github.com/scverse/scanpy/pull/1506#issuecomment-734405082:28,Testability,test,test,28,"I've update the code to ; - test that the file is actually a tiff image; - automatically add the path to the image to `adata.uns['spatial'][library_id]['metadata']['tissue_image_path']`. It's looking for a tiff or jpeg file with the name `""image""` or `library_id""_image""`. This should cover most cases hopefully?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734405082
https://github.com/scverse/scanpy/pull/1506#issuecomment-734735910:532,Usability,simpl,simply,532,"> I think it would be fine to only cover the case of what `space ranger` actually outputs. I was thinking there could be an argument where the user manually passes an alternate path. This could be useful for cases where they've processed the image themselves some modifications to the image. space ranger doesn't output this image, as it's taken as input to assign spots and get scalefactors and metadata. This type of image is in the same folder just for chance in the 10x genomics dataset. ; In the `read_visium` function I would simply add an argument to pass the path of the image, and basically just assign it to the `adata.uns` metadata. Otherwise just assign None. THis way it's consistent for the spatial tool whichlater uses it in the image container.; It's also convenient to add it as argument so that `read_visium` could just be passed in that same way as it is now in `datasets.visium_sge`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-734735910
https://github.com/scverse/scanpy/pull/1506#issuecomment-735881272:18,Deployability,update,update,18,I've made a small update to add a `tissue_image_path` parameter to `read_visium` that you can use to specify the tissue image path to put in `adata.uns`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-735881272
https://github.com/scverse/scanpy/pull/1506#issuecomment-738555603:14,Deployability,Pipeline,Pipelines,14,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-738555603
https://github.com/scverse/scanpy/pull/1506#issuecomment-738555603:55,Deployability,pipeline,pipeline,55,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-738555603
https://github.com/scverse/scanpy/pull/1506#issuecomment-743084299:0,Deployability,Update,Updated,0,Updated the PR:; - renamed argument in `visium_sge` to `include_hires_tiff`; - renamed argument in `read_visium` to `source_image_path` and removed guessing of image file if `source_image_path` is None,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-743084299
https://github.com/scverse/scanpy/pull/1506#issuecomment-744276746:20,Deployability,release,release,20,"> Please also add a release note!. sorry for the naive question, but how do I add a release note? As a comment in the PR? As a commit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-744276746
https://github.com/scverse/scanpy/pull/1506#issuecomment-744276746:84,Deployability,release,release,84,"> Please also add a release note!. sorry for the naive question, but how do I add a release note? As a comment in the PR? As a commit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-744276746
https://github.com/scverse/scanpy/pull/1506#issuecomment-744281015:116,Deployability,release,release-latest,116,"just mention this PR with brief description and your name here: https://github.com/theislab/scanpy/blob/master/docs/release-latest.rst. nice! only one PR to go, thank you @hspitzer @ivirshup !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-744281015
https://github.com/scverse/scanpy/pull/1506#issuecomment-744317801:118,Deployability,release,release-latest,118,"> just mention this PR with brief description and your name here: https://github.com/theislab/scanpy/blob/master/docs/release-latest.rst. ok, done. I put it in the ""on master"" section",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-744317801
https://github.com/scverse/scanpy/pull/1507#issuecomment-733508444:96,Availability,error,errors,96,"The main change here is passing `None` instead of `0` to `total`, right?. Also: this makes some errors with files still existing make much more sense. I had no idea `KeyboardInterrupt` doesn't inherit from `Exception`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507#issuecomment-733508444
https://github.com/scverse/scanpy/pull/1507#issuecomment-733508444:193,Modifiability,inherit,inherit,193,"The main change here is passing `None` instead of `0` to `total`, right?. Also: this makes some errors with files still existing make much more sense. I had no idea `KeyboardInterrupt` doesn't inherit from `Exception`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507#issuecomment-733508444
https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404:507,Availability,error,errors,507,"Thanks for a quick response and the comments. > The main change here is passing None instead of 0 to total, right?. It was actually setting it in the contructor, rather than assigning it to the tqdm object (the latter doesn't work). Here's before:; ![old](https://user-images.githubusercontent.com/46717574/100207740-3b88d880-2f08-11eb-882f-cae14be0837e.png); and after:; ![new](https://user-images.githubusercontent.com/46717574/100207756-3fb4f600-2f08-11eb-85f8-5938ff04572d.png). > Also: this makes some errors with files still existing make much more sense. I had no idea KeyboardInterrupt doesn't inherit from Exception. I didn't know that either, so I looked it up (it actually inherits from `BaseException` among other things:; https://docs.python.org/3/library/exceptions.html#exception-hierarchy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404
https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404:602,Modifiability,inherit,inherit,602,"Thanks for a quick response and the comments. > The main change here is passing None instead of 0 to total, right?. It was actually setting it in the contructor, rather than assigning it to the tqdm object (the latter doesn't work). Here's before:; ![old](https://user-images.githubusercontent.com/46717574/100207740-3b88d880-2f08-11eb-882f-cae14be0837e.png); and after:; ![new](https://user-images.githubusercontent.com/46717574/100207756-3fb4f600-2f08-11eb-85f8-5938ff04572d.png). > Also: this makes some errors with files still existing make much more sense. I had no idea KeyboardInterrupt doesn't inherit from Exception. I didn't know that either, so I looked it up (it actually inherits from `BaseException` among other things:; https://docs.python.org/3/library/exceptions.html#exception-hierarchy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404
https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404:684,Modifiability,inherit,inherits,684,"Thanks for a quick response and the comments. > The main change here is passing None instead of 0 to total, right?. It was actually setting it in the contructor, rather than assigning it to the tqdm object (the latter doesn't work). Here's before:; ![old](https://user-images.githubusercontent.com/46717574/100207740-3b88d880-2f08-11eb-882f-cae14be0837e.png); and after:; ![new](https://user-images.githubusercontent.com/46717574/100207756-3fb4f600-2f08-11eb-85f8-5938ff04572d.png). > Also: this makes some errors with files still existing make much more sense. I had no idea KeyboardInterrupt doesn't inherit from Exception. I didn't know that either, so I looked it up (it actually inherits from `BaseException` among other things:; https://docs.python.org/3/library/exceptions.html#exception-hierarchy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404
https://github.com/scverse/scanpy/issues/1508#issuecomment-734657400:345,Deployability,release,release,345,"How I save plots is:. ```python; from matplotlib import pyplot as plt. with plt.rc_context(): # Use this to set figure params like size and dpi; sc.pl.plotting_function(..., show=False); plt.savefig(""path/to/file.extension"", bbox_inches=""tight""); ```. I think how this argument works is one of the things we would change in a major API breaking release. @fidelram, would you add anything to this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-734657400
https://github.com/scverse/scanpy/issues/1508#issuecomment-734788343:175,Deployability,release,release,175,"You could also not make this breaking by checking if there are any `/` in the string and appending `umap` or `violin` after the last `/`? Would be a bit messy, but quicker to release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-734788343
https://github.com/scverse/scanpy/issues/1508#issuecomment-735050380:77,Safety,detect,detect,77,"I don't want to do too much with string mangling. Now that mention trying to detect it though, I don't think I'd have anything against passing a `Path` to override this behaviour. We could also add a different argument with this behaviour – like `path` – then deprecate `save`, or at least passing a string to `save`. On the other hand, I wonder how much we gain by controlling the saving of a figure. Maybe the only way to save a figure should be from the user calling matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-735050380
https://github.com/scverse/scanpy/issues/1508#issuecomment-735835548:235,Usability,intuit,intuitive,235,I quite like the saving of figures as it means people can use scanpy who otherwise aren't as familiar with data science in python. Calling a function on an axis object or saving the last axis object that was is displayed is not always intuitive to new users.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-735835548
https://github.com/scverse/scanpy/issues/1508#issuecomment-841127632:237,Usability,intuit,intuitive,237,"> I quite like the saving of figures as it means people can use scanpy who otherwise aren't as familiar with data science in python. Calling a function on an axis object or saving the last axis object that was is displayed is not always intuitive to new users. How about adding a ""plotting cookbook"" section to the docs instead? `plt.rc_context` is such a neat trick (also beyond scanpy), but it wasn't obvious to me either (#1648). . Obligatory quote from the ""Zen of Python"":; ```; There should be one-- and preferably only one --obvious way to do it.; Although that way may not be obvious at first unless you're Dutch.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1508#issuecomment-841127632
https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308:124,Availability,ping,ping,124,"It looks like the new release breaks most of our usage from (at least) a change in arguments to `simplicial_set_embedding` (ping @Koncopd). <details>; <summary> Example error </summary>. ```pytb; n_epochs = 0 if maxiter is None else maxiter; > X_umap = simplicial_set_embedding(; X,; neighbors['connectivities'].tocoo(),; n_components,; alpha,; a,; b,; gamma,; negative_sample_rate,; n_epochs,; init_coords,; random_state,; neigh_params.get('metric', 'euclidean'),; neigh_params.get('metric_kwds', {}),; verbose=settings.verbosity > 3,; E TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. </details>. It looks like there is also a lot of cool stuff in the new release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308
https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308:169,Availability,error,error,169,"It looks like the new release breaks most of our usage from (at least) a change in arguments to `simplicial_set_embedding` (ping @Koncopd). <details>; <summary> Example error </summary>. ```pytb; n_epochs = 0 if maxiter is None else maxiter; > X_umap = simplicial_set_embedding(; X,; neighbors['connectivities'].tocoo(),; n_components,; alpha,; a,; b,; gamma,; negative_sample_rate,; n_epochs,; init_coords,; random_state,; neigh_params.get('metric', 'euclidean'),; neigh_params.get('metric_kwds', {}),; verbose=settings.verbosity > 3,; E TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. </details>. It looks like there is also a lot of cool stuff in the new release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308
https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308:22,Deployability,release,release,22,"It looks like the new release breaks most of our usage from (at least) a change in arguments to `simplicial_set_embedding` (ping @Koncopd). <details>; <summary> Example error </summary>. ```pytb; n_epochs = 0 if maxiter is None else maxiter; > X_umap = simplicial_set_embedding(; X,; neighbors['connectivities'].tocoo(),; n_components,; alpha,; a,; b,; gamma,; negative_sample_rate,; n_epochs,; init_coords,; random_state,; neigh_params.get('metric', 'euclidean'),; neigh_params.get('metric_kwds', {}),; verbose=settings.verbosity > 3,; E TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. </details>. It looks like there is also a lot of cool stuff in the new release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308
https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308:740,Deployability,release,release,740,"It looks like the new release breaks most of our usage from (at least) a change in arguments to `simplicial_set_embedding` (ping @Koncopd). <details>; <summary> Example error </summary>. ```pytb; n_epochs = 0 if maxiter is None else maxiter; > X_umap = simplicial_set_embedding(; X,; neighbors['connectivities'].tocoo(),; n_components,; alpha,; a,; b,; gamma,; negative_sample_rate,; n_epochs,; init_coords,; random_state,; neigh_params.get('metric', 'euclidean'),; neigh_params.get('metric_kwds', {}),; verbose=settings.verbosity > 3,; E TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. </details>. It looks like there is also a lot of cool stuff in the new release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1509#issuecomment-743966308
https://github.com/scverse/scanpy/issues/1509#issuecomment-748156450:59,Usability,learn,learn,59,This seems related to the new Densmap feature https://umap-learn.readthedocs.io/en/latest/densmap_demo.html (see https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1). Would be cool to support it in scanpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1509#issuecomment-748156450
https://github.com/scverse/scanpy/pull/1511#issuecomment-734833743:753,Deployability,update,updated,753,"```python; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc, key_added=""clusters"", resolution=0.5). marker_genes_dict = {; ""1"": [""GNLY"", ""NKG7""],; ""0"": [""CD3D""],; ""2"": [""CD79A"", ""MS4A1""],; ""4"": [""FCGR3A""],; ""3"": [""FCER1A""],; }. sc.pl.heatmap(; pbmc,; marker_genes_dict,; groupby=""clusters"",; vmin=-2,; vmax=2,; cmap=""RdBu_r"",; dendrogram=True,; swap_axes=True,; ); ```. before:; ![image](https://user-images.githubusercontent.com/25887487/100453873-0e7d2700-30bc-11eb-9451-0240f6170e2f.png). after:; ![image](https://user-images.githubusercontent.com/25887487/100453885-15a43500-30bc-11eb-9b95-66d0881aad31.png). as you can see, heatmap colors for rows (genes) is consistent with columns (clusters) ; @wkopp is this helpful? notebook still to be updated",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734833743
https://github.com/scverse/scanpy/pull/1511#issuecomment-734837826:53,Modifiability,variab,variable,53,"One thing to note:; if the categories of the groupby variable (clusters in this case) don't match up with the categories in the marker_genes_dict, there will be no matching if colors between rows and columns. This is what also happens in the tutorial, where; marker_genes_dict = {'NK': ['GNLY', 'NKG7'],; 'T-cell': ['CD3D'],; 'B-cell': ['CD79A', 'MS4A1'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; while the clusters are numbers (""1"", ""2"" etc.).; We could consider throwing a warning in that case, but I don't think it's necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734837826
https://github.com/scverse/scanpy/pull/1511#issuecomment-734846006:57,Energy Efficiency,green,green,57,"I'm confused by the figure above. In Cluster 2 (shown in green below the heatmap) the marker gene that is coming up most prominently is CD3D, which was defined as a marker for Cluster 0, though.; The color code on the right seems correct, but why is CD3D most prominently expressed in Cluster 2 and not in Cluster 0?; The same seems to be the case for the other clusters.; Or were the marker genes perhaps just mislabelled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734846006
https://github.com/scverse/scanpy/pull/1511#issuecomment-734848875:309,Modifiability,variab,variable,309,"Yeah the marker assignments here are random, that might be a bit confusing indeed. The only point is that the colors for the clusters (columns) match with the *names* of the marker gene lists (rows). So if, in an actual dataset, you now name the correct clusters ""T cells"", ""B cells"" etc. in you obs.clusters variable, and match those names with your marker gene dict keys, everything will match up. ; Does that make sense?; I just didn't go through annotations of the clusters here, that would be a bit tedious. But try it out on your own (correctly annotated) anndata object if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734848875
https://github.com/scverse/scanpy/pull/1511#issuecomment-734861544:420,Availability,error,error,420,"exactly! also this makes a lot of sense in the context of the reversed heatmap (where keys of the mapping are plotted as column annotation). ; This is also particularly useful if you have 2 annotations `cluster1=['1','2','3']` and `cluster2=['foo','bar']`, and you want to check for; ```python; markers={; 	""foo"":[""gene1"", ""gene2""],; 	""bar"":[""gene3""]; }; ```; but with respect to `cluster1`. I would have just thrown an error but this is a much more elegant solution!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734861544
https://github.com/scverse/scanpy/pull/1511#issuecomment-738555534:14,Deployability,Pipeline,Pipelines,14,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-738555534
https://github.com/scverse/scanpy/pull/1511#issuecomment-738555534:55,Deployability,pipeline,pipeline,55,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-738555534
https://github.com/scverse/scanpy/pull/1511#issuecomment-738690458:25,Testability,test,tests,25,"Hi! not sure why the two tests failed, I don't think it's related to my edits",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-738690458
https://github.com/scverse/scanpy/pull/1511#issuecomment-739663476:61,Testability,test,test,61,"@LisaSikkema, no worries there's been some flakiness of that test. Can this get a test case like ? Maybe even two, one where groups match, one where they don't?. I'm thinking something that calls `save_and_compare_images`, like https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L272",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739663476
https://github.com/scverse/scanpy/pull/1511#issuecomment-739663476:82,Testability,test,test,82,"@LisaSikkema, no worries there's been some flakiness of that test. Can this get a test case like ? Maybe even two, one where groups match, one where they don't?. I'm thinking something that calls `save_and_compare_images`, like https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L272",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739663476
https://github.com/scverse/scanpy/pull/1511#issuecomment-739663476:316,Testability,test,tests,316,"@LisaSikkema, no worries there's been some flakiness of that test. Can this get a test case like ? Maybe even two, one where groups match, one where they don't?. I'm thinking something that calls `save_and_compare_images`, like https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L272",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739663476
https://github.com/scverse/scanpy/pull/1511#issuecomment-739858710:69,Safety,detect,detected,69,"Thanks for looking at this... it is surprising that this bug was not detected earlier. . I looked at the code and looks fine but, I would like to add a test. @LisaSikkema can you check this? If this is too much trouble I can do it or I can help you because the plot test are difficult as they require similar setup as in the CI tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739858710
https://github.com/scverse/scanpy/pull/1511#issuecomment-739858710:152,Testability,test,test,152,"Thanks for looking at this... it is surprising that this bug was not detected earlier. . I looked at the code and looks fine but, I would like to add a test. @LisaSikkema can you check this? If this is too much trouble I can do it or I can help you because the plot test are difficult as they require similar setup as in the CI tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739858710
https://github.com/scverse/scanpy/pull/1511#issuecomment-739858710:266,Testability,test,test,266,"Thanks for looking at this... it is surprising that this bug was not detected earlier. . I looked at the code and looks fine but, I would like to add a test. @LisaSikkema can you check this? If this is too much trouble I can do it or I can help you because the plot test are difficult as they require similar setup as in the CI tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739858710
https://github.com/scverse/scanpy/pull/1511#issuecomment-739858710:328,Testability,test,tests,328,"Thanks for looking at this... it is surprising that this bug was not detected earlier. . I looked at the code and looks fine but, I would like to add a test. @LisaSikkema can you check this? If this is too much trouble I can do it or I can help you because the plot test are difficult as they require similar setup as in the CI tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739858710
https://github.com/scverse/scanpy/pull/1511#issuecomment-739859836:54,Testability,test,test,54,"Yes would be happy to look into it and come up with a test! I have no experience with the testing code yet though, so might take me a while to figure out how to write it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739859836
https://github.com/scverse/scanpy/pull/1511#issuecomment-739859836:90,Testability,test,testing,90,"Yes would be happy to look into it and come up with a test! I have no experience with the testing code yet though, so might take me a while to figure out how to write it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739859836
https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301:58,Availability,error,errors,58,"first try the following and check that you don't have any errors. ; ```; cd scanpy/scanpy/tests; py.test test_plotting.py; ```. If that is the case, you can go to this line: https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L36 and add a new test. I usually run the tests after adding the new test case which will fail the first time. But, it generates an image to compare. Then I copy the new image from `./figures/` to `./_images/` and add it to git. Off course you want to check that the image is what you expect. If you re-run the tests they should work now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301
https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301:90,Testability,test,tests,90,"first try the following and check that you don't have any errors. ; ```; cd scanpy/scanpy/tests; py.test test_plotting.py; ```. If that is the case, you can go to this line: https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L36 and add a new test. I usually run the tests after adding the new test case which will fail the first time. But, it generates an image to compare. Then I copy the new image from `./figures/` to `./_images/` and add it to git. Off course you want to check that the image is what you expect. If you re-run the tests they should work now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301
https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301:100,Testability,test,test,100,"first try the following and check that you don't have any errors. ; ```; cd scanpy/scanpy/tests; py.test test_plotting.py; ```. If that is the case, you can go to this line: https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L36 and add a new test. I usually run the tests after adding the new test case which will fail the first time. But, it generates an image to compare. Then I copy the new image from `./figures/` to `./_images/` and add it to git. Off course you want to check that the image is what you expect. If you re-run the tests they should work now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301
https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301:262,Testability,test,tests,262,"first try the following and check that you don't have any errors. ; ```; cd scanpy/scanpy/tests; py.test test_plotting.py; ```. If that is the case, you can go to this line: https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L36 and add a new test. I usually run the tests after adding the new test case which will fail the first time. But, it generates an image to compare. Then I copy the new image from `./figures/` to `./_images/` and add it to git. Off course you want to check that the image is what you expect. If you re-run the tests they should work now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301
https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301:303,Testability,test,test,303,"first try the following and check that you don't have any errors. ; ```; cd scanpy/scanpy/tests; py.test test_plotting.py; ```. If that is the case, you can go to this line: https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L36 and add a new test. I usually run the tests after adding the new test case which will fail the first time. But, it generates an image to compare. Then I copy the new image from `./figures/` to `./_images/` and add it to git. Off course you want to check that the image is what you expect. If you re-run the tests they should work now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301
https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301:327,Testability,test,tests,327,"first try the following and check that you don't have any errors. ; ```; cd scanpy/scanpy/tests; py.test test_plotting.py; ```. If that is the case, you can go to this line: https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L36 and add a new test. I usually run the tests after adding the new test case which will fail the first time. But, it generates an image to compare. Then I copy the new image from `./figures/` to `./_images/` and add it to git. Off course you want to check that the image is what you expect. If you re-run the tests they should work now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301
https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301:354,Testability,test,test,354,"first try the following and check that you don't have any errors. ; ```; cd scanpy/scanpy/tests; py.test test_plotting.py; ```. If that is the case, you can go to this line: https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L36 and add a new test. I usually run the tests after adding the new test case which will fail the first time. But, it generates an image to compare. Then I copy the new image from `./figures/` to `./_images/` and add it to git. Off course you want to check that the image is what you expect. If you re-run the tests they should work now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301
https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301:596,Testability,test,tests,596,"first try the following and check that you don't have any errors. ; ```; cd scanpy/scanpy/tests; py.test test_plotting.py; ```. If that is the case, you can go to this line: https://github.com/theislab/scanpy/blob/02fc946a8ce3c2e456dbc6e026ee068734f11e1e/scanpy/tests/test_plotting.py#L36 and add a new test. I usually run the tests after adding the new test case which will fail the first time. But, it generates an image to compare. Then I copy the new image from `./figures/` to `./_images/` and add it to git. Off course you want to check that the image is what you expect. If you re-run the tests they should work now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-739930301
https://github.com/scverse/scanpy/pull/1511#issuecomment-743121097:22,Testability,test,test,22,"thanks for making the test, was planning to do it today (Friday = scanpy day)! Read through your code, so now I'll know how to do it next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-743121097
https://github.com/scverse/scanpy/pull/1512#issuecomment-734934976:201,Testability,log,logic,201,"the problem this PR try to address is the following.; We construct the spatial graph and put keys of obsp in `adata.uns[""spatial""]`. Problem is that there is also the library id in case of Visium. The logic before would work only if there was a single key in `adata.uns[""spatial""]`. Now instead, we need to account for `adata.uns[""spatial""]` being empty, and also having `adata.uns[""spatial""][""connectivities_key""]` or `adata.uns[""spatial""][""distances_key""]`.; It doesn't look really good because of the hard coded keys. Any suggestion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-734934976
https://github.com/scverse/scanpy/pull/1512#issuecomment-738555461:14,Deployability,Pipeline,Pipelines,14,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-738555461
https://github.com/scverse/scanpy/pull/1512#issuecomment-738555461:55,Deployability,pipeline,pipeline,55,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-738555461
https://github.com/scverse/scanpy/pull/1512#issuecomment-738578221:166,Testability,log,logic,166,"Question for this, what heuristics have you tried? My guess would be that `min(distances_between_points) / 3` should be fine for an upper bound. Second, I think this logic is a little convoluted, and I don't know that `library_id` will always be associated with visium only. Would a better check be for `[""metadata""][""software_version""]` or something like that?. It might help for me to know what exactly you're planning on putting in the `""spatial""` entry.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-738578221
https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626:899,Testability,log,logic,899,"all very good points, and I don't think I have a clear solution, it's more that we have to decide how to go about this:; > Question for this, what heuristics have you tried? My guess would be that min(distances_between_points) / 3 should be fine for an upper bound. this indeed could be solved, but we still would have to set this heuristics differently according to the spatial data type in question. E.g. for visium `size=1` is correct, because coordinates are in pixel measure. In the dataset I have now (seqFISH) the coordinates are essentially z-score and so would have to change for instance to the one you proposed. However, why would we want to have `circle` at all in that t case, and not just scatterplot? Since there is no real notion of size, I think a scatterplot is actually more appropriate. We discussed this already but back then we didn't have this example. > Second, I think this logic is a little convoluted, and I don't know that library_id will always be associated with visium only. Would a better check be for [""metadata""][""software_version""] or something like that?. It definitely is, there might be a better solution but I couldn't come up with it. The problem stems in the fact that we have a `library_id` key in `adata.uns[""spatial""]`. In `spatial` we also put this; ```python; {'connectivities_key': 'spatial_connectivities',; 'distances_key': 'spatial_distances',; 'params': {'n_neighbors': 6, 'coord_type': None, 'radius': None}}; ``` ; this is needed for plotting. The default in `sc.pl.spatial` now is that if library_id is `_empty`, then it iteratively search for some keys in `adata.uns[""spatial""]`.; ```python; try: # check if key is empty; spatial_data = adata.uns['spatial']; library_id = next(; (; i; for i in spatial_data.keys(); if i not in [""connectivities_key"", ""distances_key""]; ); ); except (KeyError, StopIteration) as e:; 	library_id = None; ```; The point is that it should only assign whatever key it finds that is not `[""connectivities_key"", ""distance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626
https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626:49,Usability,clear,clear,49,"all very good points, and I don't think I have a clear solution, it's more that we have to decide how to go about this:; > Question for this, what heuristics have you tried? My guess would be that min(distances_between_points) / 3 should be fine for an upper bound. this indeed could be solved, but we still would have to set this heuristics differently according to the spatial data type in question. E.g. for visium `size=1` is correct, because coordinates are in pixel measure. In the dataset I have now (seqFISH) the coordinates are essentially z-score and so would have to change for instance to the one you proposed. However, why would we want to have `circle` at all in that t case, and not just scatterplot? Since there is no real notion of size, I think a scatterplot is actually more appropriate. We discussed this already but back then we didn't have this example. > Second, I think this logic is a little convoluted, and I don't know that library_id will always be associated with visium only. Would a better check be for [""metadata""][""software_version""] or something like that?. It definitely is, there might be a better solution but I couldn't come up with it. The problem stems in the fact that we have a `library_id` key in `adata.uns[""spatial""]`. In `spatial` we also put this; ```python; {'connectivities_key': 'spatial_connectivities',; 'distances_key': 'spatial_distances',; 'params': {'n_neighbors': 6, 'coord_type': None, 'radius': None}}; ``` ; this is needed for plotting. The default in `sc.pl.spatial` now is that if library_id is `_empty`, then it iteratively search for some keys in `adata.uns[""spatial""]`.; ```python; try: # check if key is empty; spatial_data = adata.uns['spatial']; library_id = next(; (; i; for i in spatial_data.keys(); if i not in [""connectivities_key"", ""distances_key""]; ); ); except (KeyError, StopIteration) as e:; 	library_id = None; ```; The point is that it should only assign whatever key it finds that is not `[""connectivities_key"", ""distance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-738701626
https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:334,Integrability,wrap,wrap,334,"Thanks for bearing with me Isaac 😅 🙏 took some of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, coul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306
https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:999,Integrability,wrap,wrap,999,"e of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306
https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:1790,Testability,test,tests,1790,"mbedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": {; ""library1"": {...},; ""library2"": {...},; ...; },; ""spatial_neighbors"": {; ""library_ids"": [""library1"", ...],; ""connectivities_key"": ...,; ""distances_key"": ...,; ""params"": {...},; },; }; ```. yes indeed, I will change all occurrences in squidpy so that `sc.pl.spatial` can simply work as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306
https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:1867,Testability,log,logic,1867,"mbedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": {; ""library1"": {...},; ""library2"": {...},; ...; },; ""spatial_neighbors"": {; ""library_ids"": [""library1"", ...],; ""connectivities_key"": ...,; ""distances_key"": ...,; ""params"": {...},; },; }; ```. yes indeed, I will change all occurrences in squidpy so that `sc.pl.spatial` can simply work as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306
https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:327,Usability,simpl,simply,327,"Thanks for bearing with me Isaac 😅 🙏 took some of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, coul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306
https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:1876,Usability,clear,clearer,1876,"mbedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": {; ""library1"": {...},; ""library2"": {...},; ...; },; ""spatial_neighbors"": {; ""library_ids"": [""library1"", ...],; ""connectivities_key"": ...,; ""distances_key"": ...,; ""params"": {...},; },; }; ```. yes indeed, I will change all occurrences in squidpy so that `sc.pl.spatial` can simply work as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306
https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:2318,Usability,simpl,simply,2318,"mbedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": {; ""library1"": {...},; ""library2"": {...},; ...; },; ""spatial_neighbors"": {; ""library_ids"": [""library1"", ...],; ""connectivities_key"": ...,; ""distances_key"": ...,; ""params"": {...},; },; }; ```. yes indeed, I will change all occurrences in squidpy so that `sc.pl.spatial` can simply work as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306
https://github.com/scverse/scanpy/pull/1512#issuecomment-740744921:654,Integrability,wrap,wraps,654,"> Do you think you could provide me with some example objects that are giving you trouble?. you can take any `sc.datasets.visium_sge` and play around with inverting/not inverting second axis,and plotting using `sc.pl.spatial` or `sc.pl.embedding` where coordinates are in `adata.obsm[""coords""]`. I'll give another summary on current situation and goals:. **Type of spatial data**; 1. data with coordinates centered bottom left and no image (non visium); 2. data with coordinates centered top left and no image (visium); 3. data with coordinates centered top left and image (visium). `sc.pl.spatial` should support all of the above cases. In all cases it wraps embedding but in 1. it uses scatterplot, in 2. and 3. it uses circles (with a specified radius, present only in visium). The inversion is needed in case 2., because in case 3. this is already handled by the image axis plot.; My solution for this is to pass inverted coordinates for y axis in 2. to `embedding`, so that everything can be handled by the function independently. ; What do you think is a good way to go about this ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-740744921
https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756:768,Availability,mask,mask,768,"thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. So now by default everything that calls spatial inverts y axis. I also added two lines in the doc to clarify for user. I think this account for all the cases above, which are also presents in tests. > For case 1, when there is no image, what is the advantage to using sc.pl.spatial over just sc.pl.embedding?. indeed none, but I still like that user could use `sc.pl.spatial` which in that case is a simple call `sc.pl.embedding(adata, basis=""spatial"", **kwargs)`. > Also what about non-visium data with an image?. in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot) unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units (e.g see [this](https://www.biorxiv.org/content/10.1101/800748v2.abstract) and [this](https://www.biorxiv.org/content/10.1101/2020.02.12.945345v1) paper).; It can be also more complicated if the data has subcellular resolution, like [this](https://science.sciencemag.org/content/361/6401/eaar7042); For all these cases, we'll rely on Napari, I'm in the process of building a class that maps anndata+img container to napari https://github.com/theislab/squidpy/pull/184",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756
https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756:295,Testability,test,tests,295,"thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. So now by default everything that calls spatial inverts y axis. I also added two lines in the doc to clarify for user. I think this account for all the cases above, which are also presents in tests. > For case 1, when there is no image, what is the advantage to using sc.pl.spatial over just sc.pl.embedding?. indeed none, but I still like that user could use `sc.pl.spatial` which in that case is a simple call `sc.pl.embedding(adata, basis=""spatial"", **kwargs)`. > Also what about non-visium data with an image?. in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot) unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units (e.g see [this](https://www.biorxiv.org/content/10.1101/800748v2.abstract) and [this](https://www.biorxiv.org/content/10.1101/2020.02.12.945345v1) paper).; It can be also more complicated if the data has subcellular resolution, like [this](https://science.sciencemag.org/content/361/6401/eaar7042); For all these cases, we'll rely on Napari, I'm in the process of building a class that maps anndata+img container to napari https://github.com/theislab/squidpy/pull/184",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756
https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756:64,Usability,clear,clear,64,"thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. So now by default everything that calls spatial inverts y axis. I also added two lines in the doc to clarify for user. I think this account for all the cases above, which are also presents in tests. > For case 1, when there is no image, what is the advantage to using sc.pl.spatial over just sc.pl.embedding?. indeed none, but I still like that user could use `sc.pl.spatial` which in that case is a simple call `sc.pl.embedding(adata, basis=""spatial"", **kwargs)`. > Also what about non-visium data with an image?. in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot) unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units (e.g see [this](https://www.biorxiv.org/content/10.1101/800748v2.abstract) and [this](https://www.biorxiv.org/content/10.1101/2020.02.12.945345v1) paper).; It can be also more complicated if the data has subcellular resolution, like [this](https://science.sciencemag.org/content/361/6401/eaar7042); For all these cases, we'll rely on Napari, I'm in the process of building a class that maps anndata+img container to napari https://github.com/theislab/squidpy/pull/184",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756
https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756:503,Usability,simpl,simple,503,"thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. So now by default everything that calls spatial inverts y axis. I also added two lines in the doc to clarify for user. I think this account for all the cases above, which are also presents in tests. > For case 1, when there is no image, what is the advantage to using sc.pl.spatial over just sc.pl.embedding?. indeed none, but I still like that user could use `sc.pl.spatial` which in that case is a simple call `sc.pl.embedding(adata, basis=""spatial"", **kwargs)`. > Also what about non-visium data with an image?. in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot) unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units (e.g see [this](https://www.biorxiv.org/content/10.1101/800748v2.abstract) and [this](https://www.biorxiv.org/content/10.1101/2020.02.12.945345v1) paper).; It can be also more complicated if the data has subcellular resolution, like [this](https://science.sciencemag.org/content/361/6401/eaar7042); For all these cases, we'll rely on Napari, I'm in the process of building a class that maps anndata+img container to napari https://github.com/theislab/squidpy/pull/184",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-741689756
https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703:686,Availability,mask,mask,686,"> thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. No worries!. I think communicating about the ideas we have for these tools can be fraught. > in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot). I don't think this is the case. . First, I believe there are non-visium grid based spatial methods (I remember seeing a product page for one, but can't find it atm). Second, I think you don't need segmentation info to use this function. You just need coordinates (probably derived from segmentation) and possibly an image. Like this:. > unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units. But I think a user already having done the segmentation, then coming to scanpy is a reasonable workflow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703
https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703:66,Usability,clear,clear,66,"> thank you for bearing with me 😅 I understand I should have been clear like that from beginning, sorry. No worries!. I think communicating about the ideas we have for these tools can be fraught. > in that case, we essentially don't strictly have a direct mapping to our observation uni (i.e. cell/spot). I don't think this is the case. . First, I believe there are non-visium grid based spatial methods (I remember seeing a product page for one, but can't find it atm). Second, I think you don't need segmentation info to use this function. You just need coordinates (probably derived from segmentation) and possibly an image. Like this:. > unless the user also specify a segmentation mask or some other way of annotating molecular probes in the image to observation units. But I think a user already having done the segmentation, then coming to scanpy is a reasonable workflow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-742217703
https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:2755,Availability,avail,available,2755," ```python; sc.pl.spatial(adata, color=""leiden"", groups=[""0""]); ```; <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102686727-cc8e5f00-41e9-11eb-8d61-5c53700b39d7.png). </details>. Finally, all the image processing part is removed from embedding and only present in spatial. --------------------. > No behaviour changes in embedding if the basis is called ""spatial"" vs anything else, this should be triggered by calling the spatial function. this is addressed, embedding changes behaviour only if img is passed, but has nothing to do with spatial, there is a small trick, and has to do with `ax.invert_yaxis()`. See following point. --------------------. > When spatial is called, it’s always shapes being drawn on an image. If there isn’t an image passed, an empty image would be generated. There would be no scatter plot case here. I played around with this and decided to go against. Here's the following reasons; - if no img is passed, then we should assume that also no `scale_basis` is provided/available. Thus, the empty img to be created has to be of the size of the spatial coordinates system. In the case of visium (but would be even worse for larger field of views) the ""blank source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514
https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:3012,Availability,down,down,3012,"by calling the spatial function. this is addressed, embedding changes behaviour only if img is passed, but has nothing to do with spatial, there is a small trick, and has to do with `ax.invert_yaxis()`. See following point. --------------------. > When spatial is called, it’s always shapes being drawn on an image. If there isn’t an image passed, an empty image would be generated. There would be no scatter plot case here. I played around with this and decided to go against. Here's the following reasons; - if no img is passed, then we should assume that also no `scale_basis` is provided/available. Thus, the empty img to be created has to be of the size of the spatial coordinates system. In the case of visium (but would be even worse for larger field of views) the ""blank source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. -------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514
https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:3355,Testability,test,test,3355,"xis()`. See following point. --------------------. > When spatial is called, it’s always shapes being drawn on an image. If there isn’t an image passed, an empty image would be generated. There would be no scatter plot case here. I played around with this and decided to go against. Here's the following reasons; - if no img is passed, then we should assume that also no `scale_basis` is provided/available. Thus, the empty img to be created has to be of the size of the spatial coordinates system. In the case of visium (but would be even worse for larger field of views) the ""blank source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514
https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:4842,Testability,test,tests,4842,"source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter` but invert coordinate since expected origin is top left. Furthermore, `sc.pl.embedding` now simply support the possibility to add an image in the background and accepts the relevant arguments needed for the image to be displayed correctly and modified, as well as a scale_basis argument to match the image coordinate if needed. That's it, looking forward to hear what are your thoughts and if you agree with current behaviour I'll go on and changing the docs as well as writing more tests (especially for first example where `groups` is used, where the background color is different)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514
https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:3551,Usability,simpl,simply,3551,"layed around with this and decided to go against. Here's the following reasons; - if no img is passed, then we should assume that also no `scale_basis` is provided/available. Thus, the empty img to be created has to be of the size of the spatial coordinates system. In the case of visium (but would be even worse for larger field of views) the ""blank source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter` but invert coordinate since expected origin is top left. Furthermore, `sc.pl.embedding` now simply support the possibility to add an image in the background and accepts the relevant arguments needed for the image to be displayed corr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514
https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514:4451,Usability,simpl,simply,4451,"source image"" would be very often a 10k * 10k empry array. This slows down the plotting and create an unneccesary large object; - if no img is passed, there really shouldn't be any need for using `circles` instead of `scatter` , since there is no notion of ""spot radius"" or ""spot size"" (this was my first idea since the very beginning, but eventually agreed to still use scale factor. This is also the reason why test is failing with empty visium). However, if no img is passed, when calling spatial the scatterplot should still have inverted coordinates (because we assume origin to be top left). I ended up simply setting `img = _empty` and adding it in embedding:; ```python; if img is _empty:; 	ax.invert_yaxis(); ```; This is the behviour; ```python; sc.pl.embedding(adata, color=""leiden"", basis=""spatial""); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687092-e1b8bd00-41ec-11eb-9970-4a9b98a9e68f.png). </details>. ```python; sc.pl.spatial(adata, color=""leiden"", img_key=None); ```. <details>; <summary>Details</summary>. ![image](https://user-images.githubusercontent.com/25887487/102687110-feed8b80-41ec-11eb-9063-3c3167c9b6b7.png). </details>. ----------------. TO summarize, what `sc.pl.spatial` does is:; - if an image is present, process and scale accordingly and use `circles` instead of `scatter`; - if an image is not present, use `scatter` but invert coordinate since expected origin is top left. Furthermore, `sc.pl.embedding` now simply support the possibility to add an image in the background and accepts the relevant arguments needed for the image to be displayed correctly and modified, as well as a scale_basis argument to match the image coordinate if needed. That's it, looking forward to hear what are your thoughts and if you agree with current behaviour I'll go on and changing the docs as well as writing more tests (especially for first example where `groups` is used, where the background color is different)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-748455514
https://github.com/scverse/scanpy/pull/1512#issuecomment-750338610:686,Integrability,interface,interface,686,"> Can scale_factor be removed as an argument to embedding, and instead have that handling occur inside spatial?. no, otherwise I would have to modify the adata or pass a copy, and this would break other functionalities (first that come to mind, categorical colors saved in adata.uns). > Otherwise we assume it's already an array, and make sure it's the right shape. what do you mean by that>? What you are proposing is to pass the adata.obsm as array in question and not as a string basis right? Is that possible now>? Would be happy to do that but don't want to create and pass an adata copy. > Can an image be passed directly into spatial? I'd prefer this as being the ""expert users"" interface for plotting over an image, and think passing an image to embedding could be removed altogether in the future. done, also there is no image in embedding now. Everything is handled by spatial. It is possible to do something like this:. ```python; img = adata.uns[""spatial""][""V1_Adult_Mouse_Brain""][""images""][""hires""]; scalef = adata.uns[""spatial""][""V1_Adult_Mouse_Brain""][""scalefactors""][; ""tissue_hires_scalef""; ]; sc.pl.spatial(; adata,; color=""leiden"",; scale_factor=scalef,; img=img,; size=100,; basis=""spatial"",; groups=[""0""],; ); ```; ![image](https://user-images.githubusercontent.com/25887487/103009720-7eee5b00-4537-11eb-9bbf-39751493890f.png). I would still like to have this in 1.7 if possible, I can write docs and additional tests real quick tomorrow early morning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-750338610
https://github.com/scverse/scanpy/pull/1512#issuecomment-750338610:1433,Testability,test,tests,1433,"> Can scale_factor be removed as an argument to embedding, and instead have that handling occur inside spatial?. no, otherwise I would have to modify the adata or pass a copy, and this would break other functionalities (first that come to mind, categorical colors saved in adata.uns). > Otherwise we assume it's already an array, and make sure it's the right shape. what do you mean by that>? What you are proposing is to pass the adata.obsm as array in question and not as a string basis right? Is that possible now>? Would be happy to do that but don't want to create and pass an adata copy. > Can an image be passed directly into spatial? I'd prefer this as being the ""expert users"" interface for plotting over an image, and think passing an image to embedding could be removed altogether in the future. done, also there is no image in embedding now. Everything is handled by spatial. It is possible to do something like this:. ```python; img = adata.uns[""spatial""][""V1_Adult_Mouse_Brain""][""images""][""hires""]; scalef = adata.uns[""spatial""][""V1_Adult_Mouse_Brain""][""scalefactors""][; ""tissue_hires_scalef""; ]; sc.pl.spatial(; adata,; color=""leiden"",; scale_factor=scalef,; img=img,; size=100,; basis=""spatial"",; groups=[""0""],; ); ```; ![image](https://user-images.githubusercontent.com/25887487/103009720-7eee5b00-4537-11eb-9bbf-39751493890f.png). I would still like to have this in 1.7 if possible, I can write docs and additional tests real quick tomorrow early morning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-750338610
https://github.com/scverse/scanpy/pull/1512#issuecomment-754420131:464,Testability,test,tests,464,"Oh, wow, sorry! I completely missed your comment here!. > What you are proposing is to pass the adata.obsm as array in question and not as a string basis right? . What I was thinking: if it's a string get the array from `obsm`, if it's an array, check that it's shape is right, then use the array directly. > It is possible to do something like this. That looks great, thanks!. > I would still like to have this in 1.7 if possible, I can write docs and additional tests real quick tomorrow early morning. For sure! I was waiting on this actually, just managed to miss any notifications about it. Sorry again about that!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-754420131
https://github.com/scverse/scanpy/pull/1512#issuecomment-755199967:601,Testability,test,test,601,"> What I was thinking: if it's a string get the array from obsm, if it's an array, check that it's shape is right, then use the array directly. I'm really sorry but I still don't get it 😅 . What I unnderstand is to modify `adata.obsm[spatial]` in `pl.spatial` and pass that to emebdding. However, I don't want to modify the adata in place or pass a copy. Maybe I'm missing something fundamental, but I could see doing this only if modifying the adata or passing the spatial coordinates as array (but is it possible in sc.pl.embedding? I couldn't find a way). For the rest, I've added docs and added a test, should be ready to go (?). Thanks again for bearing with me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-755199967
https://github.com/scverse/scanpy/pull/1512#issuecomment-756087364:778,Usability,simpl,simply,778,"looks really great, I'd say Yes! to pretty much everything, thanks for looking into this.; > Getting the spot size even if the image isn't used. so this is something we are going back and forth a lot, I still think it has pros and cons, and also agree with the point below re having a separate argument `spot_size`. ; If `sc.pl.spatial` *always* plot `circles` and not `scatter`, then the size of the radius needs to be inferred from the data: this makes me a bit worried for the different cases that could arise. I understand that is much nicer that the function returns always the same type of plot (circles), but it might be a bit forcing in this context (given the heterogeneity of the data). What I would agree instead is to pass the `size_spot` and use circles, otherwise simply use scatter (and then set size). What do you think? . > Using ax.set_aspect(""equal"") when there is no image, so that the aspect ratio is equivalent (coordinates are assumed to be pixel space); this is really nice, I'd say yes in principle, would like to try it out though for couple of plots. > If crop_coords is not passed, use the cropping matplotlib would have used if there was no image. This is done by getting the axis limits before the image is added. this is also fine and probably cleaner than having an heuristic for the offset. > I feel like it would make sense for these to crop to the same part of the image or embedding:. missed this, ok yes it makes sense, then metric of `crop_coord` is the same as `adata.obsm[""spatial""]`. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-756087364
https://github.com/scverse/scanpy/pull/1512#issuecomment-759329104:80,Availability,down,down,80,"Merging this, docs are failing to build from intersphinx since numpy's docs are down. This commit has built on read the docs before, so I'm happy to assume it still does. Thanks @giovp!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-759329104
https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689:1710,Availability,mainten,maintenance,1710,"I've used it a bit, and have gotten nice results. I think I've mentioned it before (#938), but that was on an unrelated issue so it's good to have. The results are nice:. <details>; <summary> Example usage </summary>. ```python; from adjustText import adjust_text. def gen_mpl_labels(; adata, groupby, exclude=(), ax=None, adjust_kwargs=None, text_kwargs=None; ):; if adjust_kwargs is None:; adjust_kwargs = {""text_from_points"": False}; if text_kwargs is None:; text_kwargs = {}. medians = {}. for g, g_idx in adata.obs.groupby(groupby).groups.items():; if g in exclude:; continue; medians[g] = np.median(adata[g_idx].obsm[""X_umap""], axis=0). if ax is None:; texts = [; plt.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items(); ]; else:; texts = [ax.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items()]. adjust_text(texts, **adjust_kwargs). with plt.rc_context({""figure.figsize"": (8, 8), ""figure.dpi"": 300, ""figure.frameon"": False}):; ax = sc.pl.umap(pbmc, color=""Low-level celltypes"", show=False, legend_loc=None, frameon=False); gen_mpl_labels(; pbmc,; ""Low-level celltypes"",; exclude=(""None"",), # This was before we had the `nan` behaviour; ax=ax,; adjust_kwargs=dict(arrowprops=dict(arrowstyle='-', color='black')),; text_kwargs=dict(fontsize=14),; ); fig = ax.get_figure(); fig.tight_layout(); plt.show(); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/100496350-81af9780-31a7-11eb-8b38-2eb7f914c1a1.png). I believe you're also supposed to be able to make the text repel from points, so they don't sit on top of your data, but I had some trouble getting that working at the time. I'm a bit antsy about having this as a required dependency since maintenance [doesn't seem too active](https://pypi.org/project/adjustText/#history). Could be an optional dependency, used with `legend_loc=""adjust_text""`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689
https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689:1693,Integrability,depend,dependency,1693,"I've used it a bit, and have gotten nice results. I think I've mentioned it before (#938), but that was on an unrelated issue so it's good to have. The results are nice:. <details>; <summary> Example usage </summary>. ```python; from adjustText import adjust_text. def gen_mpl_labels(; adata, groupby, exclude=(), ax=None, adjust_kwargs=None, text_kwargs=None; ):; if adjust_kwargs is None:; adjust_kwargs = {""text_from_points"": False}; if text_kwargs is None:; text_kwargs = {}. medians = {}. for g, g_idx in adata.obs.groupby(groupby).groups.items():; if g in exclude:; continue; medians[g] = np.median(adata[g_idx].obsm[""X_umap""], axis=0). if ax is None:; texts = [; plt.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items(); ]; else:; texts = [ax.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items()]. adjust_text(texts, **adjust_kwargs). with plt.rc_context({""figure.figsize"": (8, 8), ""figure.dpi"": 300, ""figure.frameon"": False}):; ax = sc.pl.umap(pbmc, color=""Low-level celltypes"", show=False, legend_loc=None, frameon=False); gen_mpl_labels(; pbmc,; ""Low-level celltypes"",; exclude=(""None"",), # This was before we had the `nan` behaviour; ax=ax,; adjust_kwargs=dict(arrowprops=dict(arrowstyle='-', color='black')),; text_kwargs=dict(fontsize=14),; ); fig = ax.get_figure(); fig.tight_layout(); plt.show(); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/100496350-81af9780-31a7-11eb-8b38-2eb7f914c1a1.png). I believe you're also supposed to be able to make the text repel from points, so they don't sit on top of your data, but I had some trouble getting that working at the time. I'm a bit antsy about having this as a required dependency since maintenance [doesn't seem too active](https://pypi.org/project/adjustText/#history). Could be an optional dependency, used with `legend_loc=""adjust_text""`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689
https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689:1816,Integrability,depend,dependency,1816,"I've used it a bit, and have gotten nice results. I think I've mentioned it before (#938), but that was on an unrelated issue so it's good to have. The results are nice:. <details>; <summary> Example usage </summary>. ```python; from adjustText import adjust_text. def gen_mpl_labels(; adata, groupby, exclude=(), ax=None, adjust_kwargs=None, text_kwargs=None; ):; if adjust_kwargs is None:; adjust_kwargs = {""text_from_points"": False}; if text_kwargs is None:; text_kwargs = {}. medians = {}. for g, g_idx in adata.obs.groupby(groupby).groups.items():; if g in exclude:; continue; medians[g] = np.median(adata[g_idx].obsm[""X_umap""], axis=0). if ax is None:; texts = [; plt.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items(); ]; else:; texts = [ax.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items()]. adjust_text(texts, **adjust_kwargs). with plt.rc_context({""figure.figsize"": (8, 8), ""figure.dpi"": 300, ""figure.frameon"": False}):; ax = sc.pl.umap(pbmc, color=""Low-level celltypes"", show=False, legend_loc=None, frameon=False); gen_mpl_labels(; pbmc,; ""Low-level celltypes"",; exclude=(""None"",), # This was before we had the `nan` behaviour; ax=ax,; adjust_kwargs=dict(arrowprops=dict(arrowstyle='-', color='black')),; text_kwargs=dict(fontsize=14),; ); fig = ax.get_figure(); fig.tight_layout(); plt.show(); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/100496350-81af9780-31a7-11eb-8b38-2eb7f914c1a1.png). I believe you're also supposed to be able to make the text repel from points, so they don't sit on top of your data, but I had some trouble getting that working at the time. I'm a bit antsy about having this as a required dependency since maintenance [doesn't seem too active](https://pypi.org/project/adjustText/#history). Could be an optional dependency, used with `legend_loc=""adjust_text""`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689
https://github.com/scverse/scanpy/issues/1513#issuecomment-839597935:384,Availability,down,down,384,"I have been unable to get this to look good by default. It can be made to look good by playing around with the parameters, but then we're not really saving the user much effort. A strategy that seemed to work okay was to repel the labels from the points, followed by a second repulsion from other labels. But then I had to redraw the lines manually. Current thoughts are to punt this down the road. Maybe there will be a better solution in the future, or maybe there's a clever parameterization fix I hadn't thought of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-839597935
https://github.com/scverse/scanpy/issues/1513#issuecomment-839597935:478,Modifiability,parameteriz,parameterization,478,"I have been unable to get this to look good by default. It can be made to look good by playing around with the parameters, but then we're not really saving the user much effort. A strategy that seemed to work okay was to repel the labels from the points, followed by a second repulsion from other labels. But then I had to redraw the lines manually. Current thoughts are to punt this down the road. Maybe there will be a better solution in the future, or maybe there's a clever parameterization fix I hadn't thought of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-839597935
https://github.com/scverse/scanpy/issues/1513#issuecomment-839982675:68,Performance,optimiz,optimization,68,"I never get adjustText to work without numerous rounds of parameter optimization, so yeah, I agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-839982675
https://github.com/scverse/scanpy/issues/1513#issuecomment-2048182372:449,Integrability,message,message,449,"> > @VladimirShitov can you give an example of how you use the gen_mpl_labels function? I tried it and got somewhat different results. For example it lacked the lines pointing to the cluster centers. Thanks!; > ; > Have you solved this problem? I still can't show the lines pointing to the cluster centers. Hi @nnnanchen. I apologize, I forgot to add the `adjust_text` call in the very last line of the `gen_mpl_labels` above. I edited the previous message. Can you try again?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-2048182372
https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619:176,Deployability,pipeline,pipelines,176,"This is fantastic, thank you!. A few things I'm unclear on:. * Why is this PR getting a build if there is no [`pr` trigger entry](https://docs.microsoft.com/en-us/azure/devops/pipelines/repos/github?view=azure-devops&tabs=yaml#pr-triggers) in the `yaml`?; * Why isn't travis running on this PR? It might be that we've turned off branch CI since it was causing double runs with branches on this repo which were being used in PRs, but I thought it would still trigger once a pr was made. I think I'm just going to try and merge this, since it seems to be working. We can fine tune it via PRs as we go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619
https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619:574,Performance,tune,tune,574,"This is fantastic, thank you!. A few things I'm unclear on:. * Why is this PR getting a build if there is no [`pr` trigger entry](https://docs.microsoft.com/en-us/azure/devops/pipelines/repos/github?view=azure-devops&tabs=yaml#pr-triggers) in the `yaml`?; * Why isn't travis running on this PR? It might be that we've turned off branch CI since it was causing double runs with branches on this repo which were being used in PRs, but I thought it would still trigger once a pr was made. I think I'm just going to try and merge this, since it seems to be working. We can fine tune it via PRs as we go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619
https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275:98,Availability,down,down,98,"> Why is this PR getting a build if there is no `pr` trigger entry in the yaml?. See 3 paragraphs down:. > If no pr triggers appear in your YAML file, pull request validations are automatically enabled for all branches, as if you wrote the following pr trigger. This configuration triggers a build when any pull request is created, and when commits come into the source branch of any active pull request.; > ; > ```; > pr:; > branches:; > include:; > - '*' # must quote since ""*"" is a YAML reserved character; we want a string; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275
https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275:267,Deployability,configurat,configuration,267,"> Why is this PR getting a build if there is no `pr` trigger entry in the yaml?. See 3 paragraphs down:. > If no pr triggers appear in your YAML file, pull request validations are automatically enabled for all branches, as if you wrote the following pr trigger. This configuration triggers a build when any pull request is created, and when commits come into the source branch of any active pull request.; > ; > ```; > pr:; > branches:; > include:; > - '*' # must quote since ""*"" is a YAML reserved character; we want a string; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275
https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275:267,Modifiability,config,configuration,267,"> Why is this PR getting a build if there is no `pr` trigger entry in the yaml?. See 3 paragraphs down:. > If no pr triggers appear in your YAML file, pull request validations are automatically enabled for all branches, as if you wrote the following pr trigger. This configuration triggers a build when any pull request is created, and when commits come into the source branch of any active pull request.; > ; > ```; > pr:; > branches:; > include:; > - '*' # must quote since ""*"" is a YAML reserved character; we want a string; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275
https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275:164,Security,validat,validations,164,"> Why is this PR getting a build if there is no `pr` trigger entry in the yaml?. See 3 paragraphs down:. > If no pr triggers appear in your YAML file, pull request validations are automatically enabled for all branches, as if you wrote the following pr trigger. This configuration triggers a build when any pull request is created, and when commits come into the source branch of any active pull request.; > ; > ```; > pr:; > branches:; > include:; > - '*' # must quote since ""*"" is a YAML reserved character; we want a string; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275
https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:295,Modifiability,flexible,flexible,295,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259
https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:65,Testability,test,tested,65,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259
https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:151,Testability,test,tested,151,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259
https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:242,Usability,clear,clear,242,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259
https://github.com/scverse/scanpy/issues/1519#issuecomment-744303985:476,Modifiability,flexible,flexible,476,"I see. Yes then maybe we should change this annotation:; `groups : {‘all’}, Iterable[str] (default: 'all').`; `Subset of groups, e.g. ['g1', 'g2', 'g3'], to which comparison shall be restricted, or 'all' (default), for all groups.`; to something like:; `Subset of groups, e.g. ['g1', 'g2', 'g3'], for which differentially expressed genes should be calculated, or 'all' (default) for all groups.`; ?; I could also take a look to see how we can make the reference argument more flexible, if you agree that would be a good feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-744303985
https://github.com/scverse/scanpy/issues/1519#issuecomment-747197402:93,Usability,simpl,simple,93,That doc change looks good to me!. For any change to `reference` it would be good to keep it simple. Maybe it could accept a list of groups?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-747197402
https://github.com/scverse/scanpy/pull/1520#issuecomment-738557948:366,Deployability,pipeline,pipelines,366,"AFAICT `ubuntu 16.04` is what they use in most of their examples. Considering many of our users will be on academic clusters, I think old-ish versions of linux are reasonable to test against. The alternative would probably be `ubuntu-18.04`, which we should switch to when `16` is out of support. [Here are the options](https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops&tabs=yaml#software).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1520#issuecomment-738557948
https://github.com/scverse/scanpy/pull/1520#issuecomment-738557948:178,Testability,test,test,178,"AFAICT `ubuntu 16.04` is what they use in most of their examples. Considering many of our users will be on academic clusters, I think old-ish versions of linux are reasonable to test against. The alternative would probably be `ubuntu-18.04`, which we should switch to when `16` is out of support. [Here are the options](https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops&tabs=yaml#software).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1520#issuecomment-738557948
https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883:31,Deployability,install,install,31,"Current problems:. - [x] `flit install --pth-file --deps=production` doesn’t work with setuptools-scm (just in conda?); - [x] it needs `setuptools_scm`, which is in the `dev` extra. We need to document this.; - [x] circumvent pypa/setuptools#2531; - [x] flit doesn’t work if setup.py exists (still? where’s the issue?); - [x] `pip install -e` with the setup.py doesn’t install deps if the metadata is broken. Not a problem (to my knowledge), but because you mentioned it:. conda doesn’t identify flit installed distributions as `<develop>`. This is because flit installs a regular `.dist-info` directory instead of dumping an `.egg-info` into your dev directory and adding an `.egg-link` file to the `site-packages`; That needs to be fixed by conda, [I suggest they should parse symlinks](https://discuss.python.org/t/standardising-editable-mode-installs-runtime-layout-not-hooks/4098).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883
https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883:331,Deployability,install,install,331,"Current problems:. - [x] `flit install --pth-file --deps=production` doesn’t work with setuptools-scm (just in conda?); - [x] it needs `setuptools_scm`, which is in the `dev` extra. We need to document this.; - [x] circumvent pypa/setuptools#2531; - [x] flit doesn’t work if setup.py exists (still? where’s the issue?); - [x] `pip install -e` with the setup.py doesn’t install deps if the metadata is broken. Not a problem (to my knowledge), but because you mentioned it:. conda doesn’t identify flit installed distributions as `<develop>`. This is because flit installs a regular `.dist-info` directory instead of dumping an `.egg-info` into your dev directory and adding an `.egg-link` file to the `site-packages`; That needs to be fixed by conda, [I suggest they should parse symlinks](https://discuss.python.org/t/standardising-editable-mode-installs-runtime-layout-not-hooks/4098).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883
https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883:369,Deployability,install,install,369,"Current problems:. - [x] `flit install --pth-file --deps=production` doesn’t work with setuptools-scm (just in conda?); - [x] it needs `setuptools_scm`, which is in the `dev` extra. We need to document this.; - [x] circumvent pypa/setuptools#2531; - [x] flit doesn’t work if setup.py exists (still? where’s the issue?); - [x] `pip install -e` with the setup.py doesn’t install deps if the metadata is broken. Not a problem (to my knowledge), but because you mentioned it:. conda doesn’t identify flit installed distributions as `<develop>`. This is because flit installs a regular `.dist-info` directory instead of dumping an `.egg-info` into your dev directory and adding an `.egg-link` file to the `site-packages`; That needs to be fixed by conda, [I suggest they should parse symlinks](https://discuss.python.org/t/standardising-editable-mode-installs-runtime-layout-not-hooks/4098).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883
https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883:501,Deployability,install,installed,501,"Current problems:. - [x] `flit install --pth-file --deps=production` doesn’t work with setuptools-scm (just in conda?); - [x] it needs `setuptools_scm`, which is in the `dev` extra. We need to document this.; - [x] circumvent pypa/setuptools#2531; - [x] flit doesn’t work if setup.py exists (still? where’s the issue?); - [x] `pip install -e` with the setup.py doesn’t install deps if the metadata is broken. Not a problem (to my knowledge), but because you mentioned it:. conda doesn’t identify flit installed distributions as `<develop>`. This is because flit installs a regular `.dist-info` directory instead of dumping an `.egg-info` into your dev directory and adding an `.egg-link` file to the `site-packages`; That needs to be fixed by conda, [I suggest they should parse symlinks](https://discuss.python.org/t/standardising-editable-mode-installs-runtime-layout-not-hooks/4098).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883
https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883:562,Deployability,install,installs,562,"Current problems:. - [x] `flit install --pth-file --deps=production` doesn’t work with setuptools-scm (just in conda?); - [x] it needs `setuptools_scm`, which is in the `dev` extra. We need to document this.; - [x] circumvent pypa/setuptools#2531; - [x] flit doesn’t work if setup.py exists (still? where’s the issue?); - [x] `pip install -e` with the setup.py doesn’t install deps if the metadata is broken. Not a problem (to my knowledge), but because you mentioned it:. conda doesn’t identify flit installed distributions as `<develop>`. This is because flit installs a regular `.dist-info` directory instead of dumping an `.egg-info` into your dev directory and adding an `.egg-link` file to the `site-packages`; That needs to be fixed by conda, [I suggest they should parse symlinks](https://discuss.python.org/t/standardising-editable-mode-installs-runtime-layout-not-hooks/4098).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883
https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883:846,Deployability,install,installs-runtime-layout-not-hooks,846,"Current problems:. - [x] `flit install --pth-file --deps=production` doesn’t work with setuptools-scm (just in conda?); - [x] it needs `setuptools_scm`, which is in the `dev` extra. We need to document this.; - [x] circumvent pypa/setuptools#2531; - [x] flit doesn’t work if setup.py exists (still? where’s the issue?); - [x] `pip install -e` with the setup.py doesn’t install deps if the metadata is broken. Not a problem (to my knowledge), but because you mentioned it:. conda doesn’t identify flit installed distributions as `<develop>`. This is because flit installs a regular `.dist-info` directory instead of dumping an `.egg-info` into your dev directory and adding an `.egg-link` file to the `site-packages`; That needs to be fixed by conda, [I suggest they should parse symlinks](https://discuss.python.org/t/standardising-editable-mode-installs-runtime-layout-not-hooks/4098).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760095883
https://github.com/scverse/scanpy/pull/1527#issuecomment-760135174:7,Deployability,install,install,7,"> flit install --pth-file --deps=production doesn’t work with setuptools-scm (just in conda?); >; > Fix: it needs setuptools_scm, which is in the dev extra. We need to document this. I had run into a problem with. ```; flit install --pth-file --deps=develop; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760135174
https://github.com/scverse/scanpy/pull/1527#issuecomment-760135174:224,Deployability,install,install,224,"> flit install --pth-file --deps=production doesn’t work with setuptools-scm (just in conda?); >; > Fix: it needs setuptools_scm, which is in the dev extra. We need to document this. I had run into a problem with. ```; flit install --pth-file --deps=develop; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760135174
https://github.com/scverse/scanpy/pull/1527#issuecomment-760622794:70,Testability,test,testing,70,"I'm not comfortable with doing an absolute switch without significant testing. I would want a few frequent contributors (Fidel, Sergei, Goecken?) to try it out, try and break it, and make sure we could get things done. I also still have concerns about the limitations of what flit can distribute, and would like to hear other's thoughts on this. If a `setuptools` based workflow can happen, then I don't think these steps are necessarily blocking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760622794
https://github.com/scverse/scanpy/pull/1527#issuecomment-760774086:134,Testability,test,test,134,"Ha, got it! Seems like all tools break differently on multiline strings in the metadata. I changed the string back to single ticks to test that metadata problem and then forgot. Now it works and is tested!. Also I checked and `flit build` seems to be working with `setup.py` existing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760774086
https://github.com/scverse/scanpy/pull/1527#issuecomment-760774086:198,Testability,test,tested,198,"Ha, got it! Seems like all tools break differently on multiline strings in the metadata. I changed the string back to single ticks to test that metadata problem and then forgot. Now it works and is tested!. Also I checked and `flit build` seems to be working with `setup.py` existing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-760774086
https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746:74,Deployability,install,install,74,"Poetry is great! But i remember two problems:. 1. no good way to editably install into some env: python-poetry/poetry#34; 2. doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140. Maybe @ivirshup knows more. We talked about it way back when. The things I’m missing from flit are better dynamic version support (currently a bit hacky, but a PR exists) and sth. like `poetry install --no-root`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746
https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746:419,Deployability,install,install,419,"Poetry is great! But i remember two problems:. 1. no good way to editably install into some env: python-poetry/poetry#34; 2. doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140. Maybe @ivirshup knows more. We talked about it way back when. The things I’m missing from flit are better dynamic version support (currently a bit hacky, but a PR exists) and sth. like `poetry install --no-root`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746
https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746:141,Modifiability,plugin,plugins,141,"Poetry is great! But i remember two problems:. 1. no good way to editably install into some env: python-poetry/poetry#34; 2. doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140. Maybe @ivirshup knows more. We talked about it way back when. The things I’m missing from flit are better dynamic version support (currently a bit hacky, but a PR exists) and sth. like `poetry install --no-root`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746
https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:103,Deployability,install,install,103,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434
https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:292,Deployability,install,install,292,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434
https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:691,Deployability,install,install,691,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434
https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:1410,Deployability,install,install,1410,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434
https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:481,Integrability,depend,dependency,481,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434
https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:169,Modifiability,plugin,plugins,169,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434
https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:508,Modifiability,plugin,plugins,508,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434
https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:1375,Modifiability,plugin,plugin,1375,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434
https://github.com/scverse/scanpy/pull/1527#issuecomment-765904146:43,Usability,learn,learn,43,"Flit has a very tiny surface area. You can learn its full CLI in literally 2 minutes, as it doesn’t include any kind of new concept (like Poetry’s venv management).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904146
https://github.com/scverse/scanpy/pull/1527#issuecomment-765904369:158,Usability,learn,learn,158,"Yeah, I agree. Go for it ;) Saturday, 23 January 2021, 11:48AM +01:00 from Philipp A. notifications@github.com :. >Flit has a very tiny surface area. You can learn its full CLI in literally 2 minutes, as it doesn’t include any kind of new concept (like Poetry’s venv management).; >—; >You are receiving this because you commented.; >Reply to this email directly, view it on GitHub , or unsubscribe .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765904369
https://github.com/scverse/scanpy/pull/1527#issuecomment-777322129:342,Deployability,install,installation,342,"About the version: That’s because `__file__` in `get_version(root='..', relative_to=__file__)` is a path where no component is in a git repository. Therefore the `setuptools_scm` code fails, we enter the `else` branch, and the (stale) metadata is used. Something like the following could make the `_metadata` module figure out we’re in a dev installation after all. (Needs some more as `here.readlink()` returns a relative path. I’m against `resolve()` as we really just want that one link to be replaced and `resolve()` is a sledge hammer). ```py; file_resolved = str(here.readlink() / f'{__name__}.py') if here.is_symlink() else __file__; ```. But the real issue is that dev installs are a hack and scanpy’s package metadata is stale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777322129
https://github.com/scverse/scanpy/pull/1527#issuecomment-777322129:677,Deployability,install,installs,677,"About the version: That’s because `__file__` in `get_version(root='..', relative_to=__file__)` is a path where no component is in a git repository. Therefore the `setuptools_scm` code fails, we enter the `else` branch, and the (stale) metadata is used. Something like the following could make the `_metadata` module figure out we’re in a dev installation after all. (Needs some more as `here.readlink()` returns a relative path. I’m against `resolve()` as we really just want that one link to be replaced and `resolve()` is a sledge hammer). ```py; file_resolved = str(here.readlink() / f'{__name__}.py') if here.is_symlink() else __file__; ```. But the real issue is that dev installs are a hack and scanpy’s package metadata is stale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777322129
https://github.com/scverse/scanpy/pull/1527#issuecomment-777364572:5,Availability,failure,failures,5,"Test failures are not mine, seems like numba breaks on python 3.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777364572
https://github.com/scverse/scanpy/pull/1527#issuecomment-777364572:0,Testability,Test,Test,0,"Test failures are not mine, seems like numba breaks on python 3.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777364572
https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118:7,Availability,failure,failures,7,"> Test failures are not mine. It's not numba, it's annoy #1638 (hadn't realized scrublet uses it too). Asking for a rebuild will make it go away, but we should see if we can (1) avoid annoy in that test or (2) disable that test on 3.6 in a separate PR. Doc builds failures do seem related to this, however. Something about the way the `pip` requirement is formatted?. ------------------------. In future, could you not force push while responding to review? It makes it difficult for me to figure out what changed since my last review. History cleanup can happen pre or post review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118
https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118:264,Availability,failure,failures,264,"> Test failures are not mine. It's not numba, it's annoy #1638 (hadn't realized scrublet uses it too). Asking for a rebuild will make it go away, but we should see if we can (1) avoid annoy in that test or (2) disable that test on 3.6 in a separate PR. Doc builds failures do seem related to this, however. Something about the way the `pip` requirement is formatted?. ------------------------. In future, could you not force push while responding to review? It makes it difficult for me to figure out what changed since my last review. History cleanup can happen pre or post review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118
https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118:178,Safety,avoid,avoid,178,"> Test failures are not mine. It's not numba, it's annoy #1638 (hadn't realized scrublet uses it too). Asking for a rebuild will make it go away, but we should see if we can (1) avoid annoy in that test or (2) disable that test on 3.6 in a separate PR. Doc builds failures do seem related to this, however. Something about the way the `pip` requirement is formatted?. ------------------------. In future, could you not force push while responding to review? It makes it difficult for me to figure out what changed since my last review. History cleanup can happen pre or post review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118
https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118:2,Testability,Test,Test,2,"> Test failures are not mine. It's not numba, it's annoy #1638 (hadn't realized scrublet uses it too). Asking for a rebuild will make it go away, but we should see if we can (1) avoid annoy in that test or (2) disable that test on 3.6 in a separate PR. Doc builds failures do seem related to this, however. Something about the way the `pip` requirement is formatted?. ------------------------. In future, could you not force push while responding to review? It makes it difficult for me to figure out what changed since my last review. History cleanup can happen pre or post review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118
https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118:198,Testability,test,test,198,"> Test failures are not mine. It's not numba, it's annoy #1638 (hadn't realized scrublet uses it too). Asking for a rebuild will make it go away, but we should see if we can (1) avoid annoy in that test or (2) disable that test on 3.6 in a separate PR. Doc builds failures do seem related to this, however. Something about the way the `pip` requirement is formatted?. ------------------------. In future, could you not force push while responding to review? It makes it difficult for me to figure out what changed since my last review. History cleanup can happen pre or post review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118
https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118:223,Testability,test,test,223,"> Test failures are not mine. It's not numba, it's annoy #1638 (hadn't realized scrublet uses it too). Asking for a rebuild will make it go away, but we should see if we can (1) avoid annoy in that test or (2) disable that test on 3.6 in a separate PR. Doc builds failures do seem related to this, however. Something about the way the `pip` requirement is formatted?. ------------------------. In future, could you not force push while responding to review? It makes it difficult for me to figure out what changed since my last review. History cleanup can happen pre or post review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118
https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179:383,Availability,failure,failures,383,"> it's annoy. Sounds like annoy is being … annoying :smile:. > In future, could you not force push while responding to review?. Okay! Hmm, generally IDK what the best approach is since I now know how I want to rebase the commits but I’ll probably forget later … Maybe indicate in the message which commit they “fixup”?. Also: can we reenable squash/rebase merges soon?. > Doc builds failures do seem related to this, however. The docs failure is ugly to fix, but I did it …. Since very shortly ago, (pypa/pip#9320) pip validates wheels and for some reason decided that pluses in wheel filenames are not valid (I couldn’t find that in any spec). I hope takluyver/flit#388 gets merged soon to circumvent/fix that. If we want to temporarily circumvent that we’d have to tell readthedocs to use pip 20.3.3 version (like I did in the pipelines yaml). And that’s ugly because we’d have to add a requirements file that contains just `pip==20.3.3`, since readthedocs doesn’t allow to specify a pip version or a literal list of requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179
https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179:435,Availability,failure,failure,435,"> it's annoy. Sounds like annoy is being … annoying :smile:. > In future, could you not force push while responding to review?. Okay! Hmm, generally IDK what the best approach is since I now know how I want to rebase the commits but I’ll probably forget later … Maybe indicate in the message which commit they “fixup”?. Also: can we reenable squash/rebase merges soon?. > Doc builds failures do seem related to this, however. The docs failure is ugly to fix, but I did it …. Since very shortly ago, (pypa/pip#9320) pip validates wheels and for some reason decided that pluses in wheel filenames are not valid (I couldn’t find that in any spec). I hope takluyver/flit#388 gets merged soon to circumvent/fix that. If we want to temporarily circumvent that we’d have to tell readthedocs to use pip 20.3.3 version (like I did in the pipelines yaml). And that’s ugly because we’d have to add a requirements file that contains just `pip==20.3.3`, since readthedocs doesn’t allow to specify a pip version or a literal list of requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179
https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179:829,Deployability,pipeline,pipelines,829,"> it's annoy. Sounds like annoy is being … annoying :smile:. > In future, could you not force push while responding to review?. Okay! Hmm, generally IDK what the best approach is since I now know how I want to rebase the commits but I’ll probably forget later … Maybe indicate in the message which commit they “fixup”?. Also: can we reenable squash/rebase merges soon?. > Doc builds failures do seem related to this, however. The docs failure is ugly to fix, but I did it …. Since very shortly ago, (pypa/pip#9320) pip validates wheels and for some reason decided that pluses in wheel filenames are not valid (I couldn’t find that in any spec). I hope takluyver/flit#388 gets merged soon to circumvent/fix that. If we want to temporarily circumvent that we’d have to tell readthedocs to use pip 20.3.3 version (like I did in the pipelines yaml). And that’s ugly because we’d have to add a requirements file that contains just `pip==20.3.3`, since readthedocs doesn’t allow to specify a pip version or a literal list of requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179
https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179:284,Integrability,message,message,284,"> it's annoy. Sounds like annoy is being … annoying :smile:. > In future, could you not force push while responding to review?. Okay! Hmm, generally IDK what the best approach is since I now know how I want to rebase the commits but I’ll probably forget later … Maybe indicate in the message which commit they “fixup”?. Also: can we reenable squash/rebase merges soon?. > Doc builds failures do seem related to this, however. The docs failure is ugly to fix, but I did it …. Since very shortly ago, (pypa/pip#9320) pip validates wheels and for some reason decided that pluses in wheel filenames are not valid (I couldn’t find that in any spec). I hope takluyver/flit#388 gets merged soon to circumvent/fix that. If we want to temporarily circumvent that we’d have to tell readthedocs to use pip 20.3.3 version (like I did in the pipelines yaml). And that’s ugly because we’d have to add a requirements file that contains just `pip==20.3.3`, since readthedocs doesn’t allow to specify a pip version or a literal list of requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179
https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179:519,Security,validat,validates,519,"> it's annoy. Sounds like annoy is being … annoying :smile:. > In future, could you not force push while responding to review?. Okay! Hmm, generally IDK what the best approach is since I now know how I want to rebase the commits but I’ll probably forget later … Maybe indicate in the message which commit they “fixup”?. Also: can we reenable squash/rebase merges soon?. > Doc builds failures do seem related to this, however. The docs failure is ugly to fix, but I did it …. Since very shortly ago, (pypa/pip#9320) pip validates wheels and for some reason decided that pluses in wheel filenames are not valid (I couldn’t find that in any spec). I hope takluyver/flit#388 gets merged soon to circumvent/fix that. If we want to temporarily circumvent that we’d have to tell readthedocs to use pip 20.3.3 version (like I did in the pipelines yaml). And that’s ugly because we’d have to add a requirements file that contains just `pip==20.3.3`, since readthedocs doesn’t allow to specify a pip version or a literal list of requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179
https://github.com/scverse/scanpy/pull/1527#issuecomment-778296130:139,Deployability,install,install,139,"Came across this, and just want to add we are using poetry on scvi-tools and it's been pretty painless thus far. > no good way to editably install into some env:. If you look at our pyproject file, you can add one line that allows `pip -e .` type installation. I don't actually use poetry to create the development environment. The only issue that I haven't quite figured out is how to get the `scvi.__version__` to work on editable install.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778296130
https://github.com/scverse/scanpy/pull/1527#issuecomment-778296130:247,Deployability,install,installation,247,"Came across this, and just want to add we are using poetry on scvi-tools and it's been pretty painless thus far. > no good way to editably install into some env:. If you look at our pyproject file, you can add one line that allows `pip -e .` type installation. I don't actually use poetry to create the development environment. The only issue that I haven't quite figured out is how to get the `scvi.__version__` to work on editable install.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778296130
https://github.com/scverse/scanpy/pull/1527#issuecomment-778296130:433,Deployability,install,install,433,"Came across this, and just want to add we are using poetry on scvi-tools and it's been pretty painless thus far. > no good way to editably install into some env:. If you look at our pyproject file, you can add one line that allows `pip -e .` type installation. I don't actually use poetry to create the development environment. The only issue that I haven't quite figured out is how to get the `scvi.__version__` to work on editable install.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778296130
https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479:21,Deployability,install,install,21,Okay all done. `flit install -s` was getting too messy as some dependency installs scanpy and then things can’t be symlinked …. better leave the `pip install -r` in temporarily,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479
https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479:74,Deployability,install,installs,74,Okay all done. `flit install -s` was getting too messy as some dependency installs scanpy and then things can’t be symlinked …. better leave the `pip install -r` in temporarily,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479
https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479:150,Deployability,install,install,150,Okay all done. `flit install -s` was getting too messy as some dependency installs scanpy and then things can’t be symlinked …. better leave the `pip install -r` in temporarily,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479
https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479:63,Integrability,depend,dependency,63,Okay all done. `flit install -s` was getting too messy as some dependency installs scanpy and then things can’t be symlinked …. better leave the `pip install -r` in temporarily,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:152,Availability,down,downloaded,152,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:1793,Availability,avail,available,1793,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:2169,Availability,robust,robust,2169,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:102,Deployability,install,install,102,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:207,Deployability,release,release,207,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:237,Deployability,install,installed,237,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:337,Deployability,install,install,337,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:371,Deployability,install,install,371,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:542,Deployability,install,installs,542,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:590,Deployability,integrat,integrated,590,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:658,Deployability,install,install,658,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:697,Deployability,install,installs,697,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:743,Deployability,install,install,743,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:1592,Deployability,install,installed,1592,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:1831,Deployability,install,installed,1831,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:177,Integrability,depend,depends,177,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:590,Integrability,integrat,integrated,590,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:1639,Integrability,depend,dependency,1639,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:1810,Integrability,depend,dependencies,1810,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. That’s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there can’t be pluses in there, only that you can’t upload packages with local specifiers in their version to PyPI. Which we don’t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:45,Deployability,release,release,45,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:243,Deployability,install,install,243,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:421,Deployability,install,install,421,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:498,Deployability,install,install,498,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:527,Deployability,install,install,527,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:559,Deployability,install,installs,559,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:604,Deployability,install,install,604,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:751,Deployability,install,installation,751,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:15,Integrability,depend,depends,15,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:73,Integrability,depend,depends,73,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:1010,Security,validat,validating,1010,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your install’s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:250,Deployability,install,installed,250,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:307,Deployability,configurat,configuration,307,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:324,Deployability,upgrade,upgrades,324,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:406,Deployability,update,update,406,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:441,Deployability,update,update,441,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:532,Deployability,install,install,532,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:607,Deployability,install,install,607,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:635,Deployability,install,install,635,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:752,Deployability,install,installed,752,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:788,Deployability,update,update,788,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:906,Deployability,install,install,906,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:981,Deployability,install,install,981,"cvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1022,Deployability,install,install,1022,"cvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1158,Deployability,install,installing,1158,"de counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1269,Deployability,install,installation,1269,"resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in pypa/pip#9628, so I guess that’s the better place for following that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1569,Energy Efficiency,adapt,adapted,1569,"resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in pypa/pip#9628, so I guess that’s the better place for following that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:9,Integrability,depend,depends,9,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:202,Integrability,depend,dependencies,202,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:307,Modifiability,config,configuration,307,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1569,Modifiability,adapt,adapted,1569,"resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in pypa/pip#9628, so I guess that’s the better place for following that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1557,Availability,error,error,1557,"add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:2289,Availability,robust,robust,2289,"r, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; return True; ```. I don't see why `flit` couldn't just change the name of a function that is called internally at any point. I also think that at the moment, you and I are the only contributors who would have any idea what was going on if this acts up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:12,Deployability,install,installs,12,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:112,Deployability,update,update,112,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:147,Deployability,update,update,147,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:286,Deployability,install,install,286,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:539,Deployability,install,install,539,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:627,Deployability,install,installation,627,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1391,Deployability,Install,Installing,1391,"ith this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1536,Deployability,install,install,1536,"ack `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_st",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1631,Deployability,release,release,1631,"add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1723,Deployability,install,install,1723,"rom you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; return True; ```. I don't see why `flit` couldn't just change the name of a function that is called internally at any point. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1870,Deployability,install,install,1870,", our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; return True; ```. I don't see why `flit` couldn't just change the name of a function that is called internally at any point. I also think that at the moment, you and I are the only contributors who would have any idea what was going on if this acts up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:1891,Deployability,install,install,1891,", our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ### Version resolution. > No. Either we hardcode a string constant in the __init__.py or we leave it like it is until flit allows an alternative.; >; > That’s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented. On how version strings are handled/ generated:. I would be more comfortable using a solution that other packages used too. In particular, this looks very brittle to me:. ```python; for frame in traceback.extract_stack():; if frame.name == 'get_docstring_and_version_via_import':; return True; ```. I don't see why `flit` couldn't just change the name of a function that is called internally at any point. I also think that at the moment, you and I are the only contributors who would have any idea what was going on if this acts up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:937,Energy Efficiency,adapt,adapted,937,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:937,Modifiability,adapt,adapted,937,"### symlink installs being uninstalled **(most important)**. > No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659
https://github.com/scverse/scanpy/pull/1527#issuecomment-784973197:281,Deployability,install,install,281,"I believe the issues are related. I think pip is trying to get the version of the wheel by parsing it's name instead. For example, I just ran into this with where scanpy's version was being reported as: `1.8.0.dev34-g8d8039d9` – which I don't think is a valid version. I then `pip install -e`'d and got a version of `1.8.0.dev29+gc8309488` (different branches) and it worked fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-784973197
https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453:96,Deployability,install,install,96,"> Yeah, this is super weird. I think it's also blocking for adopting flit as recommended way to install scanpy to a dev environment. How has that to do with flit? Will pip just block upgrades things it identifies as “editable installs” from being upgraded while happily upgrading “normally installed” release candidates?. > I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. Exactly!. > I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. It wasn’t an issue for the year this PR has lingered, and now it’s pypa/pip#9628 and related discussions (which are already converging). I would like to meaningfully contribute to scanpy again instead of having to fix merge conflicts in this and discussing what pip broke this time. > In particular, this looks very brittle to me:. It isn’t, as you agreed on like 8 months ago. Worst that happens is “ImportError: cannot import ‘foo’” when building, which we can fix in 5 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453
https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453:183,Deployability,upgrade,upgrades,183,"> Yeah, this is super weird. I think it's also blocking for adopting flit as recommended way to install scanpy to a dev environment. How has that to do with flit? Will pip just block upgrades things it identifies as “editable installs” from being upgraded while happily upgrading “normally installed” release candidates?. > I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. Exactly!. > I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. It wasn’t an issue for the year this PR has lingered, and now it’s pypa/pip#9628 and related discussions (which are already converging). I would like to meaningfully contribute to scanpy again instead of having to fix merge conflicts in this and discussing what pip broke this time. > In particular, this looks very brittle to me:. It isn’t, as you agreed on like 8 months ago. Worst that happens is “ImportError: cannot import ‘foo’” when building, which we can fix in 5 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453
https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453:226,Deployability,install,installs,226,"> Yeah, this is super weird. I think it's also blocking for adopting flit as recommended way to install scanpy to a dev environment. How has that to do with flit? Will pip just block upgrades things it identifies as “editable installs” from being upgraded while happily upgrading “normally installed” release candidates?. > I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. Exactly!. > I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. It wasn’t an issue for the year this PR has lingered, and now it’s pypa/pip#9628 and related discussions (which are already converging). I would like to meaningfully contribute to scanpy again instead of having to fix merge conflicts in this and discussing what pip broke this time. > In particular, this looks very brittle to me:. It isn’t, as you agreed on like 8 months ago. Worst that happens is “ImportError: cannot import ‘foo’” when building, which we can fix in 5 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453
https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453:247,Deployability,upgrade,upgraded,247,"> Yeah, this is super weird. I think it's also blocking for adopting flit as recommended way to install scanpy to a dev environment. How has that to do with flit? Will pip just block upgrades things it identifies as “editable installs” from being upgraded while happily upgrading “normally installed” release candidates?. > I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. Exactly!. > I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. It wasn’t an issue for the year this PR has lingered, and now it’s pypa/pip#9628 and related discussions (which are already converging). I would like to meaningfully contribute to scanpy again instead of having to fix merge conflicts in this and discussing what pip broke this time. > In particular, this looks very brittle to me:. It isn’t, as you agreed on like 8 months ago. Worst that happens is “ImportError: cannot import ‘foo’” when building, which we can fix in 5 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453
https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453:290,Deployability,install,installed,290,"> Yeah, this is super weird. I think it's also blocking for adopting flit as recommended way to install scanpy to a dev environment. How has that to do with flit? Will pip just block upgrades things it identifies as “editable installs” from being upgraded while happily upgrading “normally installed” release candidates?. > I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. Exactly!. > I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. It wasn’t an issue for the year this PR has lingered, and now it’s pypa/pip#9628 and related discussions (which are already converging). I would like to meaningfully contribute to scanpy again instead of having to fix merge conflicts in this and discussing what pip broke this time. > In particular, this looks very brittle to me:. It isn’t, as you agreed on like 8 months ago. Worst that happens is “ImportError: cannot import ‘foo’” when building, which we can fix in 5 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453
https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453:301,Deployability,release,release,301,"> Yeah, this is super weird. I think it's also blocking for adopting flit as recommended way to install scanpy to a dev environment. How has that to do with flit? Will pip just block upgrades things it identifies as “editable installs” from being upgraded while happily upgrading “normally installed” release candidates?. > I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. Exactly!. > I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. It wasn’t an issue for the year this PR has lingered, and now it’s pypa/pip#9628 and related discussions (which are already converging). I would like to meaningfully contribute to scanpy again instead of having to fix merge conflicts in this and discussing what pip broke this time. > In particular, this looks very brittle to me:. It isn’t, as you agreed on like 8 months ago. Worst that happens is “ImportError: cannot import ‘foo’” when building, which we can fix in 5 minutes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-785809453
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:1816,Availability,error,error,1816," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:1318,Deployability,install,install,1318," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:1362,Deployability,install,install,1362," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:1387,Deployability,install,install,1387," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:1404,Deployability,Install,Install,1404," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:1494,Deployability,install,installation,1494," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:2062,Deployability,install,installation,2062," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:2105,Deployability,install,install,2105," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:2169,Deployability,install,installations,2169," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852:1425,Integrability,depend,depends,1425," it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. > It isn’t, as you agreed on like 8 months ago. I don't recall this specifically from 8 months ago. Theres a good chance that because I don't have as great of a knowledge about how packaging works, I understood our conversation in a different way. Because this is new, there's definitley going to be bugs. These are bugs with part of the stack that we don't have a lot of expertise in, so I'd like to minimize these before they become blockers. ```; $ conda create -yn flit-deps python=3.8 flit; $ conda activate flit-deps; $ flit install -s --dep=develop # Make development install of scanpy; $ pip install scvelo # Install project that depends on scanty; ...; Attempting uninstall: scanpy; Found existing installation: scanpy 1.8.0.dev49-ge715cd98; Uninstalling scanpy-1.8.0.dev49-ge715cd98:; Successfully uninstalled scanpy-1.8.0.dev49-ge715cd98; ...; # Development version of scanpy has now been uninstalled; ```. This is bad, and should not be the default experience for people who want to contribute. This does not give an error, or a warning. Yes there are solutions being proposed upstream, but we don't know how long until they are implemented. Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have `pip install -e` listed, and there has to be a note saying `flit -s` installations will be overridden due to a bug in `pip`. This stuff can be removed once this is fixed upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787407852
https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064:1458,Deployability,install,installation,1458,"> I would also like to see this merge. I've put a lot of time and effort into reviewing it, so we can get this over and done with.; > Your contributions are invaluable to the project, and I'd really like to see you contributing to other things. thank you, I really appreciate this :heart: . > The reason I'm so hard on this is that it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; > I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. Totally understood. My motivation to use flit is that it’s simpler and therefore better both for first-time contributors (to get started) and experienced people (to debug), whereas CLI, metadata, and code of setuptools/pip is very complex and a nightmare to debug. I know that due to flit being used less, there needs to be someone who understands the packaging ecosystem to fix things when they’re broken instead of cargo-culting one of the million answers around setuptools on StackOverflow. > Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have pip install -e listed, and there has to be a note saying flit -s installations will be overridden due to a bug in pip. This stuff can be removed once this is fixed upstream. OK, will do! Can you link me tp the upstream discussion of this problem please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064
https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064:1500,Deployability,install,install,1500,"> I would also like to see this merge. I've put a lot of time and effort into reviewing it, so we can get this over and done with.; > Your contributions are invaluable to the project, and I'd really like to see you contributing to other things. thank you, I really appreciate this :heart: . > The reason I'm so hard on this is that it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; > I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. Totally understood. My motivation to use flit is that it’s simpler and therefore better both for first-time contributors (to get started) and experienced people (to debug), whereas CLI, metadata, and code of setuptools/pip is very complex and a nightmare to debug. I know that due to flit being used less, there needs to be someone who understands the packaging ecosystem to fix things when they’re broken instead of cargo-culting one of the million answers around setuptools on StackOverflow. > Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have pip install -e listed, and there has to be a note saying flit -s installations will be overridden due to a bug in pip. This stuff can be removed once this is fixed upstream. OK, will do! Can you link me tp the upstream discussion of this problem please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064
https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064:1561,Deployability,install,installations,1561,"> I would also like to see this merge. I've put a lot of time and effort into reviewing it, so we can get this over and done with.; > Your contributions are invaluable to the project, and I'd really like to see you contributing to other things. thank you, I really appreciate this :heart: . > The reason I'm so hard on this is that it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; > I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. Totally understood. My motivation to use flit is that it’s simpler and therefore better both for first-time contributors (to get started) and experienced people (to debug), whereas CLI, metadata, and code of setuptools/pip is very complex and a nightmare to debug. I know that due to flit being used less, there needs to be someone who understands the packaging ecosystem to fix things when they’re broken instead of cargo-culting one of the million answers around setuptools on StackOverflow. > Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have pip install -e listed, and there has to be a note saying flit -s installations will be overridden due to a bug in pip. This stuff can be removed once this is fixed upstream. OK, will do! Can you link me tp the upstream discussion of this problem please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064
https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064:900,Usability,simpl,simpler,900,"> I would also like to see this merge. I've put a lot of time and effort into reviewing it, so we can get this over and done with.; > Your contributions are invaluable to the project, and I'd really like to see you contributing to other things. thank you, I really appreciate this :heart: . > The reason I'm so hard on this is that it's critical to our project (and getting new contributors), and it's a part of the stack I don't understand.; > I think you're the only one on the team who has a lot of understanding of the packaging ecosystem. The practical effect of this is that when things around this break, most of us have no idea what could be going wrong. What we have on master right now pretty much works. We've run into issues before, but it's been a while. Right now it's pretty smooth to set up a dev environment and contribute. Totally understood. My motivation to use flit is that it’s simpler and therefore better both for first-time contributors (to get started) and experienced people (to debug), whereas CLI, metadata, and code of setuptools/pip is very complex and a nightmare to debug. I know that due to flit being used less, there needs to be someone who understands the packaging ecosystem to fix things when they’re broken instead of cargo-culting one of the million answers around setuptools on StackOverflow. > Here's what I propose. I think this can be merged basically as is. However, until these issues are resolved: development installation instructions has to have pip install -e listed, and there has to be a note saying flit -s installations will be overridden due to a bug in pip. This stuff can be removed once this is fixed upstream. OK, will do! Can you link me tp the upstream discussion of this problem please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-787454064
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:13,Testability,test,tests,13,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:126,Testability,test,tests,126,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:190,Testability,test,tests,190,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:207,Testability,test,tests,207,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:283,Testability,test,tests,283,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:346,Testability,test,tests,346,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:362,Testability,test,tests,362,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:419,Testability,test,tests,419,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:437,Testability,test,tests,437,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:510,Testability,test,tests,510,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:690,Testability,test,tests,690,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:471,Usability,learn,learn,471,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250:484,Usability,learn,learn,484,"Why move the tests out of the package? It's what I've done in the past, but it certainly doesn't seem like the norm ([`pandas/tests`](https://github.com/pandas-dev/pandas/tree/master/pandas/tests), [`altair/tests`](https://github.com/altair-viz/altair/tree/master/altair), [`seaborn/tests`](https://github.com/mwaskom/seaborn/tree/master/seaborn/tests), [`numba/tests`](https://github.com/numba/numba/tree/master/numba/tests), [`sklearn/tests`](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/tests)). I personally think fixtures in `conftest.py` is poor style (why would these things all go in one file? Why is this the one place which gets implicitly imported in all the tests?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738553250
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1679,Deployability,install,installing,1679,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1967,Integrability,depend,depending,1967,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:950,Modifiability,config,config,950,"## Pytest architecture. `tests` directories and `test_*.py` collections aren’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `te",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:25,Testability,test,tests,25,"## Pytest architecture. `tests` directories and `test_*.py` collections aren’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `te",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:257,Testability,test,tests,257,"## Pytest architecture. `tests` directories and `test_*.py` collections aren’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `te",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:428,Testability,test,test,428,"## Pytest architecture. `tests` directories and `test_*.py` collections aren’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `te",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:797,Testability,test,testing,797,"## Pytest architecture. `tests` directories and `test_*.py` collections aren’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `te",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:976,Testability,test,tests,976,"## Pytest architecture. `tests` directories and `test_*.py` collections aren’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `te",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1102,Testability,test,tests,1102,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1174,Testability,test,test,1174,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1235,Testability,test,tests,1235,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1600,Testability,test,test,1600,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1702,Testability,test,tests,1702,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1714,Testability,test,testing,1714,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1901,Testability,test,testing,1901,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1999,Testability,test,tests,1999,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:2017,Testability,test,tests,2017,"’t intended to be packages, so you shouldn’t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if there’s a `__init__.py` somewhere in your `tests` directory when using pytest, you’re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what they’re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think it’s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as there’s only one `tests` directory with a structure that doesn’t neatly map to your module hierarchy, having it outside is cleaner because people can’t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects don’t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every user’s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290
https://github.com/scverse/scanpy/pull/1528#issuecomment-741748332:296,Deployability,release,releases,296,"I also just found this: https://docs.pytest.org/en/stable/pythonpath.html#import-modes. > `importlib`: new in pytest-6.0, this mode uses importlib to import test modules.; > […]; > makes test modules non-importable by each other.; > […]; > ; > **We intend to make importlib the default in future releases.**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-741748332
https://github.com/scverse/scanpy/pull/1528#issuecomment-741748332:157,Testability,test,test,157,"I also just found this: https://docs.pytest.org/en/stable/pythonpath.html#import-modes. > `importlib`: new in pytest-6.0, this mode uses importlib to import test modules.; > […]; > makes test modules non-importable by each other.; > […]; > ; > **We intend to make importlib the default in future releases.**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-741748332
https://github.com/scverse/scanpy/pull/1528#issuecomment-741748332:187,Testability,test,test,187,"I also just found this: https://docs.pytest.org/en/stable/pythonpath.html#import-modes. > `importlib`: new in pytest-6.0, this mode uses importlib to import test modules.; > […]; > makes test modules non-importable by each other.; > […]; > ; > **We intend to make importlib the default in future releases.**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-741748332
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:79,Testability,test,tests,79,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:149,Testability,test,tests,149,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:417,Testability,test,tests,417,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:505,Testability,test,testing,505,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:567,Testability,test,tests,567,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:689,Testability,test,test,689,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:790,Testability,test,tests,790,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:1181,Testability,test,tests,1181,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:1225,Testability,test,test,1225,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:1244,Testability,test,tests,1244,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296:635,Usability,clear,clear,635,"I've thought about it a bit more, and now think I agree with having the static tests in a separate job. I would like if this could also add `flake8` tests, and was setup so they would all run, regardless if any failed (`continueOnError: 'true'`). --------------------------------------. I don't think I agree with the rest, but am only going to give a partial response for now. . I'm not convinced we should move the tests out of the package. Broadly, I don't think `pytest` is a particularly opinionated testing tool, so I'm not sure one can use it wrong unless the tests aren't actually running. I do think their docs are not always clear/ correct. For example, we currently import from test modules https://github.com/theislab/scanpy/blob/8d9eec4c4763edb4a522dbec3fa5ea48832ff0f8/scanpy/tests/test_embedding_plots.py#L12. But:. ```sh; isaac@Mimir:~/github/scanpy ‹master›; $ pytest --version; pytest 6.1.2; isaac@Mimir:~/github/scanpy ‹master›; $ pytest -n 6 --import-mode=importlib; ...; ================================ 587 passed, 17 skipped, 1 xfailed, 172 warnings in 84.39s (0:01:24) ================================; ```. For good measure I also chucked a `import scanpy.tests.test_embedding_plots` into one of the test files and the tests still ran with `--import-mode=importlib`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-742213296
https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565:94,Availability,down,download,94,"We should also move them out because of file size, I don’t think everyone should be forced to download all our test data when installing scanpy. We should separate importable test tools (that e.g. other packages can import too) and our internal test tools. We can then document the test tools. > we currently import from test modules . yes, my PR fixes that. ----. But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. Since there’s no obvious reason to not do it, why struggle to find any? We can just take the obvious advantages (however slight or non-slight they may be) and do it. So is it OK if I go ahead and merge this before more PRs come in with conflicts? It’s getting a bit tiring to resolve those.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565
https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565:126,Deployability,install,installing,126,"We should also move them out because of file size, I don’t think everyone should be forced to download all our test data when installing scanpy. We should separate importable test tools (that e.g. other packages can import too) and our internal test tools. We can then document the test tools. > we currently import from test modules . yes, my PR fixes that. ----. But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. Since there’s no obvious reason to not do it, why struggle to find any? We can just take the obvious advantages (however slight or non-slight they may be) and do it. So is it OK if I go ahead and merge this before more PRs come in with conflicts? It’s getting a bit tiring to resolve those.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565
https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565:111,Testability,test,test,111,"We should also move them out because of file size, I don’t think everyone should be forced to download all our test data when installing scanpy. We should separate importable test tools (that e.g. other packages can import too) and our internal test tools. We can then document the test tools. > we currently import from test modules . yes, my PR fixes that. ----. But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. Since there’s no obvious reason to not do it, why struggle to find any? We can just take the obvious advantages (however slight or non-slight they may be) and do it. So is it OK if I go ahead and merge this before more PRs come in with conflicts? It’s getting a bit tiring to resolve those.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565
https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565:175,Testability,test,test,175,"We should also move them out because of file size, I don’t think everyone should be forced to download all our test data when installing scanpy. We should separate importable test tools (that e.g. other packages can import too) and our internal test tools. We can then document the test tools. > we currently import from test modules . yes, my PR fixes that. ----. But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. Since there’s no obvious reason to not do it, why struggle to find any? We can just take the obvious advantages (however slight or non-slight they may be) and do it. So is it OK if I go ahead and merge this before more PRs come in with conflicts? It’s getting a bit tiring to resolve those.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565
https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565:245,Testability,test,test,245,"We should also move them out because of file size, I don’t think everyone should be forced to download all our test data when installing scanpy. We should separate importable test tools (that e.g. other packages can import too) and our internal test tools. We can then document the test tools. > we currently import from test modules . yes, my PR fixes that. ----. But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. Since there’s no obvious reason to not do it, why struggle to find any? We can just take the obvious advantages (however slight or non-slight they may be) and do it. So is it OK if I go ahead and merge this before more PRs come in with conflicts? It’s getting a bit tiring to resolve those.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565
https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565:282,Testability,test,test,282,"We should also move them out because of file size, I don’t think everyone should be forced to download all our test data when installing scanpy. We should separate importable test tools (that e.g. other packages can import too) and our internal test tools. We can then document the test tools. > we currently import from test modules . yes, my PR fixes that. ----. But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. Since there’s no obvious reason to not do it, why struggle to find any? We can just take the obvious advantages (however slight or non-slight they may be) and do it. So is it OK if I go ahead and merge this before more PRs come in with conflicts? It’s getting a bit tiring to resolve those.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565
https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565:321,Testability,test,test,321,"We should also move them out because of file size, I don’t think everyone should be forced to download all our test data when installing scanpy. We should separate importable test tools (that e.g. other packages can import too) and our internal test tools. We can then document the test tools. > we currently import from test modules . yes, my PR fixes that. ----. But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. Since there’s no obvious reason to not do it, why struggle to find any? We can just take the obvious advantages (however slight or non-slight they may be) and do it. So is it OK if I go ahead and merge this before more PRs come in with conflicts? It’s getting a bit tiring to resolve those.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-743207565
https://github.com/scverse/scanpy/pull/1528#issuecomment-744154930:548,Deployability,release,release,548,"> So is it OK if I go ahead and merge this before more PRs come in with conflicts? . No. There are already open PRs which I'm working on merging, and this will cause conflicts in those. > But even if you don’t fully agree with all of my arguments, there’s still arguments, and zero for not doing it. I've only partially responded because I'm low on time. At first glance, there are a number of things I'm against here. But I'll be able to consider them more thoroughly, and tell you my arguments, once I've got more time – sometime after the 1.7.0 release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-744154930
https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545:79,Testability,test,test,79,"@ivirshup I'd actually be interest in hearing those. My packages also have the test folder outside the package, but I am happy to learn why many major packages have theirs in the actual package and why that might be a good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545
https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545:130,Usability,learn,learn,130,"@ivirshup I'd actually be interest in hearing those. My packages also have the test folder outside the package, but I am happy to learn why many major packages have theirs in the actual package and why that might be a good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-1089291545
https://github.com/scverse/scanpy/pull/1529#issuecomment-738210245:164,Testability,log,logFC,164,"Are the bottom ranked really not expressed, or just not differentially expressed? The former could still have significant p-values. I guess I wonder if you rank by logFC or by adjusted p-value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738210245
https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854:188,Testability,log,log,188,"@LuckyMD genes at the bottom simply have the lowest rank but they could be expressed. By default the ranking is taking directly from `sc.get.rank_genes_groups_df` which ranks the genes by log fold change. Bottom genes tend to have significant p-value. . To make this more transparent we can add a parameter to select how to rank for example by p-value or log fold change. . But, first I need to figure out what is this mess with the new tests....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854
https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854:355,Testability,log,log,355,"@LuckyMD genes at the bottom simply have the lowest rank but they could be expressed. By default the ranking is taking directly from `sc.get.rank_genes_groups_df` which ranks the genes by log fold change. Bottom genes tend to have significant p-value. . To make this more transparent we can add a parameter to select how to rank for example by p-value or log fold change. . But, first I need to figure out what is this mess with the new tests....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854
https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854:437,Testability,test,tests,437,"@LuckyMD genes at the bottom simply have the lowest rank but they could be expressed. By default the ranking is taking directly from `sc.get.rank_genes_groups_df` which ranks the genes by log fold change. Bottom genes tend to have significant p-value. . To make this more transparent we can add a parameter to select how to rank for example by p-value or log fold change. . But, first I need to figure out what is this mess with the new tests....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854
https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854:29,Usability,simpl,simply,29,"@LuckyMD genes at the bottom simply have the lowest rank but they could be expressed. By default the ranking is taking directly from `sc.get.rank_genes_groups_df` which ranks the genes by log fold change. Bottom genes tend to have significant p-value. . To make this more transparent we can add a parameter to select how to rank for example by p-value or log fold change. . But, first I need to figure out what is this mess with the new tests....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738292854
https://github.com/scverse/scanpy/pull/1529#issuecomment-738459239:80,Testability,log,logFC,80,"Yeah, I recently found out that `rank_genes_groups` doesn't just filter for +ve logFC, but ranks by it. I used to think that it's a filtering and you needed to do A vs B and B vs A to get all results ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738459239
https://github.com/scverse/scanpy/pull/1529#issuecomment-738733928:1430,Testability,log,logfoldchange,1430,"@LuckyMD Your impression is right, but after changes to `sc.tl.rank_genes_groups` were introduced, now by default the full list of genes is returned and is not necessary to do A vs. B and then B vs A. In my impression this change opened new opportunities, like looking at specific genes or looking at the bottom ranked. However, I think it is worth to make the ranking and selection more transparent and I am open here for suggestions. For background the current state is:; * `sc.tl.filter_rank_genes_groups` can be used to filter the results in different ways like fold change or fraction of cells expressing the gene in a given cluster or outside a given cluster. The goal was to allow identification of markers quite specific to a cluster. Although, I made this function I think we should not use it as it is not up to date and creates confusion because it replaces genes by NaNs to allow the filtering. This was pre `sc.get.rank_genes_groups_df` and some other changes. Also is complicated to use because is run, a new rank_genes_groups key is created with the filtering and this key has to be added to the plotting functions to see the results. ; * The `sc.pl.rank_genes_groups_*` plots have the option `min_logfoldchage` for filtering. I find this useful but limited because is not possible to filter by p-value for example. As a solution, plots could have a filtering option that uses pandas query syntax like: `filtering='logfoldchange>1 & p-value<0.0001'` and for the sorting something like `sortby=('logfoldchange', 'ascend')`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738733928
https://github.com/scverse/scanpy/pull/1529#issuecomment-738733928:1510,Testability,log,logfoldchange,1510,"@LuckyMD Your impression is right, but after changes to `sc.tl.rank_genes_groups` were introduced, now by default the full list of genes is returned and is not necessary to do A vs. B and then B vs A. In my impression this change opened new opportunities, like looking at specific genes or looking at the bottom ranked. However, I think it is worth to make the ranking and selection more transparent and I am open here for suggestions. For background the current state is:; * `sc.tl.filter_rank_genes_groups` can be used to filter the results in different ways like fold change or fraction of cells expressing the gene in a given cluster or outside a given cluster. The goal was to allow identification of markers quite specific to a cluster. Although, I made this function I think we should not use it as it is not up to date and creates confusion because it replaces genes by NaNs to allow the filtering. This was pre `sc.get.rank_genes_groups_df` and some other changes. Also is complicated to use because is run, a new rank_genes_groups key is created with the filtering and this key has to be added to the plotting functions to see the results. ; * The `sc.pl.rank_genes_groups_*` plots have the option `min_logfoldchage` for filtering. I find this useful but limited because is not possible to filter by p-value for example. As a solution, plots could have a filtering option that uses pandas query syntax like: `filtering='logfoldchange>1 & p-value<0.0001'` and for the sorting something like `sortby=('logfoldchange', 'ascend')`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738733928
https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770:264,Performance,perform,perform,264,I like your suggestions. Especially the `filter_rank_genes_groups` use makes a lot of sense to me. The one thing I would suggest to take into account is that some of these filtering steps can be done before significance testing and therefore you would not have to perform multiple testing correction on the filtered out genes. This may be quite useful to some. That precludes filtering on p-value though. It also makes a case for filtering already in `rank_genes_groups` rather than in `sc.get`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770
https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770:220,Testability,test,testing,220,I like your suggestions. Especially the `filter_rank_genes_groups` use makes a lot of sense to me. The one thing I would suggest to take into account is that some of these filtering steps can be done before significance testing and therefore you would not have to perform multiple testing correction on the filtered out genes. This may be quite useful to some. That precludes filtering on p-value though. It also makes a case for filtering already in `rank_genes_groups` rather than in `sc.get`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770
https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770:281,Testability,test,testing,281,I like your suggestions. Especially the `filter_rank_genes_groups` use makes a lot of sense to me. The one thing I would suggest to take into account is that some of these filtering steps can be done before significance testing and therefore you would not have to perform multiple testing correction on the filtered out genes. This may be quite useful to some. That precludes filtering on p-value though. It also makes a case for filtering already in `rank_genes_groups` rather than in `sc.get`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770
https://github.com/scverse/scanpy/pull/1529#issuecomment-781059091:114,Availability,avail,available,114,"Re: #1649. Does this still need a max fold change argument?. More generally, how complex do we want the filtering available through these functions (and `sc.get.rank_genes_groups_df`) to be? Is it most straight forward to recommend passing the gene names, and recommend users generate these by manipulating the dataframe returned by `rank_genes_groups_df`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-781059091
https://github.com/scverse/scanpy/pull/1529#issuecomment-865866325:16,Deployability,update,updated,16,"@fidelram, I've updated this so the tests pass, and think I've caught a few more bugs. Hopefully I didn't misinterpret your intent here, but I'm merging as we'd like to get a release out. Please let me know if I've messed anything up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-865866325
https://github.com/scverse/scanpy/pull/1529#issuecomment-865866325:175,Deployability,release,release,175,"@fidelram, I've updated this so the tests pass, and think I've caught a few more bugs. Hopefully I didn't misinterpret your intent here, but I'm merging as we'd like to get a release out. Please let me know if I've messed anything up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-865866325
https://github.com/scverse/scanpy/pull/1529#issuecomment-865866325:36,Testability,test,tests,36,"@fidelram, I've updated this so the tests pass, and think I've caught a few more bugs. Hopefully I didn't misinterpret your intent here, but I'm merging as we'd like to get a release out. Please let me know if I've messed anything up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-865866325
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:623,Energy Efficiency,adapt,adapted,623,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:623,Modifiability,adapt,adapted,623,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:149,Testability,log,logreg,149,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:261,Testability,log,logreg,261,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:316,Testability,test,test,316,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:348,Testability,log,logreg,348,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:388,Testability,test,test,388,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:405,Testability,test,test,405,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:458,Testability,test,test,458,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:465,Testability,test,test,465,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:500,Testability,test,test,500,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:516,Testability,test,test,516,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:545,Testability,test,test,545,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:586,Testability,test,test,586,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:662,Testability,log,logistic,662,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:715,Testability,log,logfoldchange,715,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688
https://github.com/scverse/scanpy/issues/1531#issuecomment-738828346:178,Integrability,depend,depend,178,"@LuckyMD Mine was a general answer, I agree that at this scale it may not be an issue but it may indeed be for larger atlases. @mxposed about your first question: cell distances depend on the HVG in absolute terms, but the overall structure of your data is more ""relative"". If the kNN graph topology is overall conserved you'll end up with similar populations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-738828346
https://github.com/scverse/scanpy/issues/1531#issuecomment-739436787:323,Deployability,update,updated,323,"@fidelram Thank you for pointing this out. I did miss the `n_neighbors` parameter for `sc.pp.neighbors` function. It has default 15, while Seurat's have default 20 (and yes, they then prune the kNN). Adding this parameter did solve the discrepancy in the first dataset, but not the second, in the investigation (I have not updated the notebook). @LuckyMD Thank you! I really liked applying scArches, and it's also a very natural approach: having a reference, mapping to it. I hope we're moving towards that direction generally. Thank you for pointing out that Leiden is stochastic, I didn't realize that, and the fixed default random seed obscures it a little. I'll try to look at different seeds and assess the distribution of clustering. Can't estimate to which degree different runs would disagree. Indeed, it appears that scanpy does kNN and doesn't do any pruning (judging from my brief glance at the code). I honestly expected that some kind of pruning of the kNN graph would be there. I remember two talks, one from Dana Pe'er and one from Dominic Grün, that mentioned kNN pruning as a strategy to improve analysis. @dawe Thank you for linking to the resolution limit. However, I don't think it's the case here, because 2 of the 3 strategies that I tried did resolve those populations. . If we focus on dataset 2 (SC167) in the investigation, obviously, there's some small kNN topology difference between the strategies tried, that leads to SCT+scanpy strategy being slow to separate DC1 cells from B cells. I am mostly surprised that vanilla (log-norm) strategy does separate those cells. . I wonder how to go about investigating what drives that behaviour?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-739436787
https://github.com/scverse/scanpy/issues/1531#issuecomment-739436787:1551,Testability,log,log-norm,1551,"@fidelram Thank you for pointing this out. I did miss the `n_neighbors` parameter for `sc.pp.neighbors` function. It has default 15, while Seurat's have default 20 (and yes, they then prune the kNN). Adding this parameter did solve the discrepancy in the first dataset, but not the second, in the investigation (I have not updated the notebook). @LuckyMD Thank you! I really liked applying scArches, and it's also a very natural approach: having a reference, mapping to it. I hope we're moving towards that direction generally. Thank you for pointing out that Leiden is stochastic, I didn't realize that, and the fixed default random seed obscures it a little. I'll try to look at different seeds and assess the distribution of clustering. Can't estimate to which degree different runs would disagree. Indeed, it appears that scanpy does kNN and doesn't do any pruning (judging from my brief glance at the code). I honestly expected that some kind of pruning of the kNN graph would be there. I remember two talks, one from Dana Pe'er and one from Dominic Grün, that mentioned kNN pruning as a strategy to improve analysis. @dawe Thank you for linking to the resolution limit. However, I don't think it's the case here, because 2 of the 3 strategies that I tried did resolve those populations. . If we focus on dataset 2 (SC167) in the investigation, obviously, there's some small kNN topology difference between the strategies tried, that leads to SCT+scanpy strategy being slow to separate DC1 cells from B cells. I am mostly surprised that vanilla (log-norm) strategy does separate those cells. . I wonder how to go about investigating what drives that behaviour?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-739436787
https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:450,Deployability,install,install,450,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692
https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:507,Deployability,install,install,507,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692
https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:228,Modifiability,Variab,Variable,228,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692
https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:706,Modifiability,variab,variable,706,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692
https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:1083,Modifiability,variab,variable,1083,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692
https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:1375,Modifiability,Variab,VariableFeatures,1375,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692
https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:1517,Performance,perform,perform,1517,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692
https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:906,Testability,log,log,906,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692
https://github.com/scverse/scanpy/pull/1533#issuecomment-815285176:147,Availability,avail,available,147,"Dear @LouisFaure,. thank you very much for the high quality PR.; A couple of questions:; 1. Do you think that we should check for whether GPUs are available if any of the GPU accelerated methods were chosen? This would allow us to exit more nicely if we were requesting GPU support but none were found; 2. I think that we should homogenize the parameter names for the method selection. Sometimes they are called 'method', sometimes 'flavor' and then you're also using 'device'. I myself am a fan of 'device' to switch between CPU and GPU implementations. However, then it would be unclear which method to use when several GPU accelerated algorithms for a task are implemented. Do you have better ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-815285176
https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859:402,Deployability,pipeline,pipelines,402,"Hey, just wanted to comment here on why it's taken so long for a review. I'm personally not comfortable with having significant code in the package that we cannot test on CI. We're looking into this, but it's been slow going since it looks like we have to set this up and manage it on our own. As far as I can tell this process is:. * Put money into the azure account; * Set up containers; * Configure pipelines to use these containers (not sure if we can use the standard Tasks on ""self hosted"" containers) . @Zethson, since you're actually at the institute with the money you may have better luck moving the first step forward than I've had. Do you think you'd be able to look into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859
https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859:392,Modifiability,Config,Configure,392,"Hey, just wanted to comment here on why it's taken so long for a review. I'm personally not comfortable with having significant code in the package that we cannot test on CI. We're looking into this, but it's been slow going since it looks like we have to set this up and manage it on our own. As far as I can tell this process is:. * Put money into the azure account; * Set up containers; * Configure pipelines to use these containers (not sure if we can use the standard Tasks on ""self hosted"" containers) . @Zethson, since you're actually at the institute with the money you may have better luck moving the first step forward than I've had. Do you think you'd be able to look into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859
https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859:163,Testability,test,test,163,"Hey, just wanted to comment here on why it's taken so long for a review. I'm personally not comfortable with having significant code in the package that we cannot test on CI. We're looking into this, but it's been slow going since it looks like we have to set this up and manage it on our own. As far as I can tell this process is:. * Put money into the azure account; * Set up containers; * Configure pipelines to use these containers (not sure if we can use the standard Tasks on ""self hosted"" containers) . @Zethson, since you're actually at the institute with the money you may have better luck moving the first step forward than I've had. Do you think you'd be able to look into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859
https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412:170,Availability,avail,available,170,"Thanks for your comments, I understand the struggle of implementing CI for GPU code!. @Zethson here are my answers to your questions:; 1. Instead of checking if a gpu is available, I would suggest to rather check if the related library is installed (depending on the method, it could be cugraph, cupy or cuml) since each of these libraries always require a GPU at installation and usage, I think using these as check would suffice.; 2. I agree with moving to the usage of 'device' as much as possible. It should be easily possible to rename ""method""/""flavor"" to ""device"" for `tl.draw_graph`, `tl.leiden` and `tl.louvain`, and use only ""cpu""/""gpu"" as choices as theses parameters would have only two choices anyway. In most case this would indeed remove the name of the python backend used, but one could instead mention it in the api/doc. ; `pp.neighbors` is a bit more tricky to handle, running it in gpu mode lead to a combination of distances/neighbors calculation with gpu/cuml backend and then connectivities calculations on cpu/umap backend, this could be solved if maintainers of cuml decide to allow the latter to be computed with cuml: https://github.com/rapidsai/cuml/issues/3123. Since it will take time before CI can be implemented, I can just add the easy small changes proposed on 2. and let the PR open so you decide what to do later!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412
https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412:239,Deployability,install,installed,239,"Thanks for your comments, I understand the struggle of implementing CI for GPU code!. @Zethson here are my answers to your questions:; 1. Instead of checking if a gpu is available, I would suggest to rather check if the related library is installed (depending on the method, it could be cugraph, cupy or cuml) since each of these libraries always require a GPU at installation and usage, I think using these as check would suffice.; 2. I agree with moving to the usage of 'device' as much as possible. It should be easily possible to rename ""method""/""flavor"" to ""device"" for `tl.draw_graph`, `tl.leiden` and `tl.louvain`, and use only ""cpu""/""gpu"" as choices as theses parameters would have only two choices anyway. In most case this would indeed remove the name of the python backend used, but one could instead mention it in the api/doc. ; `pp.neighbors` is a bit more tricky to handle, running it in gpu mode lead to a combination of distances/neighbors calculation with gpu/cuml backend and then connectivities calculations on cpu/umap backend, this could be solved if maintainers of cuml decide to allow the latter to be computed with cuml: https://github.com/rapidsai/cuml/issues/3123. Since it will take time before CI can be implemented, I can just add the easy small changes proposed on 2. and let the PR open so you decide what to do later!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412
https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412:364,Deployability,install,installation,364,"Thanks for your comments, I understand the struggle of implementing CI for GPU code!. @Zethson here are my answers to your questions:; 1. Instead of checking if a gpu is available, I would suggest to rather check if the related library is installed (depending on the method, it could be cugraph, cupy or cuml) since each of these libraries always require a GPU at installation and usage, I think using these as check would suffice.; 2. I agree with moving to the usage of 'device' as much as possible. It should be easily possible to rename ""method""/""flavor"" to ""device"" for `tl.draw_graph`, `tl.leiden` and `tl.louvain`, and use only ""cpu""/""gpu"" as choices as theses parameters would have only two choices anyway. In most case this would indeed remove the name of the python backend used, but one could instead mention it in the api/doc. ; `pp.neighbors` is a bit more tricky to handle, running it in gpu mode lead to a combination of distances/neighbors calculation with gpu/cuml backend and then connectivities calculations on cpu/umap backend, this could be solved if maintainers of cuml decide to allow the latter to be computed with cuml: https://github.com/rapidsai/cuml/issues/3123. Since it will take time before CI can be implemented, I can just add the easy small changes proposed on 2. and let the PR open so you decide what to do later!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412
https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412:250,Integrability,depend,depending,250,"Thanks for your comments, I understand the struggle of implementing CI for GPU code!. @Zethson here are my answers to your questions:; 1. Instead of checking if a gpu is available, I would suggest to rather check if the related library is installed (depending on the method, it could be cugraph, cupy or cuml) since each of these libraries always require a GPU at installation and usage, I think using these as check would suffice.; 2. I agree with moving to the usage of 'device' as much as possible. It should be easily possible to rename ""method""/""flavor"" to ""device"" for `tl.draw_graph`, `tl.leiden` and `tl.louvain`, and use only ""cpu""/""gpu"" as choices as theses parameters would have only two choices anyway. In most case this would indeed remove the name of the python backend used, but one could instead mention it in the api/doc. ; `pp.neighbors` is a bit more tricky to handle, running it in gpu mode lead to a combination of distances/neighbors calculation with gpu/cuml backend and then connectivities calculations on cpu/umap backend, this could be solved if maintainers of cuml decide to allow the latter to be computed with cuml: https://github.com/rapidsai/cuml/issues/3123. Since it will take time before CI can be implemented, I can just add the easy small changes proposed on 2. and let the PR open so you decide what to do later!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412
https://github.com/scverse/scanpy/pull/1533#issuecomment-816784411:197,Availability,down,down,197,@LouisFaure Great!; While I agree with your comments and suggestions I think that for now you can save yourself the time to implement them since they are likely to run into further merge conflicts down the road. ; The GPU CI is certainly off weeks if not months. As soon as it's ready I would ping you again and we can get this PR ready. Does this sound fine to you? Thanks again! Looking forward to GPU accelerated Scanpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816784411
https://github.com/scverse/scanpy/pull/1533#issuecomment-816784411:293,Availability,ping,ping,293,@LouisFaure Great!; While I agree with your comments and suggestions I think that for now you can save yourself the time to implement them since they are likely to run into further merge conflicts down the road. ; The GPU CI is certainly off weeks if not months. As soon as it's ready I would ping you again and we can get this PR ready. Does this sound fine to you? Thanks again! Looking forward to GPU accelerated Scanpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816784411
https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960:100,Availability,avail,available,100,"For those interested in using the GPU accelerated functions leiden, draw_graph_fa, I have made them available on the following gist:; https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. I have also included in that code `load_mtx`, which reads and convert mtx files into anndata using cudf. I tested on a 654Mo mtx containing 56621 cells x 20222 genes, I can obtain a 13X speedup (using RTX8000)! . ![image](https://user-images.githubusercontent.com/27488782/164707560-30c0c9fe-6bfe-4fcb-ac2c-0d8a503081b6.png). I expect this to scale even better with higher number of cells. I could also add this wrapper into scanpy once CI is ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960
https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960:615,Integrability,wrap,wrapper,615,"For those interested in using the GPU accelerated functions leiden, draw_graph_fa, I have made them available on the following gist:; https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. I have also included in that code `load_mtx`, which reads and convert mtx files into anndata using cudf. I tested on a 654Mo mtx containing 56621 cells x 20222 genes, I can obtain a 13X speedup (using RTX8000)! . ![image](https://user-images.githubusercontent.com/27488782/164707560-30c0c9fe-6bfe-4fcb-ac2c-0d8a503081b6.png). I expect this to scale even better with higher number of cells. I could also add this wrapper into scanpy once CI is ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960
https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960:310,Testability,test,tested,310,"For those interested in using the GPU accelerated functions leiden, draw_graph_fa, I have made them available on the following gist:; https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. I have also included in that code `load_mtx`, which reads and convert mtx files into anndata using cudf. I tested on a 654Mo mtx containing 56621 cells x 20222 genes, I can obtain a 13X speedup (using RTX8000)! . ![image](https://user-images.githubusercontent.com/27488782/164707560-30c0c9fe-6bfe-4fcb-ac2c-0d8a503081b6.png). I expect this to scale even better with higher number of cells. I could also add this wrapper into scanpy once CI is ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960
https://github.com/scverse/scanpy/pull/1533#issuecomment-1106490794:286,Performance,load,loading,286,"@Intron7 I think the aim here is indeed to not keep anything in VRAM anyway. In the code/functions I propose here, the data is only transiently stored in device memory for calculation and the resulting output is always transfered back to host once finished. Moreover, I also think that loading a huge mtx file with a 4Go GPU is not impossible. From what I understood rmm should allow oversubscription on host RAM using the following command:; ```python; rmm.reinitialize(managed_memory=True); cp.cuda.set_allocator(rmm.rmm_cupy_allocator); ```. I had a look at your code and GPU accelerated preprocessing functions would be also welcome in scanpy in my opinion! I feel that `scale` and `regress_out` could benefit from such speedup for example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106490794
https://github.com/scverse/scanpy/pull/1533#issuecomment-1107449372:166,Availability,error,error,166,Using RMM works but only to a certain extend. As far as I understand it you can oversubscribe VRAM to a maximum of 2X. If you go above that you’ll get a memory alloc error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1107449372
https://github.com/scverse/scanpy/pull/1533#issuecomment-1107449372:38,Modifiability,extend,extend,38,Using RMM works but only to a certain extend. As far as I understand it you can oversubscribe VRAM to a maximum of 2X. If you go above that you’ll get a memory alloc error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1107449372
https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:14,Deployability,update,updates,14,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110
https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:470,Deployability,pipeline,pipeline,470,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110
https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:393,Performance,perform,performed,393,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110
https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:484,Performance,load,loading,484,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110
https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:77,Security,expose,exposed,77,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110
https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:711,Testability,test,tested,711,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110
https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:463,Usability,simpl,simple,463,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110
https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399:171,Modifiability,layers,layers,171,"I created a PR to this branch to add GPU support for :; *`tl.rank_gene_groups` with method='logreg'; *`tl.embedding_density`; *`correlation_matrix`; *`diffmap`; I added `.layers` support for `pp.pca`. This helps with the ""Pearson Residuals"" workflow.; The default pca solver for device GPU is now ""auto""; I also fixed a bug in `tl.rank_gene_groups` with `method='logreg'` with selecting groups (eg. groups = [""2"",""1"",""5""]) that is currently still in scanpy.; ![image](https://user-images.githubusercontent.com/37635888/179788802-6783f87d-19eb-497c-922e-59c18d6015d5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399
https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399:92,Testability,log,logreg,92,"I created a PR to this branch to add GPU support for :; *`tl.rank_gene_groups` with method='logreg'; *`tl.embedding_density`; *`correlation_matrix`; *`diffmap`; I added `.layers` support for `pp.pca`. This helps with the ""Pearson Residuals"" workflow.; The default pca solver for device GPU is now ""auto""; I also fixed a bug in `tl.rank_gene_groups` with `method='logreg'` with selecting groups (eg. groups = [""2"",""1"",""5""]) that is currently still in scanpy.; ![image](https://user-images.githubusercontent.com/37635888/179788802-6783f87d-19eb-497c-922e-59c18d6015d5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399
https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399:363,Testability,log,logreg,363,"I created a PR to this branch to add GPU support for :; *`tl.rank_gene_groups` with method='logreg'; *`tl.embedding_density`; *`correlation_matrix`; *`diffmap`; I added `.layers` support for `pp.pca`. This helps with the ""Pearson Residuals"" workflow.; The default pca solver for device GPU is now ""auto""; I also fixed a bug in `tl.rank_gene_groups` with `method='logreg'` with selecting groups (eg. groups = [""2"",""1"",""5""]) that is currently still in scanpy.; ![image](https://user-images.githubusercontent.com/37635888/179788802-6783f87d-19eb-497c-922e-59c18d6015d5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399
https://github.com/scverse/scanpy/pull/1533#issuecomment-1671325998:353,Deployability,update,updated,353,"Hey @LouisFaure,. During the Hackathlon last week we talked again about this PR. For the time being we will keep GPU computing functionality out of scanpy and in rapids-singlecell. RSC is now tested with a CI solution. If you want to contribute to rapids-singlecell I would be very happy. Missing functions like Umap and Neighbors are currently getting updated and also ported to RSC.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1671325998
https://github.com/scverse/scanpy/pull/1533#issuecomment-1671325998:192,Testability,test,tested,192,"Hey @LouisFaure,. During the Hackathlon last week we talked again about this PR. For the time being we will keep GPU computing functionality out of scanpy and in rapids-singlecell. RSC is now tested with a CI solution. If you want to contribute to rapids-singlecell I would be very happy. Missing functions like Umap and Neighbors are currently getting updated and also ported to RSC.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1671325998
https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392:471,Safety,avoid,avoid,471,"It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of `sc.pl.umap(adata, color='clusters')` -> `sc.pl.umap(adata, 'clusters')`. About the changes that you suggest: I have concerns with breaking previous functionality and I wonder what is your position with respect to this. The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage from the very firsts versions of scanpy but maybe you have some good ideas for transitioning this from a string to a tuple. . The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. . For the plots being the product of `color` and `components`: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. . For your question about replacing `components` by `dimensions`. We need to be careful here because in many places the use of components is in the context of PCA as in `sc.pp.neighbors` with the parameter `n_pcs`. I think that the replacement of `components` by `dimensions` should only be done for the embedding functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392
https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392:115,Usability,simpl,simple,115,"It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of `sc.pl.umap(adata, color='clusters')` -> `sc.pl.umap(adata, 'clusters')`. About the changes that you suggest: I have concerns with breaking previous functionality and I wonder what is your position with respect to this. The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage from the very firsts versions of scanpy but maybe you have some good ideas for transitioning this from a string to a tuple. . The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. . For the plots being the product of `color` and `components`: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. . For your question about replacing `components` by `dimensions`. We need to be careful here because in many places the use of components is in the context of PCA as in `sc.pp.neighbors` with the parameter `n_pcs`. I think that the replacement of `components` by `dimensions` should only be done for the embedding functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-743241392
https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877:329,Safety,avoid,avoid,329,"> It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of sc.pl.umap(adata, color='clusters') -> sc.pl.umap(adata, 'clusters'). 👍 . > The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage. The idea is to parse `components`, but allow you to directly pass the indices with dimensions. > The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. To me, I think it makes more sense to be consistent with python. I feel like it's very clear what is happening if the default argument is `sc.pl.pca(adata, dimensions=(0, 1))`. It makes it easier to work with programmatically if the values are equivalent to what you could use to index the array directly. For example, say you find the dimension which is maximally correlated with some gene. You can just pass the result of that into dimensions without having to remember to add 1. > For the plots being the product of color and components: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. Cool. I feel like this can be useful, but it would be useful if I could choose which arguments it worked with. I think this is a different function call though. For example, I might was to look at a gene under multiple embeddings, so pairwise combinations of `basis` and `colors`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877
https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877:117,Usability,simpl,simple,117,"> It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of sc.pl.umap(adata, color='clusters') -> sc.pl.umap(adata, 'clusters'). 👍 . > The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage. The idea is to parse `components`, but allow you to directly pass the indices with dimensions. > The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. To me, I think it makes more sense to be consistent with python. I feel like it's very clear what is happening if the default argument is `sc.pl.pca(adata, dimensions=(0, 1))`. It makes it easier to work with programmatically if the values are equivalent to what you could use to index the array directly. For example, say you find the dimension which is maximally correlated with some gene. You can just pass the result of that into dimensions without having to remember to add 1. > For the plots being the product of color and components: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. Cool. I feel like this can be useful, but it would be useful if I could choose which arguments it worked with. I think this is a different function call though. For example, I might was to look at a gene under multiple embeddings, so pairwise combinations of `basis` and `colors`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877
https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877:733,Usability,clear,clear,733,"> It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of sc.pl.umap(adata, color='clusters') -> sc.pl.umap(adata, 'clusters'). 👍 . > The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage. The idea is to parse `components`, but allow you to directly pass the indices with dimensions. > The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. To me, I think it makes more sense to be consistent with python. I feel like it's very clear what is happening if the default argument is `sc.pl.pca(adata, dimensions=(0, 1))`. It makes it easier to work with programmatically if the values are equivalent to what you could use to index the array directly. For example, say you find the dimension which is maximally correlated with some gene. You can just pass the result of that into dimensions without having to remember to add 1. > For the plots being the product of color and components: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. Cool. I feel like this can be useful, but it would be useful if I could choose which arguments it worked with. I think this is a different function call though. For example, I might was to look at a gene under multiple embeddings, so pairwise combinations of `basis` and `colors`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877
https://github.com/scverse/scanpy/pull/1540#issuecomment-748754780:54,Deployability,release,release,54,Could this get a notice in the docs/ something in the release notes?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1540#issuecomment-748754780
https://github.com/scverse/scanpy/pull/1540#issuecomment-762565049:15,Deployability,update,update,15,Thanks for the update! Merged via #1595,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1540#issuecomment-762565049
https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865:299,Deployability,Update,Updated,299,"I think this is getting to a good place for an initial addition. Parts that definitely need expanding include:. * Code guidelines; * These are pretty minimal at the moment.; * Documentation; * Information on restructured text ; * sphinx extensions we use; * More on the structure of a doc-string; * Updated examples (I took these from the existing `CONTRIBUTING.md`). I think these can be expanded at a later date. The main goal here was to make sure there was some base organization for a contributing guide and dev-docs. . I'm probably also not the best person to expand on the documentation, since I still barely understand sphinx :wink:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865
https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865:119,Usability,guid,guidelines,119,"I think this is getting to a good place for an initial addition. Parts that definitely need expanding include:. * Code guidelines; * These are pretty minimal at the moment.; * Documentation; * Information on restructured text ; * sphinx extensions we use; * More on the structure of a doc-string; * Updated examples (I took these from the existing `CONTRIBUTING.md`). I think these can be expanded at a later date. The main goal here was to make sure there was some base organization for a contributing guide and dev-docs. . I'm probably also not the best person to expand on the documentation, since I still barely understand sphinx :wink:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865
https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865:503,Usability,guid,guide,503,"I think this is getting to a good place for an initial addition. Parts that definitely need expanding include:. * Code guidelines; * These are pretty minimal at the moment.; * Documentation; * Information on restructured text ; * sphinx extensions we use; * More on the structure of a doc-string; * Updated examples (I took these from the existing `CONTRIBUTING.md`). I think these can be expanded at a later date. The main goal here was to make sure there was some base organization for a contributing guide and dev-docs. . I'm probably also not the best person to expand on the documentation, since I still barely understand sphinx :wink:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1544#issuecomment-748856865
https://github.com/scverse/scanpy/issues/1545#issuecomment-1222015647:94,Testability,log,logarithmic,94,I'm confused too. The documentation says that flavor ='seurat' or flavor ='cell_ranger' needs logarithmic data. Why the data is transformed back out of logspace using X=np.expm1(X) if flavor='seurat' ? Doesn't this do nothing if expm1(log1p(X))?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1545#issuecomment-1222015647
https://github.com/scverse/scanpy/issues/1545#issuecomment-1222015647:152,Testability,log,logspace,152,I'm confused too. The documentation says that flavor ='seurat' or flavor ='cell_ranger' needs logarithmic data. Why the data is transformed back out of logspace using X=np.expm1(X) if flavor='seurat' ? Doesn't this do nothing if expm1(log1p(X))?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1545#issuecomment-1222015647
https://github.com/scverse/scanpy/issues/1545#issuecomment-1501448104:249,Testability,log,log,249,"Hi, same confusion here.; According to: https://github.com/scverse/scanpy/issues/969#issuecomment-629667682; If I set `flavor ='cell_ranger'`, dose it mean I should not use `sc.pp.log1p(adata)` to ensure use the ""library size normalized counts""(not log)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1545#issuecomment-1501448104
https://github.com/scverse/scanpy/issues/1549#issuecomment-1298339775:191,Testability,log,logg,191,"I found the same problem in `sc.pl.dotplot`, but i found in `\scanpy\plotting\_anndata.py` 2236th line：; ```; if dendrogram_key not in adata.uns:; from ..tools._dendrogram import dendrogram. logg.warning(; f""dendrogram data not found (using key={dendrogram_key}). ""; ""Running `sc.tl.dendrogram` with default parameters. For fine ""; ""tuning it is recommended to run `sc.tl.dendrogram` independently.""; ); dendrogram(adata, groupby, key_added=dendrogram_key); ```; `dendrogram` is not add `var_names`, and i fixed it in my source code. ------; anndata 0.7.8; scanpy 1.9.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1549#issuecomment-1298339775
https://github.com/scverse/scanpy/issues/1549#issuecomment-1739649092:4,Deployability,update,update,4,Any update on this? I encountered the same issue,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1549#issuecomment-1739649092
https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721:258,Deployability,continuous,continuous,258,"Scanpy has enhanced sc.pl.umap function last year. For example, now sc.pl.umap(adata,color=[""louvain""],groups=""1"") can highligt cluster 1 while displaying other clusters in gray color. I think they are very similar, excepting that gene expressing values are continuous variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721
https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721:11,Modifiability,enhance,enhanced,11,"Scanpy has enhanced sc.pl.umap function last year. For example, now sc.pl.umap(adata,color=[""louvain""],groups=""1"") can highligt cluster 1 while displaying other clusters in gray color. I think they are very similar, excepting that gene expressing values are continuous variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721
https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721:269,Modifiability,variab,variables,269,"Scanpy has enhanced sc.pl.umap function last year. For example, now sc.pl.umap(adata,color=[""louvain""],groups=""1"") can highligt cluster 1 while displaying other clusters in gray color. I think they are very similar, excepting that gene expressing values are continuous variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721
https://github.com/scverse/scanpy/issues/1550#issuecomment-748139666:24,Usability,guid,guidance,24,Thanks for you detailed guidance. All I want is to display zero count cells as some color (gray) and this is what cellranger and seurat do. I can do it now following the above code. This is just what I want !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748139666
https://github.com/scverse/scanpy/pull/1551#issuecomment-748422767:293,Availability,error,error,293,"I like this idea. * While we should be conservative about adding new keywords, this fits well with `vmin` and `vmax`; * The docs for this argument should mention that users should pass a diverging palette with it, and probably have an example; * If `norm` is passed along at the same time, an error should be thrown; * It looks like there is a bunch of repeated code handling generating the `norm`, could this get put into a common utility function?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748422767
https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569:47,Availability,error,error,47,"> If norm is passed along at the same time, an error should be thrown. Following up on this a bit, I realized I didn't actually know what matplotlib would do if you passed `norm` and a bound, so I checked it out. Turns out they currently allow it, but it's deprecated, so throwing an error is the right thing to do. ```python; import vega_datasets; import matplotlib as mpl, matplotlib.pyplot as plt. iris = vega_datasets.data.iris(). norm = mpl.colors.LogNorm(). plt.scatter(; iris[""sepalLength""],; iris[""sepalWidth""],; c=iris[""petalLength""],; norm=norm,; vmin=3,; ); plt.colorbar(); ```. ```; MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax ; simultaneously is deprecated since 3.3 and will become an error two minor releases ; later. Please pass vmin/vmax directly to the norm when creating it.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569
https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569:284,Availability,error,error,284,"> If norm is passed along at the same time, an error should be thrown. Following up on this a bit, I realized I didn't actually know what matplotlib would do if you passed `norm` and a bound, so I checked it out. Turns out they currently allow it, but it's deprecated, so throwing an error is the right thing to do. ```python; import vega_datasets; import matplotlib as mpl, matplotlib.pyplot as plt. iris = vega_datasets.data.iris(). norm = mpl.colors.LogNorm(). plt.scatter(; iris[""sepalLength""],; iris[""sepalWidth""],; c=iris[""petalLength""],; norm=norm,; vmin=3,; ); plt.colorbar(); ```. ```; MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax ; simultaneously is deprecated since 3.3 and will become an error two minor releases ; later. Please pass vmin/vmax directly to the norm when creating it.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569
https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569:723,Availability,error,error,723,"> If norm is passed along at the same time, an error should be thrown. Following up on this a bit, I realized I didn't actually know what matplotlib would do if you passed `norm` and a bound, so I checked it out. Turns out they currently allow it, but it's deprecated, so throwing an error is the right thing to do. ```python; import vega_datasets; import matplotlib as mpl, matplotlib.pyplot as plt. iris = vega_datasets.data.iris(). norm = mpl.colors.LogNorm(). plt.scatter(; iris[""sepalLength""],; iris[""sepalWidth""],; c=iris[""petalLength""],; norm=norm,; vmin=3,; ); plt.colorbar(); ```. ```; MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax ; simultaneously is deprecated since 3.3 and will become an error two minor releases ; later. Please pass vmin/vmax directly to the norm when creating it.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569
https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569:739,Deployability,release,releases,739,"> If norm is passed along at the same time, an error should be thrown. Following up on this a bit, I realized I didn't actually know what matplotlib would do if you passed `norm` and a bound, so I checked it out. Turns out they currently allow it, but it's deprecated, so throwing an error is the right thing to do. ```python; import vega_datasets; import matplotlib as mpl, matplotlib.pyplot as plt. iris = vega_datasets.data.iris(). norm = mpl.colors.LogNorm(). plt.scatter(; iris[""sepalLength""],; iris[""sepalWidth""],; c=iris[""petalLength""],; norm=norm,; vmin=3,; ); plt.colorbar(); ```. ```; MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax ; simultaneously is deprecated since 3.3 and will become an error two minor releases ; later. Please pass vmin/vmax directly to the norm when creating it.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569
https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569:453,Testability,Log,LogNorm,453,"> If norm is passed along at the same time, an error should be thrown. Following up on this a bit, I realized I didn't actually know what matplotlib would do if you passed `norm` and a bound, so I checked it out. Turns out they currently allow it, but it's deprecated, so throwing an error is the right thing to do. ```python; import vega_datasets; import matplotlib as mpl, matplotlib.pyplot as plt. iris = vega_datasets.data.iris(). norm = mpl.colors.LogNorm(). plt.scatter(; iris[""sepalLength""],; iris[""sepalWidth""],; c=iris[""petalLength""],; norm=norm,; vmin=3,; ); plt.colorbar(); ```. ```; MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax ; simultaneously is deprecated since 3.3 and will become an error two minor releases ; later. Please pass vmin/vmax directly to the norm when creating it.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748567569
https://github.com/scverse/scanpy/pull/1551#issuecomment-748679208:139,Availability,error,error,139,"> ```; > MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax ; > simultaneously is deprecated since 3.3 and will become an error two minor releases ; > later. Please pass vmin/vmax directly to the norm when creating it.; > ```. Yeah, that actually re-ignited my idea of adding support for vcenter after upgrading mpl :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748679208
https://github.com/scverse/scanpy/pull/1551#issuecomment-748679208:155,Deployability,release,releases,155,"> ```; > MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax ; > simultaneously is deprecated since 3.3 and will become an error two minor releases ; > later. Please pass vmin/vmax directly to the norm when creating it.; > ```. Yeah, that actually re-ignited my idea of adding support for vcenter after upgrading mpl :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-748679208
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:993,Deployability,update,updated,993,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:38,Testability,test,tests,38,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:104,Testability,test,test,104,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:221,Testability,test,tests,221,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:357,Testability,test,tests,357,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:674,Testability,test,tests,674,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:1067,Testability,test,tests,1067,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:1190,Testability,test,tests,1190,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:1268,Testability,test,test,1268,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:1317,Testability,test,tests,1317,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676:1356,Testability,test,tests,1356,"@fidelram Some help with the plotting tests is very much appreciated :) I don't get why only Python 3.6 test passes and others fail. For example; why does [this plot](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) which is produced by [this code](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_plotting.py#L357) where the color legend is defined [here](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_matrixplot.py#L74) as `Mean expression in group` have the old title which is `Expression level in group`? If the ""ground truth plots"" in the ""_images"" folder are outdated, aren't the tests supposed to fail since the [this commit](https://github.com/theislab/scanpy/commit/d4d373ea58b9add4451091c5650d4da245d025dc#diff-421b3afbcd51c81c45f23dd7e8483697b68668de9172f124ec6bf2f0523840d9L95) i.e. `Expression level in group` -> `Mean expression in group`, but they don't fail, why?. Another example: I have updated [this file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot_with_totals.png) but not [that file](https://github.com/theislab/scanpy/blob/master/scanpy/tests/_images/master_matrixplot.png) which seem both outdated. But Python 3.6 test still seems to pass. How come? Are plotting tests disabled in Python3.6?. Plotting tests are extremely tricky and make it pretty difficult to contribute I must say :/ I think twice or three times when I want to change anything about plotting... (not a complaint addressed to @fidelram but something we should consider as a team)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-751396676
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:619,Availability,mask,masking,619,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:568,Deployability,install,installed,568,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:786,Deployability,update,updated,786,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:454,Safety,avoid,avoid,454,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:29,Testability,test,tests,29,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:120,Testability,test,test,120,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:246,Testability,test,tests,246,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:334,Testability,test,test,334,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:460,Testability,test,tests,460,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:1009,Testability,test,tests,1009,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523:766,Usability,clear,clearly,766,"@gokceneraslan Regarding the tests: yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. I agree that this limits contribution because the mountain of work to get the tests working puts one off. For the particular question about the title difference: the test may be passing because of the 'threshold' used to call the images as different. Why we use a threshold? This is to avoid tests from failing due to small differences between matplotlib or other graphic libraries versions or fonts installed. However, sometimes the threshold may be masking some small problems, although in general I am quite happy because important differences not missed. . BTW: The image that you point out is clearly wrong but I updated it recently for other reason (PR #1584). Regarding the issue about adding `norm` as explicit parameter. I would suggest to add it if this just mean changing very few lines but I know this is lot of work (do we want tests for this?) for something that is already working. . Besides the very good review by Isaac I don't have much to add and will be happy to merge once some of the changes are taken care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1551#issuecomment-761117523
https://github.com/scverse/scanpy/issues/1552#issuecomment-1110180699:29,Deployability,update,update,29,Also looking forward to this update,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1552#issuecomment-1110180699
https://github.com/scverse/scanpy/issues/1552#issuecomment-1435832592:24,Deployability,update,update,24,Looking forward to this update or is there any other way to achieve this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1552#issuecomment-1435832592
https://github.com/scverse/scanpy/issues/1560#issuecomment-753982630:344,Integrability,depend,depending,344,"Hi,; It looks like this code comes from the [single-cell-tutorial github](https://github.com/theislab/single-cell-tutorial). It might be best to report the issue there. It looks like you haven't filtered out genes that are not expressed in your dataset via `sc.pp.filter_genes()`. If you filter the dataset (maybe with `min_cells` set to 5-50, depending on the size of your dataset), then this shouldn't happen. Here, you have too many genes with the same mean, which is very low.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560#issuecomment-753982630
https://github.com/scverse/scanpy/pull/1561#issuecomment-753488211:816,Integrability,interface,interface,816,"Hi Pavlin, Happy New Year!. Great to see this going forward!. > We'd probably have to do something similar to the gauss option and just overwrite the UMAP weights after the fact. Does this sound reasonable?. Yes, I also thought we'd need to do it similar to the `gauss` option. Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? I think it may be possible to use openTSNE's function to compute the affinities and then get the weights out of there?. One question here though: how would `tsne` function know if it should use the uniform kernel or the weights constructed by the `neighbors` function? Can some flag be switched if `neighbors` is run with `mode='tsne'` so that the `tsne()` function later on uses those weights? Not sure what's the best interface here. Alternatively the `tsne()` function could check if the weights conform to what t-SNE expects (sum to 1). Maybe that's better actually. Apart from that, I noticed that you implemented ; ```; class UniformAffinities(openTSNE.affinity.Affinities):; ``` ; in here, but isn't it part of `openTSNE` already?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753488211
https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:1472,Deployability,release,release,1472,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428
https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:722,Integrability,wrap,wrapper,722,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428
https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:1153,Usability,simpl,simple,1153,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428
https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515:392,Deployability,release,release,392,"> As you can see, it's a pretty trivial wrapper anyway. Yes, makes sense. > Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. Yes, I like this. > I added exaggeration=None, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release. Ah, right, I somehow overlooked that you did add the exaggeration parameter. That's fine then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515
https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515:40,Integrability,wrap,wrapper,40,"> As you can see, it's a pretty trivial wrapper anyway. Yes, makes sense. > Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. Yes, I like this. > I added exaggeration=None, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release. Ah, right, I somehow overlooked that you did add the exaggeration parameter. That's fine then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515
https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515:134,Usability,simpl,simple,134,"> As you can see, it's a pretty trivial wrapper anyway. Yes, makes sense. > Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. Yes, I like this. > I added exaggeration=None, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release. Ah, right, I somehow overlooked that you did add the exaggeration parameter. That's fine then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515
https://github.com/scverse/scanpy/pull/1561#issuecomment-757850250:49,Availability,ping,pinging,49,I hope everybody had a good New Year break! Just pinging @ivirshup again because I think it'd be great to move forward with this. Also wanted to ping @falexwolf as I know he did a lot of work on dim reduction etc. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-757850250
https://github.com/scverse/scanpy/pull/1561#issuecomment-757850250:145,Availability,ping,ping,145,I hope everybody had a good New Year break! Just pinging @ivirshup again because I think it'd be great to move forward with this. Also wanted to ping @falexwolf as I know he did a lot of work on dim reduction etc. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-757850250
https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:1055,Availability,avail,available,1055,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448
https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:195,Deployability,integrat,integration,195,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448
https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:195,Integrability,integrat,integration,195,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448
https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:1230,Usability,simpl,simple,1230,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448
https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450:268,Availability,mainten,maintenance,268,"> I think it may be possible to use openTSNE's function to compute the affinities and then get the weights out of there?. I would definitely like this to be the case. I'm not sure I see . > Why? `sc.pp.neighbors` already has `method='gauss'`. To me, it’s largely of a maintenance and documentation issue. Most bugs I fix (here, and in upstream libraries) come from argument handling. The more features you lump into a function, the more complicated argument handling gets. There are questions of default values and fallbacks for different backends, and being sure users understand which arguments are valid for each backend. The use of the `Neighbors` class ends up making the `neighbors` function much more complicated than it needs to be. I think skipping out on that here can make this implementation much more simple. From an API stand point, I would like the ""blessed"" `tsne` workflow to be dead obvious. I'm thinking:. ```python; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```. How many arguments is it going to take to make this work if this functionality is in `sc.pp.neighbors`? At a minimum, `k=30, method=tsne_affinity, nn_method=""annoy""`, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450
https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450:814,Usability,simpl,simple,814,"> I think it may be possible to use openTSNE's function to compute the affinities and then get the weights out of there?. I would definitely like this to be the case. I'm not sure I see . > Why? `sc.pp.neighbors` already has `method='gauss'`. To me, it’s largely of a maintenance and documentation issue. Most bugs I fix (here, and in upstream libraries) come from argument handling. The more features you lump into a function, the more complicated argument handling gets. There are questions of default values and fallbacks for different backends, and being sure users understand which arguments are valid for each backend. The use of the `Neighbors` class ends up making the `neighbors` function much more complicated than it needs to be. I think skipping out on that here can make this implementation much more simple. From an API stand point, I would like the ""blessed"" `tsne` workflow to be dead obvious. I'm thinking:. ```python; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```. How many arguments is it going to take to make this work if this functionality is in `sc.pp.neighbors`? At a minimum, `k=30, method=tsne_affinity, nn_method=""annoy""`, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759238450
https://github.com/scverse/scanpy/pull/1561#issuecomment-759329085:1290,Availability,down,downstream,1290,"Thanks! So my understanding is that you are saying that `neighbors` function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that `gauss` out of it, I guess?). > From an API stand point, I would like the ""blessed"" tsne workflow to be dead obvious. I'm thinking:; > `sc.pp.neighbors_tsne(adata)`; > `sc.tl.tsne(adata)`; > How many arguments is it going to take to make this work if this functionality is in sc.pp.neighbors? At a minimum, k=30, method=tsne_affinity, nn_method=""annoy"", right?. I'd say `perplexity=30, method='tsne'`. I don't see why t-SNE should use annoy if UMAP uses pynndescent. This does not matter for the algorithm. So `nn_method` could either be included in all `neighbors` functions or into none, IMHO. In any case,; ```; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```. would also be fine with me. I think it's important that the following works:. ```; sc.pp.neighbors(adata); sc.tl.tsne(adata); ```. and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). But if somebody wants to do the ""actual"" t-SNE, then they can run `sc.pp.neighbors_tsne(adata)` first. I think this makes sense. . One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs `neighbors_tsne` (or both `neighbors` and `neighbors_tsne`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759329085
https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:1406,Availability,down,downstream,1406,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128
https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:521,Integrability,interface,interface,521,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128
https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:287,Usability,simpl,simpler,287,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128
https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:1003,Usability,clear,clear,1003,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128
https://github.com/scverse/scanpy/pull/1561#issuecomment-759340575:643,Usability,clear,clear,643,"> Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. My argument was mostly about API. But one big benefit of using kNN with k=15 is that it's *much faster* then using k=90 (with perplexity=30) which is what t-SNE is using by default (for historical reasons). It's not about uniform vs non-uniform, it's about k=15 vs k=90. Uniform on k=15 just happens to give almost the same results as perplexity=30 on k=90. So it's a lucky coincidence that I thought we could benefit from. > Second API: I'm not sure I completely agree with this. I think it would be the most clear for sc.pp.neighbors to essentially mean ""build umap's connectivity graph"", and functions like sc.tl.tsne or sc.tl.umap to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via connectivities_key) are the weights that get used. I understand what you saying, but the situation won't be symmetric because `neighbors` already exists, is *not* called `neighbors_umap`, and all users of Scanpy are very familiar with this function. I'd like to make it very easy to use t-SNE in scanpy and that it naturally fits into the scanpy's established workflow. That's why I think simply running `tsne()` after running `neighbors()` should be possible. Please note that t-SNE requires normalized weights (they should sum to 1). The weights constructed by UMAP in `neighbors` are not normalized. So if you run `neighbors()` and then `tsne()` then t-SNE should do *something* in order to be able to use this graph. My suggestion is that it discards the weights and uses normalized affinity matrix. I am not sure what exactly is your suggestion? Take UMAP weights and normalize them? This has never been explored in the literature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759340575
https://github.com/scverse/scanpy/pull/1561#issuecomment-759340575:1286,Usability,simpl,simply,1286,"> Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. My argument was mostly about API. But one big benefit of using kNN with k=15 is that it's *much faster* then using k=90 (with perplexity=30) which is what t-SNE is using by default (for historical reasons). It's not about uniform vs non-uniform, it's about k=15 vs k=90. Uniform on k=15 just happens to give almost the same results as perplexity=30 on k=90. So it's a lucky coincidence that I thought we could benefit from. > Second API: I'm not sure I completely agree with this. I think it would be the most clear for sc.pp.neighbors to essentially mean ""build umap's connectivity graph"", and functions like sc.tl.tsne or sc.tl.umap to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via connectivities_key) are the weights that get used. I understand what you saying, but the situation won't be symmetric because `neighbors` already exists, is *not* called `neighbors_umap`, and all users of Scanpy are very familiar with this function. I'd like to make it very easy to use t-SNE in scanpy and that it naturally fits into the scanpy's established workflow. That's why I think simply running `tsne()` after running `neighbors()` should be possible. Please note that t-SNE requires normalized weights (they should sum to 1). The weights constructed by UMAP in `neighbors` are not normalized. So if you run `neighbors()` and then `tsne()` then t-SNE should do *something* in order to be able to use this graph. My suggestion is that it discards the weights and uses normalized affinity matrix. I am not sure what exactly is your suggestion? Take UMAP weights and normalize them? This has never been explored in the literature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759340575
https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:141,Deployability,integrat,integration,141,"I'm a little late to the party, but here's my 0.02$. > What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ ingest functionality happening separately, or would you like to do it all at once?. I think we can split it into two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009
https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:2789,Deployability,release,release,2789,"ustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since the goal is to use the same KNNG construction procedure, t-SNE will have to call the same UMAP function, and override the weights afterward. Much like `gauss` does now. It would probably make more sense to split out the KNNG construction from the UMAP weight calculation, but that seems like a lot of work. Or maybe not. In the latest UMAP release from a few days ago, they split out the graph construction into `pynndescent`. Either way, refactoring this doesn't belong in this PR. Alternatively, we could construct our own KNNG in `sc.pp.tsne_neighbors` using Annoy, which openTSNE does by default. But that seems suboptimal, because the design philosophy seems to be re-use the same graph for everything. . What I think would make more sense is to remove the connectivity calculation from the `sc.pp.neighbors` altogether, and have that calculate an unweighted KNNG. Since different methods need their own connectivities anyways, that should be done in each method separately. So UMAP connectivities would be calculated in `sc.tl.umap`, and the Louvain Jaccard connectivities in `sc.tl.louvain`. Then, you wouldn't be assigning any preference to any one connectivity method. From what I can tell, there's no evidence the UMAP connectivities are better than the others in any way, especially not for clustering. If you have any information on this, I'd be really curious ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009
https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:141,Integrability,integrat,integration,141,"I'm a little late to the party, but here's my 0.02$. > What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ ingest functionality happening separately, or would you like to do it all at once?. I think we can split it into two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009
https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:1441,Integrability,rout,route,1441,"methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009
https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:2888,Modifiability,refactor,refactoring,2888,"ould I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since the goal is to use the same KNNG construction procedure, t-SNE will have to call the same UMAP function, and override the weights afterward. Much like `gauss` does now. It would probably make more sense to split out the KNNG construction from the UMAP weight calculation, but that seems like a lot of work. Or maybe not. In the latest UMAP release from a few days ago, they split out the graph construction into `pynndescent`. Either way, refactoring this doesn't belong in this PR. Alternatively, we could construct our own KNNG in `sc.pp.tsne_neighbors` using Annoy, which openTSNE does by default. But that seems suboptimal, because the design philosophy seems to be re-use the same graph for everything. . What I think would make more sense is to remove the connectivity calculation from the `sc.pp.neighbors` altogether, and have that calculate an unweighted KNNG. Since different methods need their own connectivities anyways, that should be done in each method separately. So UMAP connectivities would be calculated in `sc.tl.umap`, and the Louvain Jaccard connectivities in `sc.tl.louvain`. Then, you wouldn't be assigning any preference to any one connectivity method. From what I can tell, there's no evidence the UMAP connectivities are better than the others in any way, especially not for clustering. If you have any information on this, I'd be really curious to know. Ultimately, the decision is up to you guys, since this is more of a desi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009
https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:1290,Usability,simpl,simple,1290,"nto two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009
https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:1402,Usability,simpl,simple,1402," them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the cod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009
https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:506,Availability,avail,available,506,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200
https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:713,Deployability,integrat,integrate,713,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200
https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:763,Deployability,integrat,integrated,763,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200
https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:165,Integrability,depend,depend,165,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200
https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:713,Integrability,integrat,integrate,713,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200
https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:763,Integrability,integrat,integrated,763,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200
https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:1125,Integrability,Wrap,WrappedAffinities,1125,"ave multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already done this way in `openTSNE`, I think this could help with that goal. > From what I can tell, the standard way of weighing the KNNG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200
https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:2549,Testability,benchmark,benchmarks,2549,"s constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already done this way in `openTSNE`, I think this could help with that goal. > From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). `Seurat` does this, but I'm not sure this has been standardized much otherwise. Off the top of my head, I'd guess that the Jaccard method is going to be much more sensitive to `k` as a parameter, particularly how it relates to cluster size. I'm not really aware of any consensus made on this or benchmarks comparing approaches. When I've looked into it, using the weights (as opposed to just binarized edges) seems to give more stable results, particularly for small clusters. We've had some previous discussions on this here: #586, #240.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200
https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:531,Usability,clear,clear,531,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200
https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738:1311,Modifiability,refactor,refactoring,1311,"My primary wish here is very simple. I'd like the following sequence of commands:; ```; sc.pp.neighbors(adata); sc.tl.tsne(adata); ```; to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. What you suggest @ivirshup (t-SNE on normalized UMAP affinities) could maybe achieve that, but we would need to check. As I said, I don't think anybody ever has tried that. I could imagine that it would roughly correspond to t-SNE with perplexity less than 30, perhaps 20 or so, but this is just a wild guess. . I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications, because it's really t-SNE on normalized UMAP affinities which is an odd-sounding hybrid. But if the result is similar enough to t-SNE, then maybe it's okay to call it simply ""t-SNE (as implemented in Scanpy)""... A *separate* question is how a user would be able to achieve t-SNE *proper*, and here I could live with either; ```; sc.pp.neighbors(adata, method='tsne') # this would use perplexity=30 by default; sc.tl.tsne(adata); ```; or; ```; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```; This is just a question of API, and is less important for me personally. I agree that it could be better to have `neighbors()` compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738
https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738:29,Usability,simpl,simple,29,"My primary wish here is very simple. I'd like the following sequence of commands:; ```; sc.pp.neighbors(adata); sc.tl.tsne(adata); ```; to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. What you suggest @ivirshup (t-SNE on normalized UMAP affinities) could maybe achieve that, but we would need to check. As I said, I don't think anybody ever has tried that. I could imagine that it would roughly correspond to t-SNE with perplexity less than 30, perhaps 20 or so, but this is just a wild guess. . I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications, because it's really t-SNE on normalized UMAP affinities which is an odd-sounding hybrid. But if the result is similar enough to t-SNE, then maybe it's okay to call it simply ""t-SNE (as implemented in Scanpy)""... A *separate* question is how a user would be able to achieve t-SNE *proper*, and here I could live with either; ```; sc.pp.neighbors(adata, method='tsne') # this would use perplexity=30 by default; sc.tl.tsne(adata); ```; or; ```; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```; This is just a question of API, and is less important for me personally. I agree that it could be better to have `neighbors()` compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738
https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738:783,Usability,simpl,simply,783,"My primary wish here is very simple. I'd like the following sequence of commands:; ```; sc.pp.neighbors(adata); sc.tl.tsne(adata); ```; to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. What you suggest @ivirshup (t-SNE on normalized UMAP affinities) could maybe achieve that, but we would need to check. As I said, I don't think anybody ever has tried that. I could imagine that it would roughly correspond to t-SNE with perplexity less than 30, perhaps 20 or so, but this is just a wild guess. . I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications, because it's really t-SNE on normalized UMAP affinities which is an odd-sounding hybrid. But if the result is similar enough to t-SNE, then maybe it's okay to call it simply ""t-SNE (as implemented in Scanpy)""... A *separate* question is how a user would be able to achieve t-SNE *proper*, and here I could live with either; ```; sc.pp.neighbors(adata, method='tsne') # this would use perplexity=30 by default; sc.tl.tsne(adata); ```; or; ```; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```; This is just a question of API, and is less important for me personally. I agree that it could be better to have `neighbors()` compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738
https://github.com/scverse/scanpy/pull/1561#issuecomment-762595475:1477,Availability,down,down,1477,"> I'd like the following sequence of commands ... to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. ; > I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications. I share the same worry, but am not qualified to answer when something becomes ""t-SNE"". I think it would be sufficient for `sc.tl.tsne` to warn users if the graph it was passed looks unexpected (or if it could tell it was generated by a different method). > What you suggest (t-SNE on normalized UMAP affinities) could maybe achieve that. From an API point of view, we don't control weights at the `sc.tl.umap` call, so I think it would be strange to control weights at the `sc.tl.tsne` call. I'm also not sure if binarizing the graph would be closer to ""t-SNE"". ----------------------. About `sc.pp.neighbors` vs `sc.pp.neighbors_tsne`. > This is just a question of API, and is less important for me personally. I agree that it could be better to have neighbors() compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR. I think for backwards compatibility I would like to keep neighbors pretty much as is. I think new functions like `distance_neighbors`, `umap_neighbors`, `tsne_neighbors` could be reasonable to add. It's also possible we could add a `""tsne""` method to `neighbors`, but I think the implementation can look very similar to having a `tsne_neighbors` function, so this can be kicked down the road a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762595475
https://github.com/scverse/scanpy/pull/1561#issuecomment-762595475:1058,Modifiability,refactor,refactoring,1058,"> I'd like the following sequence of commands ... to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. ; > I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications. I share the same worry, but am not qualified to answer when something becomes ""t-SNE"". I think it would be sufficient for `sc.tl.tsne` to warn users if the graph it was passed looks unexpected (or if it could tell it was generated by a different method). > What you suggest (t-SNE on normalized UMAP affinities) could maybe achieve that. From an API point of view, we don't control weights at the `sc.tl.umap` call, so I think it would be strange to control weights at the `sc.tl.tsne` call. I'm also not sure if binarizing the graph would be closer to ""t-SNE"". ----------------------. About `sc.pp.neighbors` vs `sc.pp.neighbors_tsne`. > This is just a question of API, and is less important for me personally. I agree that it could be better to have neighbors() compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR. I think for backwards compatibility I would like to keep neighbors pretty much as is. I think new functions like `distance_neighbors`, `umap_neighbors`, `tsne_neighbors` could be reasonable to add. It's also possible we could add a `""tsne""` method to `neighbors`, but I think the implementation can look very similar to having a `tsne_neighbors` function, so this can be kicked down the road a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762595475
https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727:179,Integrability,message,message,179,"> I think it would be sufficient for sc.tl.tsne to warn users if the graph it was passed looks unexpected. Yes, warning is a good idea. But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. If you really want to get exactly t-SNE, run the following command instead: ..., but note that it can be slower and the result will likely look around the same"". > From an API point of view, we don't control weights at the sc.tl.umap call, so I think it would be strange to control weights at the sc.tl.tsne call. But we will *have* to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. > I'm also not sure if binarizing the graph would be closer to ""t-SNE"". Maybe not, but; (1) it will be further away from UMAP, so that e.g. UMAP paper does not need to be cited when using such t-SNE. ; (2) There is citeable literature showing that binary affinities yield practically the same result as t-SNE proper. We can cite this in Scanpy docs. I am not aware of any such literature for normalized UMAP affinities in t-SNE. Stepping back, I am not sure I managed to convey my main point here. Which is: Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, *UMAP-independent*, and nearly equivalent replacement for k=90, perplexity=30 affinities. . I agree with Pavlin though that pretty much any decision would be better than the current situation :-)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727
https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727:239,Integrability,message,message,239,"> I think it would be sufficient for sc.tl.tsne to warn users if the graph it was passed looks unexpected. Yes, warning is a good idea. But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. If you really want to get exactly t-SNE, run the following command instead: ..., but note that it can be slower and the result will likely look around the same"". > From an API point of view, we don't control weights at the sc.tl.umap call, so I think it would be strange to control weights at the sc.tl.tsne call. But we will *have* to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. > I'm also not sure if binarizing the graph would be closer to ""t-SNE"". Maybe not, but; (1) it will be further away from UMAP, so that e.g. UMAP paper does not need to be cited when using such t-SNE. ; (2) There is citeable literature showing that binary affinities yield practically the same result as t-SNE proper. We can cite this in Scanpy docs. I am not aware of any such literature for normalized UMAP affinities in t-SNE. Stepping back, I am not sure I managed to convey my main point here. Which is: Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, *UMAP-independent*, and nearly equivalent replacement for k=90, perplexity=30 affinities. . I agree with Pavlin though that pretty much any decision would be better than the current situation :-)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727
https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727:350,Integrability,message,message,350,"> I think it would be sufficient for sc.tl.tsne to warn users if the graph it was passed looks unexpected. Yes, warning is a good idea. But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. If you really want to get exactly t-SNE, run the following command instead: ..., but note that it can be slower and the result will likely look around the same"". > From an API point of view, we don't control weights at the sc.tl.umap call, so I think it would be strange to control weights at the sc.tl.tsne call. But we will *have* to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. > I'm also not sure if binarizing the graph would be closer to ""t-SNE"". Maybe not, but; (1) it will be further away from UMAP, so that e.g. UMAP paper does not need to be cited when using such t-SNE. ; (2) There is citeable literature showing that binary affinities yield practically the same result as t-SNE proper. We can cite this in Scanpy docs. I am not aware of any such literature for normalized UMAP affinities in t-SNE. Stepping back, I am not sure I managed to convey my main point here. Which is: Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, *UMAP-independent*, and nearly equivalent replacement for k=90, perplexity=30 affinities. . I agree with Pavlin though that pretty much any decision would be better than the current situation :-)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727
https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:569,Availability,error,error,569,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636
https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:1183,Availability,error,erroring,1183," this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warning to notify the user if the weights were computed in a non-standard way; * There should be a function for computing a perplexity weighted nearest neighbor gr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636
https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:55,Deployability,release,release,55,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636
https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:1324,Energy Efficiency,efficient,efficient,1324,"s!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warning to notify the user if the weights were computed in a non-standard way; * There should be a function for computing a perplexity weighted nearest neighbor graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636
https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:142,Integrability,message,message,142,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636
https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:202,Integrability,message,message,202,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636
https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:313,Integrability,message,message,313,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636
https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:609,Performance,optimiz,optimization,609,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636
https://github.com/scverse/scanpy/pull/1561#issuecomment-773303341:441,Availability,error,erroring,441,"> In general, are we agreed on these points? ; > tsne should allow weights to be passed through (whether perplexity based, or not) ; > There should be a warning to notify the user if the weights were computed in a non-standard way ; > There should be a function for computing a perplexity weighted nearest neighbor graph. . Yes. I agree. > Perhaps there needs to be a weights option on tsne which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. This sounds good. IMHO erroring is not necessary. There will be a warning anyway. > Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Actually UMAP weights *are* symmetric. So it would be enough to normalize the entire weight matrix to sum to 1. -----------------. I think there are two different choices that we have been disagreeing about:. * Choice 1: whether `preprocess_weights='normalize'` or `preprocess_weights='binarize'` is default for `tl.tsne()` if the passed weights do not sum to 1.; * Argument for `normalize` (Isaac): closer to originally calculated weights;; * Argument for `binarize` (Dmitry and Pavlin): will make it UMAP-independent if `tl.tsne()` is run after default `tt.neighbors()`.; * Choice 2: whether perplexity weights are given by `pp.neighbors_tsne()` or by `pp.neighbors(method='tsne')`; * Argument for `neighbors_tsne` (Isaac): the existing function is complicated enough, so let's not make it even more complicated;; * Argument for `neighbors(method='tsne')` (Dmitry and Pavlin): the other option would make the API for UMAP weights and for tSNE weights assymetric and even confusing, as `neighbors` is not called `neighbors_umap`. Does this summarize the arguments from both sides?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773303341
https://github.com/scverse/scanpy/pull/1561#issuecomment-773303341:525,Availability,error,erroring,525,"> In general, are we agreed on these points? ; > tsne should allow weights to be passed through (whether perplexity based, or not) ; > There should be a warning to notify the user if the weights were computed in a non-standard way ; > There should be a function for computing a perplexity weighted nearest neighbor graph. . Yes. I agree. > Perhaps there needs to be a weights option on tsne which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. This sounds good. IMHO erroring is not necessary. There will be a warning anyway. > Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Actually UMAP weights *are* symmetric. So it would be enough to normalize the entire weight matrix to sum to 1. -----------------. I think there are two different choices that we have been disagreeing about:. * Choice 1: whether `preprocess_weights='normalize'` or `preprocess_weights='binarize'` is default for `tl.tsne()` if the passed weights do not sum to 1.; * Argument for `normalize` (Isaac): closer to originally calculated weights;; * Argument for `binarize` (Dmitry and Pavlin): will make it UMAP-independent if `tl.tsne()` is run after default `tt.neighbors()`.; * Choice 2: whether perplexity weights are given by `pp.neighbors_tsne()` or by `pp.neighbors(method='tsne')`; * Argument for `neighbors_tsne` (Isaac): the existing function is complicated enough, so let's not make it even more complicated;; * Argument for `neighbors(method='tsne')` (Dmitry and Pavlin): the other option would make the API for UMAP weights and for tSNE weights assymetric and even confusing, as `neighbors` is not called `neighbors_umap`. Does this summarize the arguments from both sides?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773303341
https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:1732,Availability,down,downstream,1732," not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more and more functionality to scanpy, things are inevitably going to get more complicated, and patching the existing API will just lead to thousands of parameters. The API in #1739 seems like the logical next step. I'll try to work on this in the coming days/weeks, so we can see what's really necessary, and we can work out the exact API after we have a working prototype. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797
https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:155,Deployability,release,release,155,"So, having re-read the thread, the steps forward seem pretty clear, and we're really just debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are conside",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797
https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:203,Deployability,release,release,203,"So, having re-read the thread, the steps forward seem pretty clear, and we're really just debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are conside",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797
https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:2179,Deployability,patch,patching,2179," not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more and more functionality to scanpy, things are inevitably going to get more complicated, and patching the existing API will just lead to thousands of parameters. The API in #1739 seems like the logical next step. I'll try to work on this in the coming days/weeks, so we can see what's really necessary, and we can work out the exact API after we have a working prototype. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797
https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:1024,Safety,avoid,avoid,1024,"st debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797
https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:2280,Testability,log,logical,2280," not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more and more functionality to scanpy, things are inevitably going to get more complicated, and patching the existing API will just lead to thousands of parameters. The API in #1739 seems like the logical next step. I'll try to work on this in the coming days/weeks, so we can see what's really necessary, and we can work out the exact API after we have a working prototype. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797
https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:61,Usability,clear,clear,61,"So, having re-read the thread, the steps forward seem pretty clear, and we're really just debating the API, which wouldn't be that hard to change before a release anyways. It becomes much harder after a release because they you have to worry about backward compatibility. So, I suggest the following. First, calling `sc.pp.neighbors` followed by `sc.tl.tsne` should not recompute the nearest neighbors, and use the existing KNNG. To get around the whole ""should we binarize or not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are conside",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797
https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797:1606,Usability,clear,clear,1606," not"", I suggest adding a parameter to `sc.tl.tsne(binarize: bool = ""auto"")`. If `binarize=True`, we binarize the KNNG, regardless of input. If `binarize=False`, we just re-normalize the weights if needed. This way, we can potentially use UMAP connectivities. As for the default option `binarize=""auto""`, this would automatically binarize weights if they don't come from `sc.pp.neighbors_tsne`. This way, the default would either use t-SNE proper, or the uniform kernel t-SNE, which is close enough. Since most users use default values, this would avoid people running a strange combination of UMAP and t-SNE, and have something close to t-SNE proper, and would only have to cite the t-SNE paper (as implemented in scanpy). This way, we can run any of the three scenarios. Second, I agree that adding more parameters to `sc.pp.neighbors` is not a good idea, so, at least for now, the least bad solution seems to add `sc.pp.neighbors_tsne`. This way, we can see what parameters are needed and not need to work around the existing implementation. That said, this is not a good solution, just not as bad as the other one. This gives clear preferential treatment to UMAP weights. I am still confused why the UMAP weights are the used for everything, including downstream clustering (e.g. `sc.pp.neighbors(...); sc.tl.leiden(...)`). I haven't been following single-cell literature as much lately, but from what I can tell, there's no evidence that shows this is better than anything else. From #1739, it seems that you are considering a change in the API, and I would definitely be in favour of that. As you add more and more functionality to scanpy, things are inevitably going to get more complicated, and patching the existing API will just lead to thousands of parameters. The API in #1739 seems like the logical next step. I'll try to work on this in the coming days/weeks, so we can see what's really necessary, and we can work out the exact API after we have a working prototype. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-801745797
https://github.com/scverse/scanpy/pull/1561#issuecomment-802136594:291,Deployability,pipeline,pipeline,291,Sounds great to me! Looking forward. Would be interesting to compare these three t-SNEs: ; ```; sc.pp.neighbors(); sc.tl.tsne(binarize=True); sc.tl.tsne(binarize=False); sc.pp.neighbors_tsne(); sc.tl.tsne(binarize=False); ```; on a couple of datasets after the standard scanpy preprocessing pipeline.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-802136594
https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944:222,Deployability,release,release,222,"@ivirshup @dkobak I've fixed up this PR, so now it implements what I mentioned in my comment above. I've left a couple of comments on the code, commenting on anything noteworthy. The tests and everything will fail until I release a new version of openTSNE, which I'll do in the coming days. But please look through the changes and let me know if there's anything you'd like me to change, so we can get this merged. Also, I haven't updated the docstrings at all. The most glaring thing is `neighbors_tsne`. Over 90% of the code here is identical to `neighbors`. Really, the only difference is that I changed the `n_neighbors` parameter to `perplexity`. But there was no elegant way to incorporate that into `neighbors`. I've also tried refactoring the duplicated code that saves the settings into `adata.uns`, but doingt that would also make the code pretty messy. Obviously, it's not a good idea to have duplicated code like this. What do you think would be the best way to handle this?. Functionally, this now works as agreed. Let me know how you want to proceed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944
https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944:431,Deployability,update,updated,431,"@ivirshup @dkobak I've fixed up this PR, so now it implements what I mentioned in my comment above. I've left a couple of comments on the code, commenting on anything noteworthy. The tests and everything will fail until I release a new version of openTSNE, which I'll do in the coming days. But please look through the changes and let me know if there's anything you'd like me to change, so we can get this merged. Also, I haven't updated the docstrings at all. The most glaring thing is `neighbors_tsne`. Over 90% of the code here is identical to `neighbors`. Really, the only difference is that I changed the `n_neighbors` parameter to `perplexity`. But there was no elegant way to incorporate that into `neighbors`. I've also tried refactoring the duplicated code that saves the settings into `adata.uns`, but doingt that would also make the code pretty messy. Obviously, it's not a good idea to have duplicated code like this. What do you think would be the best way to handle this?. Functionally, this now works as agreed. Let me know how you want to proceed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944
https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944:735,Modifiability,refactor,refactoring,735,"@ivirshup @dkobak I've fixed up this PR, so now it implements what I mentioned in my comment above. I've left a couple of comments on the code, commenting on anything noteworthy. The tests and everything will fail until I release a new version of openTSNE, which I'll do in the coming days. But please look through the changes and let me know if there's anything you'd like me to change, so we can get this merged. Also, I haven't updated the docstrings at all. The most glaring thing is `neighbors_tsne`. Over 90% of the code here is identical to `neighbors`. Really, the only difference is that I changed the `n_neighbors` parameter to `perplexity`. But there was no elegant way to incorporate that into `neighbors`. I've also tried refactoring the duplicated code that saves the settings into `adata.uns`, but doingt that would also make the code pretty messy. Obviously, it's not a good idea to have duplicated code like this. What do you think would be the best way to handle this?. Functionally, this now works as agreed. Let me know how you want to proceed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944
https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944:183,Testability,test,tests,183,"@ivirshup @dkobak I've fixed up this PR, so now it implements what I mentioned in my comment above. I've left a couple of comments on the code, commenting on anything noteworthy. The tests and everything will fail until I release a new version of openTSNE, which I'll do in the coming days. But please look through the changes and let me know if there's anything you'd like me to change, so we can get this merged. Also, I haven't updated the docstrings at all. The most glaring thing is `neighbors_tsne`. Over 90% of the code here is identical to `neighbors`. Really, the only difference is that I changed the `n_neighbors` parameter to `perplexity`. But there was no elegant way to incorporate that into `neighbors`. I've also tried refactoring the duplicated code that saves the settings into `adata.uns`, but doingt that would also make the code pretty messy. Obviously, it's not a good idea to have duplicated code like this. What do you think would be the best way to handle this?. Functionally, this now works as agreed. Let me know how you want to proceed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944
https://github.com/scverse/scanpy/pull/1561#issuecomment-822057782:76,Availability,error,error,76,"For some reason I don't see the figures here on the Github page (and get an error message when I click on the link), but they showed fine in the email notification I received. Looks good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822057782
https://github.com/scverse/scanpy/pull/1561#issuecomment-822057782:82,Integrability,message,message,82,"For some reason I don't see the figures here on the Github page (and get an error message when I click on the link), but they showed fine in the email notification I received. Looks good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822057782
https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389:242,Availability,error,error,242,"The problem is not the collapsible thing. I still don't see the images here but only links instead, and when I click on a link e.g. https://user-images.githubusercontent.com/5758119/115158730-f0768a00-a08f-11eb-939a-1b9c35373fae.png I get an error message that the image cannot be displayed because it contains errors. Odd. Maybe it's something weird on my side.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389
https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389:311,Availability,error,errors,311,"The problem is not the collapsible thing. I still don't see the images here but only links instead, and when I click on a link e.g. https://user-images.githubusercontent.com/5758119/115158730-f0768a00-a08f-11eb-939a-1b9c35373fae.png I get an error message that the image cannot be displayed because it contains errors. Odd. Maybe it's something weird on my side.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389
https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389:248,Integrability,message,message,248,"The problem is not the collapsible thing. I still don't see the images here but only links instead, and when I click on a link e.g. https://user-images.githubusercontent.com/5758119/115158730-f0768a00-a08f-11eb-939a-1b9c35373fae.png I get an error message that the image cannot be displayed because it contains errors. Odd. Maybe it's something weird on my side.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389
https://github.com/scverse/scanpy/pull/1561#issuecomment-871319496:58,Availability,ping,ping,58,Just got reminded of this PR... @ivirshup Should we maybe ping somebody else from the Scanpy team to join the discussion / review here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-871319496
https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079:391,Deployability,integrat,integration,391,"@ivirshup @giovp I'm wondering whether you've had the time to look over this. If this PR is maybe too big a change, then perhaps it would make more sense to migrate to openTSNE in a more iterative approach. For instance, we could just replace the t-SNE implementation to openTSNE, ignoring the API discussion and ignoring the precomputed graphs. I think switching to openTSNE, regardless of integration, would make the t-SNE implementation faster. We could then go for tighter integration step by step. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079
https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079:477,Deployability,integrat,integration,477,"@ivirshup @giovp I'm wondering whether you've had the time to look over this. If this PR is maybe too big a change, then perhaps it would make more sense to migrate to openTSNE in a more iterative approach. For instance, we could just replace the t-SNE implementation to openTSNE, ignoring the API discussion and ignoring the precomputed graphs. I think switching to openTSNE, regardless of integration, would make the t-SNE implementation faster. We could then go for tighter integration step by step. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079
https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079:391,Integrability,integrat,integration,391,"@ivirshup @giovp I'm wondering whether you've had the time to look over this. If this PR is maybe too big a change, then perhaps it would make more sense to migrate to openTSNE in a more iterative approach. For instance, we could just replace the t-SNE implementation to openTSNE, ignoring the API discussion and ignoring the precomputed graphs. I think switching to openTSNE, regardless of integration, would make the t-SNE implementation faster. We could then go for tighter integration step by step. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079
https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079:477,Integrability,integrat,integration,477,"@ivirshup @giovp I'm wondering whether you've had the time to look over this. If this PR is maybe too big a change, then perhaps it would make more sense to migrate to openTSNE in a more iterative approach. For instance, we could just replace the t-SNE implementation to openTSNE, ignoring the API discussion and ignoring the precomputed graphs. I think switching to openTSNE, regardless of integration, would make the t-SNE implementation faster. We could then go for tighter integration step by step. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079
https://github.com/scverse/scanpy/pull/1561#issuecomment-1288913427:0,Deployability,Update,Update,0,"Update here: I met @ivirshup at the SCG conference in Utrecht and we briefly chatted about this issue on the way to a pub. Isaac seemed supportive of the whole setup in this PR, which would allow to do this:. ``` Python; sc.pp.neighbors(); sc.tl.tsne() # default binarize='auto' resolves to binarize=True here. UMAP kNN graph but binary weights; sc.tl.tsne(binarize=False) # this would use UMAP weights but normalize to sum to 1; sc.pp.neighbors_tsne(); sc.tl.tsne() # default binarize='auto' resolves to binarize=False here; ```. As @pavlin-policar showed above, these three t-SNE calls yield very similar embeddings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-1288913427
https://github.com/scverse/scanpy/issues/1562#issuecomment-759557571:455,Deployability,update,update,455,"If I understand correctly, pandas > 1.2.0 `plot.add_totals(sort='descending')` returns a different sorting compared to previous version. However, most of the categories have the same number of cells and thus a different sorting is equally valid. ![image](https://user-images.githubusercontent.com/4964309/104478840-0a9d5980-55c3-11eb-94b0-b0e2092c55c3.png). My suggestion is to use other dataset in which the number of cells per category is different and update the image.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1562#issuecomment-759557571
https://github.com/scverse/scanpy/issues/1563#issuecomment-753945848:35,Usability,learn,learned,35,I like this idea a lot. I recently learned about automated linting and blacking of code per commit and have started using it for the single cell open problems project upon the suggestion of @scottgigante.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-753945848
https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509:127,Modifiability,variab,variables,127,"@ivirshup . I'd recommend these:; - `mypy` for type checking (I've caught a few bugs using it); - `autoflake` to remove unused variables/imports (by default removes only imports from Python's standard library); - `yesqa` to automatically remove unused `# noqa` for flake8; - `check-yaml` and `pretty-format-yaml` I also found useful; - `requirements-txt-fixer` to sort the reqs.; - `check-added-large-files` have prevented my from commiting some `.h5ad` files that shouldn't be there; - `rstcheck` to check the syntax of .rst files; - some flake8: `pytest-style` (enforces some pytest conventions, like non-returning fixtures beginning with `_`), `blind-except` (no blanket exceptions) and `comprehensions` (helps you rewrite comprehensions). Here's also an exhaustive list from which I picked the ones I use: https://pre-commit.com/hooks.html. As for any problems, some of them came from `rstcheck` as; `docs/source/classes.rst:9: (INFO/1) No directive entry for ""autoclass"" in module ""docutils.parsers.rst.languages.en"".`, that's why `.rstcheck.cfg` might be necessary. Also fixing types for `mypy` takes a while, I'd do it as last.; Also `flake8-comprehensions` forces you to abandon `dict(foo=..., bar=...)` calls, unless you disable it.; And `[tool.isort]` in `pyproject.toml` should have `profile = ""black""`. I had also planed to use `pylint` to enforce naming conventions, etc., but it has large overlap with `flake8` + the config file can be rather large, see https://gist.github.com/dkatz23238/08d79a76911ddaaa6171dff9cddd5772",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509
https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509:718,Modifiability,rewrite,rewrite,718,"@ivirshup . I'd recommend these:; - `mypy` for type checking (I've caught a few bugs using it); - `autoflake` to remove unused variables/imports (by default removes only imports from Python's standard library); - `yesqa` to automatically remove unused `# noqa` for flake8; - `check-yaml` and `pretty-format-yaml` I also found useful; - `requirements-txt-fixer` to sort the reqs.; - `check-added-large-files` have prevented my from commiting some `.h5ad` files that shouldn't be there; - `rstcheck` to check the syntax of .rst files; - some flake8: `pytest-style` (enforces some pytest conventions, like non-returning fixtures beginning with `_`), `blind-except` (no blanket exceptions) and `comprehensions` (helps you rewrite comprehensions). Here's also an exhaustive list from which I picked the ones I use: https://pre-commit.com/hooks.html. As for any problems, some of them came from `rstcheck` as; `docs/source/classes.rst:9: (INFO/1) No directive entry for ""autoclass"" in module ""docutils.parsers.rst.languages.en"".`, that's why `.rstcheck.cfg` might be necessary. Also fixing types for `mypy` takes a while, I'd do it as last.; Also `flake8-comprehensions` forces you to abandon `dict(foo=..., bar=...)` calls, unless you disable it.; And `[tool.isort]` in `pyproject.toml` should have `profile = ""black""`. I had also planed to use `pylint` to enforce naming conventions, etc., but it has large overlap with `flake8` + the config file can be rather large, see https://gist.github.com/dkatz23238/08d79a76911ddaaa6171dff9cddd5772",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509
https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509:1431,Modifiability,config,config,1431,"@ivirshup . I'd recommend these:; - `mypy` for type checking (I've caught a few bugs using it); - `autoflake` to remove unused variables/imports (by default removes only imports from Python's standard library); - `yesqa` to automatically remove unused `# noqa` for flake8; - `check-yaml` and `pretty-format-yaml` I also found useful; - `requirements-txt-fixer` to sort the reqs.; - `check-added-large-files` have prevented my from commiting some `.h5ad` files that shouldn't be there; - `rstcheck` to check the syntax of .rst files; - some flake8: `pytest-style` (enforces some pytest conventions, like non-returning fixtures beginning with `_`), `blind-except` (no blanket exceptions) and `comprehensions` (helps you rewrite comprehensions). Here's also an exhaustive list from which I picked the ones I use: https://pre-commit.com/hooks.html. As for any problems, some of them came from `rstcheck` as; `docs/source/classes.rst:9: (INFO/1) No directive entry for ""autoclass"" in module ""docutils.parsers.rst.languages.en"".`, that's why `.rstcheck.cfg` might be necessary. Also fixing types for `mypy` takes a while, I'd do it as last.; Also `flake8-comprehensions` forces you to abandon `dict(foo=..., bar=...)` calls, unless you disable it.; And `[tool.isort]` in `pyproject.toml` should have `profile = ""black""`. I had also planed to use `pylint` to enforce naming conventions, etc., but it has large overlap with `flake8` + the config file can be rather large, see https://gist.github.com/dkatz23238/08d79a76911ddaaa6171dff9cddd5772",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:229,Deployability,configurat,configurations,229,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:952,Deployability,release,release,952,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:1707,Deployability,install,install,1707," `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:722,Integrability,depend,dependencies,722,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:2219,Integrability,message,messages,2219," `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:185,Modifiability,config,configuring,185,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:229,Modifiability,config,configurations,229,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:1481,Modifiability,config,configuring,1481," Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your conce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:312,Performance,load,load,312,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:515,Testability,stub,stubs,515,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:173,Usability,learn,learn,173,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:1469,Usability,learn,learn,1469," Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your conce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635
https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096:338,Performance,load,load,338,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it.; yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096
https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096:698,Usability,simpl,simplified,698,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it.; yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096
https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096:745,Usability,guid,guide,745,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it.; yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096
https://github.com/scverse/scanpy/issues/1563#issuecomment-757907537:41,Modifiability,config,configured,41,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort!. Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757907537
https://github.com/scverse/scanpy/issues/1563#issuecomment-757907537:96,Testability,test,tests,96,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort!. Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757907537
https://github.com/scverse/scanpy/issues/1563#issuecomment-777223132:625,Availability,error,errors,625,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>; <summary> Some initial settings for `isort` </summary>. ```toml; [tool.isort]; profile = ""black""; multi_line_output = 3; skip = ""scanpy/__init__.py""; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-777223132
https://github.com/scverse/scanpy/issues/1563#issuecomment-784244194:103,Modifiability,config,config,103,"I just added `black`, `isort`, and `autopep8` to `scprep` which worked pretty seamlessly. ; pre-commit config: https://github.com/KrishnaswamyLab/scprep/blob/dev/.pre-commit-config.yaml; precommit github action: https://github.com/KrishnaswamyLab/scprep/blob/dev/.github/workflows/pre-commit.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784244194
https://github.com/scverse/scanpy/issues/1563#issuecomment-784244194:174,Modifiability,config,config,174,"I just added `black`, `isort`, and `autopep8` to `scprep` which worked pretty seamlessly. ; pre-commit config: https://github.com/KrishnaswamyLab/scprep/blob/dev/.pre-commit-config.yaml; precommit github action: https://github.com/KrishnaswamyLab/scprep/blob/dev/.github/workflows/pre-commit.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784244194
https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825:171,Deployability,install,install,171,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825
https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825:262,Deployability,configurat,configuration,262,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825
https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825:262,Modifiability,config,configuration,262,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825
https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825:253,Security,hash,hash,253,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825
https://github.com/scverse/scanpy/issues/1563#issuecomment-785092164:143,Availability,error,errors,143,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-785092164
https://github.com/scverse/scanpy/issues/1563#issuecomment-785092164:305,Testability,test,testing,305,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-785092164
https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136:297,Modifiability,config,configure,297,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136
https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136:80,Performance,queue,queued,80,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136
https://github.com/scverse/scanpy/issues/1563#issuecomment-787870905:19,Availability,down,down,19,Github actions are down. It seems like they have problems at least once a week: https://twitter.com/githubstatus. I'd be fine with having pre-commit be a required check. I'm a little antsy about having something that goes down frequently be required though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-787870905
https://github.com/scverse/scanpy/issues/1563#issuecomment-787870905:222,Availability,down,down,222,Github actions are down. It seems like they have problems at least once a week: https://twitter.com/githubstatus. I'd be fine with having pre-commit be a required check. I'm a little antsy about having something that goes down frequently be required though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-787870905
https://github.com/scverse/scanpy/issues/1563#issuecomment-842799143:365,Availability,down,down,365,"I'd be up for all of the numbered ones. IIRC, I had some issues with the trailing whitespace/ end of file fixers and some binary files/ csvs in the test suite. I'm a bit worried about false positives with `check-large-files`, but so long as it's easy to allow certain things (e.g. intentionally added test data) it should be fine. In terms of breaking these things down into small tasks/ PRs how about: (1), (2, 3), (4, 5)?. `prettier` looks a bit heavy and like it's targeting a lot of stuff we don't use, so you'd have to make a good case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-842799143
https://github.com/scverse/scanpy/issues/1563#issuecomment-842799143:148,Testability,test,test,148,"I'd be up for all of the numbered ones. IIRC, I had some issues with the trailing whitespace/ end of file fixers and some binary files/ csvs in the test suite. I'm a bit worried about false positives with `check-large-files`, but so long as it's easy to allow certain things (e.g. intentionally added test data) it should be fine. In terms of breaking these things down into small tasks/ PRs how about: (1), (2, 3), (4, 5)?. `prettier` looks a bit heavy and like it's targeting a lot of stuff we don't use, so you'd have to make a good case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-842799143
https://github.com/scverse/scanpy/issues/1563#issuecomment-842799143:301,Testability,test,test,301,"I'd be up for all of the numbered ones. IIRC, I had some issues with the trailing whitespace/ end of file fixers and some binary files/ csvs in the test suite. I'm a bit worried about false positives with `check-large-files`, but so long as it's easy to allow certain things (e.g. intentionally added test data) it should be fine. In terms of breaking these things down into small tasks/ PRs how about: (1), (2, 3), (4, 5)?. `prettier` looks a bit heavy and like it's targeting a lot of stuff we don't use, so you'd have to make a good case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-842799143
https://github.com/scverse/scanpy/pull/1564#issuecomment-754438743:77,Testability,test,test,77,"I think I'll leave codecov to a separate PR, since this immediately improves test reporting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564#issuecomment-754438743
https://github.com/scverse/scanpy/issues/1566#issuecomment-753946626:55,Deployability,install,installed,55,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566#issuecomment-753946626
https://github.com/scverse/scanpy/issues/1566#issuecomment-753946626:208,Deployability,install,install,208,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566#issuecomment-753946626
https://github.com/scverse/scanpy/issues/1566#issuecomment-753946626:123,Testability,log,logging,123,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566#issuecomment-753946626
https://github.com/scverse/scanpy/issues/1566#issuecomment-753962579:2,Deployability,install,installed,2,I installed again louvain with this command (although i already tried this command 3 times) and it work for me now. Thank you very much for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566#issuecomment-753962579
https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:227,Availability,failure,failure,227,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843
https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:183,Integrability,message,message,183,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843
https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:235,Integrability,message,message,235,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843
https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:330,Usability,learn,learn,330,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843
https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053:138,Availability,failure,failure,138,It causes exactly the same issue when I run:. ```; import numpy as np; import umap; ```; There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053
https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053:110,Integrability,message,message,110,It causes exactly the same issue when I run:. ```; import numpy as np; import umap; ```; There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053
https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053:146,Integrability,message,message,146,It causes exactly the same issue when I run:. ```; import numpy as np; import umap; ```; There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053
https://github.com/scverse/scanpy/issues/1567#issuecomment-755097946:158,Availability,error,error,158,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh; $ python3 -c ""import numpy; import umap""; ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to; * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-755097946
https://github.com/scverse/scanpy/issues/1567#issuecomment-755097946:383,Availability,fault,fault,383,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh; $ python3 -c ""import numpy; import umap""; ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to; * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-755097946
https://github.com/scverse/scanpy/issues/1567#issuecomment-755220002:30,Testability,log,logs,30,sometimes the jupyter console logs give you an idea of what is happening as well. I've had this happen when you run out of memory.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-755220002
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:167,Deployability,upgrade,upgrade,167,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:257,Deployability,install,install,257,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:267,Deployability,upgrade,upgrade,267,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:286,Deployability,install,install,286,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:296,Deployability,upgrade,upgrade,296,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:384,Deployability,install,installation,384,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:418,Deployability,install,install,418,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:481,Deployability,install,install,481,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:533,Deployability,install,install,533,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:658,Deployability,install,install,658,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:764,Deployability,install,installations,764,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:309,Usability,learn,learn,309,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500:441,Usability,learn,learn,441,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; ```bash; pip install --upgrade numba; pip install --upgrade umap-learn; ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy; ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash; pip install numpy==1.20; ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; ```bash; python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968181500
https://github.com/scverse/scanpy/issues/1567#issuecomment-968903708:104,Deployability,install,installation,104,"I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-968903708
https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:1040,Availability,error,error,1040,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927
https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:347,Deployability,update,updated,347,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927
https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:436,Modifiability,config,config,436,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927
https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:757,Modifiability,layers,layers,757,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927
https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:558,Usability,learn,learn,558,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:1167,Availability,error,errors,1167,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:1379,Availability,fault,fault,1379,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:176,Deployability,upgrade,upgrade,176,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:275,Deployability,install,install,275,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:285,Deployability,upgrade,upgrade,285,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:306,Deployability,install,install,306,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:316,Deployability,upgrade,upgrade,316,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:412,Deployability,install,installation,412,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:456,Deployability,install,install,456,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:521,Deployability,install,install,521,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:575,Deployability,install,install,575,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:717,Deployability,install,install,717,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:831,Deployability,install,installations,831,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:1238,Integrability,message,messagestream,1238,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:1252,Integrability,Message,MessageStream,1252,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:329,Usability,learn,learn,329,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:479,Usability,learn,learn,479,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606
https://github.com/scverse/scanpy/issues/1567#issuecomment-1063371834:106,Deployability,install,installation,106,"> I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state. Creating a new environment and reinstalling everything worked for me. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063371834
https://github.com/scverse/scanpy/issues/1567#issuecomment-1211520944:5,Deployability,install,install,5,`pip install -U pynndescent`; This fix my problems; I hope it's able to help others!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1211520944
https://github.com/scverse/scanpy/issues/1567#issuecomment-1364481075:6,Deployability,install,install,6,conda install -c conda-forge pynndescent. This also fixed my problem. Thanks @FlyPythons for the hint.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1364481075
https://github.com/scverse/scanpy/issues/1567#issuecomment-1592461176:8,Deployability,install,install,8,> conda install -c conda-forge pynndescent; > ; > This also fixed my problem. Thanks @FlyPythons for the hint. Thanks!This also work for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1592461176
https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730:177,Deployability,integrat,integration,177,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy?. In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730
https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730:177,Integrability,integrat,integration,177,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy?. In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730
https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730:189,Testability,benchmark,benchmarking,189,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy?. In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730
https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191:210,Deployability,integrat,integration,210,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191
https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191:210,Integrability,integrat,integration,210,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191
https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191:194,Performance,perform,performing,194,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191
https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093:148,Deployability,integrat,integrate,148,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093
https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093:148,Integrability,integrat,integrate,148,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093
https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093:218,Testability,test,tests,218,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093
https://github.com/scverse/scanpy/issues/1574#issuecomment-757802073:53,Deployability,update,updated,53,"Hi @vitkl !; that's a great idea yes, should be kept updated with most recent methods. ; Unfortuantely I don't have capacity now, do you mind opening an issue in https://github.com/theislab/scanpy-tutorials to keep as reminder? I will close this but of of course you could reference this in the tutorial repo.; Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574#issuecomment-757802073
https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276:23,Availability,avail,available,23,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276
https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276:376,Availability,avail,available,376,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276
https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276:102,Deployability,pipeline,pipelines,102,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276
https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276:281,Deployability,Pipeline,Pipelines,281,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276
https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276:112,Testability,test,test,112,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576#issuecomment-758366276
https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430:31,Availability,down,downstream,31,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):; 1. Using fewer genes is computationally less expensive for downstream analysis.; 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430
https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430:165,Availability,down,downstream,165,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):; 1. Using fewer genes is computationally less expensive for downstream analysis.; 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430
https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430:238,Modifiability,variab,variable,238,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):; 1. Using fewer genes is computationally less expensive for downstream analysis.; 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430
https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:670,Deployability,integrat,integration,670,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020
https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:717,Deployability,integrat,integrated,717,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020
https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:670,Integrability,integrat,integration,670,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020
https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:717,Integrability,integrat,integrated,717,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020
https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:307,Modifiability,variab,variable,307,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020
https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:586,Testability,log,log,586,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020
https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023:81,Deployability,integrat,integration,81,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023
https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023:81,Integrability,integrat,integration,81,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023
https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023:319,Modifiability,variab,variable,319,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023
https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:19,Deployability,update,updated,19,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701
https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:64,Integrability,interface,interface,64,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701
https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:12,Usability,learn,learn,12,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701
https://github.com/scverse/scanpy/issues/1579#issuecomment-758850929:82,Deployability,release,release,82,Looks like this was solved on master last month. Any chance we could get a bugfix release?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758850929
https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261:24,Deployability,release,release,24,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon.; I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261
https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261:114,Deployability,release,release,114,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon.; I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261
https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261:199,Testability,test,tests,199,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon.; I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760014261
https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475:18,Deployability,release,release,18,Thank you for the release update! I just had to; `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475
https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475:26,Deployability,update,update,26,Thank you for the release update! I just had to; `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475
https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475:54,Deployability,install,install,54,Thank you for the release update! I just had to; `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760095475
https://github.com/scverse/scanpy/issues/1579#issuecomment-760127465:59,Deployability,release,release,59,"I've just pushed 1.6.1 (pinning umap), but you can get the release candidate for 1.7.0 with `pip install ""scanpy==1.7.0rc1""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760127465
https://github.com/scverse/scanpy/issues/1579#issuecomment-760127465:97,Deployability,install,install,97,"I've just pushed 1.6.1 (pinning umap), but you can get the release candidate for 1.7.0 with `pip install ""scanpy==1.7.0rc1""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-760127465
https://github.com/scverse/scanpy/issues/1579#issuecomment-876093110:9,Availability,error,error,9,Has this error been fixed? Facing the same issue for scanpy version 1.7.2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-876093110
https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114:8,Availability,down,downgrading,8,I tried downgrading umap-learn to 0.4.6 but then sc.pp.neighbors won't work. I've been using scanpy 1.5.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114
https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114:25,Usability,learn,learn,25,I tried downgrading umap-learn to 0.4.6 but then sc.pp.neighbors won't work. I've been using scanpy 1.5.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909192114
https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091:60,Availability,down,downgraded,60,OK I computed the neighbors using umap-learn 0.5.1 and then downgraded to 0.4.6 for UMAP. Not elegant but so far so good.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091
https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091:39,Usability,learn,learn,39,OK I computed the neighbors using umap-learn 0.5.1 and then downgraded to 0.4.6 for UMAP. Not elegant but so far so good.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-909198091
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:100,Availability,error,error,100,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:; ```; mat_all = sc.read_loom(filename=""RSV.loom""); sc.pp.pca(mat_all); sc.pp.neighbors(mat_all); sc.tl.umap(mat_all); ```; The error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:234,Availability,error,error,234,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:; ```; mat_all = sc.read_loom(filename=""RSV.loom""); sc.pp.pca(mat_all); sc.pp.neighbors(mat_all); sc.tl.umap(mat_all); ```; The error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1732,Deployability,install,install,1732,"d, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460
