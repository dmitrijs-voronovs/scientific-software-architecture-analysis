id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:616,Availability,error,error,616,"@danielecook ; Ok; I try the command that you write it runs successfully; the updated command is; BIN_VERSION=""1.4.0""; nproc=8; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref= Homo_sapiens.GRCh38.dna.alt.fa\; --reads=data/hg005_gm26107.mrna.grch38.bam\; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed\; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir. the error ; ***** Running the command:*****; time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref """" --reads ""data/hg005_gm26107.mrna.grch38.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:1127,Availability,error,errors,1127,"nproc=8; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref= Homo_sapiens.GRCh38.dna.alt.fa\; --reads=data/hg005_gm26107.mrna.grch38.bam\; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed\; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir. the error ; ***** Running the command:*****; time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref """" --reads ""data/hg005_gm26107.mrna.grch38.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:1333,Availability,error,errors,1333,".dna.alt.fa\; --reads=data/hg005_gm26107.mrna.grch38.bam\; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed\; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir. the error ; ***** Running the command:*****; time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref """" --reads ""data/hg005_gm26107.mrna.grch38.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:1539,Availability,error,errors,1539,"true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir. the error ; ***** Running the command:*****; time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref """" --reads ""data/hg005_gm26107.mrna.grch38.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:56.718800 140108024964928 errors.py:61] ref argument is required.; I1104 15:05:58.405512 140044150212416 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:1745,Availability,error,errors,1745,"ples --mode calling --ref """" --reads ""data/hg005_gm26107.mrna.grch38.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:56.718800 140108024964928 errors.py:61] ref argument is required.; I1104 15:05:58.405512 140044150212416 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:58.408862 140044150212416 errors.py:61] ref argument is required. **(newenv) fci@fci-V530-15ICR:~$ echo $(pwd); /home/fci**; and I will attach screen of my home that ref file is found ; **https:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:1951,Availability,error,errors,1951,"les ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:56.718800 140108024964928 errors.py:61] ref argument is required.; I1104 15:05:58.405512 140044150212416 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:58.408862 140044150212416 errors.py:61] ref argument is required. **(newenv) fci@fci-V530-15ICR:~$ echo $(pwd); /home/fci**; and I will attach screen of my home that ref file is found ; **https://drive.google.com/file/d/1ztB19IhHsgxeUMBVzWhgjVwuPYZhvulV/view?usp=sharing**; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:2157,Availability,error,errors,2157,"les ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:56.718800 140108024964928 errors.py:61] ref argument is required.; I1104 15:05:58.405512 140044150212416 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:58.408862 140044150212416 errors.py:61] ref argument is required. **(newenv) fci@fci-V530-15ICR:~$ echo $(pwd); /home/fci**; and I will attach screen of my home that ref file is found ; **https://drive.google.com/file/d/1ztB19IhHsgxeUMBVzWhgjVwuPYZhvulV/view?usp=sharing**; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:2363,Availability,error,errors,2363,"les ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:56.718800 140108024964928 errors.py:61] ref argument is required.; I1104 15:05:58.405512 140044150212416 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:58.408862 140044150212416 errors.py:61] ref argument is required. **(newenv) fci@fci-V530-15ICR:~$ echo $(pwd); /home/fci**; and I will attach screen of my home that ref file is found ; **https://drive.google.com/file/d/1ztB19IhHsgxeUMBVzWhgjVwuPYZhvulV/view?usp=sharing**; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:2569,Availability,error,errors,2569,"les ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:56.718800 140108024964928 errors.py:61] ref argument is required.; I1104 15:05:58.405512 140044150212416 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:58.408862 140044150212416 errors.py:61] ref argument is required. **(newenv) fci@fci-V530-15ICR:~$ echo $(pwd); /home/fci**; and I will attach screen of my home that ref file is found ; **https://drive.google.com/file/d/1ztB19IhHsgxeUMBVzWhgjVwuPYZhvulV/view?usp=sharing**; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:2642,Availability,echo,echo,2642,"les ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:05:55.213266 139684413855552 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.217037 139684413855552 errors.py:61] ref argument is required.; I1104 15:05:56.714641 140108024964928 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:56.718800 140108024964928 errors.py:61] ref argument is required.; I1104 15:05:58.405512 140044150212416 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:58.408862 140044150212416 errors.py:61] ref argument is required. **(newenv) fci@fci-V530-15ICR:~$ echo $(pwd); /home/fci**; and I will attach screen of my home that ref file is found ; **https://drive.google.com/file/d/1ztB19IhHsgxeUMBVzWhgjVwuPYZhvulV/view?usp=sharing**; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303717562:78,Deployability,update,updated,78,"@danielecook ; Ok; I try the command that you write it runs successfully; the updated command is; BIN_VERSION=""1.4.0""; nproc=8; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref= Homo_sapiens.GRCh38.dna.alt.fa\; --reads=data/hg005_gm26107.mrna.grch38.bam\; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed\; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir. the error ; ***** Running the command:*****; time seq 0 7 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref """" --reads ""data/hg005_gm26107.mrna.grch38.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@8.gz"" --channels '' --regions ""data/chr20_CDS_3x.bed"" --split_skip_reads --task {}. I1104 15:05:38.471258 140090698385216 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:38.490578 140090698385216 errors.py:61] ref argument is required.; I1104 15:05:52.866427 139657534048064 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:52.881974 139657534048064 errors.py:61] ref argument is required.; I1104 15:05:55.227194 140033931474752 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.231749 140033931474752 errors.py:61] ref argument is required.; I1104 15:05:55.349858 140679315765056 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.355002 140679315765056 errors.py:61] ref argument is required.; I1104 15:05:55.350152 140625211275072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; E1104 15:05:55.364406 140625211275072 errors.py:61] ref argument is required.; I1104 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303717562
https://github.com/google/deepvariant/issues/581#issuecomment-1303724043:104,Availability,error,error,104,You have a space after the --ref flag. Be sure that is removed when you try running. That will cause an error.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303724043
https://github.com/google/deepvariant/issues/581#issuecomment-1303761079:3361,Testability,test,test,3361,"_deepvariant/deepvariant/make_examples_core.py"", line 472, in build_calling_regions; ranges.RangeSet.from_regions(regions_to_include, contig_dict)); File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions; for elt in reader(region):; File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 458, in bed_parser; with bed.BedReader(filename) as fin:; File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: OUT_OF_RANGE: EOF; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref GRCh38_no_alt_analysis_set.fasta --reads data/hg005_gm26107.mrna.grch38.bam --examples output/intermediate_results_dir/make_examples.tfrecord@8.gz --channels '' --regions data/chr20_CDS_3x.bed --split_skip_reads --task 3. real	0m9.092s; user	0m3.463s; sys	0m0.757s. I use the case study with all test files(https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) put in it and now I remove the space that you tell me about this , what is the problem ?; @danielecook",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1303761079
https://github.com/google/deepvariant/issues/581#issuecomment-1307848624:36,Availability,error,error,36,"@Rofidagamal I'm a bit stumped. The error suggests there is an issue with the bed file provided. ```; ValueError: OUT_OF_RANGE: EOF; ```. Maybe we can look a little closer at that and see if there is any sign of an issue there. Can you run:. ```; cut -f 1 data/chr20_CDS_3x.bed | wc; cut -f 1 data/chr20_CDS_3x.bed | uniq -c; ```. I wonder if it could also be related to the amount of space being allocated / avail. Can you also provide the output from the following:. ```; sudo docker run; -v ""$(pwd):$(pwd)""; -w $(pwd); google/deepvariant:""${BIN_VERSION}"" /bin/bash. # Once the image is running, run:; df -h; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1307848624
https://github.com/google/deepvariant/issues/581#issuecomment-1307848624:409,Availability,avail,avail,409,"@Rofidagamal I'm a bit stumped. The error suggests there is an issue with the bed file provided. ```; ValueError: OUT_OF_RANGE: EOF; ```. Maybe we can look a little closer at that and see if there is any sign of an issue there. Can you run:. ```; cut -f 1 data/chr20_CDS_3x.bed | wc; cut -f 1 data/chr20_CDS_3x.bed | uniq -c; ```. I wonder if it could also be related to the amount of space being allocated / avail. Can you also provide the output from the following:. ```; sudo docker run; -v ""$(pwd):$(pwd)""; -w $(pwd); google/deepvariant:""${BIN_VERSION}"" /bin/bash. # Once the image is running, run:; df -h; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1307848624
https://github.com/google/deepvariant/issues/581#issuecomment-1307848624:397,Energy Efficiency,allocate,allocated,397,"@Rofidagamal I'm a bit stumped. The error suggests there is an issue with the bed file provided. ```; ValueError: OUT_OF_RANGE: EOF; ```. Maybe we can look a little closer at that and see if there is any sign of an issue there. Can you run:. ```; cut -f 1 data/chr20_CDS_3x.bed | wc; cut -f 1 data/chr20_CDS_3x.bed | uniq -c; ```. I wonder if it could also be related to the amount of space being allocated / avail. Can you also provide the output from the following:. ```; sudo docker run; -v ""$(pwd):$(pwd)""; -w $(pwd); google/deepvariant:""${BIN_VERSION}"" /bin/bash. # Once the image is running, run:; df -h; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1307848624
https://github.com/google/deepvariant/issues/581#issuecomment-1317489201:76,Usability,feedback,feedback,76,"Great! Thank you for letting us know!; If you're interested in sharing more feedback to us, that will be great too. :); I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/581#issuecomment-1317489201
https://github.com/google/deepvariant/issues/583#issuecomment-1309166060:23,Testability,test,tested,23,"Hello Amy,. We haven't tested DeepVariant with HaloPlex data. WES model would be the best fit for this kind of data but there is no guaranty. Please let us know if you have any further questions. . Thanks; Alex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/583#issuecomment-1309166060
https://github.com/google/deepvariant/issues/584#issuecomment-1309177398:764,Energy Efficiency,reduce,reduce,764,"Hi @ASLeonard here is a table from the [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1). We stratify the F1-score across different region types. The published RNA-seq model is `DV RNA-seq [GTEx]`:. <img width=""795"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200906677-4b6e2f11-8b29-44e3-871e-299f46d1cd64.png"">. The model has no problem running genome wide, but accuracy will vary by region type due to the nature of RNA-seq data. We observe the highest accuracy in CDS regions which is why the case study is limited to these regions. Users should filter variants depending on their use case. This might mean filtering by region, but you can also consider filtering by genotype quality (or both). We show how you can reduce the false-discovery rate in figure 5 of the preprint by filtering on genotype quality:. <img width=""648"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200907763-1d21cc44-daff-47d2-87c6-e7917ea62a32.png"">. > Is that applicable with the RNA-seq model, or is that primarily trained on CDS/exome only?. The model is trained on exonic regions. We found this to give the best performance in our evaluations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/584#issuecomment-1309177398
https://github.com/google/deepvariant/issues/584#issuecomment-1309177398:611,Integrability,depend,depending,611,"Hi @ASLeonard here is a table from the [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1). We stratify the F1-score across different region types. The published RNA-seq model is `DV RNA-seq [GTEx]`:. <img width=""795"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200906677-4b6e2f11-8b29-44e3-871e-299f46d1cd64.png"">. The model has no problem running genome wide, but accuracy will vary by region type due to the nature of RNA-seq data. We observe the highest accuracy in CDS regions which is why the case study is limited to these regions. Users should filter variants depending on their use case. This might mean filtering by region, but you can also consider filtering by genotype quality (or both). We show how you can reduce the false-discovery rate in figure 5 of the preprint by filtering on genotype quality:. <img width=""648"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200907763-1d21cc44-daff-47d2-87c6-e7917ea62a32.png"">. > Is that applicable with the RNA-seq model, or is that primarily trained on CDS/exome only?. The model is trained on exonic regions. We found this to give the best performance in our evaluations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/584#issuecomment-1309177398
https://github.com/google/deepvariant/issues/584#issuecomment-1309177398:1162,Performance,perform,performance,1162,"Hi @ASLeonard here is a table from the [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1). We stratify the F1-score across different region types. The published RNA-seq model is `DV RNA-seq [GTEx]`:. <img width=""795"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200906677-4b6e2f11-8b29-44e3-871e-299f46d1cd64.png"">. The model has no problem running genome wide, but accuracy will vary by region type due to the nature of RNA-seq data. We observe the highest accuracy in CDS regions which is why the case study is limited to these regions. Users should filter variants depending on their use case. This might mean filtering by region, but you can also consider filtering by genotype quality (or both). We show how you can reduce the false-discovery rate in figure 5 of the preprint by filtering on genotype quality:. <img width=""648"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200907763-1d21cc44-daff-47d2-87c6-e7917ea62a32.png"">. > Is that applicable with the RNA-seq model, or is that primarily trained on CDS/exome only?. The model is trained on exonic regions. We found this to give the best performance in our evaluations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/584#issuecomment-1309177398
https://github.com/google/deepvariant/issues/586#issuecomment-1320619115:468,Deployability,release,releases,468,"Hi @JakeHagen , . My guess is that our model isn't as confident, because 100bp reads is not the main type of data our model is trained on. Glad to hear that the number of calls are expected though. Certainly interesting to see that the VCF report here. (Side note: Maybe we should consider attaching these reports as part of our documentations like [metrics.md](https://github.com/google/deepvariant/blob/r1.4/docs/metrics.md). I'll take a note to consider for future releases!). By the way, In the past (starting v1.2), we did try augmenting the training data by creating 100bp and 125bp reads, but we did so by trimming. See this document: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-details-training-data.md#vfootnote12; But it's possible that our model still didn't feel confident enough with your datatype. I'll also ask around on my team to see if anyone else has other thoughts. Thanks for reporting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1320619115
https://github.com/google/deepvariant/issues/586#issuecomment-1320629275:1321,Modifiability,variab,variability,1321,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1320629275
https://github.com/google/deepvariant/issues/586#issuecomment-1320629275:230,Performance,scalab,scalable,230,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1320629275
https://github.com/google/deepvariant/issues/586#issuecomment-1320638764:98,Testability,log,logging,98,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1320638764
https://github.com/google/deepvariant/issues/586#issuecomment-1331125414:36,Security,access,access,36,"Hi @JakeHagen . Although we do have access to some dbGaP datasets, I don't believe that this is one of them. Let me conduct some experiments from our benchmark data and see if I can replicate the effect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1331125414
https://github.com/google/deepvariant/issues/586#issuecomment-1331125414:150,Testability,benchmark,benchmark,150,"Hi @JakeHagen . Although we do have access to some dbGaP datasets, I don't believe that this is one of them. Let me conduct some experiments from our benchmark data and see if I can replicate the effect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1331125414
https://github.com/google/deepvariant/issues/586#issuecomment-1334570619:394,Usability,simpl,simply,394,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads; 2. last 100bp of the reads; 3. first 75bp of the reads; 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1334570619
https://github.com/google/deepvariant/issues/586#issuecomment-1341790338:159,Energy Efficiency,reduce,reduces,159,"Hi @JakeHagen . I will take a look at running a similar analysis on our exome samples. I suppose one remaining possibility is that the truncation of the reads reduces how far beyond the capture region the sequencing is getting. The edges of the capture region tend to both have less coverage and it's harder to sample both alleles. That's just a guess, I don't have a clear answer and will still try to collect more data. When you run DeepVariant for the exome, do you restrict to the capture regions only and do you add any padding to those?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1341790338
https://github.com/google/deepvariant/issues/586#issuecomment-1341790338:368,Usability,clear,clear,368,"Hi @JakeHagen . I will take a look at running a similar analysis on our exome samples. I suppose one remaining possibility is that the truncation of the reads reduces how far beyond the capture region the sequencing is getting. The edges of the capture region tend to both have less coverage and it's harder to sample both alleles. That's just a guess, I don't have a clear answer and will still try to collect more data. When you run DeepVariant for the exome, do you restrict to the capture regions only and do you add any padding to those?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1341790338
https://github.com/google/deepvariant/issues/586#issuecomment-1352251354:104,Availability,Down,Downloaded,104,"Hi @AndrewCarroll . I believe I have recreated this issue with HG002 from GIAB.; This is what I did:; - Downloaded ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam; - Converted the bam to fastq; - Made a 100bp and 75bp fastq; - Aligned all three fastqs (original-127bp, 100bp, 75bp) to https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz; - Marked dups etc; - Ran deep variant 1.4 on 100bp bam, 75bp bam, and original bam using WES model; - Use bed file consisting of regions from the 100bp bam with depth over 10 intersected with CDS regions.; ; [over10.cds.bed.txt](https://github.com/google/deepvariant/files/10231884/over10.cds.bed.txt). This is how the distributions look; Original-127bp; <img width=""786"" alt=""original_127bp"" src=""https://user-images.githubusercontent.com/8237552/207720422-ea64ddad-1290-4cc3-819d-4b9829e56981.png"">. 100bp; <img width=""786"" alt=""100bp"" src=""https://user-images.githubusercontent.com/8237552/207713090-c669cdbf-544c-4190-ac6b-6c091123c551.png"">. 75bp. <img width=""786"" alt=""75bp"" src=""https://user-images.githubusercontent.com/8237552/207713065-cdb71b28-8d69-45df-81e3-53b2f1d3aecf.png"">. This is what my command looked like; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /share/terra docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /share/terra/rsrc/hg38/ref/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz \; --reads ./HG002.proc.bam \; --output_vcf HG002.over10dp.vcf.gz \; --output_gvcf HG002.over10dp.gvcf.gz \; --num_shards 8 \; --intermediate_results_dir ./dv_int_results \; --regions ../100bp/over10.cds.bed; ```. Thanks again for looking into this",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1352251354
https://github.com/google/deepvariant/issues/586#issuecomment-1356675582:591,Deployability,release,release,591,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1356675582
https://github.com/google/deepvariant/issues/586#issuecomment-1356675582:525,Security,validat,validate,525,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1356675582
https://github.com/google/deepvariant/issues/586#issuecomment-1457417030:87,Deployability,release,releases,87,"Hi @JakeHagen ,; this problem should be fixed in https://github.com/google/deepvariant/releases/tag/v1.5.0. And, starting in this release, we added the VCF stats plots to https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md and https://github.com/google/deepvariant/blob/r1.5/docs/metrics-deeptrio.md. We're glad to see that @MariaNattestad 's VCF stats tool was useful for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1457417030
https://github.com/google/deepvariant/issues/586#issuecomment-1457417030:130,Deployability,release,release,130,"Hi @JakeHagen ,; this problem should be fixed in https://github.com/google/deepvariant/releases/tag/v1.5.0. And, starting in this release, we added the VCF stats plots to https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md and https://github.com/google/deepvariant/blob/r1.5/docs/metrics-deeptrio.md. We're glad to see that @MariaNattestad 's VCF stats tool was useful for you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/586#issuecomment-1457417030
https://github.com/google/deepvariant/issues/587#issuecomment-1319519059:376,Availability,down,down,376,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
https://github.com/google/deepvariant/issues/587#issuecomment-1319519059:581,Availability,robust,robustly,581,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
https://github.com/google/deepvariant/issues/587#issuecomment-1319519059:66,Deployability,pipeline,pipeline,66,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
https://github.com/google/deepvariant/issues/587#issuecomment-1319519059:560,Safety,detect,detect,560,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
https://github.com/google/deepvariant/issues/587#issuecomment-1319519059:125,Usability,resume,resume,125,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
https://github.com/google/deepvariant/issues/587#issuecomment-1319519059:511,Usability,resume,resume,511,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059
https://github.com/google/deepvariant/issues/588#issuecomment-1321390027:16,Availability,error,error,16,"@zivlang ,. The error says: `ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram`. Can you please do `ls /input/1115492_23181_0_0.cram` and see if the file exists in this file path?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588#issuecomment-1321390027
https://github.com/google/deepvariant/issues/588#issuecomment-1323439639:45,Availability,error,error,45,"Thanks. That helped. Now I'm facing the next error:; For every header in the reference file it prints; > ""header name"" is n bp and IS MISSING,. and then; > Please make sure the --ref input matches the build used for the input in --reads. That means that the reference genome that was used for creating the input I used in the variant calling pipe line command is different than the reference genome I used in that command? in which case, is there a way to find which reference genome was used for the creation of the input I used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/588#issuecomment-1323439639
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1086,Availability,Down,Download,1086,"ry Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1477,Availability,Down,Download,1477,"u"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:3536,Availability,mainten,maintenance,3536,"r20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. ```; python bin/make_examples.zip \; --mode calling \; --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; --channels ""insert_size""; ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step.; If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:268,Deployability,install,install,268,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1336,Deployability,install,install,1336,"ries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:435,Testability,test,test,435,"Before you proceed, if you can't use Docker because of root permission, I recommend that you try Singularity: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity. If you don't have root permission, you won't be able to install necessary things before running the binaries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1486,Testability,test,test,1486,"u"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1532,Testability,test,testdata,1532,"rm"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. ```; python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1612,Testability,test,testdata,1612,"d"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. ```; python bin/make_examples.zip \; --mode calling \; --ref ""${INPUT_DIR}/ucsc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:3227,Testability,test,tested,3227,"r20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. ```; python bin/make_examples.zip \; --mode calling \; --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; --channels ""insert_size""; ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step.; If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:3797,Testability,test,test,3797,"r20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. ```; python bin/make_examples.zip \; --mode calling \; --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; --channels ""insert_size""; ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step.; If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:1267,Availability,Down,Download,1267,"Singularity:; > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity; >; > If you don't have root permission, you won't be able to install necessary; > things before running the binaries either.; > ------------------------------; >; > Here is what I did:; >; > Get a machine. (Not required to run on GCP. I just use this to get a; > machine to test); >; > gcloud compute instances create ""${USER}-cpu"" --scopes; > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts""; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; > wget -P ${INPUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:1675,Availability,Down,Download,1675,"; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:3809,Availability,mainten,maintenance,3809,"TTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > mkdir -p ""${OUTPUT_DIR}""; >; > python bin/make_examples.zip \; > --mode calling \; > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; > --channels ""insert_size""; >; > (To figure out which flags you need to add for each model, you can read; > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253; > . Sorry that we don't have better documentation than that right now); >; > For how to run this with multiple shards, and how to run the rest of the; > commands, please read; > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md; >; > I just tested the steps above and confirmed that it worked for me on; > v1.4.0, at least for the make_examples step.; > If you encounter more issues with other steps, please feel free to ask; > again. I'd be happy to help.; >; > Note that I don't plan to put this into an official documentation page; > now, because that adds to our maintenance burden to keep it up to date.; > Given that we have the Docker/Singularity solution that works generally; > well for our users, I don't expect many of our users to need to use; > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for; > your question so I have a chance to test it again and document it here.; > Hopefully this is helpful for you. Happy to answer more questions if you; > encounter more problems.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:365,Deployability,install,install,365,"Thanks!.   , 21  2022, 23:50,  Pi-Chuan Chang <; ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I; > recommend that you try Singularity:; > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity; >; > If you don't have root permission, you won't be able to install necessary; > things before running the binaries either.; > ------------------------------; >; > Here is what I did:; >; > Get a machine. (Not required to run on GCP. I just use this to get a; > machine to test); >; > gcloud compute instances create ""${USER}-cpu"" --scopes; > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts""; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:1522,Deployability,install,install,1522,"machine. (Not required to run on GCP. I just use this to get a; > machine to test); >; > gcloud compute instances create ""${USER}-cpu"" --scopes; > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts""; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; > wget -P ${I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:4571,Integrability,Message,Message,4571,"TTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > mkdir -p ""${OUTPUT_DIR}""; >; > python bin/make_examples.zip \; > --mode calling \; > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; > --channels ""insert_size""; >; > (To figure out which flags you need to add for each model, you can read; > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253; > . Sorry that we don't have better documentation than that right now); >; > For how to run this with multiple shards, and how to run the rest of the; > commands, please read; > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md; >; > I just tested the steps above and confirmed that it worked for me on; > v1.4.0, at least for the make_examples step.; > If you encounter more issues with other steps, please feel free to ask; > again. I'd be happy to help.; >; > Note that I don't plan to put this into an official documentation page; > now, because that adds to our maintenance burden to keep it up to date.; > Given that we have the Docker/Singularity solution that works generally; > well for our users, I don't expect many of our users to need to use; > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for; > your question so I have a chance to test it again and document it here.; > Hopefully this is helpful for you. Happy to answer more questions if you; > encounter more problems.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:578,Testability,test,test,578,"Thanks!.   , 21  2022, 23:50,  Pi-Chuan Chang <; ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I; > recommend that you try Singularity:; > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity; >; > If you don't have root permission, you won't be able to install necessary; > things before running the binaries either.; > ------------------------------; >; > Here is what I did:; >; > Get a machine. (Not required to run on GCP. I just use this to get a; > machine to test); >; > gcloud compute instances create ""${USER}-cpu"" --scopes; > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts""; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:1684,Testability,test,test,1684,"; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:1730,Testability,test,testdata,1730,"; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:1812,Testability,test,testdata,1812,"-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > mkdir -p ""${OUTPUT_DIR}""; >; > python bin/make_examples.zip \; > --mode calling \; > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.uni",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:3483,Testability,test,tested,3483,"} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > mkdir -p ""${OUTPUT_DIR}""; >; > python bin/make_examples.zip \; > --mode calling \; > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; > --channels ""insert_size""; >; > (To figure out which flags you need to add for each model, you can read; > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253; > . Sorry that we don't have better documentation than that right now); >; > For how to run this with multiple shards, and how to run the rest of the; > commands, please read; > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md; >; > I just tested the steps above and confirmed that it worked for me on; > v1.4.0, at least for the make_examples step.; > If you encounter more issues with other steps, please feel free to ask; > again. I'd be happy to help.; >; > Note that I don't plan to put this into an official documentation page; > now, because that adds to our maintenance burden to keep it up to date.; > Given that we have the Docker/Singularity solution that works generally; > well for our users, I don't expect many of our users to need to use; > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for; > your question so I have a chance to test it again and document it here.; > Hopefully this is helpful for you. Happy to answer more questions if you; > encounter more problems.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:4111,Testability,test,test,4111,"TTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > mkdir -p ""${OUTPUT_DIR}""; >; > python bin/make_examples.zip \; > --mode calling \; > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; > --channels ""insert_size""; >; > (To figure out which flags you need to add for each model, you can read; > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253; > . Sorry that we don't have better documentation than that right now); >; > For how to run this with multiple shards, and how to run the rest of the; > commands, please read; > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md; >; > I just tested the steps above and confirmed that it worked for me on; > v1.4.0, at least for the make_examples step.; > If you encounter more issues with other steps, please feel free to ask; > again. I'd be happy to help.; >; > Note that I don't plan to put this into an official documentation page; > now, because that adds to our maintenance burden to keep it up to date.; > Given that we have the Docker/Singularity solution that works generally; > well for our users, I don't expect many of our users to need to use; > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for; > your question so I have a chance to test it again and document it here.; > Hopefully this is helpful for you. Happy to answer more questions if you; > encounter more problems.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566
https://github.com/google/deepvariant/issues/590#issuecomment-1602332965:20,Energy Efficiency,efficient,efficient,20,@George-du The most efficient pre-built binaries would be the Docker/Singularity approach.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/590#issuecomment-1602332965
https://github.com/google/deepvariant/issues/591#issuecomment-1329920517:36,Deployability,install,install,36,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1329920517
https://github.com/google/deepvariant/issues/591#issuecomment-1329920517:126,Integrability,rout,route,126,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1329920517
https://github.com/google/deepvariant/issues/591#issuecomment-1329920517:193,Testability,test,test,193,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1329920517
https://github.com/google/deepvariant/issues/591#issuecomment-1329988277:38,Deployability,install,install,38,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work.; I need to debug some parts of this project to better understand it, so I have to build it from source.; Do you mean there is a route to build DV using Docker or Singularity?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277
https://github.com/google/deepvariant/issues/591#issuecomment-1329988277:133,Integrability,rout,route,133,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work.; I need to debug some parts of this project to better understand it, so I have to build it from source.; Do you mean there is a route to build DV using Docker or Singularity?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277
https://github.com/google/deepvariant/issues/591#issuecomment-1329988277:425,Integrability,rout,route,425,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work.; I need to debug some parts of this project to better understand it, so I have to build it from source.; Do you mean there is a route to build DV using Docker or Singularity?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277
https://github.com/google/deepvariant/issues/591#issuecomment-1329988277:200,Testability,test,test,200,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work.; I need to debug some parts of this project to better understand it, so I have to build it from source.; Do you mean there is a route to build DV using Docker or Singularity?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277
https://github.com/google/deepvariant/issues/591#issuecomment-1334703761:38,Deployability,install,install,38,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761
https://github.com/google/deepvariant/issues/591#issuecomment-1334703761:133,Integrability,rout,route,133,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761
https://github.com/google/deepvariant/issues/591#issuecomment-1334703761:392,Integrability,depend,depend,392,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761
https://github.com/google/deepvariant/issues/591#issuecomment-1334703761:200,Testability,test,test,200,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761
https://github.com/google/deepvariant/issues/591#issuecomment-1334703761:261,Usability,learn,learned,261,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761
https://github.com/google/deepvariant/issues/592#issuecomment-1332875716:2112,Safety,detect,detect,2112," GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1332875716
https://github.com/google/deepvariant/issues/592#issuecomment-1332875716:1909,Usability,learn,learning,1909," GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1332875716
https://github.com/google/deepvariant/issues/592#issuecomment-1334238783:58,Availability,down,download,58,"Hi @li1ba . Thank you for the BAM file, I've been able to download it and look at the region in IGV. To my eye, I can't see the reason DeepVariant is calling 0/1 based on the pileup I see. We're going to run a few experiments with this and see if we can identify either something DeepVariant sees that we don't or if this is highlighting some sort of edge case or bug in the code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1334238783
https://github.com/google/deepvariant/issues/592#issuecomment-1335960453:336,Usability,learn,learned,336,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```; chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0; ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. ; ```; chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1; ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1335960453
https://github.com/google/deepvariant/issues/592#issuecomment-1335960453:946,Usability,learn,learned,946,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```; chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0; ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. ; ```; chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1; ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1335960453
https://github.com/google/deepvariant/issues/592#issuecomment-1457415830:55,Deployability,release,release,55,"Hi @li1ba , this problem should be fixed in the latest release: https://github.com/google/deepvariant/releases/tag/v1.5.0. Let us know if it works. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1457415830
https://github.com/google/deepvariant/issues/592#issuecomment-1457415830:102,Deployability,release,releases,102,"Hi @li1ba , this problem should be fixed in the latest release: https://github.com/google/deepvariant/releases/tag/v1.5.0. Let us know if it works. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/592#issuecomment-1457415830
https://github.com/google/deepvariant/issues/593#issuecomment-1333821000:75,Availability,error,error,75,@sivianil it looks to me like an issue with the location of your data. The error is saying that the reference file does not exist. Can you double check the following:. 1. Did you define `${INPUT_DIR}`?; 2. Do you have the `ucsc.hg19.chr20.unittest.fasta` in the input directory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/593#issuecomment-1333821000
https://github.com/google/deepvariant/issues/596#issuecomment-1338619194:21,Availability,avail,available,21,nproc command is not available on your machine. You may replace $(nproc) with the number of cores on your computer.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/596#issuecomment-1338619194
https://github.com/google/deepvariant/issues/597#issuecomment-1350569462:286,Integrability,message,message,286,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350569462
https://github.com/google/deepvariant/issues/597#issuecomment-1350569462:260,Usability,clear,clear,260,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350569462
https://github.com/google/deepvariant/issues/597#issuecomment-1350769545:492,Integrability,message,message,492,"Thanks for the feedback, Im using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:; Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup.; In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag); Please let me know if it works after you remove the openvino flag. Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545
https://github.com/google/deepvariant/issues/597#issuecomment-1350769545:739,Integrability,Message,Message,739,"Thanks for the feedback, Im using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:; Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup.; In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag); Please let me know if it works after you remove the openvino flag. Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545
https://github.com/google/deepvariant/issues/597#issuecomment-1350769545:15,Usability,feedback,feedback,15,"Thanks for the feedback, Im using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:; Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup.; In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag); Please let me know if it works after you remove the openvino flag. Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545
https://github.com/google/deepvariant/issues/597#issuecomment-1350769545:466,Usability,clear,clear,466,"Thanks for the feedback, Im using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:; Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup.; In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag); Please let me know if it works after you remove the openvino flag. Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545
https://github.com/google/deepvariant/issues/597#issuecomment-1356038369:49,Deployability,update,update,49,I'll close this one for now. Please feel free to update and let me know if it worked or not.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1356038369
https://github.com/google/deepvariant/issues/597#issuecomment-1423319704:28,Deployability,update,update,28,"Hi @serverchief , ; a quick update for you:; In the next release, we'll make sure that if you accidentally run with `--call_variants_extra_args=use_openvino=true`, the program will exit earlier (before it starts make_examples) and warn you that openvino is no longer built in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1423319704
https://github.com/google/deepvariant/issues/597#issuecomment-1423319704:57,Deployability,release,release,57,"Hi @serverchief , ; a quick update for you:; In the next release, we'll make sure that if you accidentally run with `--call_variants_extra_args=use_openvino=true`, the program will exit earlier (before it starts make_examples) and warn you that openvino is no longer built in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/597#issuecomment-1423319704
https://github.com/google/deepvariant/issues/598#issuecomment-1354327575:150,Deployability,install,install,150,Hi @Zero-Sun ; `dv_make_examples.py` isn't a file that our GitHub repo provided. Can you tell me a bit more about what's in that file and how did you install DeepVariant?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1354327575
https://github.com/google/deepvariant/issues/598#issuecomment-1354787986:23,Deployability,install,install,23,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`.; ```; $ mamba search deepvariant; Name Version Build Channel; deepvariant 1.3.0 py36hf3e76ba_0 bioconda; deepvariant 1.4.0 py36hf3e76ba_0 bioconda; $ mamba install deepvariant; ```; When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site.; [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url); ```; $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/; $ls; call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh; call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip; deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1354787986
https://github.com/google/deepvariant/issues/598#issuecomment-1354787986:280,Deployability,install,install,280,"Sorry, I used mamba to install it. And then I get `dv_make_examples.py`,`dv_call_variants.py`,`dv_postprocess_variants.py`.; ```; $ mamba search deepvariant; Name Version Build Channel; deepvariant 1.3.0 py36hf3e76ba_0 bioconda; deepvariant 1.4.0 py36hf3e76ba_0 bioconda; $ mamba install deepvariant; ```; When I run `dv_make_examples.py`, this is the directory where my `python` executes `make_examples.zip`, the same directory as the file you provide at this site.; [https://console.cloud.google.com/storage/browser/deepvariant/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0](url); ```; $cd /path/deepvariant-1.4.0-0/binaries/DeepVariant/1.4.0/DeepVariant-1.4.0/; $ls; call_variants_keras.zip freeze_graph.zip model_eval.zip postprocess_variants.zip settings.sh; call_variants.zip licenses.zip model_train.zip run-prereq.sh show_examples.zip; deeptrio make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1354787986
https://github.com/google/deepvariant/issues/598#issuecomment-1356305359:186,Availability,error,error,186,"Thank you for your reply!; For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either.; As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet.; ```; File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1356305359
https://github.com/google/deepvariant/issues/598#issuecomment-1356305359:242,Usability,learn,learn,242,"Thank you for your reply!; For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either.; As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet.; ```; File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1356305359
https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:308,Availability,error,error,308,"Thank you for your reply!; 1My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260
https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:1160,Availability,error,error,1160,"Thank you for your reply!; 1My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260
https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:1503,Availability,ERROR,ERROR,1503,"Thank you for your reply!; 1My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260
https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:199,Deployability,install,install,199,"Thank you for your reply!; 1My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260
https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:446,Deployability,install,install,446,"Thank you for your reply!; 1My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260
https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:1267,Modifiability,sandbox,sandbox,1267,"Thank you for your reply!; 1My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260
https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:1210,Performance,cache,cached,1210,"Thank you for your reply!; 1My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260
https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:1333,Performance,cache,cache,1333,"Thank you for your reply!; 1My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260
https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:1267,Testability,sandbox,sandbox,1267,"Thank you for your reply!; 1My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260
https://github.com/google/deepvariant/issues/599#issuecomment-1360396137:69,Availability,error,error,69,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599#issuecomment-1360396137
https://github.com/google/deepvariant/issues/599#issuecomment-1360396137:75,Integrability,message,message,75,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/599#issuecomment-1360396137
https://github.com/google/deepvariant/issues/601#issuecomment-1363548041:64,Security,access,access,64,figured it out by adding more bind path for the files I need to access to,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/601#issuecomment-1363548041
https://github.com/google/deepvariant/issues/602#issuecomment-1381120099:270,Availability,error,error,270,"Hi kmarianski,. This line is suspicious. ; ```; Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory?. The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602#issuecomment-1381120099
https://github.com/google/deepvariant/issues/602#issuecomment-1381120099:283,Testability,log,log,283,"Hi kmarianski,. This line is suspicious. ; ```; Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; ```. Each DeepVariant job needs a separate `intermediate_directory`. Could you verify that each job uses different temp directory?. The error in the log comes from TensorFlow library. Unfortunately, we are unable to help with TensorFlow (which is a third party tool).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/602#issuecomment-1381120099
https://github.com/google/deepvariant/issues/606#issuecomment-1404771799:415,Availability,error,error,415,"Hello Andrew,; ; I got the VCF line from only DeepVariant generated file for this specific position. From the DeepVariant REF column have G and the ALT column have GCTCT,GCTCTCT. Two different alternate alleles of both insertion types. So I can say this comes under Multiallelic Insertion category. . From the truth VCF file, there is no line for this coordinate position. When I evaluated using hap.py, it returns error as I commented earlier in the above post. . Does the model consider both Multiallelic variant type and Biallelic or only Biallelic during evaluation? Is there any way I can filter the VCF lines having Multiallelic variant types?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1404771799
https://github.com/google/deepvariant/issues/606#issuecomment-1405862600:60,Availability,error,error,60,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same?. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1405862600
https://github.com/google/deepvariant/issues/606#issuecomment-1405862600:346,Availability,error,error,346,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same?. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1405862600
https://github.com/google/deepvariant/issues/606#issuecomment-1405862600:66,Integrability,message,message,66,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same?. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1405862600
https://github.com/google/deepvariant/issues/606#issuecomment-1406491035:149,Availability,error,error,149,"@sivianil if the BAM file and reference FASTA files are publicly sharable as well, can you share them?; It'll be good if we can try to reproduce the error. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1406491035
https://github.com/google/deepvariant/issues/606#issuecomment-1406588815:191,Deployability,release,releases,191,"Well, I generated the BAM file using bam and samtools from fastq files. The file is too large to share. I'll add the one drive link to access the file.; [https://1001genomes.org/data/GMI-MPI/releases/v3.1/pseudogenomes/fasta/pseudo801.fasta.gz](url); [https://unipotsdamde-my.sharepoint.com/:u:/g/personal/anil_kumar_boddapati_uni-potsdam_de/EYyX5a4xEqBBrH4aXAID8KcB9__Q3s34vU3cTfav0J-VrA?e=owCGZI](url). Best,; Anil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1406588815
https://github.com/google/deepvariant/issues/606#issuecomment-1406588815:135,Security,access,access,135,"Well, I generated the BAM file using bam and samtools from fastq files. The file is too large to share. I'll add the one drive link to access the file.; [https://1001genomes.org/data/GMI-MPI/releases/v3.1/pseudogenomes/fasta/pseudo801.fasta.gz](url); [https://unipotsdamde-my.sharepoint.com/:u:/g/personal/anil_kumar_boddapati_uni-potsdam_de/EYyX5a4xEqBBrH4aXAID8KcB9__Q3s34vU3cTfav0J-VrA?e=owCGZI](url). Best,; Anil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1406588815
https://github.com/google/deepvariant/issues/606#issuecomment-1407228342:677,Availability,error,error,677,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```; 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16; ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1407228342
https://github.com/google/deepvariant/issues/606#issuecomment-1407228342:748,Availability,error,error,748,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```; 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16; ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1407228342
https://github.com/google/deepvariant/issues/606#issuecomment-1407228342:1204,Availability,avail,available,1204,"Hi @sivianil . Thank you for the files. With them, I was able to get a better idea of the problem. The problem is that the file that you are providing as your truth set file (801_snp_short_indel_with_quality_reference.vcf.gz) has an entry on chr1 position 8669 which disagrees in the REF field with the file that you are providing as your reference (pseudo801.fasta.gz). According to *pseudo801.fasta.gz* the reference base is *G*. The line in *801_snp_short_indel_with_quality_reference.vcf.gz* has an entry on this field as:. ```; 1 8669 . C . 38 PASS DP=16 GT:GQ:DP 0|0:38:16; ```. According to this VCF, the reference base listed at this position is *C*. You will get this error with hap.py regardless the content of the DeepVariant file. This error does not relate to DeepVariant at all. If you want to do any comparison with 801_snp_short_indel_with_quality_reference.vcf.gz you will need to reconcile the difference between the reference and the contents of the VCF. . Does it really make sense to apply hap.py to this dataset? Hap.py was developed for comparison with highly curated truth datasets made with multiple technologies and/or corroborated by extensive pedigree information (e.g. as is available in Genome in a Bottle or Platinum Genomes). I'm not sure that the data present in the 1001 Arabidopsis Genomes projects would sufficiently allow such a comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1407228342
https://github.com/google/deepvariant/issues/606#issuecomment-1408682802:726,Availability,error,error,726,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,; Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1408682802
https://github.com/google/deepvariant/issues/606#issuecomment-1408682802:82,Deployability,pipeline,pipeline,82,"Hi Andrew,. The Gold standard vcf file you mentioned above is only based on SHORE pipeline. Yes, you were absolutely correct! The reference bases acc to fasta file and VCF file doesn't match. Excluding that, when I tried to run the model on subset of positions for chr1:100-249000 [chr1:250951-250951 disagree on what the reference bases should be! (A != T) ] , I can't get any true positives in the result. The result contains only FN, so the model didn't make call for single variant position which are present in Truth VCF file. Please look at the screenshot I attached. I will ask my supervisor, if there is any possibility to reconcile the difference b/w the reference and bases of the VCF. Thanks for finding the actual error. . Best,; Anil. <img width=""1505"" alt=""Screenshot 2023-01-30 at 15 04 23"" src=""https://user-images.githubusercontent.com/75676816/215498753-e4340ba2-0573-4b13-a10d-33fb593cfd9f.png"">",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/606#issuecomment-1408682802
https://github.com/google/deepvariant/issues/607#issuecomment-1412596619:21,Availability,error,errors,21,"Hello Saurabh,; What errors do you get? Could you paste the output?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1412596619
https://github.com/google/deepvariant/issues/607#issuecomment-1414165945:68,Deployability,update,update,68,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1414165945
https://github.com/google/deepvariant/issues/607#issuecomment-1414165945:98,Deployability,release,release,98,"Updating TF version might not be straightforward. We're planning to update to TF 2.11 in the next release, which should be out in the next few months.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1414165945
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:67,Availability,error,error-log,67,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:298,Availability,Error,Error,298,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:324,Availability,ERROR,ERROR,324,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:458,Availability,error,error,458,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:989,Availability,Down,Download,989,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:108,Deployability,upgrade,upgraded,108,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:131,Integrability,depend,dependencies,131,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:494,Performance,cache,cache,494,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:73,Testability,log,log,73,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:304,Testability,log,log,304,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705
https://github.com/google/deepvariant/issues/607#issuecomment-1457417936:33,Deployability,release,release,33,"Hi @SaurabhKalikar ,; The latest release (v1.5.0) is now using TensorFlow 2.11. We'll also update Nucleus in a few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1457417936
https://github.com/google/deepvariant/issues/607#issuecomment-1457417936:91,Deployability,update,update,91,"Hi @SaurabhKalikar ,; The latest release (v1.5.0) is now using TensorFlow 2.11. We'll also update Nucleus in a few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/607#issuecomment-1457417936
https://github.com/google/deepvariant/issues/609#issuecomment-1421504703:610,Deployability,release,released,610,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. ; DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). ; In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models.; If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703
https://github.com/google/deepvariant/issues/609#issuecomment-1421504703:216,Modifiability,extend,extended,216,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. ; DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). ; In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models.; If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703
https://github.com/google/deepvariant/issues/609#issuecomment-1421504703:570,Modifiability,extend,extend,570,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. ; DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). ; In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models.; If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703
https://github.com/google/deepvariant/issues/609#issuecomment-1421504703:340,Safety,predict,predict,340,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. ; DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). ; In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models.; If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703
https://github.com/google/deepvariant/issues/609#issuecomment-1421504703:362,Safety,predict,predict,362,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. ; DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). ; In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models.; If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703
https://github.com/google/deepvariant/issues/609#issuecomment-1423013212:505,Testability,benchmark,benchmarks,505,"Hi @karoliinas . As Pi-Chuan mentioned, it is (in theory) possible to train a quartet model, but it would take work. If you want to always have consistent calls between parents and child, the best thing is probably to either use parent calling from only 1 DeepTrio run, or to run DeepVariant instead on the full quartet. In the event that you do want to use DeepTrio on both trios as opposed to DeepVariant on the four, there are some things you can look at to increase sensitivity for de novos. From our benchmarks, DeepTrio has a higher overall accuracy for de novos, but is more conservative (lower sensitivity, higher specificity). You could potentially use the GQ/PL values for no-call sites that are potential de novos sites and rank these by confidence, setting your own threshold. I mention the above possibility as an option. It sounds like for your use case of having a quartet and wanting higher sensitivity, the most straightfoward approach is to just use DeepVariant for joint genotyping. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/609#issuecomment-1423013212
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:88,Availability,down,down,88,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1345,Availability,Down,Downloads,1345,"\; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1654,Availability,down,download,1654,"l-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. This worked for me. Check nump",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:4683,Availability,error,error,4683,"/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; python -c 'import numpy as np; print(np.version.version)'; ```; Which shows:; ```; 1.19.2; ```. It seems like my machine doesn't already have numpy, though:. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy; WARNING: Package(s) not found: numpy; ```; doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:; ```; pip3.8 install numpy==1.23.0; ```; (Because you mentioned your cluster has 1.23.0). Now this shows:. ```; [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy; Name: numpy; Version: 1.23.0; Summary: NumPy is the fundamental package for array computing with Python.; Home-page: https://www.numpy.org; Author: Travis E. Oliphant et al.; Author-email: None; License: BSD; Location: /home/pichuan/.local/lib/python3.8/site-packages; Requires: ; Required-by: ; ```. Then I re-ran:. ```; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. Which still seems to work fine for me. And, this actually shows 1.23.0 as well:; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; python -c 'import numpy as np; print(np.version.version)'; ```. ```; 1.23.0; ```. I also checked TensorFlow version:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; python -c 'import tensorflow; print(tensorflow.__version__)'; ```; This shows:; ```; 2.7.0; ```. @asherrar Let me know what you think I need to change to try to reproduce the error you saw. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:588,Deployability,Install,Install,588,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:625,Deployability,update,update,625,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:642,Deployability,install,install,642,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:706,Deployability,install,install,706,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:732,Deployability,install,install,732,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:938,Deployability,install,install,938,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1072,Deployability,Install,Install,1072,"o write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1107,Deployability,update,update,1107,"cloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1183,Deployability,install,install,1183,"cloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1645,Deployability,release,releases,1645,"l-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. This worked for me. Check nump",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1859,Deployability,install,install,1859,"on-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. This worked for me. Check numpy version:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; python -c 'import numpy as np; print(np.version.version)'; ```; Which ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:3063,Deployability,Install,Install,3063,"larity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. This worked for me. Check numpy version:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; python -c 'import numpy as np; print(np.version.version)'; ```; Which shows:; ```; 1.19.2; ```. It seems like my machine doesn't already have numpy, though:. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy; WARNING: Package(s) not found: numpy; ```; doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:; ```; pip3.8 install numpy==1.23.0; ```; (Because you mentioned your cluster has 1.23.0). Now this shows:. ```; [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy; Name: numpy; Version: 1.23.0; Summary: NumPy is the fundamental package for array computing with Python.; Home-page: https://www.numpy.org; Author: Travis E. Oliphant et al.; Author-email: None; License: BSD; Location: /home/pichuan/.local/lib/python3.8/site-packages; Requires: ; Required-by: ; ```. Then I re-ran:. ```; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/interm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:3129,Deployability,install,install,3129,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. This worked for me. Check numpy version:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; python -c 'import numpy as np; print(np.version.version)'; ```; Which shows:; ```; 1.19.2; ```. It seems like my machine doesn't already have numpy, though:. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy; WARNING: Package(s) not found: numpy; ```; doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:; ```; pip3.8 install numpy==1.23.0; ```; (Because you mentioned your cluster has 1.23.0). Now this shows:. ```; [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy; Name: numpy; Version: 1.23.0; Summary: NumPy is the fundamental package for array computing with Python.; Home-page: https://www.numpy.org; Author: Travis E. Oliphant et al.; Author-email: None; License: BSD; Location: /home/pichuan/.local/lib/python3.8/site-packages; Requires: ; Required-by: ; ```. Then I re-ran:. ```; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. Which still seems to wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:885,Modifiability,config,configure,885,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:904,Performance,optimiz,optimizations,904,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:179,Testability,test,test,179,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759
https://github.com/google/deepvariant/issues/611#issuecomment-1431715409:82,Availability,checkpoint,checkpoints,82,"Hi @adamnovak , ; I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611#issuecomment-1431715409
https://github.com/google/deepvariant/issues/611#issuecomment-1431715409:187,Availability,checkpoint,checkpoints,187,"Hi @adamnovak , ; I can confirm that our current code is designed to wait for new checkpoints, and only evaluate new ones. I understand that it'll be great retrospectively evaluate older checkpoints too. But we haven't experimented with that. I haven't got time to do so; but in the next few weeks (to a month) I'll likely need to re-run the training tutorial anyway. I'll try to find a time to see if it could be easy to tweak that behavior (but I don't know for sure). I'll keep this issue open and assigned to me for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611#issuecomment-1431715409
https://github.com/google/deepvariant/issues/611#issuecomment-1513714977:37,Availability,checkpoint,checkpoint,37,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611#issuecomment-1513714977
https://github.com/google/deepvariant/issues/611#issuecomment-1513714977:417,Availability,checkpoint,checkpoint,417,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611#issuecomment-1513714977
https://github.com/google/deepvariant/issues/611#issuecomment-1513714977:515,Availability,checkpoint,checkpoints,515,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611#issuecomment-1513714977
https://github.com/google/deepvariant/issues/611#issuecomment-1513714977:81,Deployability,update,updated,81,"@adamnovak,. Just try to copy an old checkpoint file as a new file so it gets an updated timestamp, since just quickly looking at the tensorflow source code it seems to just look for the latest file:. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_utils.py#L150-L178. https://github.com/tensorflow/tensorflow/blob/55d62330dd9197e69ff8f1f03981784184706b2a/tensorflow/python/checkpoint/checkpoint_management.py#L326-L363. It if complains then it would be easy to tweak the checkpoints for what tensorflow is looking for in that directory. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/611#issuecomment-1513714977
https://github.com/google/deepvariant/issues/613#issuecomment-1431702886:1117,Availability,error,error,1117," consult you.; First I verify that my host directory is successfully mounted to the container directory.`/mnt/QJref.fa /mnt/input.bam`; ```; singularity exec -B $TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt /dellfsqd2/ST_OCEAN/USER/sunzhilong/1_Software/dpv/deepvariant_1.4.0.sif bash; Singularity> cd / && ls; bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var; Singularity> cd mnt && ls; QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir; ```; Then I ran the following script.; ```; cat test0215.sh; WORK_DIR=/path1/4_Test/qingjiang/dpv; export TMPDIR=""$PWD/tmp_dir""; singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \; /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \; --num_shards=3 \; --model_type=PACBIO \; --ref=/mnt/QJref.fa \; --reads=/mnt/input.bam \; --output_vcf=/mnt/output.vcf.gz \; --output_gvcf=/mnt/output.g.vcf.gz \; --intermediate_results_dir /mnt/dpv \; ```; The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam; [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`.; The complete error information is as follows.; Sincerely look forward to your help! thank you; ```; sh test0215.sh; I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****; time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613#issuecomment-1431702886
https://github.com/google/deepvariant/issues/613#issuecomment-1431702886:1341,Availability,error,error,1341,"1_Software/dpv/deepvariant_1.4.0.sif bash; Singularity> cd / && ls; bin boot dellfsqd2 dev environment etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin singularity srv sys tmp usr var; Singularity> cd mnt && ls; QJref.fa input.bam QJref.fa.fai input.bam.bai tmp_dir; ```; Then I ran the following script.; ```; cat test0215.sh; WORK_DIR=/path1/4_Test/qingjiang/dpv; export TMPDIR=""$PWD/tmp_dir""; singularity run -B$TMPDIR:$TMPDIR,""${WORK_DIR}"":/mnt \; /path1/1_Software/dpv/deepvariant_1.4.0.sif /opt/deepvariant/bin/run_deepvariant \; --num_shards=3 \; --model_type=PACBIO \; --ref=/mnt/QJref.fa \; --reads=/mnt/input.bam \; --output_vcf=/mnt/output.vcf.gz \; --output_gvcf=/mnt/output.g.vcf.gz \; --intermediate_results_dir /mnt/dpv \; ```; The core error is `ValueError: NOT_FOUND: Could not open /mnt/input.bam; [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory`However, I have verified the existence of `/mnt/input.bam`.; The complete error information is as follows.; Sincerely look forward to your help! thank you; ```; sh test0215.sh; I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****; time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613#issuecomment-1431702886
https://github.com/google/deepvariant/issues/613#issuecomment-1431702886:2406,Deployability,install,installed,2406," help! thank you; ```; sh test0215.sh; I0216 00:56:07.446549 140582811191104 run_deepvariant.py:342] Re-using the directory for intermediate results in /mnt/dpv. ***** Intermediate results will be written to /mnt/dpv in docker. ****. ***** Running the command:*****; time seq 0 2 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory; Traceback (most recent call last):; File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613#issuecomment-1431702886
https://github.com/google/deepvariant/issues/613#issuecomment-1431702886:2690,Deployability,install,installed,2690,"rallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory; Traceback (most recent call last):; File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/613#issuecomment-1431702886
https://github.com/google/deepvariant/issues/614#issuecomment-1440666950:190,Availability,Down,Downsampling,190,"What is the coverage, approximately? DeepVariant's pileup images can only really fit ~100 reads at each locus, but if you have very high coverage that could cause it to use a lot of memory. Downsampling in that case to about 100X would help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614#issuecomment-1440666950
https://github.com/google/deepvariant/issues/614#issuecomment-1440694835:209,Availability,down,downsampling,209,"Thank you for the response. Using samtools coverage, I see that mean coverage are in ~130X for some chromosomes and 224X with mitochondria with one of the files, and I suspect that other files are similar. By downsampling, do you mean something like splitting fastq files (say 3 files with 1/3 of original using something like fastqsplitter)? I have never done this before, so an explicit answer would be greatly appreciated. Also, do you have suggestions on what to use to merge gvcf or vcf output?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614#issuecomment-1440694835
https://github.com/google/deepvariant/issues/614#issuecomment-1457774639:121,Usability,clear,clear,121,Closing this. DeepVariant ran 'successfully' after splitting fastq files - although the output was bizarre. It is pretty clear that using DeepVariant with older PacBio data is not worthwhile -- my impression is that older PacBio data is only useful for finding larger SVs and worth using only if there is nothing better (e.g. PacBio HiFi or ONT).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/614#issuecomment-1457774639
https://github.com/google/deepvariant/issues/616#issuecomment-1440512685:145,Availability,error,errors,145,"You're very welcome :); We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616#issuecomment-1440512685
https://github.com/google/deepvariant/issues/616#issuecomment-1440512685:190,Availability,error,errors,190,"You're very welcome :); We can speculate, but it might just be sort of random things happening on the edges of the regions due to low numbers of errors overall. If you look at the number of errors different between these runs, it's like 26 vs 24 false positives for indels. I wouldn't draw any conclusions of trends from such small numbers. If you're curious you could inspect them in IGV though. I don't think we have any more insight into this from the DeepVariant side than you do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616#issuecomment-1440512685
https://github.com/google/deepvariant/issues/616#issuecomment-1444172404:82,Availability,error,errors,82,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616#issuecomment-1444172404
https://github.com/google/deepvariant/issues/616#issuecomment-1444172404:563,Integrability,depend,depends,563,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/616#issuecomment-1444172404
https://github.com/google/deepvariant/issues/617#issuecomment-1450589919:169,Testability,test,tested,169,"Hi @WenyuLiang ,. We have used the standard ONT mapping command to map Duplex reads to fastq:; ```; minimap2 -k 17 -ax map-ont -t <CPUS> <REF> <FASTQ>; ```; We have not tested other mapping modes for R10.4 to see if it improves overall mapping. If that is something you explore on your end and find improvements, please let us know by opening a github issue or reaching out over an email as we have not explored this extensively.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617#issuecomment-1450589919
https://github.com/google/deepvariant/issues/617#issuecomment-1470887594:66,Deployability,update,update,66,"Hi @WenyuLiang ,; I'll close this issue now. Feel free to give us update here if you have found anything that might be useful to share with other users too. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/617#issuecomment-1470887594
https://github.com/google/deepvariant/issues/618#issuecomment-1461845240:191,Safety,detect,detect,191,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C; - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):; - CAGCAGCGCT -> C; - CAGCAGCGCT -> T. Am I misunderstanding this?. Going back to the GT field, can it be that this behavior is changed in version 1.5.0?; Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:; Result of deepvariant version 1.2.0:; ```; NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61; ```; Result of deepvariant version 1.5.0:; ```; NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66; ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618#issuecomment-1461845240
https://github.com/google/deepvariant/issues/618#issuecomment-1461845240:1201,Safety,safe,safe,1201,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C; - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):; - CAGCAGCGCT -> C; - CAGCAGCGCT -> T. Am I misunderstanding this?. Going back to the GT field, can it be that this behavior is changed in version 1.5.0?; Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:; Result of deepvariant version 1.2.0:; ```; NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61; ```; Result of deepvariant version 1.5.0:; ```; NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66; ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618#issuecomment-1461845240
https://github.com/google/deepvariant/issues/618#issuecomment-1470986150:1322,Availability,down,downstream,1322,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C; CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618#issuecomment-1470986150
https://github.com/google/deepvariant/issues/618#issuecomment-1470986150:237,Usability,clear,clear,237,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C; CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/618#issuecomment-1470986150
https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:188,Availability,error,error,188,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851
https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:309,Availability,error,error,309,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851
https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:938,Availability,error,errors,938,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851
https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:1201,Availability,error,error,1201,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851
https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:1018,Modifiability,variab,variable,1018,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851
https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:504,Performance,optimiz,optimized,504,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851
https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:604,Performance,perform,performance-critical,604,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851
https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:273,Testability,test,test,273,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851
https://github.com/google/deepvariant/issues/619#issuecomment-1471268664:17,Deployability,install,installed,17,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664
https://github.com/google/deepvariant/issues/619#issuecomment-1471268664:67,Deployability,install,installed,67,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664
https://github.com/google/deepvariant/issues/619#issuecomment-1471268664:197,Deployability,install,installed,197,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664
https://github.com/google/deepvariant/issues/619#issuecomment-1471268664:276,Deployability,install,installation-guide-linux,276,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664
https://github.com/google/deepvariant/issues/619#issuecomment-1471268664:319,Deployability,install,install,319,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664
https://github.com/google/deepvariant/issues/619#issuecomment-1471268664:289,Usability,guid,guide-linux,289,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664
https://github.com/google/deepvariant/issues/619#issuecomment-1471272942:37,Deployability,install,installed,37,"Hi, @pichuan.; I am sure the CUDA is installed on the host. The CUDA version is V11.8.89.; ![image](https://user-images.githubusercontent.com/43125963/225511464-62c75283-b925-4b19-aea4-a75913ffd224.png)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471272942
https://github.com/google/deepvariant/issues/619#issuecomment-1471281516:181,Testability,test,test,181,I see. Right you mentioned that before.; It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471281516
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:135,Availability,mainten,maintenance-policy,135,"I got a machine to test:. ```; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I install nvidia driver first:. ```; sudo yum update -y && sudo yum install -y python3; curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py; sudo python3 install_gpu_driver.py; ```. After that, I can confirm that nvidia-smi exists:; ```; [pichuan@pichuan-gpu2 ~]$ nvidia-smi; Thu Mar 16 04:47:54 2023 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:1927,Availability,down,downloads,1927,".85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --version; singularity version 3.7.0; ```. The rest is similar to https://githu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:1969,Availability,down,download,1969,"-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --version; singularity version 3.7.0; ```. The rest is similar to https://github.com/google/deepvariant/issue",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:423,Deployability,install,install,423,"I got a machine to test:. ```; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I install nvidia driver first:. ```; sudo yum update -y && sudo yum install -y python3; curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py; sudo python3 install_gpu_driver.py; ```. After that, I can confirm that nvidia-smi exists:; ```; [pichuan@pichuan-gpu2 ~]$ nvidia-smi; Thu Mar 16 04:47:54 2023 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:467,Deployability,update,update,467,"I got a machine to test:. ```; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I install nvidia driver first:. ```; sudo yum update -y && sudo yum install -y python3; curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py; sudo python3 install_gpu_driver.py; ```. After that, I can confirm that nvidia-smi exists:; ```; [pichuan@pichuan-gpu2 ~]$ nvidia-smi; Thu Mar 16 04:47:54 2023 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:489,Deployability,install,install,489,"I got a machine to test:. ```; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I install nvidia driver first:. ```; sudo yum update -y && sudo yum install -y python3; curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py; sudo python3 install_gpu_driver.py; ```. After that, I can confirm that nvidia-smi exists:; ```; [pichuan@pichuan-gpu2 ~]$ nvidia-smi; Thu Mar 16 04:47:54 2023 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:580,Deployability,install,installation,580,"I got a machine to test:. ```; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I install nvidia driver first:. ```; sudo yum update -y && sudo yum install -y python3; curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py; sudo python3 install_gpu_driver.py; ```. After that, I can confirm that nvidia-smi exists:; ```; [pichuan@pichuan-gpu2 ~]$ nvidia-smi; Thu Mar 16 04:47:54 2023 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:1864,Deployability,install,install,1864,"--------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:2455,Deployability,release,release,2455,"0W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --version; singularity version 3.7.0; ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```; # Pull the image.; BIN_VERSION=1.5.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:2592,Deployability,Install,Install,2592,"CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --version; singularity version 3.7.0; ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```; # Pull the image.; BIN_VERSION=1.5.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:19,Testability,test,test,19,"I got a machine to test:. ```; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I install nvidia driver first:. ```; sudo yum update -y && sudo yum install -y python3; curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py; sudo python3 install_gpu_driver.py; ```. After that, I can confirm that nvidia-smi exists:; ```; [pichuan@pichuan-gpu2 ~]$ nvidia-smi; Thu Mar 16 04:47:54 2023 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:2578,Testability,test,test,2578,"Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --version; singularity version 3.7.0; ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```; # Pull the image.; BIN_VERSION=1.5.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553
https://github.com/google/deepvariant/issues/619#issuecomment-1471376570:210,Availability,error,error,210,"Actually, when I took a closer look at the logs, it says:. ```; 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2; ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471376570
https://github.com/google/deepvariant/issues/619#issuecomment-1471376570:1004,Deployability,release,release,1004,"Actually, when I took a closer look at the logs, it says:. ```; 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2; ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471376570
https://github.com/google/deepvariant/issues/619#issuecomment-1471376570:43,Testability,log,logs,43,"Actually, when I took a closer look at the logs, it says:. ```; 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2; ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471376570
https://github.com/google/deepvariant/issues/619#issuecomment-1471376570:967,Testability,test,test,967,"Actually, when I took a closer look at the logs, it says:. ```; 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2; ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471376570
https://github.com/google/deepvariant/issues/619#issuecomment-1471376570:990,Testability,test,tested,990,"Actually, when I took a closer look at the logs, it says:. ```; 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2; ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471376570
https://github.com/google/deepvariant/issues/619#issuecomment-1471376570:1030,Testability,test,test,1030,"Actually, when I took a closer look at the logs, it says:. ```; 2023-03-16 05:35:30.141288: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 05:35:30.141355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 05:35:30.141366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 05:35:30.141423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 05:35:30.141471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.30.2; ```. So it seems like I'm able to reproduce this issue. . Let me take a closer look. I'll also want to test this on Ubuntu. I tested before release, but I'll want to test it again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471376570
https://github.com/google/deepvariant/issues/619#issuecomment-1471378680:164,Testability,test,test,164,"I ran:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471378680
https://github.com/google/deepvariant/issues/619#issuecomment-1471378680:286,Testability,test,test,286,"I ran:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471378680
https://github.com/google/deepvariant/issues/619#issuecomment-1471410648:169,Availability,down,download-archive,169,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```; wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run; sudo sh cuda_11.3.0_465.19.01_linux.run; ```. ```; export PATH=/usr/local/cuda-11.3/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Now, with this, I tried:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648
https://github.com/google/deepvariant/issues/619#issuecomment-1471410648:231,Availability,down,download,231,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```; wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run; sudo sh cuda_11.3.0_465.19.01_linux.run; ```. ```; export PATH=/usr/local/cuda-11.3/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Now, with this, I tried:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648
https://github.com/google/deepvariant/issues/619#issuecomment-1471410648:1031,Availability,error,error,1031,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```; wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run; sudo sh cuda_11.3.0_465.19.01_linux.run; ```. ```; export PATH=/usr/local/cuda-11.3/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Now, with this, I tried:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648
https://github.com/google/deepvariant/issues/619#issuecomment-1471410648:17,Deployability,install,installing,17,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```; wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run; sudo sh cuda_11.3.0_465.19.01_linux.run; ```. ```; export PATH=/usr/local/cuda-11.3/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Now, with this, I tried:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648
https://github.com/google/deepvariant/issues/619#issuecomment-1471410648:697,Deployability,release,release,697,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```; wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run; sudo sh cuda_11.3.0_465.19.01_linux.run; ```. ```; export PATH=/usr/local/cuda-11.3/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Now, with this, I tried:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648
https://github.com/google/deepvariant/issues/619#issuecomment-1471410648:951,Testability,test,test,951,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```; wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run; sudo sh cuda_11.3.0_465.19.01_linux.run; ```. ```; export PATH=/usr/local/cuda-11.3/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Now, with this, I tried:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:400,Availability,down,download,400,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```; sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:1334,Availability,error,error,1334,"su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances reset --zone us-west1-b pichuan-gpu2; ```. and then ssh back to the machine. ```; BIN_VERSION=1.5.0; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:2033,Availability,reboot,rebooting,2033,"T_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances reset --zone us-west1-b pichuan-gpu2; ```. and then ssh back to the machine. ```; BIN_VERSION=1.5.0; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:179,Deployability,install,installed,179,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```; sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:222,Deployability,install,install,222,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```; sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:511,Deployability,install,install,511,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```; sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:869,Deployability,release,release,869,"I'm going to try out this:. https://stackoverflow.com/questions/67045622/tensorflow-stream-executor-cuda-cuda-driver-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```; sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:2049,Deployability,install,installing,2049,"T_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances reset --zone us-west1-b pichuan-gpu2; ```. and then ssh back to the machine. ```; BIN_VERSION=1.5.0; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:1114,Testability,test,test,1114,"-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```; sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:2367,Testability,test,test,2367,"T_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances reset --zone us-west1-b pichuan-gpu2; ```. and then ssh back to the machine. ```; BIN_VERSION=1.5.0; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:2727,Testability,test,test,2727,"T_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances reset --zone us-west1-b pichuan-gpu2; ```. and then ssh back to the machine. ```; BIN_VERSION=1.5.0; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355
https://github.com/google/deepvariant/issues/619#issuecomment-1471459575:286,Availability,reboot,rebooting,286,"Hi, @pichuan . I am trying to run with the followings command:; ```; singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu; ```; But it still doesn't work.; There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. ; ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. ; Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471459575
https://github.com/google/deepvariant/issues/619#issuecomment-1471459575:454,Availability,reboot,reboot,454,"Hi, @pichuan . I am trying to run with the followings command:; ```; singularity shell -B /usr/lib/locale/:/usr/lib/locale/ --nv ~/data/images/deepvariant_gpu; ```; But it still doesn't work.; There are two files in the `/usr/lib/locale/` folder, so I don't know whether it works after rebooting the machine. ; ![image](https://user-images.githubusercontent.com/43125963/225547562-bf86b5df-60cd-45d4-8d7c-a6ff986090f4.png). I will ask the HPC manager to reboot the machine. ; Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471459575
https://github.com/google/deepvariant/issues/619#issuecomment-1471488330:301,Modifiability,variab,variable,301,"Hi, @pgrosu. Thanks for your reply. ; I find a lot of lib files following your command:; ```; cd /usr; find | grep cuda | grep lib; ```; Here is the output file.; [find_cude_lib.txt](https://github.com/google/deepvariant/files/10988201/find_cude_lib.txt). Do you have any suggestion for me to set the variable before starting the deepvariant-gpu singularity program?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471488330
https://github.com/google/deepvariant/issues/619#issuecomment-1471943687:101,Performance,load,load,101,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:; `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471943687
https://github.com/google/deepvariant/issues/619#issuecomment-1471943687:238,Performance,load,load,238,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:; `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471943687
https://github.com/google/deepvariant/issues/619#issuecomment-1471943687:137,Safety,detect,detected,137,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:; `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/619#issuecomment-1471943687
https://github.com/google/deepvariant/issues/620#issuecomment-1470895246:295,Testability,test,test,295,"Hi @pamelameza . The contents of the INFO field are generally used by other calling methods to describe statistics about the variant position that are used in the process of variant calling (for example ""##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">""). . In DeepVariant, instead of directly calculating many of these values to use in a statistical model, the more raw read-level data is directly presented to a neural network and the neural network itself determines what properties in the raw data are useful for variant calling. So we don't populate the INFO field as we don't calculate many of those derived features. One thing that people often use the INFO field for is to filter variant calls by different properties. We've looked into the most effective ways to filter calls, and it is consistently the case that the sample level GQ field is well-calibrated to the probability of a call being correct. If you want to see some data for this, you can look at the calibration in Figure 2 of the DeepVariant paper (https://www.nature.com/articles/nbt.4235) or at the general work in our cohort calling paper (https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). Are there fields in particular you generally use within INFO? It's unlikely we could easily add them, but it might be useful for us to know. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/620#issuecomment-1470895246
https://github.com/google/deepvariant/issues/623#issuecomment-1482167397:64,Deployability,release,release,64,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623#issuecomment-1482167397
https://github.com/google/deepvariant/issues/623#issuecomment-1482167397:239,Deployability,release,releases,239,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623#issuecomment-1482167397
https://github.com/google/deepvariant/issues/623#issuecomment-1482167397:194,Performance,perform,performs,194,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623#issuecomment-1482167397
https://github.com/google/deepvariant/issues/623#issuecomment-1483276108:111,Availability,down,downloaded,111,"@crazysummerW ,. Your truth file looks empty, see the TRUTH.TOTAL is 0. Can you please make sure that you have downloaded the right file and it contains records in the VCF?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/623#issuecomment-1483276108
https://github.com/google/deepvariant/issues/624#issuecomment-1485499295:15,Deployability,release,release,15,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1485499295
https://github.com/google/deepvariant/issues/624#issuecomment-1485499295:82,Deployability,release,release,82,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1485499295
https://github.com/google/deepvariant/issues/624#issuecomment-1485499295:240,Testability,test,tested,240,"For the v1.5.0 release we did not train a new RNA-seq model and therefore did not release a new model. You can stick with v1.4.0 model+codebase to run the RNA-seq model or you can use this v1.4.0 model with the v1.5.0 codebase (we have not tested directly, but it should work). Please let me know if you run into any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1485499295
https://github.com/google/deepvariant/issues/624#issuecomment-1496232959:52,Usability,feedback,feedback,52,Excellent! Please let us know if you have any other feedback. I will close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1496232959
https://github.com/google/deepvariant/issues/624#issuecomment-1498623984:28,Performance,perform,performance,28,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1498623984
https://github.com/google/deepvariant/issues/624#issuecomment-1498623984:96,Performance,perform,performance,96,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1498623984
https://github.com/google/deepvariant/issues/624#issuecomment-1499324669:15,Deployability,update,update,15,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1499324669
https://github.com/google/deepvariant/issues/624#issuecomment-1499324669:120,Usability,feedback,feedback,120,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/624#issuecomment-1499324669
https://github.com/google/deepvariant/issues/626#issuecomment-1504023688:217,Deployability,release,release,217,"Hi @srbehera . The PrecisionFDA submission reflects a pre-DV1.0 code base. The most comparable version would be v1.0, but there are some code and data changes that occurred between the pFDA submission and the general release. There is one minor difference that could be partially relevant here, which is that I believe that the PrecisionFDA results were generated with --min_mapping_quality=1 where our default for DV1.0-1.5 is min_mapping_quality=5. We generally don't see a large accuracy improvement from that, though. The precisionFDA submission does not have additional filtering, and reflects the output of DeepVariant from the model as occurs with other versions. . One thing to note for fluctuations between precision and recall is that much of the difference can be exactly where on the ROC the model is chosen. If you would like higher recall, it may be possible to use the PL values to set a threshold below the no-call/RefCall threshold. Does this answer your question? Is there more information I can give which will be helpful?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/626#issuecomment-1504023688
https://github.com/google/deepvariant/issues/627#issuecomment-1512334269:471,Availability,error,error,471,"Hi, here is the output:; `; dv_call_variants.py -h; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_dwq0j2la/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 356; f'The file {input_shape_file} should contain 3 integers'); ^; SyntaxError: invalid syntax; `; If you check dv_make_examples.py and dv_postprocess_variants.py, you can find similar output.; I also tried to solve this syntax by replace f-strings to .format, and get another syntax error:; `Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_gxfcz7rb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 645; options: deepvariant_pb2.SampleOptions; ^; SyntaxError: invalid syntax; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1512334269
https://github.com/google/deepvariant/issues/627#issuecomment-1512358132:702,Availability,error,error,702,"@richard-nm Can you paste your command too?. I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape; 100 221 6; ```. In the later moment, we changed the format. For example, 1.4.0:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ```; (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1512358132
https://github.com/google/deepvariant/issues/627#issuecomment-1512358132:708,Integrability,message,messages,708,"@richard-nm Can you paste your command too?. I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape; 100 221 6; ```. In the later moment, we changed the format. For example, 1.4.0:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ```; (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1512358132
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:30,Availability,error,error,30,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:184,Availability,error,error,184,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:422,Availability,error,error,422,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:1266,Availability,error,error,1266,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:305,Deployability,install,install,305,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:36,Integrability,message,messages,36,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:190,Integrability,message,messages,190,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:428,Integrability,message,messages,428,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:675,Integrability,Wrap,Wrapper,675,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:859,Integrability,wrap,wrapper,859,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725
https://github.com/google/deepvariant/issues/627#issuecomment-1518213948:381,Availability,error,error,381,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:; 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all.; 2. Comment: Purely based on the error message you posted:; ```; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax; ```; The corresponding line would be this one:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374; I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:; ```; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; ```; gave me:; ```; UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5; ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948
https://github.com/google/deepvariant/issues/627#issuecomment-1518213948:32,Deployability,update,update,32,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:; 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all.; 2. Comment: Purely based on the error message you posted:; ```; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax; ```; The corresponding line would be this one:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374; I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:; ```; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; ```; gave me:; ```; UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5; ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948
https://github.com/google/deepvariant/issues/627#issuecomment-1518213948:1001,Deployability,install,installing,1001,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:; 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all.; 2. Comment: Purely based on the error message you posted:; ```; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax; ```; The corresponding line would be this one:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374; I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:; ```; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; ```; gave me:; ```; UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5; ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948
https://github.com/google/deepvariant/issues/627#issuecomment-1518213948:1067,Deployability,install,install,1067,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:; 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all.; 2. Comment: Purely based on the error message you posted:; ```; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax; ```; The corresponding line would be this one:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374; I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:; ```; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; ```; gave me:; ```; UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5; ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948
https://github.com/google/deepvariant/issues/627#issuecomment-1518213948:1231,Deployability,install,installation,1231,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:; 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all.; 2. Comment: Purely based on the error message you posted:; ```; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax; ```; The corresponding line would be this one:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374; I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:; ```; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; ```; gave me:; ```; UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5; ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948
https://github.com/google/deepvariant/issues/627#issuecomment-1518213948:387,Integrability,message,message,387,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:; 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all.; 2. Comment: Purely based on the error message you posted:; ```; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax; ```; The corresponding line would be this one:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374; I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:; ```; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; ```; gave me:; ```; UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5; ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948
https://github.com/google/deepvariant/issues/627#issuecomment-1518213948:892,Integrability,wrap,wrapped,892,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:; 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all.; 2. Comment: Purely based on the error message you posted:; ```; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax; ```; The corresponding line would be this one:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374; I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:; ```; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; ```; gave me:; ```; UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5; ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:99,Availability,error,errors,99,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:1219,Availability,avail,available,1219,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:678,Deployability,pipeline,pipeline,678,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:1182,Deployability,pipeline,pipeline,1182,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:535,Energy Efficiency,reduce,reduced,535,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:1058,Energy Efficiency,efficient,efficient,1058,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:763,Integrability,rout,routinely,763,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:587,Performance,perform,performance,587,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:773,Testability,benchmark,benchmark,773,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:841,Testability,benchmark,benchmark,841,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:1201,Testability,benchmark,benchmark,1201,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466
https://github.com/google/deepvariant/issues/630#issuecomment-1506060914:622,Security,access,access,622,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/630#issuecomment-1506060914
https://github.com/google/deepvariant/issues/631#issuecomment-1507742336:429,Availability,error,error,429,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631#issuecomment-1507742336
https://github.com/google/deepvariant/issues/631#issuecomment-1507742336:303,Deployability,continuous,continuous,303,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631#issuecomment-1507742336
https://github.com/google/deepvariant/issues/631#issuecomment-1507742336:23,Integrability,message,message,23,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631#issuecomment-1507742336
https://github.com/google/deepvariant/issues/631#issuecomment-1507742336:679,Testability,test,test,679,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/631#issuecomment-1507742336
https://github.com/google/deepvariant/issues/632#issuecomment-1512450759:953,Availability,error,error,953," now the ```rtg-tools format``` miraculously work using the following command. ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And this is the result . ```; Formatting FASTA data; Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data; Files : GRCh38_no_alt_analysis_set.fasta; Format : FASTA; Type : DNA; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422. Output Data; SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422; ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below; ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bcftools view - \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759
https://github.com/google/deepvariant/issues/632#issuecomment-1512450759:965,Availability,Error,Error,965," now the ```rtg-tools format``` miraculously work using the following command. ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And this is the result . ```; Formatting FASTA data; Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data; Files : GRCh38_no_alt_analysis_set.fasta; Format : FASTA; Type : DNA; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422. Output Data; SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422; ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below; ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bcftools view - \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759
https://github.com/google/deepvariant/issues/632#issuecomment-1512450759:0,Deployability,Update,Update,0,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And this is the result . ```; Formatting FASTA data; Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data; Files : GRCh38_no_alt_analysis_set.fasta; Format : FASTA; Type : DNA; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422. Output Data; SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422; ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below; ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bcftools view - \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bgzip -c > output/HG002_trio_merged",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759
https://github.com/google/deepvariant/issues/632#issuecomment-1512450759:467,Deployability,install,installing,467,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And this is the result . ```; Formatting FASTA data; Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data; Files : GRCh38_no_alt_analysis_set.fasta; Format : FASTA; Type : DNA; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422. Output Data; SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422; ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below; ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bcftools view - \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bgzip -c > output/HG002_trio_merged",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759
https://github.com/google/deepvariant/issues/632#issuecomment-1512450759:1700,Modifiability,config,config,1700," now the ```rtg-tools format``` miraculously work using the following command. ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And this is the result . ```; Formatting FASTA data; Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data; Files : GRCh38_no_alt_analysis_set.fasta; Format : FASTA; Type : DNA; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422. Output Data; SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422; ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below; ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bcftools view - \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759
https://github.com/google/deepvariant/issues/632#issuecomment-1512450759:424,Safety,Detect,Detected,424,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And this is the result . ```; Formatting FASTA data; Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data; Files : GRCh38_no_alt_analysis_set.fasta; Format : FASTA; Type : DNA; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422. Output Data; SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422; ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below; ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bcftools view - \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bgzip -c > output/HG002_trio_merged",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759
https://github.com/google/deepvariant/issues/632#issuecomment-1512478424:68,Availability,error,error,68,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512478424
https://github.com/google/deepvariant/issues/632#issuecomment-1512478424:26,Deployability,pipeline,pipeline,26,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512478424
https://github.com/google/deepvariant/issues/632#issuecomment-1512478424:75,Deployability,Install,Installing,75,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512478424
https://github.com/google/deepvariant/issues/632#issuecomment-1512478424:293,Modifiability,config,config,293,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1512478424
https://github.com/google/deepvariant/issues/632#issuecomment-1518194290:15,Deployability,update,update,15,Thanks for the update @ivanwilliammd !,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/632#issuecomment-1518194290
https://github.com/google/deepvariant/issues/633#issuecomment-1518235939:441,Availability,down,downstream,441,"Hello,. I think it'd be helpful if you can share individual gVCF calls for the problematic site before merging (hopefully with a fewer number of samples) so that we can try to reproduce it. As you said we'll have to ask GLnexus maintainers about this issue.; I found this issue on the GLnexus repo https://github.com/dnanexus-rnd/GLnexus/issues/286 - I'd recommend adding a reproducible example there as well. About the PASS filter, if your downstream application explicitly requires having that filter value, I'd recommend using tools like `bcftools` to add it. I think `bcftools annotate --rename-annots` would work based on this page, using `FILTER/. PASS` as the mapping: https://samtools.github.io/bcftools/bcftools.html#annotate. Thank you. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/633#issuecomment-1518235939
https://github.com/google/deepvariant/issues/634#issuecomment-1518574416:79,Availability,error,error,79,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. ; But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1518574416
https://github.com/google/deepvariant/issues/634#issuecomment-1518574416:7,Deployability,install,installed,7,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. ; But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1518574416
https://github.com/google/deepvariant/issues/634#issuecomment-1518574416:45,Deployability,install,install,45,I have installed chardet 3.0.4 by using 'pip install chardet==3.04'. ; But the error are still reported.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1518574416
https://github.com/google/deepvariant/issues/634#issuecomment-1518577444:23,Availability,error,error,23,"I have now solved this error.; Firstly, input within shell:; > singularity shell deepvariant_1.5.0.sif. Then,; > pip install --upgrade pip; > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1518577444
https://github.com/google/deepvariant/issues/634#issuecomment-1518577444:117,Deployability,install,install,117,"I have now solved this error.; Firstly, input within shell:; > singularity shell deepvariant_1.5.0.sif. Then,; > pip install --upgrade pip; > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1518577444
https://github.com/google/deepvariant/issues/634#issuecomment-1518577444:127,Deployability,upgrade,upgrade,127,"I have now solved this error.; Firstly, input within shell:; > singularity shell deepvariant_1.5.0.sif. Then,; > pip install --upgrade pip; > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1518577444
https://github.com/google/deepvariant/issues/634#issuecomment-1518577444:146,Deployability,install,install,146,"I have now solved this error.; Firstly, input within shell:; > singularity shell deepvariant_1.5.0.sif. Then,; > pip install --upgrade pip; > pip install chardet==3.04",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1518577444
https://github.com/google/deepvariant/issues/634#issuecomment-1595873180:137,Deployability,update,update,137,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1595873180
https://github.com/google/deepvariant/issues/634#issuecomment-1595873180:281,Deployability,install,install,281,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1595873180
https://github.com/google/deepvariant/issues/634#issuecomment-1595873180:337,Deployability,update,updated,337,"@CWYuan08 Basically `singularity shell` drops you with a shell prompt within your running container. The container will require that pip update. Here is a [video link](https://www.youtube.com/watch?v=97VuBVnfcwg) describing this process. Once inside the container just run the pip install and then run DeepVariant from there, to use the updated environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/634#issuecomment-1595873180
https://github.com/google/deepvariant/issues/635#issuecomment-1520135737:1061,Energy Efficiency,reduce,reduced,1061,"1. No - this should not be necessary. This operation is handled by DeepVariant.; 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`; - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads.; - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel.; 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635#issuecomment-1520135737
https://github.com/google/deepvariant/issues/635#issuecomment-1520135737:929,Integrability,depend,depending,929,"1. No - this should not be necessary. This operation is handled by DeepVariant.; 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`; - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads.; - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel.; 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635#issuecomment-1520135737
https://github.com/google/deepvariant/issues/635#issuecomment-1522115118:18,Availability,error,error,18,"@crazysummerW the error seems to suggest that the contigs in your bam file do not match the contigs in the reference you provided. Can you double check that `$star_fasta` is pointing to the same file as `hg19.fasta`, and that the contigs present in both are the same?. One possibility here could be that the `$star_fasta` provided here removed the `chr` prefix from the chromosomes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/635#issuecomment-1522115118
https://github.com/google/deepvariant/issues/636#issuecomment-1520586245:185,Modifiability,extend,extend,185,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636#issuecomment-1520586245
https://github.com/google/deepvariant/issues/636#issuecomment-1520586245:178,Usability,simpl,simply,178,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/636#issuecomment-1520586245
https://github.com/google/deepvariant/issues/638#issuecomment-1525033039:43,Testability,test,tested,43,hi; Thanks for your reply. I mean BQSR. I tested on WES data. The result : ; 1. deal with deduplication and BQSR; ![1682582003166](https://user-images.githubusercontent.com/70870741/234796615-a5ce10c7-da7b-4542-8717-b2cde03fc478.jpg); 2. only deal with deduplication:; ![1682582061403](https://user-images.githubusercontent.com/70870741/234796818-f7bb9a58-dc50-4a96-8848-6bd785b0b136.jpg). Can I think BQSR processed results are better?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638#issuecomment-1525033039
https://github.com/google/deepvariant/issues/638#issuecomment-1526875856:55,Energy Efficiency,reduce,reduce,55,"hi; I used DeepVariant1.4 and GATK.4.2.0.0. I want to reduce the running time of the process overall. So I want to confirm, is there no problem with data preprocessing without deduplication and BQSR for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/638#issuecomment-1526875856
https://github.com/google/deepvariant/issues/639#issuecomment-1526164168:1560,Testability,log,logic,1560,"requency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:; ```; 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990; ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position.; A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639#issuecomment-1526164168
https://github.com/google/deepvariant/issues/639#issuecomment-1526164168:1686,Testability,log,logic,1686,"requency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:; ```; 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990; ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position.; A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/639#issuecomment-1526164168
https://github.com/google/deepvariant/issues/640#issuecomment-1533748996:234,Availability,error,error,234,Thanks for the question!; I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1533748996
https://github.com/google/deepvariant/issues/640#issuecomment-1533748996:315,Deployability,install,install,315,Thanks for the question!; I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1533748996
https://github.com/google/deepvariant/issues/640#issuecomment-1533748996:331,Deployability,upgrade,upgrade,331,Thanks for the question!; I tried to run the quick start on a GCP VM but could not reproduce the issue. One hint from the [documentation](https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility) that the error links to is that it could have to do with numpy versions. Can you try `pip install numpy --upgrade` and then rerun?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1533748996
https://github.com/google/deepvariant/issues/640#issuecomment-1549844072:2955,Testability,test,test,2955,"pileup_images( ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images ; pileup = _pileup_for_pair_of_alts(alts) ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts ; ref_image = self.build_pileup( ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup ; build_pileup_for_one_sample(reads_for_samples[i], sample)) ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample ; rows = ([self._encoder.encode_reference(refbases)] * ; ImportError: numpy.core.multiarray failed to import ; I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json ; I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None ; I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] ; I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants ; I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples ; parallel: This job failed: ; /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels ; insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 ; ```. And the command is; ```bash; /opt/deepvariant/bin/run_deepvariant \; --ref=genome.fasta \; --reads=test.paired_end.sorted.cram \; --output_vcf=test_out.vcf.gz \; --output_gvcf=test_out.g.vcf.gz \; --model_type=WGS \; --regions genome.bed \; --intermediate_results_dir=. \; --num_shards=2; ```. Deepvariant version: 1.5.0 (the official docker container); Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1549844072
https://github.com/google/deepvariant/issues/640#issuecomment-1549844072:3213,Testability,test,test,3213,"pileup_images( ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images ; pileup = _pileup_for_pair_of_alts(alts) ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts ; ref_image = self.build_pileup( ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup ; build_pileup_for_one_sample(reads_for_samples[i], sample)) ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample ; rows = ([self._encoder.encode_reference(refbases)] * ; ImportError: numpy.core.multiarray failed to import ; I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json ; I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None ; I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] ; I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants ; I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples ; parallel: This job failed: ; /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels ; insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 ; ```. And the command is; ```bash; /opt/deepvariant/bin/run_deepvariant \; --ref=genome.fasta \; --reads=test.paired_end.sorted.cram \; --output_vcf=test_out.vcf.gz \; --output_gvcf=test_out.g.vcf.gz \; --model_type=WGS \; --regions genome.bed \; --intermediate_results_dir=. \; --num_shards=2; ```. Deepvariant version: 1.5.0 (the official docker container); Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1549844072
https://github.com/google/deepvariant/issues/640#issuecomment-1550288457:399,Security,access,access,399,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash; root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; DeepVariant version 1.5.0; root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; root@0f98b9adcd2d:/# find /tmp; /tmp; /tmp/tmpb20xyssf; /tmp/__pycache__; /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; /tmp/tmp0y_1vxbg; root@0f98b9adcd2d:/# find | grep bazel; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; root@0f98b9adcd2d:/#; ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550288457
https://github.com/google/deepvariant/issues/640#issuecomment-1550288457:779,Testability,test,testing,779,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash; root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; DeepVariant version 1.5.0; root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; root@0f98b9adcd2d:/# find /tmp; /tmp; /tmp/tmpb20xyssf; /tmp/__pycache__; /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; /tmp/tmp0y_1vxbg; root@0f98b9adcd2d:/# find | grep bazel; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; root@0f98b9adcd2d:/#; ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550288457
https://github.com/google/deepvariant/issues/640#issuecomment-1550288457:854,Testability,test,testing,854,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash; root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; DeepVariant version 1.5.0; root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; root@0f98b9adcd2d:/# find /tmp; /tmp; /tmp/tmpb20xyssf; /tmp/__pycache__; /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; /tmp/tmp0y_1vxbg; root@0f98b9adcd2d:/# find | grep bazel; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; root@0f98b9adcd2d:/#; ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550288457
https://github.com/google/deepvariant/issues/640#issuecomment-1550288457:955,Testability,test,testing,955,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash; root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; DeepVariant version 1.5.0; root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; root@0f98b9adcd2d:/# find /tmp; /tmp; /tmp/tmpb20xyssf; /tmp/__pycache__; /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; /tmp/tmp0y_1vxbg; root@0f98b9adcd2d:/# find | grep bazel; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; root@0f98b9adcd2d:/#; ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550288457
https://github.com/google/deepvariant/issues/640#issuecomment-1550288457:1054,Testability,test,testing,1054,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash; root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; DeepVariant version 1.5.0; root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; root@0f98b9adcd2d:/# find /tmp; /tmp; /tmp/tmpb20xyssf; /tmp/__pycache__; /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; /tmp/tmp0y_1vxbg; root@0f98b9adcd2d:/# find | grep bazel; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; root@0f98b9adcd2d:/#; ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550288457
https://github.com/google/deepvariant/issues/640#issuecomment-1550781075:331,Availability,error,error,331,"> @nvnieuwk Does it mean you only encounter this issue when running on a small region? Does the same setting work when you run something like the Quick Start?. Yes I've tried it with a CRAM file that contains the whole chromosome 21 and this works, but when I use a BAM with the subset `chr22:0-40001`, I get the earlier mentioned error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550781075
https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:1443,Deployability,pipeline,pipeline,1443," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872
https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:416,Security,access,access,416,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872
https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:814,Testability,test,testing,814,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872
https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:891,Testability,test,testing,891,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872
https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:994,Testability,test,testing,994,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872
https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:1095,Testability,test,testing,1095," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872
https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:1420,Testability,test,tests,1420," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872
https://github.com/google/deepvariant/issues/640#issuecomment-1550815697:67,Availability,error,error,67,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting.; If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550815697
https://github.com/google/deepvariant/issues/640#issuecomment-1550815697:562,Availability,error,error,562,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting.; If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550815697
https://github.com/google/deepvariant/issues/640#issuecomment-1550815697:164,Usability,clear,clear,164,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting.; If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550815697
https://github.com/google/deepvariant/issues/640#issuecomment-1550852287:624,Deployability,pipeline,pipeline,624,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287
https://github.com/google/deepvariant/issues/640#issuecomment-1550852287:646,Deployability,configurat,configuration,646,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287
https://github.com/google/deepvariant/issues/640#issuecomment-1550852287:890,Deployability,pipeline,pipelines,890,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287
https://github.com/google/deepvariant/issues/640#issuecomment-1550852287:235,Modifiability,variab,variables,235,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287
https://github.com/google/deepvariant/issues/640#issuecomment-1550852287:559,Modifiability,config,configure,559,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287
https://github.com/google/deepvariant/issues/640#issuecomment-1550852287:646,Modifiability,config,configuration,646,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287
https://github.com/google/deepvariant/issues/640#issuecomment-1550852287:959,Modifiability,variab,variables,959,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287
https://github.com/google/deepvariant/issues/641#issuecomment-1535452815:149,Deployability,release,releases,149,"Thanks for the question!; We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data.; See https://github.com/google/deepvariant/releases/tag/v1.5.0 ; ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641#issuecomment-1535452815
https://github.com/google/deepvariant/issues/641#issuecomment-1535452815:278,Performance,perform,performs,278,"Thanks for the question!; We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data.; See https://github.com/google/deepvariant/releases/tag/v1.5.0 ; ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641#issuecomment-1535452815
https://github.com/google/deepvariant/issues/641#issuecomment-1535591093:59,Testability,benchmark,benchmark,59,Thanks for your reply and i will try DeepVariant v1.5.0 to benchmark the HG002 Revio data.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/641#issuecomment-1535591093
https://github.com/google/deepvariant/issues/642#issuecomment-1535446429:2717,Availability,error,error,2717,".g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the high mappability BED file from genome in a bottle. **--filter ""SOR > 50.0 "" --filter-name LowSOR** This filter relates to the strand of reads over a variant. Strand bias is something which DeepVariant can see in its inputs and will use this information to determine variant calls and assign confidence to those calls. However, strand bias is not reported a a separate value in the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. I would say that use of this additional filter isn't needed. **--filter ""AN <150 "" --filter-name LowMiss""** As I understand this filter, it quantifies the number of different alleles identified at a position. I suppose this is some measure of the error rate around a position. If I understand the annotation correctly, DeepVariant should never produce value which come remotely close to this value in the cohort. You can exclude this filter as I don't think it would ever trigger.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642#issuecomment-1535446429
https://github.com/google/deepvariant/issues/642#issuecomment-1535446429:642,Safety,Predict,Predicted,642,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642#issuecomment-1535446429
https://github.com/google/deepvariant/issues/642#issuecomment-1535446429:710,Safety,predict,predicted-real,710,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/642#issuecomment-1535446429
https://github.com/google/deepvariant/issues/643#issuecomment-1535257357:746,Availability,avail,available,746,"Thanks for the question. Yes that is definitely some kind of bug, but I'm not able to reproduce it. It looks like something wrong with the rendering in vega-lite. How does this report look to you?: https://42basepairs.com/browse/gs/deepvariant/visual_reports/DeepVariant/1.5.0/WGS; This one has proper minus signs when I look at it. I'm asking because the report is effectively a website, so rendering can show up differently on different computers. A few things you could try:; 1. Open the HTML report in a different web browser (Chrome, Firefox, Safari, etc.); 2. Click the (...) in the top right corner of the report and go to ""Open in Vega Editor"".; 3. Play with any language settings on the computer or web browser in case it's the alphabet available that is missing the ""-"" character. If none of those solve the issue, then I'm also happy to take a look at your report, if you can share it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643#issuecomment-1535257357
https://github.com/google/deepvariant/issues/643#issuecomment-1535859706:493,Security,confidential,confidential,493,"Hi Maria,. Thanks for your kind help and time. Your linked report looks OK and shows minus symbols in chrome or safari (OSX 13.3.1 / M2) while mine are bad and were obtained on two different ubuntu22 servers with the same docker image. By contrast, the same report looks OK on the ubuntu server itself (open via VNC and using Mozilla), which points in the direction of a ubuntu 22 font specificity in my servers as you suggested. I attach here one such report for your review, the data is not confidential and comes from a SRA genome. [WT_REP1.visual_report.html.zip](https://github.com/google/deepvariant/files/11404181/WT_REP1.visual_report.html.zip). best regards",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/643#issuecomment-1535859706
https://github.com/google/deepvariant/issues/645#issuecomment-1542581479:35,Integrability,depend,dependent,35,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites?. `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness?. For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`).; * Filter sites where the average depth per sample is < 5.; * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/645#issuecomment-1542581479
https://github.com/google/deepvariant/issues/646#issuecomment-1546431553:37,Availability,avail,available,37,@yangyxt how much memory do you have available on this machine? Is it possible to increase the amount of memory?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1546431553
https://github.com/google/deepvariant/issues/646#issuecomment-1547109235:87,Energy Efficiency,schedul,scheduler,87,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:; ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547109235
https://github.com/google/deepvariant/issues/646#issuecomment-1547109235:100,Energy Efficiency,allocate,allocated,100,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:; ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547109235
https://github.com/google/deepvariant/issues/646#issuecomment-1547115920:199,Availability,error,error,199,"Hi @yangyxt ,; In your log, I saw this line which is a bit strange:. ```; Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error; ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant/; git checkout r1.4; find . -type f -exec grep -H call_deeptrio_per_pair {} \;; ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547115920
https://github.com/google/deepvariant/issues/646#issuecomment-1547115920:23,Testability,log,log,23,"Hi @yangyxt ,; In your log, I saw this line which is a bit strange:. ```; Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error; ```. I tried to search for that function in our r1.4 codebase (which is the version you mentioned you're using):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant/; git checkout r1.4; find . -type f -exec grep -H call_deeptrio_per_pair {} \;; ```. And I can't find that function in our codebase. Can you confirm that you're running our version, or maybe you're running a modified version from somewhere else?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547115920
https://github.com/google/deepvariant/issues/646#issuecomment-1547132456:63,Integrability,wrap,wrapper,63,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547132456
https://github.com/google/deepvariant/issues/646#issuecomment-1547132456:19,Testability,log,log,19,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547132456
https://github.com/google/deepvariant/issues/646#issuecomment-1547153842:984,Availability,error,error,984,"Actually I have a suggestion:; Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547153842
https://github.com/google/deepvariant/issues/646#issuecomment-1547153842:1037,Availability,error,error,1037,"Actually I have a suggestion:; Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547153842
https://github.com/google/deepvariant/issues/646#issuecomment-1547153842:1115,Availability,error,error,1115,"Actually I have a suggestion:; Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547153842
https://github.com/google/deepvariant/issues/646#issuecomment-1547153842:1121,Integrability,message,message,1121,"Actually I have a suggestion:; Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547153842
https://github.com/google/deepvariant/issues/646#issuecomment-1547543795:63,Usability,feedback,feedback,63,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547543795
https://github.com/google/deepvariant/issues/646#issuecomment-1547543795:107,Usability,simpl,simple,107,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1547543795
https://github.com/google/deepvariant/issues/646#issuecomment-1549015401:362,Availability,error,error,362,"Hi @yangyxt ,; I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1549015401
https://github.com/google/deepvariant/issues/646#issuecomment-1549015401:397,Deployability,release,release,397,"Hi @yangyxt ,; I see... For now, to help you continue your work, I suggest using run_deeptrio with the `--dry_run=true` flag to get the list of commands, and run the commands there individually for now. Sorry for the inconvenience. The goal for having the one-step run_deeptrio.py script was to make things easier. But I need to do some improvements to make the error reporting better in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1549015401
https://github.com/google/deepvariant/issues/646#issuecomment-1552499124:78,Availability,error,errors,78,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1552499124
https://github.com/google/deepvariant/issues/646#issuecomment-1552499124:99,Deployability,update,update,99,"I'll close this issue for now. @yangyxt if you are able to find more detailed errors, feel free to update and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/646#issuecomment-1552499124
https://github.com/google/deepvariant/issues/649#issuecomment-1546497696:44,Deployability,release,release,44,"@kiranpatil222 you are in luck. Our [v1.4.0 release](https://github.com/google/deepvariant/releases/tag/v1.4.0) introduced 'direct phasing,' which means you no longer need to run whatshap. . See the [PacBio case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) for further details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1546497696
https://github.com/google/deepvariant/issues/649#issuecomment-1546497696:91,Deployability,release,releases,91,"@kiranpatil222 you are in luck. Our [v1.4.0 release](https://github.com/google/deepvariant/releases/tag/v1.4.0) introduced 'direct phasing,' which means you no longer need to run whatshap. . See the [PacBio case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) for further details.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1546497696
https://github.com/google/deepvariant/issues/649#issuecomment-1546990230:1565,Energy Efficiency,consumption,consumption,1565,"3085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]; I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088.; I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]; ...; I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]; I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953.; I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]; I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496.; I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]; ....; ```. And there's also the relevant command information from the log below (formatted for human consumption). ```; time seq 0 0 | parallel -q --halt 2 \; --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \; --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \; --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \; --add_hp_channel \; --alt_aligned_pileup ""diff_channels"" \; --max_reads_per_partition ""600"" \; --min_mapping_quality ""1"" \; --parse_sam_aux_fields \; --partition_size ""25000"" \; --phase_reads \; --pileup_image_width ""199"" \; --norealign_reads \; --regions ""chr20"" \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels ""0.12"" \; --task {}; ...; ```. Here's a snapshot of the output. ```; chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35; chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59; chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1546990230
https://github.com/google/deepvariant/issues/649#issuecomment-1546990230:470,Testability,log,log,470,"Hi, . I'm sure I'm doing something wrong, but couldn't figure out where that is: the variants I get are not directly phased. I followed the PacBio case study, using the 1.5.0 docker. ; Here's the command. ```bash; $ ./run_deepvariant \; --model_type PACBIO \; --ref /shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \; --reads /shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf /tmp/only_vcf.vcf.gz \; --regions chr20; ```. What's strange in the log are the following lines. ```; ...; I0514 01:48:09.263085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]; I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088.; I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]; ...; I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]; I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953.; I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]; I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496.; I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]; ....; ```. And there's also the relevant command information from the log below (formatted for human consumption). ```; time seq 0 0 | parallel -q --halt 2 \; --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \; --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \; --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \; --add_hp_channel \; --alt_aligned_pileup ""diff_channels"" \; --max_reads_per_partition ""600"" \; --min_mapping_quali",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1546990230
https://github.com/google/deepvariant/issues/649#issuecomment-1546990230:1534,Testability,log,log,1534,"3085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]; I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088.; I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]; ...; I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]; I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953.; I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]; I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496.; I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]; ....; ```. And there's also the relevant command information from the log below (formatted for human consumption). ```; time seq 0 0 | parallel -q --halt 2 \; --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \; --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \; --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \; --add_hp_channel \; --alt_aligned_pileup ""diff_channels"" \; --max_reads_per_partition ""600"" \; --min_mapping_quality ""1"" \; --parse_sam_aux_fields \; --partition_size ""25000"" \; --phase_reads \; --pileup_image_width ""199"" \; --norealign_reads \; --regions ""chr20"" \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels ""0.12"" \; --task {}; ...; ```. Here's a snapshot of the output. ```; chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35; chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59; chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1546990230
https://github.com/google/deepvariant/issues/649#issuecomment-1547069475:1168,Security,expose,expose,1168,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:; ```; I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953.; ```; (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode.; https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260; (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.); If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know!. By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1547069475
https://github.com/google/deepvariant/issues/649#issuecomment-1547069475:62,Testability,log,log,62,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:; ```; I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953.; ```; (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode.; https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260; (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.); If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know!. By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1547069475
https://github.com/google/deepvariant/issues/649#issuecomment-1547108448:57,Usability,clear,clear,57,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1547108448
https://github.com/google/deepvariant/issues/649#issuecomment-1547127685:518,Availability,avail,available,518,"Thanks for the explanations, @pichuan !. Just to re-iterate to make sure I can understand, . - it is not expected that DV 1.4.0 and 1.5.0 will output phased variants, i.e. the GT field should still look like 0/1 not 0|1. ; - Instead, phasing is done internally to DV (like the PEPPER step), so that accuracy can benefit. ; - Therefore we should still run a standalone phasing tool (e.g. whatshap, margin phase) to phase the outputs of DV 1.4.0 and 1.5.0, if phasing is needed.; - And internal phasing results are also available if one follows your suggestion above (i.e. peeking under the hood). Is this correct?. Also, looking forward to the new doc explaining the phasing!. Thanks,; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1547127685
https://github.com/google/deepvariant/issues/649#issuecomment-1547305204:393,Usability,clear,clear,393,"Sorry for the confusion here. We're phasing the reads internally to help with DeepVariant accuracy. (""Direct"" is referring to that we're doing this now directly in DeepVariant instead of relying on external HP tags from other tools like WhatsHap). However, DeepVariant currently still doesn't generate phased variant calls. Therefore you won't see ""|"" in the VCF files. Hopefully this is more clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1547305204
https://github.com/google/deepvariant/issues/649#issuecomment-1550118274:143,Availability,down,down,143,"Given these confusions, I feel like some explanations of what ""direct phasing"" means, other than in this ticket, would reduce users' questions down the road.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1550118274
https://github.com/google/deepvariant/issues/649#issuecomment-1550118274:119,Energy Efficiency,reduce,reduce,119,"Given these confusions, I feel like some explanations of what ""direct phasing"" means, other than in this ticket, would reduce users' questions down the road.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1550118274
https://github.com/google/deepvariant/issues/649#issuecomment-1551634414:31,Deployability,release,release,31,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1551634414
https://github.com/google/deepvariant/issues/649#issuecomment-1551634414:536,Integrability,synchroniz,synchronization,536,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1551634414
https://github.com/google/deepvariant/issues/649#issuecomment-1551634414:195,Modifiability,enhance,enhances,195,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/649#issuecomment-1551634414
https://github.com/google/deepvariant/issues/650#issuecomment-1548690112:100,Deployability,release,releases,100,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112
https://github.com/google/deepvariant/issues/650#issuecomment-1548690112:696,Deployability,release,release,696,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112
https://github.com/google/deepvariant/issues/650#issuecomment-1548690112:443,Performance,optimiz,optimized,443,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112
https://github.com/google/deepvariant/issues/650#issuecomment-1548690112:744,Performance,optimiz,optimization,744,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112
https://github.com/google/deepvariant/issues/650#issuecomment-1548690112:764,Performance,perform,performance,764,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112
https://github.com/google/deepvariant/issues/650#issuecomment-1548690112:1022,Testability,benchmark,benchmarks,1022,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112
https://github.com/google/deepvariant/issues/650#issuecomment-1549350965:312,Performance,bottleneck,bottleneck,312,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1549350965
https://github.com/google/deepvariant/issues/650#issuecomment-1549350965:444,Performance,optimiz,optimizing,444,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1549350965
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:704,Availability,down,down,704,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:930,Availability,down,down,930,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:312,Deployability,pipeline,pipeline,312,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:322,Deployability,release,release,322,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:921,Deployability,pipeline,pipeline,921,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:983,Deployability,pipeline,pipeline,983,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:1158,Deployability,pipeline,pipelines,1158,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:88,Energy Efficiency,efficient,efficiently,88,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:904,Performance,optimiz,optimize,904,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:966,Performance,optimiz,optimize,966,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:1305,Performance,optimiz,optimization,1305,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017
https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:1222,Deployability,release,releases,1222,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050
https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:967,Energy Efficiency,reduce,reduce,967,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050
https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:1095,Energy Efficiency,efficient,efficient,1095,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050
https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:549,Performance,optimiz,optimizing,549,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050
https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:564,Performance,perform,performance,564,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050
https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:826,Performance,bottleneck,bottleneck,826,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050
https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:947,Performance,optimiz,optimizations,947,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050
https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:187,Testability,log,logic,187,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050
https://github.com/google/deepvariant/issues/650#issuecomment-1551851074:276,Availability,avail,available,276,"Thanks, Andrew!; I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning.; [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf); [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551851074
https://github.com/google/deepvariant/issues/650#issuecomment-1551851074:346,Energy Efficiency,allocate,allocated,346,"Thanks, Andrew!; I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning.; [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf); [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1551851074
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:10142,Deployability,integrat,integrate,10142,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:1704,Energy Efficiency,reduce,reduced,1704,"=========; str | 223354 | 35.79 MB; dict | 88941 | 25.94 MB; code | 50107 | 8.54 MB; type | 6121 | 5.65 MB; tuple | 63884 | 3.62 MB; list | 30942 | 3.19 MB; set | 2864 | 1.51 MB; weakref | 14251 | 1002.02 KB; abc.ABCMeta | 784 | 826.05 KB; cell | 20911 | 816.84 KB; int | 25259 | 697.77 KB; builtin_function_or_method | 8801 | 618.82 KB; google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB; frozenset | 1862 | 541.02 KB; function (__init__) | 3439 | 456.74 KB; ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```; Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time; List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function); 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}; 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}; 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}; 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller); 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode); 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__); 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}; 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__); 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}; 113550 0.334 0.000 0.646 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:6491,Energy Efficiency,reduce,reduced,6491,"0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}; 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}; 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}; 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code); 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs); 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 10001 0.142 0.000 0.428 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:7085,Energy Efficiency,reduce,reduce,7085,"much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}; 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}; 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}; 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code); 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs); 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 10001 0.142 0.000 0.428 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:249(with_gq_and_likelihoods); 5151 0.134 0.000 0.610 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:194(_calc_reference_confidence); 5268/5203 0.119 0.000 0.562 0.000 {built-in method builtins.__build_class__}; 1339 0.119 0.000 0.119 0.000 {method 'write' of 'third_party.nucleus.io.python.tfrecord_writer.TFRecordWriter' objects}; 1276 0.116 0.000 0.676 0.001 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:10142,Integrability,integrat,integrate,10142,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:2517,Modifiability,Extend,ExtendSession,2517,oral data:. ```; Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time; List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function); 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}; 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}; 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}; 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller); 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode); 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__); 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}; 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__); 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}; 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack); 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions); 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}; 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly); 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__); 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class); 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper); 3367 0.228 0.000 0.228 0.000 {built-in method posix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:5939,Modifiability,rewrite,rewrite,5939,"thon3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack); 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype); 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node); 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph); 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}; 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.47",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:10155,Modifiability,extend,extend,10155,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:280,Performance,perform,performance,280,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```; types | # objects | total size; ================================================================ | =========== | ============; str | 223354 | 35.79 MB; dict | 88941 | 25.94 MB; code | 50107 | 8.54 MB; type | 6121 | 5.65 MB; tuple | 63884 | 3.62 MB; list | 30942 | 3.19 MB; set | 2864 | 1.51 MB; weakref | 14251 | 1002.02 KB; abc.ABCMeta | 784 | 826.05 KB; cell | 20911 | 816.84 KB; int | 25259 | 697.77 KB; builtin_function_or_method | 8801 | 618.82 KB; google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB; frozenset | 1862 | 541.02 KB; function (__init__) | 3439 | 456.74 KB; ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```; Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time; List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function); 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}; 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.clie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:414,Performance,perform,performance-cuda,414,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```; types | # objects | total size; ================================================================ | =========== | ============; str | 223354 | 35.79 MB; dict | 88941 | 25.94 MB; code | 50107 | 8.54 MB; type | 6121 | 5.65 MB; tuple | 63884 | 3.62 MB; list | 30942 | 3.19 MB; set | 2864 | 1.51 MB; weakref | 14251 | 1002.02 KB; abc.ABCMeta | 784 | 826.05 KB; cell | 20911 | 816.84 KB; int | 25259 | 697.77 KB; builtin_function_or_method | 8801 | 618.82 KB; google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB; frozenset | 1862 | 541.02 KB; function (__init__) | 3439 | 456.74 KB; ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```; Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time; List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function); 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}; 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.clie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:5979,Performance,optimiz,optimizations,5979,"thon3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack); 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype); 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node); 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph); 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}; 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.47",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:6977,Performance,load,loads,6977,"izations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}; 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}; 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}; 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code); 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs); 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 10001 0.142 0.000 0.428 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:249(with_gq_and_likelihoods); 5151 0.134 0.000 0.610 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:194(_calc_reference_confidence); 5268/5203 0.119 0.000 0.562 0.000 {built-in method builtins.__build_class__}; 1339 0.119 0.000 0.119 0.000 {method 'write' of 'third_party.nucleus.io.python.tfrecord_writer.TFRecordWriter' objects}; 1276 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:10032,Performance,optimiz,optimized,10032,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:6092,Usability,simpl,simpler,6092,"/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node); 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph); 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}; 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}; 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}; 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}; 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826
https://github.com/google/deepvariant/issues/650#issuecomment-1559918325:19,Usability,feedback,feedback,19,Thanks for all the feedback and discussion above! I'll close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650#issuecomment-1559918325
https://github.com/google/deepvariant/issues/651#issuecomment-1554122525:454,Availability,error,error,454,"@ChristinaMulch . With respect to the mechanics of DeepVariant - You can merge a BAM file with multiple read groups or from several files with the same read group. When you do so, if there is only one Sample among these files (specifically, all read groups have the same SM: value), DeepVariant will call all of them together, and will use that sample name for the VCF header field. If the read groups have more than one Sample, DeepVariant will give an error. If you really need to run on that file even so, there is a flag you can provide to apply a sample name to the BAM. Does this answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651#issuecomment-1554122525
https://github.com/google/deepvariant/issues/651#issuecomment-1575314566:335,Availability,avail,available,335,"Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:. https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651#issuecomment-1575314566
https://github.com/google/deepvariant/issues/651#issuecomment-1575342924:337,Availability,avail,available,337,"> Probably the quickest way would be to process each sample individually and then combine them together, by first creating [gVCF outputs](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md) and then merging them using [GLnexus](https://github.com/dnanexus-rnd/GLnexus). A nice tutorial on how to do this is available at the following link:; > ; > https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md. Thanks for your reply! It seems too time and resource consuming to separately run for each sample. If I got the resource, I would have a try. Best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651#issuecomment-1575342924
https://github.com/google/deepvariant/issues/651#issuecomment-1576131534:46,Energy Efficiency,efficient,efficient,46,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651#issuecomment-1576131534
https://github.com/google/deepvariant/issues/651#issuecomment-1576131534:289,Testability,benchmark,benchmarks,289,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651#issuecomment-1576131534
https://github.com/google/deepvariant/issues/651#issuecomment-1576435077:53,Energy Efficiency,efficient,efficient,53,"> Hi @tinyfallen; > ; > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144).; > ; > So the main resource use can be estimated from the single sample runtime multiplied by sample number.; > ; > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651#issuecomment-1576435077
https://github.com/google/deepvariant/issues/651#issuecomment-1576435077:296,Testability,benchmark,benchmarks,296,"> Hi @tinyfallen; > ; > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144).; > ; > So the main resource use can be estimated from the single sample runtime multiplied by sample number.; > ; > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651#issuecomment-1576435077
https://github.com/google/deepvariant/issues/652#issuecomment-1555394452:99,Testability,test,tested,99,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652#issuecomment-1555394452
https://github.com/google/deepvariant/issues/652#issuecomment-1555394452:17,Usability,intuit,intuition,17,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/652#issuecomment-1555394452
https://github.com/google/deepvariant/issues/653#issuecomment-1559913831:20,Testability,log,log,20,"Hi @Zjianglin , the log above is a bit difficult for me to read. Can you confirm whether you have the fai file in your path `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai`?. If not, can you index your FASTA file? You can use `samtools faidx hs37d5.fa` to create the index file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1559913831
https://github.com/google/deepvariant/issues/653#issuecomment-1567721314:435,Availability,echo,echo,435,"@Zjianglin Can you check whether your singularity run can see the index file?. You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ?. By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1567721314
https://github.com/google/deepvariant/issues/653#issuecomment-1567721314:303,Testability,log,logx,303,"@Zjianglin Can you check whether your singularity run can see the index file?. You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ?. By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1567721314
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:1870,Availability,echo,echo,1870," -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:1932,Availability,echo,echo,1932," -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:1980,Availability,echo,echo,1980," -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:2054,Availability,echo,echo,2054," -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:2212,Availability,echo,echo,2212,"zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:2302,Availability,echo,echo,2302,"man_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:3313,Modifiability,sandbox,sandbox,3313,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:2731,Security,access,access,2731,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:2903,Security,access,access,2903,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:3085,Security,access,access,3085,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:3185,Security,access,access,3185,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:3345,Security,access,access,3345,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:1766,Testability,log,logdir,1766,"hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:1783,Testability,log,logs,1783,"hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:3313,Testability,sandbox,sandbox,3313,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761
https://github.com/google/deepvariant/issues/653#issuecomment-1573188242:98,Testability,test,testing,98,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp; ```; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; ```. Then I made my deepvariant.sif. ```; singularity build deepvariant.sif docker://google/deepvariant:1.5.0; ```. First, I check that I have the files; ```; REF=/tmp/ucsc.hg19.chr20.unittest.fasta; ls -al ${REF}*; ```; This worked.; (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; ls -al ${REF}*; ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1573188242
https://github.com/google/deepvariant/issues/653#issuecomment-1573188242:234,Testability,test,testing,234,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp; ```; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; ```. Then I made my deepvariant.sif. ```; singularity build deepvariant.sif docker://google/deepvariant:1.5.0; ```. First, I check that I have the files; ```; REF=/tmp/ucsc.hg19.chr20.unittest.fasta; ls -al ${REF}*; ```; This worked.; (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; ls -al ${REF}*; ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1573188242
https://github.com/google/deepvariant/issues/653#issuecomment-1573188242:387,Testability,test,testdata,387,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp; ```; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; ```. Then I made my deepvariant.sif. ```; singularity build deepvariant.sif docker://google/deepvariant:1.5.0; ```. First, I check that I have the files; ```; REF=/tmp/ucsc.hg19.chr20.unittest.fasta; ls -al ${REF}*; ```; This worked.; (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; ls -al ${REF}*; ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1573188242
https://github.com/google/deepvariant/issues/653#issuecomment-1573188242:122,Usability,simpl,simplified,122,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp; ```; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; ```. Then I made my deepvariant.sif. ```; singularity build deepvariant.sif docker://google/deepvariant:1.5.0; ```. First, I check that I have the files; ```; REF=/tmp/ucsc.hg19.chr20.unittest.fasta; ls -al ${REF}*; ```; This worked.; (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; ls -al ${REF}*; ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1573188242
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:717,Availability,echo,echo,717,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:779,Availability,echo,echo,779,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:827,Availability,echo,echo,827,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:899,Availability,echo,echo,899,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:1061,Availability,echo,echo,1061,"quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujiang",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:1118,Availability,echo,echo,1118,"hat I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:1288,Availability,echo,echo,1288,"hat I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3484,Modifiability,sandbox,sandbox,3484,_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Hum,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:4527,Modifiability,sandbox,sandbox,4527,cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3516,Security,access,access,3516, 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3626,Security,access,access,3626,ianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3741,Security,access,access,3741,oujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3855,Security,access,access,3855,ujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3969,Security,access,access,3969,jianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujiangl,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:4091,Security,access,access,4091,ujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1019,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:4205,Security,access,access,4205,ujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin z,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:613,Testability,log,logdir,613,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:630,Testability,log,logs,630,"Hi @pichuan , Thanks for your reply. As your suggestion, after removing the double quote around the *, the `ls` run normally in `bash` script. However, the `ls` in `deepvariant` image of `singularity` still failed. Notably, If I add a `which ls;` before a `ls -al $ref_idx*`, it runs normally so that I can see the detailed information for the files..; Here is the script:; ```bash; #!/bin/bash. nthreads=32; dvsif=""/lustre/Data/toolsDB/deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ${ref_idx}*; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif ls -al ${ref_idx}*; echo -e ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ $dvsif which ls; ls -al ${ref_idx}*. ```. Here is the running output:; ```shell; /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3484,Testability,sandbox,sandbox,3484,_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Hum,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:4527,Testability,sandbox,sandbox,4527,cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269
https://github.com/google/deepvariant/issues/653#issuecomment-1575943033:585,Usability,guid,guides,585,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```; singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*; ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575943033
https://github.com/google/deepvariant/issues/653#issuecomment-1575943033:601,Usability,guid,guide,601,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```; singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*; ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1575943033
https://github.com/google/deepvariant/issues/653#issuecomment-1793241251:67,Availability,error,error,67,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:; ```; docker run \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:; ```; docker run \; -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/index/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1793241251
https://github.com/google/deepvariant/issues/653#issuecomment-1793241251:107,Performance,load,load,107,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:; ```; docker run \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:; ```; docker run \; -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/index/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1793241251
https://github.com/google/deepvariant/issues/653#issuecomment-1793241251:840,Security,access,access,840,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:; ```; docker run \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:; ```; docker run \; -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/index/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653#issuecomment-1793241251
https://github.com/google/deepvariant/issues/655#issuecomment-1570674832:1402,Availability,down,downsample,1402," to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? ; 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832
https://github.com/google/deepvariant/issues/655#issuecomment-1570674832:1629,Availability,down,downstream,1629," to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? ; 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832
https://github.com/google/deepvariant/issues/655#issuecomment-1570674832:1553,Safety,avoid,avoid,1553," to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? ; 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832
https://github.com/google/deepvariant/issues/655#issuecomment-1570674832:82,Usability,clear,clear,82,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? ; 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this loo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832
https://github.com/google/deepvariant/issues/655#issuecomment-1586633037:119,Safety,avoid,avoid,119,"hi@AndrewCarroll ; Thank you for your suggestion. This is a new platform panel data, we will consider filtering GQ to avoid this problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655#issuecomment-1586633037
https://github.com/google/deepvariant/issues/656#issuecomment-1573061991:153,Integrability,message,message,153,https://github.com/google/nucleus/blob/v0.6.0/nucleus/protos/range.proto#L18. ```; // A 0-based half-open genomic coordinate range for search requests.; message Range {; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/656#issuecomment-1573061991
https://github.com/google/deepvariant/issues/657#issuecomment-1575575178:46,Availability,error,error,46,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):; ```; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped; /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575575178
https://github.com/google/deepvariant/issues/657#issuecomment-1575575178:179,Availability,avail,available,179,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):; ```; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped; /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575575178
https://github.com/google/deepvariant/issues/657#issuecomment-1575575178:239,Safety,Abort,Aborted,239,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):; ```; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped; /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575575178
https://github.com/google/deepvariant/issues/657#issuecomment-1575575178:311,Safety,Abort,Aborted,311,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):; ```; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped; /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575575178
https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:511,Availability,avail,available,511,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935
https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:69,Deployability,Install,Install,69,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935
https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:202,Deployability,install,install,202,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935
https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:221,Deployability,install,install,221,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935
https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:328,Deployability,configurat,configuration,328,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935
https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:686,Deployability,configurat,configurations,686,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935
https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:328,Modifiability,config,configuration,328,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935
https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:686,Modifiability,config,configurations,686,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:820,Availability,Down,Downloaded,820,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:1274,Availability,error,error,1274,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:1825,Availability,error,error,1825,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:60,Deployability,install,install,60,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:134,Deployability,install,install,134,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:634,Deployability,install,installation,634,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:661,Integrability,message,message,661,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:795,Integrability,message,message,795,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:1457,Integrability,message,message,1457,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:1004,Performance,optimiz,optimized,1004,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:1104,Performance,perform,performance-critical,1104,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:1579,Performance,optimiz,optimized,1579,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:1679,Performance,perform,performance-critical,1679,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623
https://github.com/google/deepvariant/issues/657#issuecomment-1575652409:184,Availability,error,error-message,184,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information.""; https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575652409
https://github.com/google/deepvariant/issues/657#issuecomment-1575652409:15,Integrability,message,messages,15,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information.""; https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575652409
https://github.com/google/deepvariant/issues/657#issuecomment-1575652409:190,Integrability,message,message,190,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information.""; https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575652409
https://github.com/google/deepvariant/issues/657#issuecomment-1575652409:231,Testability,test,test,231,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information.""; https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575652409
https://github.com/google/deepvariant/issues/657#issuecomment-1575672535:249,Deployability,install,installed,249,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575672535
https://github.com/google/deepvariant/issues/657#issuecomment-1575672535:744,Deployability,install,install,744,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575672535
https://github.com/google/deepvariant/issues/657#issuecomment-1575672535:100,Performance,perform,performs,100,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575672535
https://github.com/google/deepvariant/issues/657#issuecomment-1575677888:234,Deployability,install,installed,234,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888
https://github.com/google/deepvariant/issues/657#issuecomment-1575677888:347,Deployability,install,installed,347,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888
https://github.com/google/deepvariant/issues/657#issuecomment-1575677888:118,Safety,predict,predicted,118,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888
https://github.com/google/deepvariant/issues/657#issuecomment-1575677888:150,Security,access,access,150,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888
https://github.com/google/deepvariant/issues/657#issuecomment-1575677888:427,Usability,guid,guidance,427,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888
https://github.com/google/deepvariant/issues/657#issuecomment-1575697410:29,Security,access,access,29,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```; #!/bin/bash; source settings.sh; ./run-prereq.sh; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575697410
https://github.com/google/deepvariant/issues/657#issuecomment-1575697410:269,Security,access,access,269,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```; #!/bin/bash; source settings.sh; ./run-prereq.sh; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575697410
https://github.com/google/deepvariant/issues/657#issuecomment-1575697410:418,Security,access,access,418,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```; #!/bin/bash; source settings.sh; ./run-prereq.sh; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575697410
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2137,Availability,ERROR,ERROR,2137,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2411,Availability,ERROR,ERROR,2411,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2685,Availability,ERROR,ERROR,2685,"ARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3319,Availability,ERROR,ERROR,3319,"u have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is inc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3834,Availability,ERROR,ERROR,3834,"ckages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:4528,Availability,error,error,4528," but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:4584,Availability,avail,available,4584," incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf';",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:4608,Availability,error,error,4608,"======= [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:4626,Availability,echo,echo,4626,"======= [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6057,Availability,Error,Error,6057,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6576,Availability,error,errors,6576,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:812,Deployability,Update,Update,812,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:910,Deployability,Install,Install,910,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1031,Deployability,Install,Install,1031," this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1318,Deployability,Install,Installing,1318,"DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1396,Deployability,install,installation,1396,"DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1531,Deployability,install,installed,1531," > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1714,Deployability,install,installed,1714,"1:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2101,Deployability,Install,Install,2101,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2233,Deployability,install,installed,2233,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2507,Deployability,install,installed,2507,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2781,Deployability,install,installed,2781,"ARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3223,Deployability,Install,Install,3223,"lled. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3265,Deployability,Install,Installing,3265,"lled. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3415,Deployability,install,installed,3415,"u have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is inc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3644,Deployability,Install,Install,3644,"ckages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3720,Deployability,Install,Install,3720,"ckages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3800,Deployability,Install,Install,3800,"ckages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3930,Deployability,install,installed,3930,"ckages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6138,Deployability,configurat,configuration,6138,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6243,Deployability,configurat,configuration,6243,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:162,Integrability,message,messages,162,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:511,Integrability,message,message,511,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2150,Integrability,depend,dependency,2150,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2290,Integrability,depend,dependency,2290,"ed pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2424,Integrability,depend,dependency,2424,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2564,Integrability,depend,dependency,2564,".local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2698,Integrability,depend,dependency,2698,"ARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2838,Integrability,depend,dependency,2838,"behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3332,Integrability,depend,dependency,3332,"u have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is inc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3472,Integrability,depend,dependency,3472,"tly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' sta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3847,Integrability,depend,dependency,3847,"ckages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3987,Integrability,depend,dependency,3987,"pu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:668,Modifiability,config,config,668,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5508,Modifiability,config,config,5508,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6138,Modifiability,config,configuration,6138,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6243,Modifiability,config,configuration,6243,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6552,Modifiability,Config,Configuring,6552,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:663,Performance,Load,Load,663,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1273,Performance,cache,cached,1273," this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5742,Performance,Perform,Performing,5742,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5786,Performance,Perform,Performing,5786,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5067,Safety,Detect,Detecting,5067,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5101,Safety,Detect,Detecting,5101,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5142,Safety,Detect,Detecting,5142,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5175,Safety,Detect,Detecting,5175,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5322,Safety,Detect,Detecting,5322,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5358,Safety,Detect,Detecting,5358,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5401,Safety,Detect,Detecting,5401,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5436,Safety,Detect,Detecting,5436,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5753,Testability,Test,Test,5753,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5797,Testability,Test,Test,5797,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6644,Testability,log,log,6644,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6700,Testability,log,log,6700,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:696,Availability,error,errors,696,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:348,Deployability,install,install,348,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:463,Deployability,Install,Install,463,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:569,Deployability,Install,Install,569,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:615,Deployability,Install,Install,615,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:804,Deployability,install,installing,804,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1132,Deployability,update,update,1132,"nt/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1153,Deployability,install,install,1153,"-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1278,Deployability,install,installed,1278,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1288,Deployability,configurat,configuration,1288,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:2117,Deployability,install,installed,2117,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1288,Modifiability,config,configuration,1288,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1367,Modifiability,config,config-,1367,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1397,Modifiability,config,configs,1397,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1994,Modifiability,config,config,1994,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:335,Usability,simpl,simplify,335,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:206,Availability,echo,echo,206,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:694,Availability,failure,failures,694,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:875,Availability,echo,echo,875,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:999,Availability,echo,echo,999,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1210,Availability,echo,echo,1210,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1464,Availability,echo,echo,1464,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1662,Availability,echo,echo,1662,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2166,Availability,down,downgrade,2166,"ge.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2814,Availability,down,downloads,2814,"}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --all",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2925,Availability,echo,echo,2925,"w 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2994,Availability,echo,echo,2994," caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3074,Availability,down,download,3074," # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUD",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3380,Availability,down,download,3380,"##########; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3517,Availability,down,download,3517,"A"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3810,Availability,down,downgrades,3810,"ownloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # #####################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3956,Availability,echo,echo,3956,"ownloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # #####################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4046,Availability,echo,echo,4046,"rl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4157,Availability,down,download,4157," cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:5172,Availability,error,errors,5172,"nload.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:5274,Availability,echo,echo,5274,"dnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config set",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7743,Availability,ERROR,ERROR,7743,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8017,Availability,ERROR,ERROR,8017,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8557,Availability,ERROR,ERROR,8557,"cal/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8916,Availability,ERROR,ERROR,8916,"flow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10249,Availability,down,download,10249,"v,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: STDOUT. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10313,Availability,down,download,10313,"n-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: STDOUT. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports foca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10865,Availability,down,download,10865,"rg)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: STDOUT. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10930,Availability,down,download,10930,"ting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: STDOUT. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13117,Availability,error,error,13117,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13457,Availability,Error,Error,13457,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13976,Availability,error,errors,13976,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:119,Deployability,Install,Install,119,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:232,Deployability,install,installation,232,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:275,Deployability,install,installed,275,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:320,Deployability,install,install,320,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:740,Deployability,release,released,740,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:881,Deployability,Install,Installing,881,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:938,Deployability,install,install,938,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:965,Deployability,upgrade,upgrade,965,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1005,Deployability,Install,Installing,1005,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1059,Deployability,install,install,1059,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1086,Deployability,upgrade,upgrade,1086,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1144,Deployability,release,release,1144,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1216,Deployability,Install,Installing,1216,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1307,Deployability,install,install,1307,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1334,Deployability,upgrade,upgrade,1334,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1470,Deployability,Install,Installing,1470,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1563,Deployability,install,install,1563,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1590,Deployability,upgrade,upgrade,1590,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1668,Deployability,Install,Installing,1668,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1763,Deployability,install,install,1763,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:1790,Deployability,upgrade,upgrade,1790,"ant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2256,Deployability,install,install,2256,"U_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2283,Deployability,upgrade,upgrade,2283,"U_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2510,Deployability,Install,Install,2510,"orFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2560,Deployability,install,install,2560,"ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3000,Deployability,Install,Installing,3000," caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3597,Deployability,update,update,3597," required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3885,Deployability,upgrade,upgrade,3885,"ownloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # #####################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3912,Deployability,install,install,3912,"ownloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # #####################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4052,Deployability,Install,Installing,4052,"rl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4566,Deployability,install,install,4566,"o -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4677,Deployability,install,install,4677,"/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; #",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4724,Deployability,install,installed,4724,"-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4963,Deployability,Install,Install,4963,"""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:5230,Deployability,install,install,5230,"dnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config set",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6418,Deployability,Update,Update,6418,"); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6516,Deployability,Install,Install,6516,"{TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6637,Deployability,Install,Install,6637," a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6924,Deployability,Install,Installing,6924,"RRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7002,Deployability,install,installation,7002,"RRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7137,Deployability,install,installed,7137,"; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7320,Deployability,install,installed,7320,"1:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7707,Deployability,Install,Install,7707,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7839,Deployability,install,installed,7839,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8113,Deployability,install,installed,8113,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8503,Deployability,Install,Installing,8503,"stead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prere",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8653,Deployability,install,installed,8653,"cal/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8882,Deployability,Install,Install,8882,"flow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:9012,Deployability,install,installed,9012,"flow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10470,Deployability,update,updates,10470," 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: STDOUT. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10843,Deployability,update,update,10843,"rg)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: STDOUT. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11158,Deployability,update,updates,11158,"m.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11362,Deployability,install,install,11362,"buntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11819,Deployability,install,installed,11819,"red/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11873,Deployability,install,installed,11873,"ase ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11996,Deployability,upgrade,upgraded,11996,"cal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12014,Deployability,install,installed,12014,"cal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12047,Deployability,upgrade,upgraded,12047,"cal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12256,Deployability,upgrade,upgraded,12256,"ease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Loo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12274,Deployability,install,installed,12274,"ease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Loo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12307,Deployability,upgrade,upgraded,12307,"ease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Loo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12470,Deployability,install,installed,12470,"g dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13080,Deployability,install,installation,13080,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13538,Deployability,configurat,configuration,13538,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13643,Deployability,configurat,configuration,13643,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7756,Integrability,depend,dependency,7756,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7896,Integrability,depend,dependency,7896,"ed pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8030,Integrability,depend,dependency,8030,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8170,Integrability,depend,dependency,8170,".local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8570,Integrability,depend,dependency,8570,"cal/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8710,Integrability,depend,dependency,8710,"C] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8929,Integrability,depend,dependency,8929,"flow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:9069,Integrability,depend,dependency,9069,"ot currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: STDOUT. - 100%[===================>] 3.07K --.-KB/s in 0s . ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11457,Integrability,depend,dependency,11457,"pdates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12121,Integrability,depend,dependency,12121,"com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6274,Modifiability,config,config,6274," echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12678,Modifiability,config,config-,12678," newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were cons",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13538,Modifiability,config,configuration,13538,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13643,Modifiability,config,configuration,13643,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13952,Modifiability,Config,Configuring,13952,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6269,Performance,Load,Load,6269," echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6879,Performance,cache,cached,6879," a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13142,Performance,Perform,Performing,13142,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13186,Performance,Perform,Performing,13186,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10608,Security,secur,security,10608," > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: STDOUT. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11296,Security,secur,security,11296,"https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2238,Testability,test,tests,2238,"ho ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-rep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13153,Testability,Test,Test,13153,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13197,Testability,Test,Test,13197,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:14044,Testability,log,log,14044,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:14100,Testability,log,log,14100,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:27,Usability,guid,guidance,27,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518
https://github.com/google/deepvariant/issues/657#issuecomment-1576305703:160,Deployability,INSTALL,INSTALL,160,"So in `tools/build_build.clif` script, add the following on the [line right above](https://github.com/google/deepvariant/blob/r1.5/tools/build_clif.sh#L146) `./INSTALL.sh`:. ```; sed -i -e 's/LLVM 11.1.0/LLVM 11.0.0/g' clif/cmake/modules/CLIFUtils.cmake; ```. It should look like this:. ```; if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; sed -i -e 's/LLVM 11.1.0/LLVM 11.0.0/g' clif/cmake/modules/CLIFUtils.cmake; ./INSTALL.sh; ```; Then try it running `tools/build_build.clif` again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1576305703
https://github.com/google/deepvariant/issues/657#issuecomment-1576305703:435,Deployability,INSTALL,INSTALL,435,"So in `tools/build_build.clif` script, add the following on the [line right above](https://github.com/google/deepvariant/blob/r1.5/tools/build_clif.sh#L146) `./INSTALL.sh`:. ```; sed -i -e 's/LLVM 11.1.0/LLVM 11.0.0/g' clif/cmake/modules/CLIFUtils.cmake; ```. It should look like this:. ```; if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; sed -i -e 's/LLVM 11.1.0/LLVM 11.0.0/g' clif/cmake/modules/CLIFUtils.cmake; ./INSTALL.sh; ```; Then try it running `tools/build_build.clif` again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1576305703
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:365,Availability,error,error,365,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:505,Availability,error,error,505,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2482,Availability,ERROR,ERROR,2482,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2756,Availability,ERROR,ERROR,2756,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3121,Availability,ERROR,ERROR,3121,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3752,Availability,ERROR,ERROR,3752,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4019,Availability,ERROR,ERROR,4019,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4378,Availability,ERROR,ERROR,4378," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5495,Availability,error,error,5495," This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:6019,Availability,Down,Download,6019,"age 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7602,Availability,down,download,7602," Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7677,Availability,down,downloaded,7677," Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9338,Availability,ERROR,ERROR,9338,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10581,Availability,ERROR,ERROR,10581," > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:14174,Availability,error,error,14174, DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_TF_NUMPY_VERSION=1.19.2; ++ DV_TF_NUMPY_VERSION=1.19.2; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; ++ export PYTHON_VERSION=3.8; ++ PYTHON_VERSION=3.8; +++ which python3.8; ++ export PYTHON_BIN_PATH=/usr/bin/python3.8; ++ PYTHON_BIN_PATH=/usr/bin/python3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; + bazel; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; + PATH=/root/bin:/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:14637,Availability,error,error,14637, DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_TF_NUMPY_VERSION=1.19.2; ++ DV_TF_NUMPY_VERSION=1.19.2; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; ++ export PYTHON_VERSION=3.8; ++ PYTHON_VERSION=3.8; +++ which python3.8; ++ export PYTHON_BIN_PATH=/usr/bin/python3.8; ++ PYTHON_BIN_PATH=/usr/bin/python3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; + bazel; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; + PATH=/root/bin:/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:896,Deployability,Install,Install,896,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1157,Deployability,Update,Update,1157,"${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1255,Deployability,Install,Install,1255,"platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1376,Deployability,Install,Install,1376,"un_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1663,Deployability,Install,Installing,1663," running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1741,Deployability,install,installation,1741," running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1876,Deployability,install,installed,1876," UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2059,Deployability,install,installed,2059,"3:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires nu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2446,Deployability,Install,Install,2446,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2578,Deployability,install,installed,2578,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2852,Deployability,install,installed,2852,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3217,Deployability,install,installed,3217,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3698,Deployability,Install,Installing,3698," numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 whi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3848,Deployability,install,installed,3848,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4115,Deployability,install,installed,4115,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4344,Deployability,Install,Install,4344," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4474,Deployability,install,installed,4474," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5153,Deployability,Update,Update,5153,"ollowing dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5253,Deployability,Install,Install,5253,"tible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Count",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5374,Deployability,Install,Install,5374,"; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), don",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5911,Deployability,Install,Install,5911,"ncompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5955,Deployability,install,installed,5955,"ncompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7303,Deployability,patch,patch-,7303,"100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7335,Deployability,install,installation,7335,"100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7357,Deployability,release,release,7357,"100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7619,Deployability,release,release,7619," Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8665,Deployability,Configurat,Configuration,8665,"ation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8800,Deployability,install,installation,8800,"ault is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9296,Deployability,Install,Installing,9296,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9434,Deployability,install,installed,9434,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9682,Deployability,install,installed,9682,"ed; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10043,Deployability,install,installation,10043,"recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10539,Deployability,Install,Installing,10539," > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10677,Deployability,install,installed,10677," > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10925,Deployability,install,installed,10925,"nv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; ++ PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2495,Integrability,depend,dependency,2495,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2635,Integrability,depend,dependency,2635,"ed pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2769,Integrability,depend,dependency,2769,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2909,Integrability,depend,dependency,2909,".local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3134,Integrability,depend,dependency,3134,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3274,Integrability,depend,dependency,3274,"nstead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3765,Integrability,depend,dependency,3765,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3905,Integrability,depend,dependency,3905,"of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4032,Integrability,depend,dependency,4032,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4172,Integrability,depend,dependency,4172,"not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4391,Integrability,depend,dependency,4391," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4531,Integrability,depend,dependency,4531,"uires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9351,Integrability,depend,dependency,9351,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9491,Integrability,depend,dependency,9491," 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10594,Integrability,depend,dependency,10594," > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10734,Integrability,depend,dependency,10734,"ning pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; ++ PATH=/r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:826,Modifiability,config,config,826,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1013,Modifiability,config,config,1013,"`tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:6032,Modifiability,config,configure,6032,"age 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7179,Modifiability,config,config,7179,"esn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Buil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7186,Modifiability,variab,variable,7186,"esn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Buil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7770,Modifiability,config,config,7770,"around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7861,Modifiability,config,configure,7861," in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pypars",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7914,Modifiability,config,configuring,7914,"to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7986,Modifiability,config,configs,7986,"mits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8037,Modifiability,config,config,8037,"-c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8106,Modifiability,config,config,8106,"ion with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8148,Modifiability,config,config,8148,"ble advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8244,Modifiability,config,config,8244,"8598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8265,Modifiability,Config,Config,8265,"8598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8312,Modifiability,config,config,8312,"bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8356,Modifiability,config,config,8356," TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8444,Modifiability,config,config,8444,"sorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the sourc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8533,Modifiability,config,configs,8533," be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8577,Modifiability,config,config,8577," be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8618,Modifiability,config,config,8618,"Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8665,Modifiability,Config,Configuration,8665,"ation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:821,Performance,Load,Load,821,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1008,Performance,Load,Load,1008,"`tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1618,Performance,cache,cached,1618,"un_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7704,Performance,optimiz,optimization,7704,"around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9243,Performance,cache,cached,9243,"ig=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour wi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10486,Performance,cache,cached,10486,"dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:230,Safety,detect,detected,230,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:14407,Testability,test,test,14407, DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_TF_NUMPY_VERSION=1.19.2; ++ DV_TF_NUMPY_VERSION=1.19.2; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; ++ export PYTHON_VERSION=3.8; ++ PYTHON_VERSION=3.8; +++ which python3.8; ++ export PYTHON_BIN_PATH=/usr/bin/python3.8; ++ PYTHON_BIN_PATH=/usr/bin/python3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; + bazel; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; + PATH=/root/bin:/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7106,Usability,undo,undo,7106,"Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:959,Availability,down,download,959,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1390,Availability,Avail,Available,1390,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:632,Deployability,install,install,632,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:950,Deployability,release,releases,950,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:986,Deployability,install,installer-linux-,986,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1048,Deployability,install,installer-linux-,1048,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1188,Deployability,install,installer-linux-,1188,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1251,Deployability,install,installation,1251,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1337,Deployability,release,release,1337,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1809,Deployability,configurat,configurations,1809,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1809,Modifiability,config,configurations,1809,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1755,Performance,Load,Loads,1755,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1733,Testability,test,test,1733,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:190,Availability,down,download,190,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:532,Availability,error,error,532,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:912,Availability,error,error,912,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:181,Deployability,release,releases,181,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:217,Deployability,install,installer-linux-,217,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:311,Deployability,install,installer-linux-,311,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:380,Deployability,install,installer-linux-,380,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:736,Deployability,install,installer-linux-,736,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:1014,Deployability,install,installer-linux-,1014,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577742056:1132,Deployability,install,installed,1132,@pgrosu Thank you for the reply!; Using the commands:; ```; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; > chmod +x bazel-*.sh; chmod: changing permissions of 'bazel-5.3.0-installer-linux-x86_64.sh': Operation not permitted; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /home/user/bin/bazel: line 220: /home/user/.bazel/bin/bazel-real: Success; > sudo su; > rm .bazelrc; rm: cannot remove '.bazelrc': No such file or directory; > chmod +x bazel-*.sh; > ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; > ./root/.bazel/bin/bazel; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```. so `./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null` in root mode did not have any response.; But it seems that bazel is not installed properly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577742056
https://github.com/google/deepvariant/issues/657#issuecomment-1577767675:159,Availability,down,download,159,"I think it's because it's a Mac M1 which is `aarch64` (`arm64`), so let's try the following one:; ```; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-linux-arm64; ```; Let me know if it runs for you when you execute it via the following:. ```; chmod +x bazel-5.3.0-linux-arm64; ./bazel-5.3.0-linux-arm64; ```. Let me know if that fixes it. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577767675
https://github.com/google/deepvariant/issues/657#issuecomment-1577767675:150,Deployability,release,releases,150,"I think it's because it's a Mac M1 which is `aarch64` (`arm64`), so let's try the following one:; ```; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-linux-arm64; ```; Let me know if it runs for you when you execute it via the following:. ```; chmod +x bazel-5.3.0-linux-arm64; ./bazel-5.3.0-linux-arm64; ```. Let me know if that fixes it. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577767675
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:300,Availability,Avail,Available,300,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:1840,Availability,error,error,1840,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:223,Deployability,install,installation,223,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:247,Deployability,release,release,247,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:719,Deployability,configurat,configurations,719,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:1023,Deployability,install,install,1023,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:1031,Deployability,Install,Installs,1031,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:1150,Integrability,depend,dependency,1150,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:719,Modifiability,config,configurations,719,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:665,Performance,Load,Loads,665,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:643,Testability,test,test,643,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:1302,Testability,test,test,1302,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:1337,Testability,test,test,1337,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:332,Availability,Avail,Available,332,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3508,Availability,ERROR,ERROR,3508,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3782,Availability,ERROR,ERROR,3782,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4147,Availability,ERROR,ERROR,4147,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4778,Availability,ERROR,ERROR,4778,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5045,Availability,ERROR,ERROR,5045,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5404,Availability,ERROR,ERROR,5404," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6725,Availability,Down,Download,6725,"common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=mon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7125,Availability,down,download,7125,":31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7200,Availability,down,downloaded,7200,":31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8861,Availability,ERROR,ERROR,8861,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10104,Availability,ERROR,ERROR,10104,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10903,Availability,error,error,10903,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:279,Deployability,release,release,279,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:751,Deployability,configurat,configurations,751,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1055,Deployability,install,install,1055,"x-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1063,Deployability,Install,Installs,1063,"x-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1922,Deployability,Install,Install,1922,"ndex.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2183,Deployability,Update,Update,2183,"uery Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2281,Deployability,Install,Install,2281,"n the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2402,Deployability,Install,Install,2402,"mmand>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2689,Deployability,Install,Installing,2689," we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2767,Deployability,install,installation,2767," we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2902,Deployability,install,installed,2902," EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3085,Deployability,install,installed,3085,"0:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires nu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3472,Deployability,Install,Install,3472,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3604,Deployability,install,installed,3604,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3878,Deployability,install,installed,3878,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4243,Deployability,install,installed,4243,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4724,Deployability,Install,Installing,4724," numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 whi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4874,Deployability,install,installed,4874,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5141,Deployability,install,installed,5141,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5370,Deployability,Install,Install,5370," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5500,Deployability,install,installed,5500," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6179,Deployability,Update,Update,6179,"ollowing dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang wi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6279,Deployability,Install,Install,6279,"tible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is spe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6400,Deployability,Install,Install,6400,"dency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android build",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6521,Deployability,install,installed,6521,"core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build comman",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6617,Deployability,Install,Install,6617,"core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build comman",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6661,Deployability,install,installed,6661,"core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build comman",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6856,Deployability,patch,patch-,6856,"common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=mon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6886,Deployability,install,installed,6886,".19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dyna",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7142,Deployability,release,release,7142,":31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8188,Deployability,Configurat,Configuration,8188,"ation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8323,Deployability,install,installation,8323,"ault is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8819,Deployability,Install,Installing,8819,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8957,Deployability,install,installed,8957,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:9205,Deployability,install,installed,9205,"ed; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:9566,Deployability,install,installation,9566,"recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10062,Deployability,Install,Installing,10062,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10200,Deployability,install,installed,10200,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10448,Deployability,install,installed,10448,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1182,Integrability,depend,dependency,1182,"it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3521,Integrability,depend,dependency,3521,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3661,Integrability,depend,dependency,3661,"ed pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3795,Integrability,depend,dependency,3795,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3935,Integrability,depend,dependency,3935,".local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4160,Integrability,depend,dependency,4160,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4300,Integrability,depend,dependency,4300,"nstead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4791,Integrability,depend,dependency,4791,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4931,Integrability,depend,dependency,4931,"of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5058,Integrability,depend,dependency,5058,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5198,Integrability,depend,dependency,5198,"not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5417,Integrability,depend,dependency,5417," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5557,Integrability,depend,dependency,5557,"uires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8874,Integrability,depend,dependency,8874,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:9014,Integrability,depend,dependency,9014," 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10117,Integrability,depend,dependency,10117,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10257,Integrability,depend,dependency,10257,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:751,Modifiability,config,configurations,751,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1852,Modifiability,config,config,1852,"repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2039,Modifiability,config,config,2039,"oftware.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6738,Modifiability,config,configure,6738,"common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=mon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7293,Modifiability,config,config,7293,"stall development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7384,Modifiability,config,configure,7384,"23 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pypars",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7437,Modifiability,config,configuring,7437,"e of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7509,Modifiability,config,configs,7509,"; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7560,Modifiability,config,config,7560,"stalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7629,Modifiability,config,config,7629,"g; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7671,Modifiability,config,config,7671,"DT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7767,Modifiability,config,config,7767,"57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7788,Modifiability,Config,Config,7788,"57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7835,Modifiability,config,config,7835,"a21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7879,Modifiability,config,config,7879," TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7967,Modifiability,config,config,7967,"sorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the sourc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8056,Modifiability,config,configs,8056," be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8100,Modifiability,config,config,8100," be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8141,Modifiability,config,config,8141,"Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8188,Modifiability,Config,Configuration,8188,"ation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:697,Performance,Load,Loads,697,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1847,Performance,Load,Load,1847,"repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2034,Performance,Load,Load,2034,"oftware.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2644,Performance,cache,cached,2644,"mmand>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7227,Performance,optimiz,optimization,7227,"stall development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8766,Performance,cache,cached,8766,"ig=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour wi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10009,Performance,cache,cached,10009,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:675,Testability,test,test,675,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1334,Testability,test,test,1334,"> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Callin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1369,Testability,test,test,1369,"> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Callin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053
https://github.com/google/deepvariant/issues/657#issuecomment-1577808511:28,Availability,error,errors,28,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577808511
https://github.com/google/deepvariant/issues/657#issuecomment-1577819231:282,Integrability,wrap,wraps,282,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577819231
https://github.com/google/deepvariant/issues/657#issuecomment-1577819231:337,Modifiability,layers,layers,337,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1577819231
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:153,Availability,Down,Downloaded,153,"So from what I see you've completed the following steps:. 1) Built and installed CLIF; 2) Installed Bazel; 3) Installed the Tensorflow Python Module; 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:71,Deployability,install,installed,71,"So from what I see you've completed the following steps:. 1) Built and installed CLIF; 2) Installed Bazel; 3) Installed the Tensorflow Python Module; 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:90,Deployability,Install,Installed,90,"So from what I see you've completed the following steps:. 1) Built and installed CLIF; 2) Installed Bazel; 3) Installed the Tensorflow Python Module; 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:110,Deployability,Install,Installed,110,"So from what I see you've completed the following steps:. 1) Built and installed CLIF; 2) Installed Bazel; 3) Installed the Tensorflow Python Module; 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1595,Deployability,install,installation,1595," the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```; ...; bazel-bin/deepvariant/postprocess_variants; bazel-bin/deepvariant/postprocess_variants.zip; bazel-bin/deepvariant/runtime_by_region_vis; bazel-bin/deepvariant/runtime_by_region_vis.zip; bazel-bin/deepvariant/show_examples; bazel-bin/deepvariant/show_examples.zip; b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:2155,Deployability,patch,patching,2155,"/deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```; ...; bazel-bin/deepvariant/postprocess_variants; bazel-bin/deepvariant/postprocess_variants.zip; bazel-bin/deepvariant/runtime_by_region_vis; bazel-bin/deepvariant/runtime_by_region_vis.zip; bazel-bin/deepvariant/show_examples; bazel-bin/deepvariant/show_examples.zip; bazel-bin/deepvariant/vcf_stats_report; bazel-bin/deepvariant/vcf_stats_report.zip; (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s; (09:13:54) INFO: 47 processes: 1 internal, 46 local.; (09:13:54) INFO: Build completed successfully, 47 total actions; $; ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```; $ ls bazel-bin/deepvariant/ ; call_variants; call_variants_keras; call_variants_keras.temp; call_variants_keras.zip; call_variants_keras.zip-0.params; call_variants.temp; call_variants.zip; call_variants.zip-0.params; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:168,Modifiability,config,configured,168,"So from what I see you've completed the following steps:. 1) Built and installed CLIF; 2) Installed Bazel; 3) Installed the Tensorflow Python Module; 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1117,Modifiability,config,configure,1117," Python Module; 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1160,Modifiability,config,configure,1160,"and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1649,Modifiability,config,config,1649,"t easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```; ...; bazel-bin/deepvariant/postprocess_variants; bazel-bin/deepvariant/postprocess_variants.zip; bazel-bin/deepvariant/runtime_by_region_vis; bazel-bin/deepvariant/runtime_by_region_vis.zip; bazel-bin/deepvariant/show_examples; bazel-bin/deepvariant/show_examples.zip; bazel-bin/deepvariant/vcf_stats_report; bazel-bin/deepvariant/vcf_stats_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1230,Performance,perform,perform,1230,"sion). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:351,Availability,error,error,351,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:364,Availability,ERROR,ERROR,364,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:374,Availability,error,error,374,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1412,Availability,Error,Error,1412,"t recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1452,Availability,Error,Error,1452,"t recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1481,Availability,Error,Error,1481,"t recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1531,Availability,ERROR,ERROR,1531,"t recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2625,Availability,Error,Error,2625,"t):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2665,Availability,Error,Error,2665,"t):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2694,Availability,Error,Error,2694,"t):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3638,Availability,ERROR,ERROR,3638,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3715,Availability,Error,Error,3715,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:4219,Availability,error,error-free,4219,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:107,Deployability,install,installation,107,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1467,Deployability,Configurat,Configuration,1467,"t recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2680,Deployability,Configurat,Configuration,2680,"t):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3701,Deployability,Configurat,Configuration,3701,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1467,Modifiability,Config,Configuration,1467,"t recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2680,Modifiability,Config,Configuration,2680,"t):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3701,Modifiability,Config,Configuration,3701,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3912,Modifiability,config,configured,3912,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:499,Performance,cache,cache,499,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:735,Performance,cache,cache,735,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:979,Performance,cache,cache,979,"rent folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1228,Performance,cache,cache,1228,"n_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1712,Performance,cache,cache,1712,"3390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1948,Performance,cache,cache,1948,"cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2192,Performance,cache,cache,2192,"ser/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2441,Performance,cache,cache,2441,"thon library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2869,Performance,cache,cache,2869,"ry_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configure",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3010,Performance,cache,cache,3010,""", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3162,Performance,cache,cache,3162,"user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3312,Performance,cache,cache,3312,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3501,Performance,cache,cache,3501,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3891,Performance,load,loaded,3891,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3692,Safety,abort,aborted,3692,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:553,Testability,test,test,553,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:583,Testability,test,test,583,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:622,Testability,test,test,622,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:650,Testability,benchmark,benchmark-test,650,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:691,Testability,test,test,691,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:720,Testability,benchmark,benchmark-test,720,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:749,Testability,test,test,749,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:777,Testability,benchmark,benchmark-test,777,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:826,Testability,test,test,826,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:855,Testability,benchmark,benchmark-test,855,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250
https://github.com/google/deepvariant/issues/657#issuecomment-1622159614:77,Testability,test,tested,77,"Thanks @pgrosu for helping out!. One thing to note is that DeepVariant isn't tested on Mac, and it's not currently something that we officially support. But good to know that there seems to be workarounds. I'll keep this open for a bit longer in case @heznanda wants to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1622159614
https://github.com/google/deepvariant/issues/657#issuecomment-1622187258:224,Deployability,install,installation,224,"Thank you for saying these words, @pichuan. I believe we are at the very last steps of getting the DeepVariant running on Apple silicon, but since the singularity on linux system works for us, I think I will not pursue this installation anymore.; I am grateful to @pgrosu who has been helping me with these 2 options. ; Perhaps this thread could help others who want to pursue the same pathway. Regards,. Hez",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657#issuecomment-1622187258
https://github.com/google/deepvariant/issues/659#issuecomment-1581314748:252,Deployability,pipeline,pipeline,252,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:; 1. Did you get this Nextflow pipeline from somewhere online or write it yourself?; 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this.; 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run.; 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581314748
https://github.com/google/deepvariant/issues/659#issuecomment-1581314748:341,Integrability,message,messages,341,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:; 1. Did you get this Nextflow pipeline from somewhere online or write it yourself?; 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this.; 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run.; 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581314748
https://github.com/google/deepvariant/issues/659#issuecomment-1581314748:355,Integrability,message,message,355,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:; 1. Did you get this Nextflow pipeline from somewhere online or write it yourself?; 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this.; 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run.; 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581314748
https://github.com/google/deepvariant/issues/659#issuecomment-1581314748:337,Testability,log,log,337,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:; 1. Did you get this Nextflow pipeline from somewhere online or write it yourself?; 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this.; 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run.; 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581314748
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:3981,Availability,error,error,3981,Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipel,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4308,Availability,error,error,4308,sion await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clini,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4635,Availability,error,error,4635,md line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4897,Availability,error,error,4897,n below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 20,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:5032,Availability,ERROR,ERROR,5032,/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]; I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 ex,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:5073,Availability,Error,Error,5073, 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]; I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]; I0608 12:15:59.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:5177,Availability,error,error,5177,ocal > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]; I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]; I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 ba,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:9817,Availability,error,error,9817,"21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main; write_variants_to_vcf(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Command error:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]; I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]; I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]; I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]; I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]; I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s; user 294m6.601s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:12658,Availability,error,error,12658,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:60,Deployability,pipeline,pipeline,60,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:1774,Deployability,Pipeline,Pipeline,1774,Provider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor ',MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:1939,Deployability,Pipeline,Pipeline,1939,sion UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session -,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4031,Deployability,Pipeline,Pipeline,4031,Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipel,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4358,Deployability,Pipeline,Pipeline,4358,sion await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clini,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4685,Deployability,Pipeline,Pipeline,4685,md line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4947,Deployability,Pipeline,Pipeline,4947,n below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 20,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:12356,Deployability,Pipeline,Pipeline,12356,"nstructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEB",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:2748,Energy Efficiency,monitor,monitor,2748,red/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1);,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:3780,Energy Efficiency,monitor,monitor,3780,=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.proces,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4107,Energy Efficiency,monitor,monitor,4107,g process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varical,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4434,Energy Efficiency,monitor,monitor,4434,nt:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4761,Energy Efficiency,monitor,monitor,4761,07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:5023,Energy Efficiency,monitor,monitor,5023,/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]; I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 ex,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:12552,Energy Efficiency,monitor,monitor,12552,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:370,Modifiability,plugin,plugin,370,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:377,Modifiability,Plugin,PluginsFacade,377,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:393,Modifiability,Plugin,Plugins,393,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:455,Modifiability,plugin,plugin,455,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:462,Modifiability,Plugin,PluginsFacade,462,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:478,Modifiability,Plugin,Plugins,478,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:2192,Performance,cache,cache,2192, pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunn,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:2198,Performance,Cache,CacheFactory,2198,0; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; J,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:2228,Performance,cache,cache,2228,0; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; J,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:2252,Performance,cache,cache,2252,edBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBU,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:3483,Performance,cache,cache,3483,n-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonit,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:7033,Performance,optimiz,optimized,7033,"300001 examples in 586 batches [0.079 sec per 100]; I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]; I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]; I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s; user 294m6.601s; sys 10m18.739s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002; 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz; 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233; I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes; I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants.; I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF.; I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m840",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:7133,Performance,perform,performance-critical,7133,"300001 examples in 586 batches [0.079 sec per 100]; I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]; I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]; I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s; user 294m6.601s; sys 10m18.739s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002; 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz; 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233; I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes; I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants.; I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF.; I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m840",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:11273,Performance,optimiz,optimized,11273,"1312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]; I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]; I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]; I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s; user 294m6.601s; sys 10m18.739s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:11373,Performance,perform,performance-critical,11373,"1312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]; I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]; I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]; I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s; user 294m6.601s; sys 10m18.739s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:13018,Performance,cache,cachedCount,13018,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:13160,Performance,cache,cachedDuration,13160,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:13179,Performance,load,loadCpus,13179,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:13191,Performance,load,loadMemory,13191,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:13291,Performance,cache,cache,13291,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:13297,Performance,Cache,CacheDB,13297,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:13315,Performance,Cache,CacheDB,13315,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:12594,Safety,abort,aborted,12594,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:13099,Safety,abort,abortedCount,13099,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:121,Testability,log,log,121,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:12493,Usability,resume,resume,12493,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308
https://github.com/google/deepvariant/issues/659#issuecomment-1584844298:24,Availability,error,error,24,"Looks like the relevant error message is:; ```; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz; ```; On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```; mkdir -p /data/shared/clinical/LongRead/Data//Analysis/; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1584844298
https://github.com/google/deepvariant/issues/659#issuecomment-1584844298:30,Integrability,message,message,30,"Looks like the relevant error message is:; ```; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz; ```; On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```; mkdir -p /data/shared/clinical/LongRead/Data//Analysis/; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659#issuecomment-1584844298
https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:1776,Availability,echo,echo,1776,"ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa""; path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:; input:; expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):; return. rule extract_bam:; input:; bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792
https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:2187,Modifiability,config,config,2187,"DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792
https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:2272,Testability,log,log,2272,"Quartet.LCL5.GRCh38.HiFi.minimap2.bam"",; bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792
https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:2331,Testability,log,log,2331,"tet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_shards={threads} '; '--make_examples_extra_args min_mapping_quality=1,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792
https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:2337,Testability,benchmark,benchmark,2337,"tet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_shards={threads} '; '--make_examples_extra_args min_mapping_quality=1,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792
https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:2402,Testability,log,log,2402,"et.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_shards={threads} '; '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '; '--intermediate_results_dir /ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792
https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:3444,Testability,log,log,3444,"t}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_shards={threads} '; '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '; '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'); shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""); shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:; input:; ""{preifx}.bam""; output:; ""{preifx}.bam.bai""; run:; shell(""{samtools} index {input}""); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792
https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:3452,Testability,log,log,3452,"t}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_shards={threads} '; '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '; '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'); shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""); shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:; input:; ""{preifx}.bam""; output:; ""{preifx}.bam.bai""; run:; shell(""{samtools} index {input}""); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792
https://github.com/google/deepvariant/issues/660#issuecomment-1589808839:897,Performance,load,load,897,"Thanks @pgrosu ,. @PengJia6 ,. Here's how my IGV looks like:. <img width=""2546"" alt=""Screenshot 2023-06-13 at 11 11 39 AM"" src=""https://github.com/google/deepvariant/assets/10559039/79761e5e-5781-4094-93cd-477d00b8d445"">. And here are the variant calls with DeepVariant 1.5:; ```bash; chr10	89013075	.	T	TC	43.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:63:35,28:0.444444:43,0,47; chr10	89013076	.	CA	C	47.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:45:63:35,28:0.444444:47,0,49; chr10	89013077	.	A	C	43.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:41:34:0,34:1:43,0,43; ```. This means you need to fix your IGV setup. On IGV version 2.16.1 go to ""View > Preference > Third Gen"" and Uncheck the ""Hide indels < show indels threshold"". It should look like this:; <img width=""896"" alt=""Screenshot 2023-06-13 at 11 15 44 AM"" src=""https://github.com/google/deepvariant/assets/10559039/89f9aa13-9233-4921-9374-9449c6870c73"">. Once done, load your bams one more time and the variants will appear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1589808839
https://github.com/google/deepvariant/issues/660#issuecomment-1590162996:575,Performance,perform,perform,575,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590162996
https://github.com/google/deepvariant/issues/660#issuecomment-1590162996:742,Security,validat,validated,742,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590162996
https://github.com/google/deepvariant/issues/660#issuecomment-1590364272:277,Usability,simpl,simpler,277,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590364272
https://github.com/google/deepvariant/issues/660#issuecomment-1590741602:170,Deployability,rolling,rolling,170,"Hi @PengJia6 . Please see ; ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590741602
https://github.com/google/deepvariant/issues/660#issuecomment-1590741602:849,Deployability,release,release,849,"Hi @PengJia6 . Please see ; ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590741602
https://github.com/google/deepvariant/issues/660#issuecomment-1590741602:1432,Deployability,update,update,1432,"Hi @PengJia6 . Please see ; ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590741602
https://github.com/google/deepvariant/issues/660#issuecomment-1590741602:1561,Energy Efficiency,efficient,efficiently,1561,"Hi @PengJia6 . Please see ; ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590741602
https://github.com/google/deepvariant/issues/660#issuecomment-1590833828:291,Availability,down,down,291,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828
https://github.com/google/deepvariant/issues/660#issuecomment-1590833828:600,Deployability,update,updated,600,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828
https://github.com/google/deepvariant/issues/660#issuecomment-1590833828:207,Performance,perform,perform,207,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828
https://github.com/google/deepvariant/issues/660#issuecomment-1590833828:142,Usability,simpl,simple,142,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828
https://github.com/google/deepvariant/issues/660#issuecomment-1591182902:352,Deployability,upgrade,upgrade,352,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)?. Best! ; Peng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1591182902
https://github.com/google/deepvariant/issues/660#issuecomment-1591182902:399,Deployability,release,release,399,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)?. Best! ; Peng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1591182902
https://github.com/google/deepvariant/issues/660#issuecomment-1591182902:50,Usability,clear,clear,50,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)?. Best! ; Peng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1591182902
https://github.com/google/deepvariant/issues/660#issuecomment-1594251244:119,Modifiability,extend,extended,119,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1594251244
https://github.com/google/deepvariant/issues/660#issuecomment-1594251244:440,Modifiability,extend,extending,440,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660#issuecomment-1594251244
https://github.com/google/deepvariant/issues/661#issuecomment-1591660025:282,Deployability,release,release,282,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1591660025
https://github.com/google/deepvariant/issues/661#issuecomment-1591660025:407,Modifiability,extend,extended,407,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1591660025
https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:307,Availability,error,errors,307,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772
https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:1045,Deployability,release,release,1045,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772
https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:633,Integrability,Message,Message,633,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772
https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:1622,Integrability,Message,Message,1622,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772
https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:1176,Modifiability,extend,extended,1176,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772
https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:149,Performance,perform,perform,149,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772
https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:602,Security,secur,secure,602,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772
https://github.com/google/deepvariant/issues/661#issuecomment-1593485538:636,Deployability,release,release,636,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------; On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538
https://github.com/google/deepvariant/issues/661#issuecomment-1593485538:253,Integrability,Message,Message,253,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------; On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538
https://github.com/google/deepvariant/issues/661#issuecomment-1593485538:1213,Integrability,Message,Message,1213,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------; On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538
https://github.com/google/deepvariant/issues/661#issuecomment-1593485538:767,Modifiability,extend,extended,767,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------; On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538
https://github.com/google/deepvariant/issues/661#issuecomment-1593485538:35,Performance,perform,performance,35,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------; On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538
https://github.com/google/deepvariant/issues/661#issuecomment-1594084840:405,Availability,avail,available,405,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1594084840
https://github.com/google/deepvariant/issues/661#issuecomment-1594153599:828,Availability,avail,available,828,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------; On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),; >; > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it.; >; > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions.; >; > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results.; >; > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: **",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1594153599
https://github.com/google/deepvariant/issues/661#issuecomment-1594153599:317,Integrability,Message,Message,317,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------; On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),; >; > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it.; >; > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions.; >; > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results.; >; > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: **",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1594153599
https://github.com/google/deepvariant/issues/661#issuecomment-1594153599:1987,Integrability,Message,Message,1987,"is issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------; On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),; >; > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it.; >; > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions.; >; > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results.; >; > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.; >; > ; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661#issuecomment-1594153599
https://github.com/google/deepvariant/issues/664#issuecomment-1593775230:40,Availability,error,error,40,"The bash script does bypass the python3 error, but then other missing package errors appear:; ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils'; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593775230
https://github.com/google/deepvariant/issues/664#issuecomment-1593775230:78,Availability,error,errors,78,"The bash script does bypass the python3 error, but then other missing package errors appear:; ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils'; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593775230
https://github.com/google/deepvariant/issues/664#issuecomment-1593791623:73,Deployability,install,install,73,Can you confirm that you had activated the Conda environment you used to install DeepVariant when you ran that command?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593791623
https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:6657,Integrability,wrap,wrapt,6657,l 5.32.1 2_h7f98852_perl5 conda-forge; pip 21.3.1 pyhd8ed1ab_0 conda-forge; protobuf 3.18.0 py36hc4f0c31_0 conda-forge; psutil 5.8.0 py36h8f6f2f9_1 conda-forge; pyasn1 0.4.8 py_0 conda-forge; pyasn1-modules 0.2.7 py_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge; pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge; pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge; pysocks 1.7.1 py36h5fab9bb_3 conda-forge; python 3.6.15 hb7a2778_0_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python_abi 3.6 2_cp36m conda-forge; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; requests 2.28.1 pyhd8ed1ab_0 conda-forge; requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge; rsa 4.9 pyhd8ed1ab_0 conda-forge; scipy 1.5.3 py36h9e8f40b_0 conda-forge; setuptools 58.0.4 py36h5fab9bb_2 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge; sqlite 3.42.0 h2c6b66d_0 conda-forge; tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge; tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge; tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge; tensorflow 2.0.0 gpu_py36h6b29c10_0 ; tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 ; tensorflow-estimator 2.0.0 pyh2649769_0 ; tensorflow-gpu 2.0.0 h0d30ee6_0 ; termcolor 1.1.0 pyhd8ed1ab_3 conda-forge; tk 8.6.12 h27826a3_0 conda-forge; toolz 0.12.0 pyhd8ed1ab_0 conda-forge; typing-extensions 4.1.1 hd8ed1ab_0 conda-forge; typing_extensions 4.1.1 pyha770c72_0 conda-forge; unzip 6.0 h7f98852_3 conda-forge; urllib3 1.26.15 pyhd8ed1ab_0 conda-forge; werkzeug 0.16.1 py_0 conda-forge; wheel 0.37.1 pyhd8ed1ab_0 conda-forge; wrapt 1.13.1 py36h8f6f2f9_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; yarl 1.6.3 py36h8f6f2f9_2 conda-forge; zipp 3.6.0 pyhd8ed1ab_0 conda-forge; zlib 1.2.13 hd590300_5 conda-forge; zstd 1.4.9 ha95c52a_0 conda-forge; (dv) dpipe@4de3e1b4384c:/app/dpipe$ ; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053
https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:6095,Modifiability,plugin,plugin-wit,6095,l 5.32.1 2_h7f98852_perl5 conda-forge; pip 21.3.1 pyhd8ed1ab_0 conda-forge; protobuf 3.18.0 py36hc4f0c31_0 conda-forge; psutil 5.8.0 py36h8f6f2f9_1 conda-forge; pyasn1 0.4.8 py_0 conda-forge; pyasn1-modules 0.2.7 py_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge; pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge; pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge; pysocks 1.7.1 py36h5fab9bb_3 conda-forge; python 3.6.15 hb7a2778_0_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python_abi 3.6 2_cp36m conda-forge; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; requests 2.28.1 pyhd8ed1ab_0 conda-forge; requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge; rsa 4.9 pyhd8ed1ab_0 conda-forge; scipy 1.5.3 py36h9e8f40b_0 conda-forge; setuptools 58.0.4 py36h5fab9bb_2 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge; sqlite 3.42.0 h2c6b66d_0 conda-forge; tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge; tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge; tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge; tensorflow 2.0.0 gpu_py36h6b29c10_0 ; tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 ; tensorflow-estimator 2.0.0 pyh2649769_0 ; tensorflow-gpu 2.0.0 h0d30ee6_0 ; termcolor 1.1.0 pyhd8ed1ab_3 conda-forge; tk 8.6.12 h27826a3_0 conda-forge; toolz 0.12.0 pyhd8ed1ab_0 conda-forge; typing-extensions 4.1.1 hd8ed1ab_0 conda-forge; typing_extensions 4.1.1 pyha770c72_0 conda-forge; unzip 6.0 h7f98852_3 conda-forge; urllib3 1.26.15 pyhd8ed1ab_0 conda-forge; werkzeug 0.16.1 py_0 conda-forge; wheel 0.37.1 pyhd8ed1ab_0 conda-forge; wrapt 1.13.1 py36h8f6f2f9_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; yarl 1.6.3 py36h8f6f2f9_2 conda-forge; zipp 3.6.0 pyhd8ed1ab_0 conda-forge; zlib 1.2.13 hd590300_5 conda-forge; zstd 1.4.9 ha95c52a_0 conda-forge; (dv) dpipe@4de3e1b4384c:/app/dpipe$ ; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053
https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:1804,Performance,cache,cached-property,1804,"eepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils'; (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv; List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel ; ; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; _tflow_select 2.1.0 gpu ; absl-py 0.15.0 pyhd8ed1ab_0 conda-forge; aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge; altair 4.2.0 pyhd8ed1ab_0 conda-forge; astor 0.8.1 pyh9f0ad1d_0 conda-forge; async-timeout 3.0.1 py_1000 conda-forge; attrs 22.2.0 pyh71513ae_0 conda-forge; blinker 1.5 pyhd8ed1ab_0 conda-forge; boost 1.75.0 py36h355b2fd_0 conda-forge; boost-cpp 1.75.0 hc6e9bd1_0 conda-forge; brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.5.7 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; cachetools 5.0.0 pyhd8ed1ab_0 conda-forge; certifi 2021.5.30 py36h5fab9bb_0 conda-forge; cffi 1.14.6 py36hd8eec40_1 conda-forge; chardet 4.0.0 py36h5fab9bb_1 conda-forge; charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge; click 8.0.1 py36h5fab9bb_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; crcmod 1.7 py36h8f6f2f9_1006 conda-forge; cryptography 35.0.0 py36hb60f036_0 conda-forge; cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge; cudnn 7.6.5.32 ha8d7eb6_1 conda-forge; cupti 10.0.130 0 ; curl 7.87.0 h6312ad2_0 conda-forge; deepvariant 1.5.0 py36hf3e76ba_0 bioconda ; entrypoints 0.4 pyhd8ed1ab_0 conda-forge; enum34 1.1.10 py36h9f0ad1d_2 conda-forge; gast 0.2.2 py_0 conda-forge; google-auth 2.20.0 pyh1a96a4e_0 conda-forge; google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge; google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge; google-pasta 0.2.0 pyh8c360ce_0 conda-forge; grpcio 1.38.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053
https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:1898,Performance,cache,cachetools,1898,"th; ModuleNotFoundError: No module named 'etils'; (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv; List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel ; ; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; _tflow_select 2.1.0 gpu ; absl-py 0.15.0 pyhd8ed1ab_0 conda-forge; aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge; altair 4.2.0 pyhd8ed1ab_0 conda-forge; astor 0.8.1 pyh9f0ad1d_0 conda-forge; async-timeout 3.0.1 py_1000 conda-forge; attrs 22.2.0 pyh71513ae_0 conda-forge; blinker 1.5 pyhd8ed1ab_0 conda-forge; boost 1.75.0 py36h355b2fd_0 conda-forge; boost-cpp 1.75.0 hc6e9bd1_0 conda-forge; brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.5.7 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; cachetools 5.0.0 pyhd8ed1ab_0 conda-forge; certifi 2021.5.30 py36h5fab9bb_0 conda-forge; cffi 1.14.6 py36hd8eec40_1 conda-forge; chardet 4.0.0 py36h5fab9bb_1 conda-forge; charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge; click 8.0.1 py36h5fab9bb_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; crcmod 1.7 py36h8f6f2f9_1006 conda-forge; cryptography 35.0.0 py36hb60f036_0 conda-forge; cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge; cudnn 7.6.5.32 ha8d7eb6_1 conda-forge; cupti 10.0.130 0 ; curl 7.87.0 h6312ad2_0 conda-forge; deepvariant 1.5.0 py36hf3e76ba_0 bioconda ; entrypoints 0.4 pyhd8ed1ab_0 conda-forge; enum34 1.1.10 py36h9f0ad1d_2 conda-forge; gast 0.2.2 py_0 conda-forge; google-auth 2.20.0 pyh1a96a4e_0 conda-forge; google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge; google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge; google-pasta 0.2.0 pyh8c360ce_0 conda-forge; grpcio 1.38.1 py36h8e87921_0 conda-forge; h5py 3.1.0 nompi_py36hc1bc4f5_100 conda-forge; hdf5 1.10.6 no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053
https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:1441,Safety,timeout,timeout,1441,"rence/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils'; (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv; List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel ; ; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; _tflow_select 2.1.0 gpu ; absl-py 0.15.0 pyhd8ed1ab_0 conda-forge; aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge; altair 4.2.0 pyhd8ed1ab_0 conda-forge; astor 0.8.1 pyh9f0ad1d_0 conda-forge; async-timeout 3.0.1 py_1000 conda-forge; attrs 22.2.0 pyh71513ae_0 conda-forge; blinker 1.5 pyhd8ed1ab_0 conda-forge; boost 1.75.0 py36h355b2fd_0 conda-forge; boost-cpp 1.75.0 hc6e9bd1_0 conda-forge; brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.5.7 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; cachetools 5.0.0 pyhd8ed1ab_0 conda-forge; certifi 2021.5.30 py36h5fab9bb_0 conda-forge; cffi 1.14.6 py36hd8eec40_1 conda-forge; chardet 4.0.0 py36h5fab9bb_1 conda-forge; charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge; click 8.0.1 py36h5fab9bb_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; crcmod 1.7 py36h8f6f2f9_1006 conda-forge; cryptography 35.0.0 py36hb60f036_0 conda-forge; cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge; cudnn 7.6.5.32 ha8d7eb6_1 conda-forge; cupti 10.0.130 0 ; curl 7.87.0 h6312ad2_0 conda-f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053
https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:1758,Security,certificate,certificates,1758,"mp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils'; (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv; List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel ; ; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; _tflow_select 2.1.0 gpu ; absl-py 0.15.0 pyhd8ed1ab_0 conda-forge; aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge; altair 4.2.0 pyhd8ed1ab_0 conda-forge; astor 0.8.1 pyh9f0ad1d_0 conda-forge; async-timeout 3.0.1 py_1000 conda-forge; attrs 22.2.0 pyh71513ae_0 conda-forge; blinker 1.5 pyhd8ed1ab_0 conda-forge; boost 1.75.0 py36h355b2fd_0 conda-forge; boost-cpp 1.75.0 hc6e9bd1_0 conda-forge; brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.5.7 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; cachetools 5.0.0 pyhd8ed1ab_0 conda-forge; certifi 2021.5.30 py36h5fab9bb_0 conda-forge; cffi 1.14.6 py36hd8eec40_1 conda-forge; chardet 4.0.0 py36h5fab9bb_1 conda-forge; charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge; click 8.0.1 py36h5fab9bb_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; crcmod 1.7 py36h8f6f2f9_1006 conda-forge; cryptography 35.0.0 py36hb60f036_0 conda-forge; cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge; cudnn 7.6.5.32 ha8d7eb6_1 conda-forge; cupti 10.0.130 0 ; curl 7.87.0 h6312ad2_0 conda-forge; deepvariant 1.5.0 py36hf3e76ba_0 bioconda ; entrypoints 0.4 pyhd8ed1ab_0 conda-forge; enum34 1.1.10 py36h9f0ad1d_2 conda-forge; gast 0.2.2 py_0 conda-forge; google-auth 2.20.0 pyh1a96a4e_0 conda-forge; google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge; google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge; google-pas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053
https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:4475,Testability,mock,mock,4475,impl_linux-64 2.40 h41732ed_0 conda-forge; libblas 3.9.0 17_linux64_openblas conda-forge; libcblas 3.9.0 17_linux64_openblas conda-forge; libcurl 7.87.0 h6312ad2_0 conda-forge; libdeflate 1.18 h0b41bf4_0 conda-forge; libedit 3.1.20191231 he28a2e2_2 conda-forge; libev 4.33 h516909a_1 conda-forge; libffi 3.4.2 h7f98852_5 conda-forge; libgcc-ng 13.1.0 he5830b7_0 conda-forge; libgfortran-ng 13.1.0 h69a702a_0 conda-forge; libgfortran5 13.1.0 h15d22d2_0 conda-forge; libgomp 13.1.0 he5830b7_0 conda-forge; liblapack 3.9.0 17_linux64_openblas conda-forge; libnghttp2 1.51.0 hdcd2b5c_0 conda-forge; libnsl 2.0.0 h7f98852_0 conda-forge; libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge; libprotobuf 3.18.0 h780b84a_1 conda-forge; libsqlite 3.42.0 h2797004_0 conda-forge; libssh2 1.10.0 haa6b8db_3 conda-forge; libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge; libzlib 1.2.13 hd590300_5 conda-forge; lz4-c 1.9.3 h9c3ff4c_1 conda-forge; markdown 3.4.3 pyhd8ed1ab_0 conda-forge; markupsafe 2.0.1 py36h8f6f2f9_0 conda-forge; mock 5.0.2 pyhd8ed1ab_0 conda-forge; multidict 5.2.0 py36h8f6f2f9_0 conda-forge; ncurses 6.4 hcb278e6_0 conda-forge; numpy 1.16.6 py36h2aa4a07_0 conda-forge; oauth2client 4.1.3 py_0 conda-forge; oauthlib 3.2.2 pyhd8ed1ab_0 conda-forge; openjdk 8.0.332 h166bdaf_0 conda-forge; openssl 1.1.1u hd590300_0 conda-forge; opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge; pandas 1.1.5 py36h284efc9_0 conda-forge; parallel 20230522 ha770c72_0 conda-forge; perl 5.32.1 2_h7f98852_perl5 conda-forge; pip 21.3.1 pyhd8ed1ab_0 conda-forge; protobuf 3.18.0 py36hc4f0c31_0 conda-forge; psutil 5.8.0 py36h8f6f2f9_1 conda-forge; pyasn1 0.4.8 py_0 conda-forge; pyasn1-modules 0.2.7 py_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge; pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge; pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge; pysocks 1.7.1 py36h5fab9bb_3 conda-forge; python 3.6.15 hb7a2778_0_cpython conda-forge; python-dateutil ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053
https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:1052,Deployability,install,installed,1052,"ment python; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; /opt/conda/envs/dv/bin/python3; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>; import dataclasses; ModuleNotFoundError: No module named 'dataclasses'; ```; Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc.; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version; Python 3.6.15; (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python; /opt/conda/envs/dv/bin/python; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python; Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) ; [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import dataclasses; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ModuleNotFoundError: No module named 'dataclasses'; >>> import tensorflow; >>> quit(); (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze; absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work; aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work; altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work; astor @ file:///home/conda/feedstock_root/build_artifacts/astor_159361046425",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553
https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:1039,Integrability,depend,dependencies,1039,"ment python; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; /opt/conda/envs/dv/bin/python3; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>; import dataclasses; ModuleNotFoundError: No module named 'dataclasses'; ```; Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc.; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version; Python 3.6.15; (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python; /opt/conda/envs/dv/bin/python; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python; Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) ; [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import dataclasses; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ModuleNotFoundError: No module named 'dataclasses'; >>> import tensorflow; >>> quit(); (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze; absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work; aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work; altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work; astor @ file:///home/conda/feedstock_root/build_artifacts/astor_159361046425",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553
https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:7621,Integrability,wrap,wrapt,7621,86286081/work; pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work; pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work; requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work; requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work; rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work; scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work; six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work; sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work; tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl; tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl; tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl; tensorflow==2.0.0; tensorflow-estimator==2.0.0; termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work; tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d; toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work; typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work; urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work; Werkzeug==0.16.1; wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work; yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work; zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553
https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:6928,Modifiability,plugin,plugin-wit,6928,86286081/work; pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work; pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work; requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work; requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work; rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work; scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work; six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work; sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work; tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl; tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl; tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl; tensorflow==2.0.0; tensorflow-estimator==2.0.0; termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work; tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d; toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work; typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work; urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work; Werkzeug==0.16.1; wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work; yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work; zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553
https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:7003,Modifiability,plugin,plugin-,7003,86286081/work; pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work; pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work; requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work; requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work; rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work; scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work; six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work; sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work; tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl; tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl; tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl; tensorflow==2.0.0; tensorflow-estimator==2.0.0; termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work; tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d; toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work; typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work; urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work; Werkzeug==0.16.1; wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work; yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work; zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553
https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:2278,Performance,cache,cached-property,2278," 18:49:41) ; [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import dataclasses; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ModuleNotFoundError: No module named 'dataclasses'; >>> import tensorflow; >>> quit(); (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze; absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work; aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work; altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work; astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work; async-timeout==3.0.1; attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work; blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work; brotlipy==0.7.0; cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work; cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work; certifi==2021.5.30; cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work; chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work; charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work; click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work; contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work; crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work; cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work; entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work; gast==0.2.2; google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work; google-auth-oauthlib @ file:///home/co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553
https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:2382,Performance,cache,cachetools,2382," 18:49:41) ; [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import dataclasses; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ModuleNotFoundError: No module named 'dataclasses'; >>> import tensorflow; >>> quit(); (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze; absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work; aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work; altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work; astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work; async-timeout==3.0.1; attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work; blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work; brotlipy==0.7.0; cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work; cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work; certifi==2021.5.30; cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work; chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work; charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work; click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work; contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work; crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work; cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work; entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work; gast==0.2.2; google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work; google-auth-oauthlib @ file:///home/co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553
https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:2073,Safety,timeout,timeout,2073,"18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>; import dataclasses; ModuleNotFoundError: No module named 'dataclasses'; ```; Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc.; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version; Python 3.6.15; (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python; /opt/conda/envs/dv/bin/python; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python; Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) ; [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import dataclasses; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ModuleNotFoundError: No module named 'dataclasses'; >>> import tensorflow; >>> quit(); (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze; absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work; aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work; altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work; astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work; async-timeout==3.0.1; attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work; blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work; brotlipy==0.7.0; cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work; cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work; certifi==2021.5.30; cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work; chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work; charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work; click @ fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553
https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:4621,Testability,mock,mock,4621,uild_artifacts/h5py_1604753633596/work; httplib2 @ file:///home/conda/feedstock_root/build_artifacts/httplib2_1679483503307/work; idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work; idna-ssl @ file:///home/conda/feedstock_root/build_artifacts/idna_ssl_1636483491140/work; importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1630267465156/work; intervaltree @ file:///home/conda/feedstock_root/build_artifacts/intervaltree_1683532206518/work; Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1636510082894/work; jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema_1634752161479/work; Keras-Applications==1.0.8; Keras-Preprocessing @ file:///home/conda/feedstock_root/build_artifacts/keras-preprocessing_1610713559828/work; Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1679584000376/work; MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1621455668064/work; mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1681654098624/work; multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1633329770033/work; numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1607958944856/work; oauth2client==4.1.3; oauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1666056362788/work; opt-einsum @ file:///home/conda/feedstock_root/build_artifacts/opt_einsum_1617859230218/work; pandas==1.1.5; protobuf==3.18.0; psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1610127101219/work; pyasn1==0.4.8; pyasn1-modules==0.2.7; pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work; PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1683676063469/work; pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1663846997386/work; pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work; pyrsiste,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553
https://github.com/google/deepvariant/issues/664#issuecomment-1593865376:35,Deployability,upgrade,upgrade,35,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python?. I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593865376
https://github.com/google/deepvariant/issues/664#issuecomment-1593865376:195,Deployability,install,install,195,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python?. I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1593865376
https://github.com/google/deepvariant/issues/664#issuecomment-1595877014:352,Deployability,release,release,352,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```; cat /etc/lsb-release; cat /etc/redhat-release; cat /etc/os-release; ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case?. 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1595877014
https://github.com/google/deepvariant/issues/664#issuecomment-1595877014:377,Deployability,release,release,377,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```; cat /etc/lsb-release; cat /etc/redhat-release; cat /etc/os-release; ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case?. 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1595877014
https://github.com/google/deepvariant/issues/664#issuecomment-1595877014:398,Deployability,release,release,398,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```; cat /etc/lsb-release; cat /etc/redhat-release; cat /etc/os-release; ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case?. 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1595877014
https://github.com/google/deepvariant/issues/664#issuecomment-1595877014:541,Security,access,access,541,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```; cat /etc/lsb-release; cat /etc/redhat-release; cat /etc/os-release; ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case?. 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664#issuecomment-1595877014
https://github.com/google/deepvariant/issues/666#issuecomment-1600094995:612,Availability,error,error,612,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):; File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000; Fatal Python error: Aborted. ```; Current thread 0x00007fe7f6689740 (most recent call first):; File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region; File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region; File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1600094995
https://github.com/google/deepvariant/issues/666#issuecomment-1600094995:619,Safety,Abort,Aborted,619,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):; File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000; Fatal Python error: Aborted. ```; Current thread 0x00007fe7f6689740 (most recent call first):; File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region; File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region; File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1600094995
https://github.com/google/deepvariant/issues/666#issuecomment-1600293071:175,Availability,error,error,175,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```; samtools view -H GFX.bam | grep @SQ; ```. You should see something like this for the same naming convention for the `SN` field:. ```; @SQ SN:chr1 LN:249250621; @SQ SN:chr2 LN:243199373; @SQ SN:chr3 LN:198022430; @SQ SN:chr4 LN:191154276; @SQ SN:chr5 LN:180915260; @SQ SN:chr6 LN:171115067; @SQ SN:chr7 LN:159138663; @SQ SN:chr8 LN:146364022; ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant.; ; Let me know if there is something I should expand on. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1600293071
https://github.com/google/deepvariant/issues/666#issuecomment-1600293071:377,Availability,error,error,377,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```; samtools view -H GFX.bam | grep @SQ; ```. You should see something like this for the same naming convention for the `SN` field:. ```; @SQ SN:chr1 LN:249250621; @SQ SN:chr2 LN:243199373; @SQ SN:chr3 LN:198022430; @SQ SN:chr4 LN:191154276; @SQ SN:chr5 LN:180915260; @SQ SN:chr6 LN:171115067; @SQ SN:chr7 LN:159138663; @SQ SN:chr8 LN:146364022; ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant.; ; Let me know if there is something I should expand on. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1600293071
https://github.com/google/deepvariant/issues/666#issuecomment-1600858931:286,Deployability,pipeline,pipeline,286,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap?. 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here?. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX; 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36; 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18; 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30; 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16; 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32; --- ; 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0; 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0; 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0; 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74; 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0; ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1600858931
https://github.com/google/deepvariant/issues/666#issuecomment-1600858931:434,Deployability,pipeline,pipeline,434,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap?. 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here?. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX; 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36; 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18; 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30; 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16; 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32; --- ; 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0; 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0; 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0; 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74; 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0; ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1600858931
https://github.com/google/deepvariant/issues/666#issuecomment-1600858931:650,Deployability,pipeline,pipeline,650,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap?. 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here?. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX; 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36; 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18; 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30; 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16; 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32; --- ; 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0; 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0; 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0; 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74; 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0; ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1600858931
https://github.com/google/deepvariant/issues/666#issuecomment-1601744318:631,Deployability,update,update,631,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318
https://github.com/google/deepvariant/issues/666#issuecomment-1601744318:1149,Safety,predict,predicted,1149,"rnally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318
https://github.com/google/deepvariant/issues/666#issuecomment-1601744318:1317,Safety,predict,predictions,1317,"ob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which is equivalent to ./. genotype. This is clarified in the [`genotype field under the VariantCall`](https://github.com/googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318
https://github.com/google/deepvariant/issues/666#issuecomment-1601744318:616,Usability,guid,guide,616,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318
https://github.com/google/deepvariant/issues/666#issuecomment-1602118672:1183,Availability,reliab,reliable,1183,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1602118672
https://github.com/google/deepvariant/issues/666#issuecomment-1602118672:1651,Testability,log,logic,1651,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1602118672
https://github.com/google/deepvariant/issues/666#issuecomment-1602118672:1146,Usability,learn,learn,1146,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1602118672
https://github.com/google/deepvariant/issues/666#issuecomment-1604221424:2380,Modifiability,layers,layers,2380," these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1604221424
https://github.com/google/deepvariant/issues/666#issuecomment-1604221424:2680,Security,validat,validated,2680," have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1604221424
https://github.com/google/deepvariant/issues/666#issuecomment-1610105430:1344,Availability,down,downstream,1344,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants.; You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194; which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. ; This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1610105430
https://github.com/google/deepvariant/issues/666#issuecomment-1610105430:772,Usability,learn,learn,772,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants.; You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194; which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. ; This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1610105430
https://github.com/google/deepvariant/issues/666#issuecomment-1611159358:418,Availability,down,downstream-analysis,418,Yes. This helps and from a model perspective this makes sense to add every site to the VCF that is a potential candidate for a variant. The last question I might raise is: What is your intention to have those positions added to the VCF by default and not as a opt-in option. To me it sound rather counterintuitive because I would argue that the majority of use-cases do not need those positions and/or removed them in downstream-analysis. But ofc this is you decision and just my view on the overall concept and I might be wrong here! . Thanks for all you help and patience so far!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1611159358
https://github.com/google/deepvariant/issues/666#issuecomment-1611797347:260,Modifiability,variab,variable,260,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1611797347
https://github.com/google/deepvariant/issues/666#issuecomment-1611797347:770,Usability,simpl,simpler,770,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1611797347
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:3761,Energy Efficiency,power,power,3761,"at might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:5196,Energy Efficiency,power,power,5196,"s to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:5718,Energy Efficiency,power,power,5718,"s to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:3370,Modifiability,layers,layers,3370,"iant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:3781,Modifiability,layers,layers,3781,"at might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:5564,Performance,optimiz,optimize,5564,"s to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:854,Safety,detect,detect,854,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:1112,Safety,detect,detecting,1112,"way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:2804,Security,expose,expose,2804,"ollapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:4291,Testability,test,testing,4291,"these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:4628,Testability,test,test,4628," with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:4777,Testability,test,test,4777,"s to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:4917,Testability,test,test,4917,"s to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:294,Usability,simpl,simpler,294,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:496,Usability,simpl,simplified,496,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088
https://github.com/google/deepvariant/issues/666#issuecomment-1612713066:772,Usability,simpl,simply,772,@AndrewCarroll and @pgrosu thanks for your view on things and @pgrosu especially for your fantastic explanation of the model!. From a data scientist perspective this makes totally sense. I fully agree that one should always gather as much data as possible since one can always remove data that seems to be not useful afterwards. My intention was not to say that one do not need those data or that you should not use the information which those HomRef sites emerged from. I talked to some of my colleagues yesterday and they agreed that they would expect a variant caller to produce variant calls and information/statistics related to those and all further information would be opt-in since they would have no need for this. But my team could just be the minority here and simply an opt-out option for this behavior would be highly recommended!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1612713066
https://github.com/google/deepvariant/issues/666#issuecomment-1613234641:6,Availability,down,downstream,6,"In my downstream analysis with WhatsHap today I found that the VCF contains only 3,889,968. The data comes from PacBio CCS 10x sequencing and I wonder if this can be the correct number of SNPs for this method. E.g. I analyzed several WGS 30x datasets from Nebula so far and spotted amounts of SNPs in the range of 4.6-4.8 million. Is there anything that I missed here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1613234641
https://github.com/google/deepvariant/issues/666#issuecomment-1613598284:1311,Deployability,pipeline,pipeline,1311,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1613598284
https://github.com/google/deepvariant/issues/666#issuecomment-1614775729:1012,Safety,detect,detection,1012,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1614775729
https://github.com/google/deepvariant/issues/666#issuecomment-1614775729:255,Testability,benchmark,benchmarked,255,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1614775729
https://github.com/google/deepvariant/issues/666#issuecomment-1614995973:527,Availability,error,errors,527,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1614995973
https://github.com/google/deepvariant/issues/666#issuecomment-1615137619:231,Usability,simpl,simply,231,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1615137619
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:133,Availability,resilien,resiliency,133,"Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:907,Availability,resilien,resiliency,907,"Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:2132,Availability,resilien,resiliency,2132,"://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:1764,Deployability,update,update,1764,"://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:2019,Performance,perform,performed,2019,"://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:277,Testability,log,logits,277,"Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:355,Testability,test,test,355,"Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:449,Testability,test,test,449,"Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:543,Testability,test,test,543,"Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:887,Testability,test,testing,887,"Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:1239,Testability,test,tested,1239,"derlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels bein",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:1436,Testability,test,test,1436,"://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:1606,Testability,test,test,1606,"://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:1696,Testability,test,testing,1696,"://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040
https://github.com/google/deepvariant/issues/666#issuecomment-1636726993:130,Modifiability,layers,layers,130,"Hi Nils,. Glad to hear it was helpful, and I enjoyed it as well! Yeah the project is pretty amazing $`-`$ it's incredible how the layers perform specific segmentations and amplifications: . ![image](https://github.com/google/deepvariant/assets/6555937/331f1ab9-155b-4aa5-98ca-4b08e40fe8cd). Thank you and hope your research is also going well!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1636726993
https://github.com/google/deepvariant/issues/666#issuecomment-1636726993:137,Performance,perform,perform,137,"Hi Nils,. Glad to hear it was helpful, and I enjoyed it as well! Yeah the project is pretty amazing $`-`$ it's incredible how the layers perform specific segmentations and amplifications: . ![image](https://github.com/google/deepvariant/assets/6555937/331f1ab9-155b-4aa5-98ca-4b08e40fe8cd). Thank you and hope your research is also going well!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666#issuecomment-1636726993
https://github.com/google/deepvariant/issues/668#issuecomment-1602330299:195,Safety,timeout,timeout,195,"@George-du That's a connection issue with Docker's CDN and/or Docker the way it operates on your side. Here's a link to possible solutions:. https://forums.docker.com/t/pulling-docker-images-i-o-timeout/740/13. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668#issuecomment-1602330299
https://github.com/google/deepvariant/issues/669#issuecomment-1601287727:97,Deployability,install,install,97,"Hi @gambalab . We'll discuss this in the team today. We don't make or directly control the conda install for DeepVariant, that is done by external people. We'll try to assess our ability to debug or offer suggestions for conda issues like this today, but it might be a bit before we have a suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1601287727
https://github.com/google/deepvariant/issues/669#issuecomment-1602257279:72,Deployability,install,install,72,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1602257279
https://github.com/google/deepvariant/issues/669#issuecomment-1602257279:11,Integrability,depend,dependency,11,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1602257279
https://github.com/google/deepvariant/issues/669#issuecomment-1602257279:228,Integrability,depend,dependencies,228,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1602257279
https://github.com/google/deepvariant/issues/669#issuecomment-1602300665:177,Security,access,access,177,@gambalab You're most likely running Python 3.6 and DeepVariant needs Python 3.8. What operating system and version of the OS are you running? Do you have Docker or Singularity access?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1602300665
https://github.com/google/deepvariant/issues/669#issuecomment-1602330355:43,Deployability,install,install,43,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)""; >NAME=""Debian GNU/Linux""; >VERSION_ID=""11""; >VERSION=""11 (bullseye)""; >VERSION_CODENAME=bullseye; >ID=debian; >HOME_URL=""https://www.debian.org/""; >SUPPORT_URL=""https://www.debian.org/support""; >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1602330355
https://github.com/google/deepvariant/issues/669#issuecomment-1602330355:243,Deployability,release,release,243,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)""; >NAME=""Debian GNU/Linux""; >VERSION_ID=""11""; >VERSION=""11 (bullseye)""; >VERSION_CODENAME=bullseye; >ID=debian; >HOME_URL=""https://www.debian.org/""; >SUPPORT_URL=""https://www.debian.org/support""; >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1602330355
https://github.com/google/deepvariant/issues/669#issuecomment-1603444351:545,Availability,down,download,545,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```; wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz; tar xzvf udocker-1.3.9.tar.gz; cd udocker-1.3.9/udocker/; ./udocker pull google/deepvariant:1.5.0; ./udocker run google/deepvariant:1.5.0; ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351
https://github.com/google/deepvariant/issues/669#issuecomment-1603444351:734,Availability,down,downloaded,734,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```; wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz; tar xzvf udocker-1.3.9.tar.gz; cd udocker-1.3.9/udocker/; ./udocker pull google/deepvariant:1.5.0; ./udocker run google/deepvariant:1.5.0; ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351
https://github.com/google/deepvariant/issues/669#issuecomment-1603444351:90,Deployability,install,installed,90,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```; wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz; tar xzvf udocker-1.3.9.tar.gz; cd udocker-1.3.9/udocker/; ./udocker pull google/deepvariant:1.5.0; ./udocker run google/deepvariant:1.5.0; ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351
https://github.com/google/deepvariant/issues/669#issuecomment-1603444351:254,Deployability,install,install,254,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```; wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz; tar xzvf udocker-1.3.9.tar.gz; cd udocker-1.3.9/udocker/; ./udocker pull google/deepvariant:1.5.0; ./udocker run google/deepvariant:1.5.0; ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351
https://github.com/google/deepvariant/issues/669#issuecomment-1603444351:415,Deployability,install,install,415,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```; wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz; tar xzvf udocker-1.3.9.tar.gz; cd udocker-1.3.9/udocker/; ./udocker pull google/deepvariant:1.5.0; ./udocker run google/deepvariant:1.5.0; ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351
https://github.com/google/deepvariant/issues/669#issuecomment-1603444351:536,Deployability,release,releases,536,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```; wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz; tar xzvf udocker-1.3.9.tar.gz; cd udocker-1.3.9/udocker/; ./udocker pull google/deepvariant:1.5.0; ./udocker run google/deepvariant:1.5.0; ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351
https://github.com/google/deepvariant/issues/669#issuecomment-1603444351:229,Performance,perform,performs,229,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```; wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz; tar xzvf udocker-1.3.9.tar.gz; cd udocker-1.3.9/udocker/; ./udocker pull google/deepvariant:1.5.0; ./udocker run google/deepvariant:1.5.0; ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351
https://github.com/google/deepvariant/issues/669#issuecomment-1604068307:98,Deployability,install,installed,98,"> Hi @gambalab,; > ; > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts.; > ; > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:; > . thank you! this is a great solution for me.; you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1604068307
https://github.com/google/deepvariant/issues/669#issuecomment-1604068307:269,Deployability,install,install,269,"> Hi @gambalab,; > ; > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts.; > ; > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:; > . thank you! this is a great solution for me.; you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1604068307
https://github.com/google/deepvariant/issues/669#issuecomment-1604068307:460,Deployability,install,installation,460,"> Hi @gambalab,; > ; > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts.; > ; > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:; > . thank you! this is a great solution for me.; you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1604068307
https://github.com/google/deepvariant/issues/669#issuecomment-1604068307:244,Performance,perform,performs,244,"> Hi @gambalab,; > ; > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts.; > ; > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:; > . thank you! this is a great solution for me.; you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1604068307
https://github.com/google/deepvariant/issues/669#issuecomment-1604566408:250,Deployability,release,release,250,"Thanks @pgrosu for the suggestion, and @gambalab for confirming it works. I don't think our team has tried `udocker` before. So it's good to know about this. I'll add an internal task to try this out and consider adding a pointer to it in the future release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669#issuecomment-1604566408
https://github.com/google/deepvariant/issues/670#issuecomment-1607949879:524,Availability,error,error,524,"Hi @zxy1555847 . It looks like this output is the result of running GLnexus on gVCFs. I believe you must be running DeepVariant on the 40 single samples and then GLnexus on the gVCF output as recommended in our best practices. The sample name that occurs in the gVCF file comes from the sample name in the BAM file tag. (as @pgrosu suggests, that should be what follows the `SM:` tag from the output of `samtools view -H ${CRAM} | grep SM`. DeepVariant should take that value as its sample name. DeepVariant should raise an error if a single BAM file has multiple sample names, suggesting that if these files were able to run through and produce output, they should be using the reads appropriately. . In either case, you can tell DeepVariant to over-ride any sample names present in a file and produce output from all reads with a sample name provided by you as a user. To do so, you can add the flag `--sample_name` with your sample name to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670#issuecomment-1607949879
https://github.com/google/deepvariant/issues/670#issuecomment-1608591412:90,Deployability,pipeline,pipelines,90,oh~~~yeap.I have find the cause of this problem! ; These samples did go through different pipelines where docker mounted to different containers; but it looks right except the sample name .; Thanks very much,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670#issuecomment-1608591412
https://github.com/google/deepvariant/issues/671#issuecomment-1609966288:150,Integrability,depend,depends,150,"Hi @geng-lee ,; DeepVariant's default should be good for achieving a good F1 metrics. Some of our users sometimes further filter the variants, but it depends on why you're doing it.; Can you tell us why you want filter the SNPs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671#issuecomment-1609966288
https://github.com/google/deepvariant/issues/671#issuecomment-1610040880:264,Availability,Error,Error,264,"Hi @geng-lee,. Just to expand on a few things. To understand the impact of the information, you would need to look at each one individually, for example:. - $`GQ`$ $`(genotype`$ $`quality)`$: Tells you the genotype is incorrect using a Phred-scaled $`-10*log_{10}(Error)`$ value. So with low error you will get a higher score -- a higher score you are confident the call is correct. In this case your highest is 19, which is borderline for DeepVariant which has it at 20. A score of 20 means, an error of 0.01, so 99% probability that the call is correct. This has a relationship to the PL value, which gives normalized Phred-scaled scores per genotype, but in this case would be too low to call confidently. - $`DP`$ $`(read`$ $`depth)`$: This tells you the filtered reads that support the call. Here your depth is a bit low for a couple of them. For research purposes the cutoff is usually 10 or higher. For clinical 30 would be nice but anything higher than 18 can be good. You will see an AD value (which includes all reads at that position) as well, but that means you are including reads that could be problematic. . - $`VAF`$ $`(variant`$ $`allele`$ $`frequency)`$: This is the percentage of reads for a specific variant divided by the overall coverage at that locus. For heterozygous it would have a value of 50%, for homozygous close to 100% while matching the reference would be 0%. You will notice one is 75% and it denotes it as heterozygous, but the GQ and PL values are very low to call it confidently. - $`QUAL`$: These are Phred-scaled values that the probability the genotype is reference (0/0). For instance a QUAL score of 20 means that your are 99% confident there is a variant at that site, but as before with much lower values you would not be. You can read more about what these columns mean, and how to interpret them at the [following site](https://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format). As @pichuan mention, other metrics you can use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671#issuecomment-1610040880
https://github.com/google/deepvariant/issues/671#issuecomment-1610040880:292,Availability,error,error,292,"Hi @geng-lee,. Just to expand on a few things. To understand the impact of the information, you would need to look at each one individually, for example:. - $`GQ`$ $`(genotype`$ $`quality)`$: Tells you the genotype is incorrect using a Phred-scaled $`-10*log_{10}(Error)`$ value. So with low error you will get a higher score -- a higher score you are confident the call is correct. In this case your highest is 19, which is borderline for DeepVariant which has it at 20. A score of 20 means, an error of 0.01, so 99% probability that the call is correct. This has a relationship to the PL value, which gives normalized Phred-scaled scores per genotype, but in this case would be too low to call confidently. - $`DP`$ $`(read`$ $`depth)`$: This tells you the filtered reads that support the call. Here your depth is a bit low for a couple of them. For research purposes the cutoff is usually 10 or higher. For clinical 30 would be nice but anything higher than 18 can be good. You will see an AD value (which includes all reads at that position) as well, but that means you are including reads that could be problematic. . - $`VAF`$ $`(variant`$ $`allele`$ $`frequency)`$: This is the percentage of reads for a specific variant divided by the overall coverage at that locus. For heterozygous it would have a value of 50%, for homozygous close to 100% while matching the reference would be 0%. You will notice one is 75% and it denotes it as heterozygous, but the GQ and PL values are very low to call it confidently. - $`QUAL`$: These are Phred-scaled values that the probability the genotype is reference (0/0). For instance a QUAL score of 20 means that your are 99% confident there is a variant at that site, but as before with much lower values you would not be. You can read more about what these columns mean, and how to interpret them at the [following site](https://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format). As @pichuan mention, other metrics you can use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671#issuecomment-1610040880
https://github.com/google/deepvariant/issues/671#issuecomment-1610040880:496,Availability,error,error,496,"Hi @geng-lee,. Just to expand on a few things. To understand the impact of the information, you would need to look at each one individually, for example:. - $`GQ`$ $`(genotype`$ $`quality)`$: Tells you the genotype is incorrect using a Phred-scaled $`-10*log_{10}(Error)`$ value. So with low error you will get a higher score -- a higher score you are confident the call is correct. In this case your highest is 19, which is borderline for DeepVariant which has it at 20. A score of 20 means, an error of 0.01, so 99% probability that the call is correct. This has a relationship to the PL value, which gives normalized Phred-scaled scores per genotype, but in this case would be too low to call confidently. - $`DP`$ $`(read`$ $`depth)`$: This tells you the filtered reads that support the call. Here your depth is a bit low for a couple of them. For research purposes the cutoff is usually 10 or higher. For clinical 30 would be nice but anything higher than 18 can be good. You will see an AD value (which includes all reads at that position) as well, but that means you are including reads that could be problematic. . - $`VAF`$ $`(variant`$ $`allele`$ $`frequency)`$: This is the percentage of reads for a specific variant divided by the overall coverage at that locus. For heterozygous it would have a value of 50%, for homozygous close to 100% while matching the reference would be 0%. You will notice one is 75% and it denotes it as heterozygous, but the GQ and PL values are very low to call it confidently. - $`QUAL`$: These are Phred-scaled values that the probability the genotype is reference (0/0). For instance a QUAL score of 20 means that your are 99% confident there is a variant at that site, but as before with much lower values you would not be. You can read more about what these columns mean, and how to interpret them at the [following site](https://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format). As @pichuan mention, other metrics you can use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671#issuecomment-1610040880
https://github.com/google/deepvariant/issues/671#issuecomment-1610040880:2109,Testability,benchmark,benchmark,2109," case your highest is 19, which is borderline for DeepVariant which has it at 20. A score of 20 means, an error of 0.01, so 99% probability that the call is correct. This has a relationship to the PL value, which gives normalized Phred-scaled scores per genotype, but in this case would be too low to call confidently. - $`DP`$ $`(read`$ $`depth)`$: This tells you the filtered reads that support the call. Here your depth is a bit low for a couple of them. For research purposes the cutoff is usually 10 or higher. For clinical 30 would be nice but anything higher than 18 can be good. You will see an AD value (which includes all reads at that position) as well, but that means you are including reads that could be problematic. . - $`VAF`$ $`(variant`$ $`allele`$ $`frequency)`$: This is the percentage of reads for a specific variant divided by the overall coverage at that locus. For heterozygous it would have a value of 50%, for homozygous close to 100% while matching the reference would be 0%. You will notice one is 75% and it denotes it as heterozygous, but the GQ and PL values are very low to call it confidently. - $`QUAL`$: These are Phred-scaled values that the probability the genotype is reference (0/0). For instance a QUAL score of 20 means that your are 99% confident there is a variant at that site, but as before with much lower values you would not be. You can read more about what these columns mean, and how to interpret them at the [following site](https://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format). As @pichuan mention, other metrics you can use are a tool such as [Illumina's Hap.py](https://github.com/Illumina/hap.py/tree/master), with which you can benchmark your VCF against gold standard truth datasets, to see the confidence of these calls in terms of number of F1 score, precision and recall. . More details are provided in the [Hap.py Manual](https://github.com/Illumina/hap.py/blob/master/doc/happy.md). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671#issuecomment-1610040880
https://github.com/google/deepvariant/issues/672#issuecomment-1613645470:28,Availability,error,error,28,"Hi @crazysummerW , from the error log, it seems like your BAM file doesn't have base quality scores. Is that expected from your pipeline?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1613645470
https://github.com/google/deepvariant/issues/672#issuecomment-1613645470:128,Deployability,pipeline,pipeline,128,"Hi @crazysummerW , from the error log, it seems like your BAM file doesn't have base quality scores. Is that expected from your pipeline?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1613645470
https://github.com/google/deepvariant/issues/672#issuecomment-1613645470:34,Testability,log,log,34,"Hi @crazysummerW , from the error log, it seems like your BAM file doesn't have base quality scores. Is that expected from your pipeline?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1613645470
https://github.com/google/deepvariant/issues/672#issuecomment-1615164671:982,Usability,clear,clear,982,"Hi @crazysummerW , ; in your first step, you mentioned ""pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@rg\tID:test1\tSM:test1'"". Can you explain to me what you're trying to do in this step?. You also mentioned DeepConsensus. Are you planning to start from your own PacBio subreads BAM?; Unless you're starting with subreads, you don't need to apply DeepConsensus. If you are starting with PacBio HiFi reads (which I think is most of the DeepVariant users), you should directly map your FASTQ files using pbmm2, which gives you a BAM file, then you apply DeepVariant on the BAM. However, if you are indeed starting from PacBio subreads, you will need to follow https://github.com/google/deepconsensus/blob/r1.2/docs/quick_start.md to obtain the FASTQ file (which is the output that DeepConsensus gives you). Then, you map the FASTQ file, which gives you a BAM file, then you apply DeepVariant on the BAM. From your original question, it isn't quite clear to me what you're trying to do here. Can you elaborate?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1615164671
https://github.com/google/deepvariant/issues/672#issuecomment-1615281965:214,Deployability,pipeline,pipeline,214,"Hi @crazysummerW ,; I noticed in your title you mentioned ""Revio data"". If you're using Revio, DeepConsensus is already run. You don't need to run it again. So, please proceed with your usual mapping + DeepVariant pipeline, and you should be able to get your variant calls that way. If I'm not interpreting your use case correctly, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1615281965
https://github.com/google/deepvariant/issues/672#issuecomment-1615974070:991,Availability,checkpoint,checkpoint,991,"Hi @crazysummerW ,. One more thought:; You mentioned ""I also tested the Revio Hifi data. It succeeded."" --> If that's the case, you don't need to re-run DeepConsensus before running DeepVariant, because DeepConsensus is already run on Revio, and in fact, the model deployed on Revio machine is the appropriate one to use. So, I am not sure why you need to re-run DeepConsensus. Maybe I miss some context here.; That's why I encourage you to post directly on https://github.com/google/deepconsensus/issues instead of here. So that we can understand your use case of DeepConsensus. One more thing:; When running DeepConsensus, the code snippet you posted is only an intermediate step. You'll need to finish running through the steps to get to the FASTQ file here: https://github.com/google/deepconsensus/blob/r1.2/docs/quick_start.md#run-deepconsensus . Specifically this step:; ```; deepconsensus run \; --subreads_to_ccs=${shard_id}.subreads_to_ccs.bam \; --ccs_bam=${shard_id}.ccs.bam \; --checkpoint=model/checkpoint \; --output=${shard_id}.output.fastq; ```. is the one that generate the FASTQ you'll use for mapping and then variant calling. Hope this helps. For more DeepConsensus questions, please post on https://github.com/google/deepconsensus/issues and we can take it from there later next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1615974070
https://github.com/google/deepvariant/issues/672#issuecomment-1615974070:1008,Availability,checkpoint,checkpoint,1008,"Hi @crazysummerW ,. One more thought:; You mentioned ""I also tested the Revio Hifi data. It succeeded."" --> If that's the case, you don't need to re-run DeepConsensus before running DeepVariant, because DeepConsensus is already run on Revio, and in fact, the model deployed on Revio machine is the appropriate one to use. So, I am not sure why you need to re-run DeepConsensus. Maybe I miss some context here.; That's why I encourage you to post directly on https://github.com/google/deepconsensus/issues instead of here. So that we can understand your use case of DeepConsensus. One more thing:; When running DeepConsensus, the code snippet you posted is only an intermediate step. You'll need to finish running through the steps to get to the FASTQ file here: https://github.com/google/deepconsensus/blob/r1.2/docs/quick_start.md#run-deepconsensus . Specifically this step:; ```; deepconsensus run \; --subreads_to_ccs=${shard_id}.subreads_to_ccs.bam \; --ccs_bam=${shard_id}.ccs.bam \; --checkpoint=model/checkpoint \; --output=${shard_id}.output.fastq; ```. is the one that generate the FASTQ you'll use for mapping and then variant calling. Hope this helps. For more DeepConsensus questions, please post on https://github.com/google/deepconsensus/issues and we can take it from there later next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1615974070
https://github.com/google/deepvariant/issues/672#issuecomment-1615974070:265,Deployability,deploy,deployed,265,"Hi @crazysummerW ,. One more thought:; You mentioned ""I also tested the Revio Hifi data. It succeeded."" --> If that's the case, you don't need to re-run DeepConsensus before running DeepVariant, because DeepConsensus is already run on Revio, and in fact, the model deployed on Revio machine is the appropriate one to use. So, I am not sure why you need to re-run DeepConsensus. Maybe I miss some context here.; That's why I encourage you to post directly on https://github.com/google/deepconsensus/issues instead of here. So that we can understand your use case of DeepConsensus. One more thing:; When running DeepConsensus, the code snippet you posted is only an intermediate step. You'll need to finish running through the steps to get to the FASTQ file here: https://github.com/google/deepconsensus/blob/r1.2/docs/quick_start.md#run-deepconsensus . Specifically this step:; ```; deepconsensus run \; --subreads_to_ccs=${shard_id}.subreads_to_ccs.bam \; --ccs_bam=${shard_id}.ccs.bam \; --checkpoint=model/checkpoint \; --output=${shard_id}.output.fastq; ```. is the one that generate the FASTQ you'll use for mapping and then variant calling. Hope this helps. For more DeepConsensus questions, please post on https://github.com/google/deepconsensus/issues and we can take it from there later next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1615974070
https://github.com/google/deepvariant/issues/672#issuecomment-1615974070:61,Testability,test,tested,61,"Hi @crazysummerW ,. One more thought:; You mentioned ""I also tested the Revio Hifi data. It succeeded."" --> If that's the case, you don't need to re-run DeepConsensus before running DeepVariant, because DeepConsensus is already run on Revio, and in fact, the model deployed on Revio machine is the appropriate one to use. So, I am not sure why you need to re-run DeepConsensus. Maybe I miss some context here.; That's why I encourage you to post directly on https://github.com/google/deepconsensus/issues instead of here. So that we can understand your use case of DeepConsensus. One more thing:; When running DeepConsensus, the code snippet you posted is only an intermediate step. You'll need to finish running through the steps to get to the FASTQ file here: https://github.com/google/deepconsensus/blob/r1.2/docs/quick_start.md#run-deepconsensus . Specifically this step:; ```; deepconsensus run \; --subreads_to_ccs=${shard_id}.subreads_to_ccs.bam \; --ccs_bam=${shard_id}.ccs.bam \; --checkpoint=model/checkpoint \; --output=${shard_id}.output.fastq; ```. is the one that generate the FASTQ you'll use for mapping and then variant calling. Hope this helps. For more DeepConsensus questions, please post on https://github.com/google/deepconsensus/issues and we can take it from there later next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1615974070
https://github.com/google/deepvariant/issues/672#issuecomment-1616079783:56,Deployability,integrat,integrated,56,"@crazysummerW . As Pi-Chuan mentioned, DeepConsensus is integrated into the Revio system; so, you will get DeepConsensus reads directly from that system. The `n1000.subreads.bam` demo dataset being discussed here is from Sequel II. It is a small number of reads from the human genome. You should be able to push it through the mechanical steps of alignment and variant calling, but the results will be limited by coverage. To figure out which mechanical step is broken here, I would recommend to pass the FASTQ directly rather than through a `fofn` to be more explicit. The pbmm2 alignment should have input of HiFi reads, not subreads. So, a typical aligned file name would be `REF.hifi_reads.bam` without a mention of subreads. > pbmm2 align hs37d5.fasta ${shard_id}.output.fastq aligned.bam --sort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1616079783
https://github.com/google/deepvariant/issues/672#issuecomment-1616079783:56,Integrability,integrat,integrated,56,"@crazysummerW . As Pi-Chuan mentioned, DeepConsensus is integrated into the Revio system; so, you will get DeepConsensus reads directly from that system. The `n1000.subreads.bam` demo dataset being discussed here is from Sequel II. It is a small number of reads from the human genome. You should be able to push it through the mechanical steps of alignment and variant calling, but the results will be limited by coverage. To figure out which mechanical step is broken here, I would recommend to pass the FASTQ directly rather than through a `fofn` to be more explicit. The pbmm2 alignment should have input of HiFi reads, not subreads. So, a typical aligned file name would be `REF.hifi_reads.bam` without a mention of subreads. > pbmm2 align hs37d5.fasta ${shard_id}.output.fastq aligned.bam --sort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1616079783
https://github.com/google/deepvariant/issues/672#issuecomment-1621111780:104,Performance,perform,performed,104,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780
https://github.com/google/deepvariant/issues/672#issuecomment-1621111780:118,Testability,test,tests,118,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780
https://github.com/google/deepvariant/issues/672#issuecomment-1621111780:282,Testability,test,tested,282,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780
https://github.com/google/deepvariant/issues/672#issuecomment-1621111780:93,Usability,clear,clearly,93,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780
https://github.com/google/deepvariant/issues/672#issuecomment-2388558929:284,Availability,error,error,284,"Hi there! Thanks for the great work.; I encountered the similar problem here. I have PacBio long-read scRNA-seq data, and I am trying to use DeepVariant on the pseudobulk level. However,`isoseq groupdedup` removed the quality scores in my bam files during preprocessing, which causes error ; As you mentioned above, should I skip the deduplication step (but keep primer removal, tag, refine, barcode correction and sorting) and then align to reference genome using `pbmm2`? Or should I use another deduplication tool that keeps base quality score?. Thanks for any help in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-2388558929
https://github.com/google/deepvariant/issues/672#issuecomment-2393536188:113,Availability,reliab,reliable,113,"Hi, sorry to bother again. I am wondering if I run DeepVariant on my non-deduplicated reads, will the results be reliable or make sense? Probably there will be artifactual mutations?; In short, I should still run DeepVariant on deduplicated reads, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672#issuecomment-2393536188
https://github.com/google/deepvariant/issues/673#issuecomment-1625547662:388,Availability,error,error,388,"Dear @kishwarshafin ; thank you for your help, the Tensorflow version is:; tensorflow 2.12.0 pypi_0 pypi; tensorflow-io-gcs-filesystem 0.32.0 pypi_0 pypi. I tried singularity exec -B /usr/lib/locale/:/usr/lib/locale/; it only gives:; Usage:; singularity [global options...] exec [exec options...] <container> <command>. and when I tried with the full command, it says FATAL Flags parsing error: Unknown command line flag 'B'. thanks again!; Best,; CW",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673#issuecomment-1625547662
https://github.com/google/deepvariant/issues/673#issuecomment-1625679434:420,Availability,error,error,420,"So yes, it's a tensorflow version issue. Our current [tensorflow version is 2.11](https://github.com/google/deepvariant/blob/r1.5/settings.sh#L72). Can you please try the [singulaity quickstart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#cpu-version) to see if that works for you. The command actually is `singularity run -B` not `singularity exec -B` which maybe the reason for the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673#issuecomment-1625679434
https://github.com/google/deepvariant/issues/673#issuecomment-1638414183:128,Usability,simpl,simplex,128,"@CWYuan08 , for Nanopore R9.4.1, we suggest using [PEPPER](https://github.com/kishwarshafin/pepper). DeepVariant supports R10.4 simplex and duplex modes. Thank you for confirming that it is working for you now. I will close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673#issuecomment-1638414183
https://github.com/google/deepvariant/issues/675#issuecomment-1629492599:374,Availability,avail,available,374,"@FraSilver You are very close. All you have to do is two things:. $`1)`$ First add the mapping of the output directory to the `docker run` command (below the one for input) as follows:; ```; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; ```. This basically makes the output directory visible from within the running Docker container, so that the results are available after Docker completes its run. $`2)`$ Next define the `FQ` variable, as it seems to be referenced as `$FQ`, but I'm not seeing it defined. If it is already defined previously, then that's fine. If you have multiple lines, you will need to also add the backslash `\` as follows, so that the command doesn't get started before the last line gets pasted:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/Homo_sapiens_assembly38.fasta \; --reads=/input/$FQ.align.sort.marked.bam \; --output_vcf=/output/$FQ.vcf.gz \; --output_gvcf=/output/$FQ.g.vcf.gz \; --num_shards=2 ; ```. Let me know if it helps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629492599
https://github.com/google/deepvariant/issues/675#issuecomment-1629492599:444,Modifiability,variab,variable,444,"@FraSilver You are very close. All you have to do is two things:. $`1)`$ First add the mapping of the output directory to the `docker run` command (below the one for input) as follows:; ```; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; ```. This basically makes the output directory visible from within the running Docker container, so that the results are available after Docker completes its run. $`2)`$ Next define the `FQ` variable, as it seems to be referenced as `$FQ`, but I'm not seeing it defined. If it is already defined previously, then that's fine. If you have multiple lines, you will need to also add the backslash `\` as follows, so that the command doesn't get started before the last line gets pasted:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/Homo_sapiens_assembly38.fasta \; --reads=/input/$FQ.align.sort.marked.bam \; --output_vcf=/output/$FQ.vcf.gz \; --output_gvcf=/output/$FQ.g.vcf.gz \; --num_shards=2 ; ```. Let me know if it helps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629492599
https://github.com/google/deepvariant/issues/675#issuecomment-1629787618:109,Modifiability,variab,variable,109,"I have copy-n-paste the wrong script. ; I have already written -v ""${OUTPUT_DIR}"":""/output"", declared my $FQ variable and used backslash.; It doesn't work.; In my ""/input"" directory I have .fasta file, .fasta.fai, bam and .bai file. I suppose I don't require nothing else. (maybe, also VCF tools if the aim is the vcf output ""prediction"". . Any suggestion?; singularity run doesn't work as well.; I get the ""deepvariant_1.5.0.sif "" image file after running singularity pull... Thanks in advance,; Fra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629787618
https://github.com/google/deepvariant/issues/675#issuecomment-1629787618:326,Safety,predict,prediction,326,"I have copy-n-paste the wrong script. ; I have already written -v ""${OUTPUT_DIR}"":""/output"", declared my $FQ variable and used backslash.; It doesn't work.; In my ""/input"" directory I have .fasta file, .fasta.fai, bam and .bai file. I suppose I don't require nothing else. (maybe, also VCF tools if the aim is the vcf output ""prediction"". . Any suggestion?; singularity run doesn't work as well.; I get the ""deepvariant_1.5.0.sif "" image file after running singularity pull... Thanks in advance,; Fra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629787618
https://github.com/google/deepvariant/issues/675#issuecomment-1629892175:1645,Availability,error,errors,1645,"mples` stage. Assuming the reads were generated from Illumina whole-exome sequencing (WES), this BAM file is fairly large. To troubleshoot, it is best to specify a smaller region you know candidates should be, as you will need significant memory for this to work for the the whole genome. The flag specifying a region is of this format, and adjust accordingly for your best candidate region of your study: . ```; --regions ""chr20:10,000,000-10,010,000""; ```. You can just use Docker as you are running using that already (via `docker run`), to keep the number of variables small. Before you run Docker again, please look at the next two items as well. $`2)`$ Assuming you used the same reference sequence file (`Homo_sapiens_assembly38.fasta`) to perform the alignment, please run the following command (also you don't need to mark duplicates with DeepVariant):. ``samtools view -H $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. $`3)`$ To perform a preliminary QC on your BAM file can you run the following command:. ``samtools flagstat $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. If you have `fastQC` you can perform the following to generate the report, which you can attach a printed PDF file of the HTML output -- or you can attach the html with the accompanying zip file, if that is easier:. ` fastqc $FQ.align.sort.marked.bam`. $`4)`$ When you run the `docker run` command for `run_deepvariant` could you please provide the full output, to see where it gets stuck with errors, or how it completes. Also how much memory and disk space is freely available on the VM? I'm assuming the BAM file is several gigabytes. DeepVariant can read BAM files and will generate the VCF and GVCF, so no additional tools are required for it to work out of the box. We are just using a few tools to troubleshoot, but usually they are not necessary. Would you be willing to share the BAM file to get a better idea what is happening?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175
https://github.com/google/deepvariant/issues/675#issuecomment-1629892175:1720,Availability,avail,available,1720,"mples` stage. Assuming the reads were generated from Illumina whole-exome sequencing (WES), this BAM file is fairly large. To troubleshoot, it is best to specify a smaller region you know candidates should be, as you will need significant memory for this to work for the the whole genome. The flag specifying a region is of this format, and adjust accordingly for your best candidate region of your study: . ```; --regions ""chr20:10,000,000-10,010,000""; ```. You can just use Docker as you are running using that already (via `docker run`), to keep the number of variables small. Before you run Docker again, please look at the next two items as well. $`2)`$ Assuming you used the same reference sequence file (`Homo_sapiens_assembly38.fasta`) to perform the alignment, please run the following command (also you don't need to mark duplicates with DeepVariant):. ``samtools view -H $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. $`3)`$ To perform a preliminary QC on your BAM file can you run the following command:. ``samtools flagstat $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. If you have `fastQC` you can perform the following to generate the report, which you can attach a printed PDF file of the HTML output -- or you can attach the html with the accompanying zip file, if that is easier:. ` fastqc $FQ.align.sort.marked.bam`. $`4)`$ When you run the `docker run` command for `run_deepvariant` could you please provide the full output, to see where it gets stuck with errors, or how it completes. Also how much memory and disk space is freely available on the VM? I'm assuming the BAM file is several gigabytes. DeepVariant can read BAM files and will generate the VCF and GVCF, so no additional tools are required for it to work out of the box. We are just using a few tools to troubleshoot, but usually they are not necessary. Would you be willing to share the BAM file to get a better idea what is happening?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175
https://github.com/google/deepvariant/issues/675#issuecomment-1629892175:666,Modifiability,variab,variables,666,"Hi Fra,. So let me go through a few items:. $`1)`$ It fails to find variant candidates in the `make_examples` stage. Assuming the reads were generated from Illumina whole-exome sequencing (WES), this BAM file is fairly large. To troubleshoot, it is best to specify a smaller region you know candidates should be, as you will need significant memory for this to work for the the whole genome. The flag specifying a region is of this format, and adjust accordingly for your best candidate region of your study: . ```; --regions ""chr20:10,000,000-10,010,000""; ```. You can just use Docker as you are running using that already (via `docker run`), to keep the number of variables small. Before you run Docker again, please look at the next two items as well. $`2)`$ Assuming you used the same reference sequence file (`Homo_sapiens_assembly38.fasta`) to perform the alignment, please run the following command (also you don't need to mark duplicates with DeepVariant):. ``samtools view -H $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. $`3)`$ To perform a preliminary QC on your BAM file can you run the following command:. ``samtools flagstat $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. If you have `fastQC` you can perform the following to generate the report, which you can attach a printed PDF file of the HTML output -- or you can attach the html with the accompanying zip file, if that is easier:. ` fastqc $FQ.align.sort.marked.bam`. $`4)`$ When you run the `docker run` command for `run_deepvariant` could you please provide the full output, to see where it gets stuck with errors, or how it completes. Also how much memory and disk space is freely available on the VM? I'm assuming the BAM file is several gigabytes. DeepVariant can read BAM files and will generate the VCF and GVCF, so no additional tools are required for it to work out of the box. We are just using a few tools to troubleshoot, but usually they are not necess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175
https://github.com/google/deepvariant/issues/675#issuecomment-1629892175:850,Performance,perform,perform,850,"Hi Fra,. So let me go through a few items:. $`1)`$ It fails to find variant candidates in the `make_examples` stage. Assuming the reads were generated from Illumina whole-exome sequencing (WES), this BAM file is fairly large. To troubleshoot, it is best to specify a smaller region you know candidates should be, as you will need significant memory for this to work for the the whole genome. The flag specifying a region is of this format, and adjust accordingly for your best candidate region of your study: . ```; --regions ""chr20:10,000,000-10,010,000""; ```. You can just use Docker as you are running using that already (via `docker run`), to keep the number of variables small. Before you run Docker again, please look at the next two items as well. $`2)`$ Assuming you used the same reference sequence file (`Homo_sapiens_assembly38.fasta`) to perform the alignment, please run the following command (also you don't need to mark duplicates with DeepVariant):. ``samtools view -H $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. $`3)`$ To perform a preliminary QC on your BAM file can you run the following command:. ``samtools flagstat $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. If you have `fastQC` you can perform the following to generate the report, which you can attach a printed PDF file of the HTML output -- or you can attach the html with the accompanying zip file, if that is easier:. ` fastqc $FQ.align.sort.marked.bam`. $`4)`$ When you run the `docker run` command for `run_deepvariant` could you please provide the full output, to see where it gets stuck with errors, or how it completes. Also how much memory and disk space is freely available on the VM? I'm assuming the BAM file is several gigabytes. DeepVariant can read BAM files and will generate the VCF and GVCF, so no additional tools are required for it to work out of the box. We are just using a few tools to troubleshoot, but usually they are not necess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175
https://github.com/google/deepvariant/issues/675#issuecomment-1629892175:1074,Performance,perform,perform,1074,"mples` stage. Assuming the reads were generated from Illumina whole-exome sequencing (WES), this BAM file is fairly large. To troubleshoot, it is best to specify a smaller region you know candidates should be, as you will need significant memory for this to work for the the whole genome. The flag specifying a region is of this format, and adjust accordingly for your best candidate region of your study: . ```; --regions ""chr20:10,000,000-10,010,000""; ```. You can just use Docker as you are running using that already (via `docker run`), to keep the number of variables small. Before you run Docker again, please look at the next two items as well. $`2)`$ Assuming you used the same reference sequence file (`Homo_sapiens_assembly38.fasta`) to perform the alignment, please run the following command (also you don't need to mark duplicates with DeepVariant):. ``samtools view -H $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. $`3)`$ To perform a preliminary QC on your BAM file can you run the following command:. ``samtools flagstat $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. If you have `fastQC` you can perform the following to generate the report, which you can attach a printed PDF file of the HTML output -- or you can attach the html with the accompanying zip file, if that is easier:. ` fastqc $FQ.align.sort.marked.bam`. $`4)`$ When you run the `docker run` command for `run_deepvariant` could you please provide the full output, to see where it gets stuck with errors, or how it completes. Also how much memory and disk space is freely available on the VM? I'm assuming the BAM file is several gigabytes. DeepVariant can read BAM files and will generate the VCF and GVCF, so no additional tools are required for it to work out of the box. We are just using a few tools to troubleshoot, but usually they are not necessary. Would you be willing to share the BAM file to get a better idea what is happening?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175
https://github.com/google/deepvariant/issues/675#issuecomment-1629892175:1280,Performance,perform,perform,1280,"mples` stage. Assuming the reads were generated from Illumina whole-exome sequencing (WES), this BAM file is fairly large. To troubleshoot, it is best to specify a smaller region you know candidates should be, as you will need significant memory for this to work for the the whole genome. The flag specifying a region is of this format, and adjust accordingly for your best candidate region of your study: . ```; --regions ""chr20:10,000,000-10,010,000""; ```. You can just use Docker as you are running using that already (via `docker run`), to keep the number of variables small. Before you run Docker again, please look at the next two items as well. $`2)`$ Assuming you used the same reference sequence file (`Homo_sapiens_assembly38.fasta`) to perform the alignment, please run the following command (also you don't need to mark duplicates with DeepVariant):. ``samtools view -H $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. $`3)`$ To perform a preliminary QC on your BAM file can you run the following command:. ``samtools flagstat $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. If you have `fastQC` you can perform the following to generate the report, which you can attach a printed PDF file of the HTML output -- or you can attach the html with the accompanying zip file, if that is easier:. ` fastqc $FQ.align.sort.marked.bam`. $`4)`$ When you run the `docker run` command for `run_deepvariant` could you please provide the full output, to see where it gets stuck with errors, or how it completes. Also how much memory and disk space is freely available on the VM? I'm assuming the BAM file is several gigabytes. DeepVariant can read BAM files and will generate the VCF and GVCF, so no additional tools are required for it to work out of the box. We are just using a few tools to troubleshoot, but usually they are not necessary. Would you be willing to share the BAM file to get a better idea what is happening?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175
https://github.com/google/deepvariant/issues/675#issuecomment-1630405873:295,Availability,Avail,Avail,295,"My BAM file is 300 MBytes. I have less than 100 Gb of disk space. I suppose is a lack of space. ; But before I was able to create the output and the equivalent report, with the same disk memory. I don't know why now it does not process!. This my memory on Ubuntu-VirtualM:; Filesystem Size Used Avail Use% Mounted on; tmpfs 794M 1.1M 793M 1% /run; /dev/sda1 126G 15G 112G 12% /; tmpfs 3.9G 0 3.9G 0% /dev/shm; tmpfs 5.0M 0 5.0M 0% /run/lock; /dev/sda15 105M 6.1M 99M 6% /boot/efi; tmpfs 794M 4.0K 794M 1% /run/user/1000; :C:/Users/frasi/OneDrive/Desktop/NEUROBLASTOMA/NEUROBLASTOMA/NEUROBLASTOMA 1000G 0 1000G 0% /mountpoint",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1630405873
https://github.com/google/deepvariant/issues/675#issuecomment-1631059899:456,Energy Efficiency,monitor,monitored,456,"Hi Fra,. That seems a bit small compared to other WES BAM files, which are between 5 - 10 GB:. https://ega-archive.org/datasets/EGAD00001005247/files. The disk space seems okay, though you have some memory-mapped file-systems, which should be okay if you have a good amount of memory. . Memory on Ubuntu can be checked with `cat /proc/meminfo` or (`top`). For example, when I ran the variant candidate selection, on just a small region of a chromosome and monitored memory usage, I saw the following trend:. ![image](https://github.com/google/deepvariant/assets/6555937/7e200851-3fde-4ad6-bf75-477ccefb9e32). You'll notice that it can be resource intensive. This is the reason I was opting we troubleshoot by first trying with a smaller region of a chromosome using `--regions`, ideally one you know variants should be present. A preliminary check with `samtools view -c BAM REGION` would ensure you have mapped reads there for DeepVariant to find, and with `samtools flagstat` to check on that subregion for the number of quality reads. . Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1631059899
https://github.com/google/deepvariant/issues/675#issuecomment-1631154925:20,Safety,sanity check,sanity check,20,"Hi @FraSilver; As a sanity check, can you try running this quick start with no modifications at all? Try just copy-pasting it exactly as written without using your own files or making any other changes to the command:; https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1631154925
https://github.com/google/deepvariant/issues/675#issuecomment-1631967811:236,Testability,test,testdata,236,"I' m using Ubuntu Virtual-Machine mount on Windows 11.; Running ""cat ${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" it displays the fasta file, but after running:; sudo docker run...., it reports -bash: --ref=/mountpoint/fastQ/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. Why?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1631967811
https://github.com/google/deepvariant/issues/675#issuecomment-1632163617:811,Testability,test,testdata,811,"Thank you Maria -- that's definitely a more elegant starting point. Hi Fra,. The reason why you are seeing that is because there are additional characters after some of the backslash characters (`\`) at the end of each line. So you need to make sure there are no spaces or characters after each backslash (i.e. it needs to be the last character on those lines) like this:. ```; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; ```. The strings that starts with ` **Replace this string...`, ` **Optional.` or ` **How many cores...` need to be removed, as they are there in the documentation only to provide additional clarity regarding those parameters. Otherwise Bash interprets the next line as a separate new command to run, like this:. ```; paul$ --ref=/mountpoint/fastQ/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; -bash: --ref=/mountpoint/fastQ/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; paul$; ```. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1632163617
https://github.com/google/deepvariant/issues/675#issuecomment-1632163617:894,Testability,test,testdata,894,"Thank you Maria -- that's definitely a more elegant starting point. Hi Fra,. The reason why you are seeing that is because there are additional characters after some of the backslash characters (`\`) at the end of each line. So you need to make sure there are no spaces or characters after each backslash (i.e. it needs to be the last character on those lines) like this:. ```; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; ```. The strings that starts with ` **Replace this string...`, ` **Optional.` or ` **How many cores...` need to be removed, as they are there in the documentation only to provide additional clarity regarding those parameters. Otherwise Bash interprets the next line as a separate new command to run, like this:. ```; paul$ --ref=/mountpoint/fastQ/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; -bash: --ref=/mountpoint/fastQ/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; paul$; ```. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1632163617
https://github.com/google/deepvariant/issues/675#issuecomment-1632242067:264,Deployability,install,installed,264,"ValueError: NOT_FOUND: Could not open /input/SAMPLE01.align.sort.marked.bam; parallel: This job failed:; /opt/deepvariant/bin/make_examples. I have a right bam file. ; Can be that ""make examples"" require a python version incompatible with my python 3.10.x version installed on Unix-virtual machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1632242067
https://github.com/google/deepvariant/issues/675#issuecomment-1632300046:523,Availability,error,error,523,"Hi Fra,. The Docker container has it's own internal version of Python so it becomes independent of any system it would be run on. . I have 4 questions:. 1) Did the sanity check Maria suggested work successfully for you? This is important to complete first as it gives context that your VM is compatible with a successful DeepVariant run. 2) Can you please provide the whole command that you just ran including any variables that were defined. Without both of these it becomes impossible to eliminate possible causes of the error, as they provide context as to how the command was set up. . 3) Please provide the `ls -l ${INPUT_DIR}` output to make sure it lists the BAM file using that variable definition. 4) Can you provide the complete output of the run, including error. This gives context as how far it ran before it stopped, which includes anything that worked. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1632300046
https://github.com/google/deepvariant/issues/675#issuecomment-1632300046:768,Availability,error,error,768,"Hi Fra,. The Docker container has it's own internal version of Python so it becomes independent of any system it would be run on. . I have 4 questions:. 1) Did the sanity check Maria suggested work successfully for you? This is important to complete first as it gives context that your VM is compatible with a successful DeepVariant run. 2) Can you please provide the whole command that you just ran including any variables that were defined. Without both of these it becomes impossible to eliminate possible causes of the error, as they provide context as to how the command was set up. . 3) Please provide the `ls -l ${INPUT_DIR}` output to make sure it lists the BAM file using that variable definition. 4) Can you provide the complete output of the run, including error. This gives context as how far it ran before it stopped, which includes anything that worked. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1632300046
https://github.com/google/deepvariant/issues/675#issuecomment-1632300046:414,Modifiability,variab,variables,414,"Hi Fra,. The Docker container has it's own internal version of Python so it becomes independent of any system it would be run on. . I have 4 questions:. 1) Did the sanity check Maria suggested work successfully for you? This is important to complete first as it gives context that your VM is compatible with a successful DeepVariant run. 2) Can you please provide the whole command that you just ran including any variables that were defined. Without both of these it becomes impossible to eliminate possible causes of the error, as they provide context as to how the command was set up. . 3) Please provide the `ls -l ${INPUT_DIR}` output to make sure it lists the BAM file using that variable definition. 4) Can you provide the complete output of the run, including error. This gives context as how far it ran before it stopped, which includes anything that worked. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1632300046
https://github.com/google/deepvariant/issues/675#issuecomment-1632300046:686,Modifiability,variab,variable,686,"Hi Fra,. The Docker container has it's own internal version of Python so it becomes independent of any system it would be run on. . I have 4 questions:. 1) Did the sanity check Maria suggested work successfully for you? This is important to complete first as it gives context that your VM is compatible with a successful DeepVariant run. 2) Can you please provide the whole command that you just ran including any variables that were defined. Without both of these it becomes impossible to eliminate possible causes of the error, as they provide context as to how the command was set up. . 3) Please provide the `ls -l ${INPUT_DIR}` output to make sure it lists the BAM file using that variable definition. 4) Can you provide the complete output of the run, including error. This gives context as how far it ran before it stopped, which includes anything that worked. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1632300046
https://github.com/google/deepvariant/issues/675#issuecomment-1632300046:164,Safety,sanity check,sanity check,164,"Hi Fra,. The Docker container has it's own internal version of Python so it becomes independent of any system it would be run on. . I have 4 questions:. 1) Did the sanity check Maria suggested work successfully for you? This is important to complete first as it gives context that your VM is compatible with a successful DeepVariant run. 2) Can you please provide the whole command that you just ran including any variables that were defined. Without both of these it becomes impossible to eliminate possible causes of the error, as they provide context as to how the command was set up. . 3) Please provide the `ls -l ${INPUT_DIR}` output to make sure it lists the BAM file using that variable definition. 4) Can you provide the complete output of the run, including error. This gives context as how far it ran before it stopped, which includes anything that worked. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1632300046
https://github.com/google/deepvariant/issues/675#issuecomment-1632602170:84,Modifiability,variab,variable,84,"It works. Both with the tutorial data and my data.; It doesn't read bam file with a variable previously declared. I've changed $FQ with SAMPLE01 and it works. Thanks. ; Kind regard,; Fra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1632602170
https://github.com/google/deepvariant/issues/675#issuecomment-1633300854:58,Safety,detect,detect,58,"Hi Fra,. That's great to hear! Hopefully you were able to detect some expected variant candidates, and ideally discover some novel and relevant ones as well. Glad this helped you out -- it was a fun team effort!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1633300854
https://github.com/google/deepvariant/issues/675#issuecomment-1651377100:340,Performance,load,load,340,"Hi Fra,. If it cannot find candidates, that happens during the `make_examples` stage. `make_examples` goes through an allele counter, and very sensitive variant caller to determine if there is enough read support for a candidate. Was the same genome (fasta) used for the alignment phase (from fastq) when generating the BAM files? When you load your BAM file in IGV to how many chromosomes and chromosomal regions does it show alignment for? . In the [quickstart tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) it uses the WGS model not the WES one (as indicated by `--model_type=WES` above). Also I believe your BAM file is only 300 Mb, which a bit small compared to other WES ones:. https://ega-archive.org/datasets/EGAD00001005247/files. Is your dataset a WES or WGS dataset? . Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1651377100
https://github.com/google/deepvariant/issues/675#issuecomment-1652409458:202,Deployability,Integrat,Integrative,202,"Hi Fra,. As far as I can infer, there is something in the BAM and reference files that the variant caller is unable to call variants for. Can you please take a look in the chromosome 8 region with the [Integrative Genomics Viewer (IGV)](https://software.broadinstitute.org/software/igv/) using your BAM aligned to the same exact reference, to confirm that you see some variation and proper alignment. I am looking at the call variant code, and the allele counts are dependent on the reference and read support by position. You should also limit your analysis with the `--regions` flag so it is quicker as well. If your data is WES then you should use the WES model, in order for the results to be scientifically valid. Maybe it might help if you realign your FASTQ files to the reference used by DeepVariant to regenerate the BAM file and the subsequent BAI file, just to be sure nothing happened along the way. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1652409458
https://github.com/google/deepvariant/issues/675#issuecomment-1652409458:202,Integrability,Integrat,Integrative,202,"Hi Fra,. As far as I can infer, there is something in the BAM and reference files that the variant caller is unable to call variants for. Can you please take a look in the chromosome 8 region with the [Integrative Genomics Viewer (IGV)](https://software.broadinstitute.org/software/igv/) using your BAM aligned to the same exact reference, to confirm that you see some variation and proper alignment. I am looking at the call variant code, and the allele counts are dependent on the reference and read support by position. You should also limit your analysis with the `--regions` flag so it is quicker as well. If your data is WES then you should use the WES model, in order for the results to be scientifically valid. Maybe it might help if you realign your FASTQ files to the reference used by DeepVariant to regenerate the BAM file and the subsequent BAI file, just to be sure nothing happened along the way. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1652409458
https://github.com/google/deepvariant/issues/675#issuecomment-1652409458:466,Integrability,depend,dependent,466,"Hi Fra,. As far as I can infer, there is something in the BAM and reference files that the variant caller is unable to call variants for. Can you please take a look in the chromosome 8 region with the [Integrative Genomics Viewer (IGV)](https://software.broadinstitute.org/software/igv/) using your BAM aligned to the same exact reference, to confirm that you see some variation and proper alignment. I am looking at the call variant code, and the allele counts are dependent on the reference and read support by position. You should also limit your analysis with the `--regions` flag so it is quicker as well. If your data is WES then you should use the WES model, in order for the results to be scientifically valid. Maybe it might help if you realign your FASTQ files to the reference used by DeepVariant to regenerate the BAM file and the subsequent BAI file, just to be sure nothing happened along the way. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675#issuecomment-1652409458
https://github.com/google/deepvariant/issues/676#issuecomment-1631277506:130,Deployability,update,update,130,Thanks for bringing this to our attention!; I'm running our tests with `nvidia/cuda:12.1.1-cudnn8-devel-ubuntu20.04` now and will update here when I have confirmed whether it's working.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676#issuecomment-1631277506
https://github.com/google/deepvariant/issues/676#issuecomment-1631277506:60,Testability,test,tests,60,Thanks for bringing this to our attention!; I'm running our tests with `nvidia/cuda:12.1.1-cudnn8-devel-ubuntu20.04` now and will update here when I have confirmed whether it's working.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676#issuecomment-1631277506
https://github.com/google/deepvariant/issues/676#issuecomment-1634645510:38,Testability,test,tested,38,"The 12.1.1 base image did NOT work. I tested `nvidia/cuda:11.3.1-cudnn8-devel-ubuntu20.04` instead, which did work correctly. . # Please use `nvidia/cuda:11.3.1-cudnn8-devel-ubuntu20.04`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676#issuecomment-1634645510
https://github.com/google/deepvariant/issues/677#issuecomment-1634515397:267,Deployability,update,updated,267,"Hi @Sami-St,. What @pichuan wrote is also very helpful to try -- I guess we were typing at the same time :) . So it works for me if I replace the beginning of the `shell` field of the Snakemake file with the following -- the input and output mapping would need to be updated based on your directory setup:. ```; docker run \; -v ""/input_files/input"":""/input"" \; -v ""/output_files/output"":""/output"" \; google/deepvariant:1.5.0 \; /opt/deepvariant/bin/make_examples \; ...; ```. This would be a first step to test before generalizing the file. Does this also work for you?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1634515397
https://github.com/google/deepvariant/issues/677#issuecomment-1634515397:507,Testability,test,test,507,"Hi @Sami-St,. What @pichuan wrote is also very helpful to try -- I guess we were typing at the same time :) . So it works for me if I replace the beginning of the `shell` field of the Snakemake file with the following -- the input and output mapping would need to be updated based on your directory setup:. ```; docker run \; -v ""/input_files/input"":""/input"" \; -v ""/output_files/output"":""/output"" \; google/deepvariant:1.5.0 \; /opt/deepvariant/bin/make_examples \; ...; ```. This would be a first step to test before generalizing the file. Does this also work for you?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1634515397
https://github.com/google/deepvariant/issues/677#issuecomment-1635046221:599,Performance,perform,performs,599,"Hi @Sami-St,. If you're really keen on using Singularity, then you can add the following two lines to your `Snakefile`:; ```; threads: 1; singularity: ""/location_of_your_sif_file/deepvariant_1.5.0.sif""; ```. And then launch it from the command-line with the following to ensure that you map (bind) shared folders within the container:. ```; snakemake -c 1 --use-singularity --singularity-args ""--bind /folder_to_share_with_singularity""; ```. It might be better if you use `/opt/deepvariant/bin/run_deepvariant` instead of `make_examples` $`-`$ unless you really know what your are doing $`-`$ as it performs the proper setup for you. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1635046221
https://github.com/google/deepvariant/issues/677#issuecomment-1635666898:454,Deployability,update,update,454,"Hey @pichuan, @pgrosu ; thx for the quick reply's :) i tested your first idea paul still had the same issue sadly, but the solution from issue 559 seems to be working (i dont understand why so but thats another problem) since the workflow is still running so i cant say for sure but it makes the tfrecords what didnt happend the last few time; ![image](https://github.com/google/deepvariant/assets/138118818/14c3b254-ec78-468c-9a2f-301443bc3a5b). I will update when the Workflow is done . Best regards ; Sami",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1635666898
https://github.com/google/deepvariant/issues/677#issuecomment-1635666898:55,Testability,test,tested,55,"Hey @pichuan, @pgrosu ; thx for the quick reply's :) i tested your first idea paul still had the same issue sadly, but the solution from issue 559 seems to be working (i dont understand why so but thats another problem) since the workflow is still running so i cant say for sure but it makes the tfrecords what didnt happend the last few time; ![image](https://github.com/google/deepvariant/assets/138118818/14c3b254-ec78-468c-9a2f-301443bc3a5b). I will update when the Workflow is done . Best regards ; Sami",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1635666898
https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:494,Availability,down,down,494,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771
https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:950,Availability,resilien,resilient,950,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771
https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:766,Deployability,configurat,configurations,766,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771
https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:693,Modifiability,flexible,flexible,693,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771
https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:766,Modifiability,config,configurations,766,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771
https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:637,Performance,optimiz,optimized,637,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771
https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:1116,Performance,optimiz,optimized,1116,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771
https://github.com/google/deepvariant/issues/678#issuecomment-1635203098:35,Deployability,update,update,35,"@tzcoolman You're very close. Just update your `singularity run` with the following two binds (`--bind ${INPUT_DIR} --bind ${OUTPUT_DIR}`), like this:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} docker://google/deepvariant:""${BIN_VERSION}"" ...; ```. This makes those directories accessible within the container. The rest can stay the same. Let me know if there is anything else. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678#issuecomment-1635203098
https://github.com/google/deepvariant/issues/678#issuecomment-1635203098:335,Security,access,accessible,335,"@tzcoolman You're very close. Just update your `singularity run` with the following two binds (`--bind ${INPUT_DIR} --bind ${OUTPUT_DIR}`), like this:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} docker://google/deepvariant:""${BIN_VERSION}"" ...; ```. This makes those directories accessible within the container. The rest can stay the same. Let me know if there is anything else. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678#issuecomment-1635203098
https://github.com/google/deepvariant/issues/679#issuecomment-1636698433:579,Availability,error,error,579,"Hi Bo,. First remove the following two from your command-line:. ```; --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp; --intermediate_results_dir=/tmp; ```. The Singularity container already has its own `/tmp` folder, and if you want an intermediate results folder you would want it be a different (unused) path name. You don't need the intermediate results folder to run DeepVariant. If you want to use stuff from your home folder (or other auto-mounted ones), then you don't need `--containall` either. Let me know if that fixes things, otherwise we can see what the next error you get and we progress like that. Let me know if it helped. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636698433
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:179,Availability,error,error,179,"Dear Paul:; Thank you so much for the speedy reply.; I tested the following two command-line operation schemes separately, and encountered some problems. I feel that the previous error report is not a problem with the (--containall; tmpDir) parameter. ## command-line plan A:; /share/app/singularity/3.8.1/bin/singularity exec \; --containall \; --bind /usr/lib/locale/:/usr/lib/locale/ \; --bind $ccsbam:$ccsbam \; --bind $ccsbam.bai:$ccsbam.bai \; --bind $fasta:$fasta \; --bind $fasta.fai:$fasta.fai \; --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output \; /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20. ***** Intermediate results will be written to /tmp/tmp8_1neaqr in docker. ****. ***** Running the command:*****; time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/Reference/PacBio/minimap2/Homo_sapiens_assembly38.fasta"" --reads ""/hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v4/R202302180101/R202302180101.pbmm2.CCS.alignment.sort.bam"" --examples ""/tmp/tmp8_1neaqr/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmp8_1neaqr/gvcf.tfrecord@20.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:2617,Availability,error,error,2617,"on ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_nwff5xo0/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; warning: /tmp/Bazel.runfiles_4ji1hg9j/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so: write error (disk full?). Continue? (y/n/^C) ; warning: /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:3867,Availability,Error,Error,3867,"run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_8twzrz82/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 0m2.302s; user 0m1.215s; sys 0m0.687s. ## command-line plan B:; /share/app/singularity/3.8.1/bin/singularity exec \; --bind /usr/lib/locale/:/usr/lib/locale/ \; --bind $ccsbam:$ccsbam \; --bind $ccsbam.bai:$ccsbam.bai \; --bind $fasta:$fasta \; --bind $fasta.fai:$fasta.fai \; --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output \; /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20. ***** Intermediate results will be wr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:3963,Availability,Error,Error,3963,"line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_8twzrz82/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 0m2.302s; user 0m1.215s; sys 0m0.687s. ## command-line plan B:; /share/app/singularity/3.8.1/bin/singularity exec \; --bind /usr/lib/locale/:/usr/lib/locale/ \; --bind $ccsbam:$ccsbam \; --bind $ccsbam.bai:$ccsbam.bai \; --bind $fasta:$fasta \; --bind $fasta.fai:$fasta.fai \; --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output \; /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20. ***** Intermediate results will be written to /tmp/6178902.1.st_supermem.q/tmpezd79ese in docker. ****; ***** Running the command:*****; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:6332,Availability,Error,Error,6332,"bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20. ***** Intermediate results will be written to /tmp/6178902.1.st_supermem.q/tmpezd79ese in docker. ****; ***** Running the command:*****; time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/Reference/PacBio/minimap2/Homo_sapiens_assembly38.fasta"" --reads ""/hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v4/T202302180201/T202302180201.pbmm2.CCS.alignment.sort.bam"" --examples ""/tmp/6178902.1.st_supermem.q/tmpezd79ese/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/6178902.1.st_supermem.q/tmpezd79ese/gvcf.tfrecord@20.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Error in tempfile() using template /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/temporary_internet_files/parXXXXX.par: Parent directory (/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/temporary_internet_files/) does not exist at /usr/bin/parallel line 3889. real 0m2.813s; user 0m0.258s; sys 0m0.406s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:5985,Deployability,install,installed,5985,"bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20. ***** Intermediate results will be written to /tmp/6178902.1.st_supermem.q/tmpezd79ese in docker. ****; ***** Running the command:*****; time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/Reference/PacBio/minimap2/Homo_sapiens_assembly38.fasta"" --reads ""/hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v4/T202302180201/T202302180201.pbmm2.CCS.alignment.sort.bam"" --examples ""/tmp/6178902.1.st_supermem.q/tmpezd79ese/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/6178902.1.st_supermem.q/tmpezd79ese/gvcf.tfrecord@20.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Error in tempfile() using template /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/temporary_internet_files/parXXXXX.par: Parent directory (/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/temporary_internet_files/) does not exist at /usr/bin/parallel line 3889. real 0m2.813s; user 0m0.258s; sys 0m0.406s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:6246,Deployability,install,installed,6246,"bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20. ***** Intermediate results will be written to /tmp/6178902.1.st_supermem.q/tmpezd79ese in docker. ****; ***** Running the command:*****; time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/Reference/PacBio/minimap2/Homo_sapiens_assembly38.fasta"" --reads ""/hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v4/T202302180201/T202302180201.pbmm2.CCS.alignment.sort.bam"" --examples ""/tmp/6178902.1.st_supermem.q/tmpezd79ese/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/6178902.1.st_supermem.q/tmpezd79ese/gvcf.tfrecord@20.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; Error in tempfile() using template /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/temporary_internet_files/parXXXXX.par: Parent directory (/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/temporary_internet_files/) does not exist at /usr/bin/parallel line 3889. real 0m2.813s; user 0m0.258s; sys 0m0.406s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:55,Testability,test,tested,55,"Dear Paul:; Thank you so much for the speedy reply.; I tested the following two command-line operation schemes separately, and encountered some problems. I feel that the previous error report is not a problem with the (--containall; tmpDir) parameter. ## command-line plan A:; /share/app/singularity/3.8.1/bin/singularity exec \; --containall \; --bind /usr/lib/locale/:/usr/lib/locale/ \; --bind $ccsbam:$ccsbam \; --bind $ccsbam.bai:$ccsbam.bai \; --bind $fasta:$fasta \; --bind $fasta.fai:$fasta.fai \; --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output \; /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20. ***** Intermediate results will be written to /tmp/tmp8_1neaqr in docker. ****. ***** Running the command:*****; time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/Reference/PacBio/minimap2/Homo_sapiens_assembly38.fasta"" --reads ""/hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v4/R202302180101/R202302180101.pbmm2.CCS.alignment.sort.bam"" --examples ""/tmp/tmp8_1neaqr/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmp8_1neaqr/gvcf.tfrecord@20.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:2240,Testability,Assert,AssertionError,2240,"piens_assembly38.fasta"" --reads ""/hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v4/R202302180101/R202302180101.pbmm2.CCS.alignment.sort.bam"" --examples ""/tmp/tmp8_1neaqr/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmp8_1neaqr/gvcf.tfrecord@20.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_nwff5xo0/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; warning: /tmp/Bazel.runfiles_4ji1hg9j/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so: write error (disk full?). Continue? (y/n/^C) ; warning: /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:3183,Testability,Assert,AssertionError,3183,"e_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_nwff5xo0/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; warning: /tmp/Bazel.runfiles_4ji1hg9j/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so: write error (disk full?). Continue? (y/n/^C) ; warning: /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_8twzrz82/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 0m2.302s; user 0m1.215s; sys 0m0.687s. ## command-line plan B:; /share/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:3717,Testability,Assert,AssertionError,3717,"ython/google/protobuf/pyext/_message.so is probably truncated; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_8twzrz82/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 0m2.302s; user 0m1.215s; sys 0m0.687s. ## command-line plan B:; /share/app/singularity/3.8.1/bin/singularity exec \; --bind /usr/lib/locale/:/usr/lib/locale/ \; --bind $ccsbam:$ccsbam \; --bind $ccsbam.bai:$ccsbam.bai \; --bind $fasta:$fasta \; --bind $fasta.fai:$fasta.fai \; --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output \; /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300
https://github.com/google/deepvariant/issues/679#issuecomment-1637342845:303,Availability,avail,available,303,"Dear Bo,. That's really awesome to hear! Yeah, sometimes how quickly distributed file systems become consistent under heavy workloads is something that has the rare possibility of happening. Today you might have hit a sweet timeslot where the cluster was not being hit very hard, so more resources were available to match (or exceed) the requirements of your run. Glad to hear it all worked out,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679#issuecomment-1637342845
https://github.com/google/deepvariant/issues/680#issuecomment-1640133876:1941,Availability,down,downstream,1941,"K-Joint genotyping. If you look at the following paper:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8023681/pdf/btaa1081.pdf). Using parameter-optimization of GLNexus (such as minimum quality thresholds, among others listed under Supplementary Table 4), the authors were able to get a slightly different number of SNPs than via GATK-Joint:. ![image](https://github.com/google/deepvariant/assets/6555937/da3459b6-cb09-45b8-8fd9-9bbcdb0d12a7) . This is from Supplementary Figure 11 (A) found under Supplementary data, listed as [a link on this page](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So merging with GLNexus for DeepVariant gVCF output files $`-`$ as @AndrewCarroll mentioned in a [previous post](https://github.com/google/deepvariant/issues/83#issuecomment-553660314) $`-`$ by using [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md), it was found to be more accurate than using those gVCFs with GATK GenotypeGVCFs. Regarding missing SNPs in individual samples, their genotype might get a no call (`./.`) as noted in [this line of the GLNexus code](https://github.com/dnanexus-rnd/GLnexus/blob/main/src/service.cc#L206):. ```; Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele; ```. Though it probably could also get called as homozygous reference, if all the QC pass. Regarding impact on downstream analysis, probably the best bet is to try both approaches (DV-GLN-OPT and GATK-Joint) in parallel, and then validate both results. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876
https://github.com/google/deepvariant/issues/680#issuecomment-1640133876:82,Performance,optimiz,optimization,82,"Dear Cheng,. Yes, there can be some differences between DeepVariant-GLNexus (with optimization) and GATK-Joint genotyping. If you look at the following paper:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8023681/pdf/btaa1081.pdf). Using parameter-optimization of GLNexus (such as minimum quality thresholds, among others listed under Supplementary Table 4), the authors were able to get a slightly different number of SNPs than via GATK-Joint:. ![image](https://github.com/google/deepvariant/assets/6555937/da3459b6-cb09-45b8-8fd9-9bbcdb0d12a7) . This is from Supplementary Figure 11 (A) found under Supplementary data, listed as [a link on this page](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So merging with GLNexus for DeepVariant gVCF output files $`-`$ as @AndrewCarroll mentioned in a [previous post](https://github.com/google/deepvariant/issues/83#issuecomment-553660314) $`-`$ by using [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md), it was found to be more accurate than using those gVCFs with GATK GenotypeGVCFs. Regarding missing SNPs in individual samples, their genotype might get a no call (`./.`) as noted in [this line of the GLNexus code](https://github.com/dnanexus-rnd/GLnexus/blob/main/src/service.cc#L206):. ```; Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele; ```. Though it probably could also get called as homozygous reference, if all the QC pass. Regarding impact on downstream analysis, probably the best bet is to try both ap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876
https://github.com/google/deepvariant/issues/680#issuecomment-1640133876:171,Performance,scalab,scalable,171,"Dear Cheng,. Yes, there can be some differences between DeepVariant-GLNexus (with optimization) and GATK-Joint genotyping. If you look at the following paper:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8023681/pdf/btaa1081.pdf). Using parameter-optimization of GLNexus (such as minimum quality thresholds, among others listed under Supplementary Table 4), the authors were able to get a slightly different number of SNPs than via GATK-Joint:. ![image](https://github.com/google/deepvariant/assets/6555937/da3459b6-cb09-45b8-8fd9-9bbcdb0d12a7) . This is from Supplementary Figure 11 (A) found under Supplementary data, listed as [a link on this page](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So merging with GLNexus for DeepVariant gVCF output files $`-`$ as @AndrewCarroll mentioned in a [previous post](https://github.com/google/deepvariant/issues/83#issuecomment-553660314) $`-`$ by using [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md), it was found to be more accurate than using those gVCFs with GATK GenotypeGVCFs. Regarding missing SNPs in individual samples, their genotype might get a no call (`./.`) as noted in [this line of the GLNexus code](https://github.com/dnanexus-rnd/GLnexus/blob/main/src/service.cc#L206):. ```; Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele; ```. Though it probably could also get called as homozygous reference, if all the QC pass. Regarding impact on downstream analysis, probably the best bet is to try both ap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876
https://github.com/google/deepvariant/issues/680#issuecomment-1640133876:320,Performance,optimiz,optimization,320,"Dear Cheng,. Yes, there can be some differences between DeepVariant-GLNexus (with optimization) and GATK-Joint genotyping. If you look at the following paper:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8023681/pdf/btaa1081.pdf). Using parameter-optimization of GLNexus (such as minimum quality thresholds, among others listed under Supplementary Table 4), the authors were able to get a slightly different number of SNPs than via GATK-Joint:. ![image](https://github.com/google/deepvariant/assets/6555937/da3459b6-cb09-45b8-8fd9-9bbcdb0d12a7) . This is from Supplementary Figure 11 (A) found under Supplementary data, listed as [a link on this page](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So merging with GLNexus for DeepVariant gVCF output files $`-`$ as @AndrewCarroll mentioned in a [previous post](https://github.com/google/deepvariant/issues/83#issuecomment-553660314) $`-`$ by using [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md), it was found to be more accurate than using those gVCFs with GATK GenotypeGVCFs. Regarding missing SNPs in individual samples, their genotype might get a no call (`./.`) as noted in [this line of the GLNexus code](https://github.com/dnanexus-rnd/GLnexus/blob/main/src/service.cc#L206):. ```; Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele; ```. Though it probably could also get called as homozygous reference, if all the QC pass. Regarding impact on downstream analysis, probably the best bet is to try both ap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876
https://github.com/google/deepvariant/issues/680#issuecomment-1640133876:2060,Security,validat,validate,2060,"K-Joint genotyping. If you look at the following paper:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8023681/pdf/btaa1081.pdf). Using parameter-optimization of GLNexus (such as minimum quality thresholds, among others listed under Supplementary Table 4), the authors were able to get a slightly different number of SNPs than via GATK-Joint:. ![image](https://github.com/google/deepvariant/assets/6555937/da3459b6-cb09-45b8-8fd9-9bbcdb0d12a7) . This is from Supplementary Figure 11 (A) found under Supplementary data, listed as [a link on this page](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So merging with GLNexus for DeepVariant gVCF output files $`-`$ as @AndrewCarroll mentioned in a [previous post](https://github.com/google/deepvariant/issues/83#issuecomment-553660314) $`-`$ by using [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md), it was found to be more accurate than using those gVCFs with GATK GenotypeGVCFs. Regarding missing SNPs in individual samples, their genotype might get a no call (`./.`) as noted in [this line of the GLNexus code](https://github.com/dnanexus-rnd/GLnexus/blob/main/src/service.cc#L206):. ```; Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele; ```. Though it probably could also get called as homozygous reference, if all the QC pass. Regarding impact on downstream analysis, probably the best bet is to try both approaches (DV-GLN-OPT and GATK-Joint) in parallel, and then validate both results. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876
https://github.com/google/deepvariant/issues/680#issuecomment-1640133876:1783,Testability,assert,assertion,1783,"K-Joint genotyping. If you look at the following paper:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8023681/pdf/btaa1081.pdf). Using parameter-optimization of GLNexus (such as minimum quality thresholds, among others listed under Supplementary Table 4), the authors were able to get a slightly different number of SNPs than via GATK-Joint:. ![image](https://github.com/google/deepvariant/assets/6555937/da3459b6-cb09-45b8-8fd9-9bbcdb0d12a7) . This is from Supplementary Figure 11 (A) found under Supplementary data, listed as [a link on this page](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So merging with GLNexus for DeepVariant gVCF output files $`-`$ as @AndrewCarroll mentioned in a [previous post](https://github.com/google/deepvariant/issues/83#issuecomment-553660314) $`-`$ by using [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md), it was found to be more accurate than using those gVCFs with GATK GenotypeGVCFs. Regarding missing SNPs in individual samples, their genotype might get a no call (`./.`) as noted in [this line of the GLNexus code](https://github.com/dnanexus-rnd/GLnexus/blob/main/src/service.cc#L206):. ```; Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele; ```. Though it probably could also get called as homozygous reference, if all the QC pass. Regarding impact on downstream analysis, probably the best bet is to try both approaches (DV-GLN-OPT and GATK-Joint) in parallel, and then validate both results. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876
https://github.com/google/deepvariant/issues/680#issuecomment-1641138568:651,Availability,recover,recovered,651,"Hi @egnarora . The way you are running DeepVariant (run on individual samples then genotype jointly with GLnexus) is correct and what we recommend. Thank you @pgrosu which is in agreement with the recommendation. @egnarora some external groups have performed analysis on strategies which use more extensive joint calling processes with DeepVariant (for example, discovering all variants in a cohort and then experimentally performing force calling on candidate positions). Regeneron is one example of a group that has conducted this analysis. Their conclusion is that there are not variant calls which are missed in the individual process that can be recovered by the more extensive joint calling, and their conclusion was that the recommendation to use GLnexus will not result in missed variants that another approach would capture. Hopefully this answers your question. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1641138568
https://github.com/google/deepvariant/issues/680#issuecomment-1641138568:249,Performance,perform,performed,249,"Hi @egnarora . The way you are running DeepVariant (run on individual samples then genotype jointly with GLnexus) is correct and what we recommend. Thank you @pgrosu which is in agreement with the recommendation. @egnarora some external groups have performed analysis on strategies which use more extensive joint calling processes with DeepVariant (for example, discovering all variants in a cohort and then experimentally performing force calling on candidate positions). Regeneron is one example of a group that has conducted this analysis. Their conclusion is that there are not variant calls which are missed in the individual process that can be recovered by the more extensive joint calling, and their conclusion was that the recommendation to use GLnexus will not result in missed variants that another approach would capture. Hopefully this answers your question. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1641138568
https://github.com/google/deepvariant/issues/680#issuecomment-1641138568:423,Performance,perform,performing,423,"Hi @egnarora . The way you are running DeepVariant (run on individual samples then genotype jointly with GLnexus) is correct and what we recommend. Thank you @pgrosu which is in agreement with the recommendation. @egnarora some external groups have performed analysis on strategies which use more extensive joint calling processes with DeepVariant (for example, discovering all variants in a cohort and then experimentally performing force calling on candidate positions). Regeneron is one example of a group that has conducted this analysis. Their conclusion is that there are not variant calls which are missed in the individual process that can be recovered by the more extensive joint calling, and their conclusion was that the recommendation to use GLnexus will not result in missed variants that another approach would capture. Hopefully this answers your question. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1641138568
https://github.com/google/deepvariant/issues/680#issuecomment-1641138568:651,Safety,recover,recovered,651,"Hi @egnarora . The way you are running DeepVariant (run on individual samples then genotype jointly with GLnexus) is correct and what we recommend. Thank you @pgrosu which is in agreement with the recommendation. @egnarora some external groups have performed analysis on strategies which use more extensive joint calling processes with DeepVariant (for example, discovering all variants in a cohort and then experimentally performing force calling on candidate positions). Regeneron is one example of a group that has conducted this analysis. Their conclusion is that there are not variant calls which are missed in the individual process that can be recovered by the more extensive joint calling, and their conclusion was that the recommendation to use GLnexus will not result in missed variants that another approach would capture. Hopefully this answers your question. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1641138568
https://github.com/google/deepvariant/issues/680#issuecomment-1641247564:254,Availability,avail,available,254,"The Honourable Andrew and Paul.; Thank you both very much for your kind answers and advice, it will be very helpful for me in my next endeavours, Deepvariant is a very efficient and useful piece of software, thank you for all your hard work in making it available to us. I will follow your advice and read some other people's research papers. Sincere thanks again.; Cheng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1641247564
https://github.com/google/deepvariant/issues/680#issuecomment-1641247564:168,Energy Efficiency,efficient,efficient,168,"The Honourable Andrew and Paul.; Thank you both very much for your kind answers and advice, it will be very helpful for me in my next endeavours, Deepvariant is a very efficient and useful piece of software, thank you for all your hard work in making it available to us. I will follow your advice and read some other people's research papers. Sincere thanks again.; Cheng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680#issuecomment-1641247564
https://github.com/google/deepvariant/issues/681#issuecomment-1641013019:495,Availability,avail,available,495,"Hi Taghrid,. I'm sorry to hear you are experiencing this. I just have a few questions:. 1) Have you tried first going through [DeepVariant Quick Start](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md) in order to check that a smaller DeepVariant run completes successfully on your system?; 2) How much free memory do you have?; 3) How much free disk space do you have?; 4) How many CPU cores do you have and how occupied are they?; 5) Do you NVIDIA GPUs that are available to you on your system?. I am assuming you are running this on a cluster as DeepVariant can be resource-intensive. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641013019
https://github.com/google/deepvariant/issues/681#issuecomment-1641030077:424,Usability,simpl,simplex,424,"Thanks @pgrosu. Knowing these would be very helpful. Besides compute, this can also be an issue with the input data. @Taghrid-M ,. Can you please tell a little more about the data in `HG004-hg38.ont.mm2.bam`:. 1) What chemistry is this data R9 or R10?; 2) What is the basecaller version you used for basecalling this data?; 3) What is the average read length of the reads?. Please note, DeepVariant currently supports R10.4 simplex and duplex variant calling for nanopore. If your data is from previous chemistry or basecaller version, please use [PEPPER](https://github.com/kishwarshafin/pepper) to call variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641030077
https://github.com/google/deepvariant/issues/681#issuecomment-1641137274:902,Availability,avail,available,902,"Thanks @pgrosu @kishwarshafin, I appreciate your swift reply!. Yes, I am using a cluster, and the data have been obtained from precisionFDA https://data.nist.gov/od/id/mds2-2336. **What chemistry is this data? Is it R9 or R10?**; This data was generated using R9.4 flow cells. **What is the basecaller version you used for basecalling this data?**; The basecalling process was performed using Guppy Version 3.6. **What is the average read length of the reads?**; 85X. **Have you tried first going through the DeepVariant Quick Start guide to check if a smaller DeepVariant run completes successfully on your system?**; Yes, I have successfully run it. **How much free memory do you have?**; 1.3T . **How much free disk space do you have?**; I have approximately 14T of free disk space. **How many CPU cores do you have, and what is their occupancy level?**; 16 CPU cores. **Do you have any NVIDIA GPUs available on your system?**; No.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274
https://github.com/google/deepvariant/issues/681#issuecomment-1641137274:377,Performance,perform,performed,377,"Thanks @pgrosu @kishwarshafin, I appreciate your swift reply!. Yes, I am using a cluster, and the data have been obtained from precisionFDA https://data.nist.gov/od/id/mds2-2336. **What chemistry is this data? Is it R9 or R10?**; This data was generated using R9.4 flow cells. **What is the basecaller version you used for basecalling this data?**; The basecalling process was performed using Guppy Version 3.6. **What is the average read length of the reads?**; 85X. **Have you tried first going through the DeepVariant Quick Start guide to check if a smaller DeepVariant run completes successfully on your system?**; Yes, I have successfully run it. **How much free memory do you have?**; 1.3T . **How much free disk space do you have?**; I have approximately 14T of free disk space. **How many CPU cores do you have, and what is their occupancy level?**; 16 CPU cores. **Do you have any NVIDIA GPUs available on your system?**; No.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274
https://github.com/google/deepvariant/issues/681#issuecomment-1641137274:533,Usability,guid,guide,533,"Thanks @pgrosu @kishwarshafin, I appreciate your swift reply!. Yes, I am using a cluster, and the data have been obtained from precisionFDA https://data.nist.gov/od/id/mds2-2336. **What chemistry is this data? Is it R9 or R10?**; This data was generated using R9.4 flow cells. **What is the basecaller version you used for basecalling this data?**; The basecalling process was performed using Guppy Version 3.6. **What is the average read length of the reads?**; 85X. **Have you tried first going through the DeepVariant Quick Start guide to check if a smaller DeepVariant run completes successfully on your system?**; Yes, I have successfully run it. **How much free memory do you have?**; 1.3T . **How much free disk space do you have?**; I have approximately 14T of free disk space. **How many CPU cores do you have, and what is their occupancy level?**; 16 CPU cores. **Do you have any NVIDIA GPUs available on your system?**; No.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274
https://github.com/google/deepvariant/issues/681#issuecomment-1641408177:254,Availability,error,error,254,"Hi @Taghrid-M,. This is good! One small thing, I think the average read length is 48,060 based on [this publication](; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9205427/pdf/main.pdf). The thing is that Guppy 3.6.0 is a bit old, and will have a higher error rate when processing the FAST5 signals from the R9 nanopore through the bidirectional RNN to generate the FASTQ file, [as shown in the following post](https://github.com/kishwarshafin/pepper/issues/90). . So that the proper SNP Pepper model gets selected internally, you can use the `--ont_r9_guppy4_hac` argument with `run_pepper_margin_deepvariant call_variants`, though not sure [version r0.8](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore) has the Guppy 4 model. Otherwise you can use [version r0.4 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.4/images/sha256-0a609f60aa45620b1949c0ffa06299554248d77e23f4186aaa1c3306a95f20a2?context=explore). . Ideally maybe you can get the FAST5 files from the [following Amazon S3 page](https://registry.opendata.aws/ont-open-data/) and reprocess them with Guppy 5 $`-`$ as that's the latest version that the Pepper model seems to be trained against $`-`$ so that you can then utilize the `--ont_r9_guppy5_sup` parameter with the [r0.8 container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore), or [version r0.5 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.5/images/sha256-13906107b84849590400074e84ce12aba051447a8118c84f31f2b20540fbd807?context=explore). Regarding troubleshooting maybe you can run it with `--dry` so you can get the individual commands, so you can run each one individually to determine where the bottleneck is stemming from. I'll wait for @kishwarshafin to confirm what would",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177
https://github.com/google/deepvariant/issues/681#issuecomment-1641408177:680,Modifiability,layers,layers,680,"Hi @Taghrid-M,. This is good! One small thing, I think the average read length is 48,060 based on [this publication](; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9205427/pdf/main.pdf). The thing is that Guppy 3.6.0 is a bit old, and will have a higher error rate when processing the FAST5 signals from the R9 nanopore through the bidirectional RNN to generate the FASTQ file, [as shown in the following post](https://github.com/kishwarshafin/pepper/issues/90). . So that the proper SNP Pepper model gets selected internally, you can use the `--ont_r9_guppy4_hac` argument with `run_pepper_margin_deepvariant call_variants`, though not sure [version r0.8](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore) has the Guppy 4 model. Otherwise you can use [version r0.4 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.4/images/sha256-0a609f60aa45620b1949c0ffa06299554248d77e23f4186aaa1c3306a95f20a2?context=explore). . Ideally maybe you can get the FAST5 files from the [following Amazon S3 page](https://registry.opendata.aws/ont-open-data/) and reprocess them with Guppy 5 $`-`$ as that's the latest version that the Pepper model seems to be trained against $`-`$ so that you can then utilize the `--ont_r9_guppy5_sup` parameter with the [r0.8 container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore), or [version r0.5 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.5/images/sha256-13906107b84849590400074e84ce12aba051447a8118c84f31f2b20540fbd807?context=explore). Regarding troubleshooting maybe you can run it with `--dry` so you can get the individual commands, so you can run each one individually to determine where the bottleneck is stemming from. I'll wait for @kishwarshafin to confirm what would",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177
https://github.com/google/deepvariant/issues/681#issuecomment-1641408177:923,Modifiability,layers,layers,923,"Hi @Taghrid-M,. This is good! One small thing, I think the average read length is 48,060 based on [this publication](; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9205427/pdf/main.pdf). The thing is that Guppy 3.6.0 is a bit old, and will have a higher error rate when processing the FAST5 signals from the R9 nanopore through the bidirectional RNN to generate the FASTQ file, [as shown in the following post](https://github.com/kishwarshafin/pepper/issues/90). . So that the proper SNP Pepper model gets selected internally, you can use the `--ont_r9_guppy4_hac` argument with `run_pepper_margin_deepvariant call_variants`, though not sure [version r0.8](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore) has the Guppy 4 model. Otherwise you can use [version r0.4 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.4/images/sha256-0a609f60aa45620b1949c0ffa06299554248d77e23f4186aaa1c3306a95f20a2?context=explore). . Ideally maybe you can get the FAST5 files from the [following Amazon S3 page](https://registry.opendata.aws/ont-open-data/) and reprocess them with Guppy 5 $`-`$ as that's the latest version that the Pepper model seems to be trained against $`-`$ so that you can then utilize the `--ont_r9_guppy5_sup` parameter with the [r0.8 container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore), or [version r0.5 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.5/images/sha256-13906107b84849590400074e84ce12aba051447a8118c84f31f2b20540fbd807?context=explore). Regarding troubleshooting maybe you can run it with `--dry` so you can get the individual commands, so you can run each one individually to determine where the bottleneck is stemming from. I'll wait for @kishwarshafin to confirm what would",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177
https://github.com/google/deepvariant/issues/681#issuecomment-1641408177:1423,Modifiability,layers,layers,1423,"hink the average read length is 48,060 based on [this publication](; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9205427/pdf/main.pdf). The thing is that Guppy 3.6.0 is a bit old, and will have a higher error rate when processing the FAST5 signals from the R9 nanopore through the bidirectional RNN to generate the FASTQ file, [as shown in the following post](https://github.com/kishwarshafin/pepper/issues/90). . So that the proper SNP Pepper model gets selected internally, you can use the `--ont_r9_guppy4_hac` argument with `run_pepper_margin_deepvariant call_variants`, though not sure [version r0.8](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore) has the Guppy 4 model. Otherwise you can use [version r0.4 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.4/images/sha256-0a609f60aa45620b1949c0ffa06299554248d77e23f4186aaa1c3306a95f20a2?context=explore). . Ideally maybe you can get the FAST5 files from the [following Amazon S3 page](https://registry.opendata.aws/ont-open-data/) and reprocess them with Guppy 5 $`-`$ as that's the latest version that the Pepper model seems to be trained against $`-`$ so that you can then utilize the `--ont_r9_guppy5_sup` parameter with the [r0.8 container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore), or [version r0.5 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.5/images/sha256-13906107b84849590400074e84ce12aba051447a8118c84f31f2b20540fbd807?context=explore). Regarding troubleshooting maybe you can run it with `--dry` so you can get the individual commands, so you can run each one individually to determine where the bottleneck is stemming from. I'll wait for @kishwarshafin to confirm what would be the most effective approach. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177
https://github.com/google/deepvariant/issues/681#issuecomment-1641408177:1625,Modifiability,layers,layers,1625,"hink the average read length is 48,060 based on [this publication](; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9205427/pdf/main.pdf). The thing is that Guppy 3.6.0 is a bit old, and will have a higher error rate when processing the FAST5 signals from the R9 nanopore through the bidirectional RNN to generate the FASTQ file, [as shown in the following post](https://github.com/kishwarshafin/pepper/issues/90). . So that the proper SNP Pepper model gets selected internally, you can use the `--ont_r9_guppy4_hac` argument with `run_pepper_margin_deepvariant call_variants`, though not sure [version r0.8](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore) has the Guppy 4 model. Otherwise you can use [version r0.4 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.4/images/sha256-0a609f60aa45620b1949c0ffa06299554248d77e23f4186aaa1c3306a95f20a2?context=explore). . Ideally maybe you can get the FAST5 files from the [following Amazon S3 page](https://registry.opendata.aws/ont-open-data/) and reprocess them with Guppy 5 $`-`$ as that's the latest version that the Pepper model seems to be trained against $`-`$ so that you can then utilize the `--ont_r9_guppy5_sup` parameter with the [r0.8 container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore), or [version r0.5 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.5/images/sha256-13906107b84849590400074e84ce12aba051447a8118c84f31f2b20540fbd807?context=explore). Regarding troubleshooting maybe you can run it with `--dry` so you can get the individual commands, so you can run each one individually to determine where the bottleneck is stemming from. I'll wait for @kishwarshafin to confirm what would be the most effective approach. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177
https://github.com/google/deepvariant/issues/681#issuecomment-1641408177:1922,Performance,bottleneck,bottleneck,1922,"hink the average read length is 48,060 based on [this publication](; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9205427/pdf/main.pdf). The thing is that Guppy 3.6.0 is a bit old, and will have a higher error rate when processing the FAST5 signals from the R9 nanopore through the bidirectional RNN to generate the FASTQ file, [as shown in the following post](https://github.com/kishwarshafin/pepper/issues/90). . So that the proper SNP Pepper model gets selected internally, you can use the `--ont_r9_guppy4_hac` argument with `run_pepper_margin_deepvariant call_variants`, though not sure [version r0.8](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore) has the Guppy 4 model. Otherwise you can use [version r0.4 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.4/images/sha256-0a609f60aa45620b1949c0ffa06299554248d77e23f4186aaa1c3306a95f20a2?context=explore). . Ideally maybe you can get the FAST5 files from the [following Amazon S3 page](https://registry.opendata.aws/ont-open-data/) and reprocess them with Guppy 5 $`-`$ as that's the latest version that the Pepper model seems to be trained against $`-`$ so that you can then utilize the `--ont_r9_guppy5_sup` parameter with the [r0.8 container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore), or [version r0.5 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.5/images/sha256-13906107b84849590400074e84ce12aba051447a8118c84f31f2b20540fbd807?context=explore). Regarding troubleshooting maybe you can run it with `--dry` so you can get the individual commands, so you can run each one individually to determine where the bottleneck is stemming from. I'll wait for @kishwarshafin to confirm what would be the most effective approach. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177
https://github.com/google/deepvariant/issues/682#issuecomment-1642808771:105,Safety,avoid,avoid,105,"He Andrew. It's a diploid organism with an ancient tetraploid structure. That's ancient enough it should avoid any mapping issue.. so, diploid it is. . Yes ... What we saw in the experimental evolution results, is that some independent lines get the same SNP. It doesn't seem random, like there were genetic clusters in the clonal ""or so we thought"" amcestral population. Still, du the mode of reproduction and very ""obbious signal"" I would not expect anything less than 10% of all alleles to be relevant. Does this answer your question? . For this question to be complete: I forgot to link the answer of Clair3 authors https://github.com/HKU-BAL/Clair3/issues/210#issuecomment-1642527205. I don't favour one over the other nor am I trying to start a conflict ^^ but I think it's an interesting technical discussion! . Thank you ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1642808771
https://github.com/google/deepvariant/issues/682#issuecomment-1642950331:451,Availability,error,error,451,"Hi @Axze-rgb . Anything recommended is going to require some investigation on your part. It might send you on a fruitless chase, but it could also be interesting. DeepVariant is trained to assess probabilities of diploid models, and its training data is diploid with some likely minor subclonal acquired variants. We've found that for diploid genomes, the output probabilities (represented as GQ values) are very well calibrated against the empirical error probability, and so are highly informative with respect to the quality of a call. . Subclonal variants will not look like the signature of true germline variants in a diploid organism, and so will not be likely to be assigned a high probability of a variant call. However, it is also the case that subclonal variants will not look like the signature of the noise-derived errors typical for rejected calls. As a result, it could be the case that the probabilities for the genotype classes for REF (0/0) or No-Call (./.) are informative in a manner you could use. DeepVariant will nominate candidates and write output for any SNP site that has the following properties: At least 2 non-variant alleles of a given candidate and a frequency of 0.12 or higher. These parameters can be changed by adding the following line to the DeepVariant command:. `--make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12`. Replacing 2 and 0.12 with the values you want. Keep in mind that decreasing these can increase runtime, so you'll likely want to experiment on running a region to estimate the proportion of candidates you create. Given that HiFi reads area are already very accurate and SNP calling on them is much easier than with other technologies, it may be the case that if you plot the GQ distributions of the REF (0/0) and No-call (./.) lines, you may be able to identify a group that looks like they may be subclonal variants. For this, you would need to do a reasonable amount of plotting, stratification by GQ and the reported V",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1642950331
https://github.com/google/deepvariant/issues/682#issuecomment-1642950331:828,Availability,error,errors,828,"Hi @Axze-rgb . Anything recommended is going to require some investigation on your part. It might send you on a fruitless chase, but it could also be interesting. DeepVariant is trained to assess probabilities of diploid models, and its training data is diploid with some likely minor subclonal acquired variants. We've found that for diploid genomes, the output probabilities (represented as GQ values) are very well calibrated against the empirical error probability, and so are highly informative with respect to the quality of a call. . Subclonal variants will not look like the signature of true germline variants in a diploid organism, and so will not be likely to be assigned a high probability of a variant call. However, it is also the case that subclonal variants will not look like the signature of the noise-derived errors typical for rejected calls. As a result, it could be the case that the probabilities for the genotype classes for REF (0/0) or No-Call (./.) are informative in a manner you could use. DeepVariant will nominate candidates and write output for any SNP site that has the following properties: At least 2 non-variant alleles of a given candidate and a frequency of 0.12 or higher. These parameters can be changed by adding the following line to the DeepVariant command:. `--make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12`. Replacing 2 and 0.12 with the values you want. Keep in mind that decreasing these can increase runtime, so you'll likely want to experiment on running a region to estimate the proportion of candidates you create. Given that HiFi reads area are already very accurate and SNP calling on them is much easier than with other technologies, it may be the case that if you plot the GQ distributions of the REF (0/0) and No-call (./.) lines, you may be able to identify a group that looks like they may be subclonal variants. For this, you would need to do a reasonable amount of plotting, stratification by GQ and the reported V",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1642950331
https://github.com/google/deepvariant/issues/682#issuecomment-1643589679:160,Deployability,update,updated,160,"Hey @AndrewCarroll . This is a very interesting idea that I am going to try asap :) This whole study is taking much more time than planned, but I will keep you updated on the results! . Thanks a lot for your commitment",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1643589679
https://github.com/google/deepvariant/issues/682#issuecomment-1647816652:337,Usability,Clear,Clearly,337,"Hello @AndrewCarroll I launched DeepVariant as suggested, and then plotted the GQ distrubution. Here is a Gaussian kernel I fitted on it to better see (some people saw 2 peaks, others 4). So, here is what Gau would see . ; ![density_gaussian](https://github.com/google/deepvariant/assets/81575666/eeb28721-204b-47b6-9ea0-750aca1ba21f). Clearly 4 peaks (github horribly compresses the plot, actually here it's hard to see the 2 peaks on the right end)... now I am gonna investigate in relation to the AF. I already had a quick look at the data themselves, that's gonna be fun ... some AF are so small. . I will keep you posted. Don't hesitate to comment. EDIT: I changed the plot type, it's much better now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1647816652
https://github.com/google/deepvariant/issues/682#issuecomment-1648181613:133,Security,validat,validate,133,"Hi @Axze-rgb,. That's good, and that just tells you that there might be high GQ regions of possible correct calls, which you need to validate. Now what you want is to determine the stationary areas of variation, which would not fall within noise signatures. As Andrew mentioned, you want to look at GQ regions with their VAF, and begin to stratify confidence regions backed up by good alignment of reads (IGV, or your favorite browser). Then you want to determine what regions of high-confidence with stable high GQ (correct call) mean in terms of VAF genotype regions, which might look like this (but not exactly):. ![image](https://github.com/google/deepvariant/assets/6555937/0847f683-8d69-4bb6-a6d4-eddd7e166993). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1648181613
https://github.com/google/deepvariant/issues/682#issuecomment-1648455718:1155,Usability,Clear,Clearly,1155,"It's all the variants that have been called RefCall, actually, see the command:. ```; bcftools query -f ""%FILTER\t[ %GQ]\n"" OUTPUT_VCF|awk '$1==""RefCall""' > RefCall_all_GQ; ```. So 1/1 and 0/1 should be excluded, unless Deepvariant consider 1/1 as a RefCall. . I am asking around about if I am allowed to share the data, they don't ""belong' to me (though we could talk days about what it means to own scientific data). I am doing my best to get a ""yes"" answer. cheers. PS: precisely, at the moment I wanted to make a kind of hexbin plot of VAF vs GQ, however I am encountering a problem with sites like those: . Chrom_3 7095984 . A ACGAACGTTTTGCGATAGTATTTAACAAAATATGTTATATGTTTTTCATTGAAATCAATCTTTAATAGAATTTCTTTTAGTATTTGTGTATACATAAATGTTGGATCTAAATGATCGATATTTTCGACGGATTTTTTGTTACTTCGAGGAACGAAACTAATCGAAATCGAATCTTGATCACAATTTTCTGCATCTTGTTTCAATTTTTGACTTAAAGAT,ATGAACGTTTTGCGATAGTATTTCACAAAATATGTTATATGTTTTTCATTGAAATCAATCTTTAATAGAATTTCTTTTAGTATTTGTGTATACATAAATGTTGGATCTAAATGATCGATATTTTCGACGGATTTTTTGTTACTTGGAGGAACGAAACTAATTGAAATCGAATCTTGATCACAATTTTGTGCATCTTGTTTCAATTTTTCACTTAAAGAT 4.3 RefCall . GT:GQ:DP:AD:VAF:PL ./.:2:217:3,47,64:0.21659,0.294931:0,12,3,12,2,3. Clearly DeepVariant thinks there are 3 potential alleles, but that in the end it can't determine ... ; While most of the time I have 35 (GQ) against whatever, but a single value. ; This is causing my code to plot complete nonsense with negative GQ showing up in the plot. At the moment I don't know if I should find a way to replace the 2 values by a single one, or find a way to deal with the complexity in my plot. I would appreciate if you have an idea ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1648455718
https://github.com/google/deepvariant/issues/682#issuecomment-1648597119:1458,Availability,error,error,1458,"Hi @Axze-rgb,. Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types $`-`$ that's the basic hypothesis here. So here's a quick way you can fix the multiallelic issue above:. $`1)`$ First split the multiallelic sites into biallelic records like this:. ```; bcftools norm -m - multi_allelic.vcf > biallelic.vcf; ```. $`2)`$ Then parse for the `0/0` and `./.` genotypes $`-`$ I'm assuming your genotypes are not phased:. ```; bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; ```. Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1648597119
https://github.com/google/deepvariant/issues/682#issuecomment-1648597119:357,Performance,optimiz,optimizes,357,"Hi @Axze-rgb,. Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types $`-`$ that's the basic hypothesis here. So here's a quick way you can fix the multiallelic issue above:. $`1)`$ First split the multiallelic sites into biallelic records like this:. ```; bcftools norm -m - multi_allelic.vcf > biallelic.vcf; ```. $`2)`$ Then parse for the `0/0` and `./.` genotypes $`-`$ I'm assuming your genotypes are not phased:. ```; bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; ```. Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1648597119
https://github.com/google/deepvariant/issues/682#issuecomment-1648597119:1378,Performance,load,load,1378,"Hi @Axze-rgb,. Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types $`-`$ that's the basic hypothesis here. So here's a quick way you can fix the multiallelic issue above:. $`1)`$ First split the multiallelic sites into biallelic records like this:. ```; bcftools norm -m - multi_allelic.vcf > biallelic.vcf; ```. $`2)`$ Then parse for the `0/0` and `./.` genotypes $`-`$ I'm assuming your genotypes are not phased:. ```; bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; ```. Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1648597119
https://github.com/google/deepvariant/issues/682#issuecomment-1648597119:791,Testability,test,test,791,"Hi @Axze-rgb,. Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types $`-`$ that's the basic hypothesis here. So here's a quick way you can fix the multiallelic issue above:. $`1)`$ First split the multiallelic sites into biallelic records like this:. ```; bcftools norm -m - multi_allelic.vcf > biallelic.vcf; ```. $`2)`$ Then parse for the `0/0` and `./.` genotypes $`-`$ I'm assuming your genotypes are not phased:. ```; bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; ```. Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1648597119
https://github.com/google/deepvariant/issues/682#issuecomment-1649296164:367,Usability,learn,learned,367,"Hello @pgrosu ; Thanks for the bcftools suggestion, exactly what's needed. I will try to digest your comment about how the model was trained. Thank you for explaining! I will report back. ; @AndrewCarroll I am gonna send you the vcf file, from a protonmail address (the proton domain is sometimes blocked by servers). . Thanks everyone for the great disussion, I hav learned a lot.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1649296164
https://github.com/google/deepvariant/issues/682#issuecomment-1650079569:503,Performance,perform,perform,503,"Hello, here is a VAF vs GQ plot. English is not my native language so I am not sure how to say it appropriately but what the?. It seems GQ is not correlated with VAF; ![histogram2d](https://github.com/google/deepvariant/assets/81575666/9dc535f1-8337-4d51-b82b-7d58ddeec006). I am not sure what to expect since we selected for RefCall in the end, maybe it's actually correct.; I am gonna redo the call with Clair3 and see if it gives results as weird as those ones. . EDIT: side note, does make_examples perform any kind of realignment?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650079569
https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:770,Availability,error,errors,770,"Hi @Axze-rgb,. So we know that this organism replicates through clonal inheritance to transfer of mutations to daughter cells that would be driven by genetic drift or selection forces, and usually not by lineages. Though you say you noticed genetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inherit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876
https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:71,Modifiability,inherit,inheritance,71,"Hi @Axze-rgb,. So we know that this organism replicates through clonal inheritance to transfer of mutations to daughter cells that would be driven by genetic drift or selection forces, and usually not by lineages. Though you say you noticed genetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inherit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876
https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:1045,Modifiability,evolve,evolve,1045,"Hi @Axze-rgb,. So we know that this organism replicates through clonal inheritance to transfer of mutations to daughter cells that would be driven by genetic drift or selection forces, and usually not by lineages. Though you say you noticed genetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inherit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876
https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:1994,Modifiability,inherit,inheritance,1994,"enetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inheritance, validated by large coverage to ensure they are true variants. This will provide you the drivers of the mutations and transmissions -- some of which might be more stable than others given different selection forces. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876
https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:1093,Security,validat,validate,1093,"ions to daughter cells that would be driven by genetic drift or selection forces, and usually not by lineages. Though you say you noticed genetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inheritance, validated by large coverage to ensure they are true variants. This will provide you the drivers ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876
https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:1222,Security,validat,validate,1222,"enetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inheritance, validated by large coverage to ensure they are true variants. This will provide you the drivers of the mutations and transmissions -- some of which might be more stable than others given different selection forces. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876
https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:2007,Security,validat,validated,2007,"enetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inheritance, validated by large coverage to ensure they are true variants. This will provide you the drivers of the mutations and transmissions -- some of which might be more stable than others given different selection forces. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876
https://github.com/google/deepvariant/issues/682#issuecomment-1650261666:141,Modifiability,inherit,inheritance,141,"Hi @pgrosu ; This is a fascinating perspective on how to use variant callers here... actually, we don't ""know"" they reproduce through clonal inheritance. No one has ever seen a male or male organ in 2 centuries. And in the labs, when we see the pattern we have here, we tend to believe indeed they reproduce through clonal inheritance. . But. there are (solid) data and papers that show that when you go outside of the lab do some population genetics, the results are perfectly compatible with sexual reproduction. That doesn't mean it's happening here, of course. But we actually can't assume they are clonal. Or, we can, but we must keep in mind it's absolutely not guaranteed. . Do you think it makes sense to compare with Clair3?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650261666
https://github.com/google/deepvariant/issues/682#issuecomment-1650261666:323,Modifiability,inherit,inheritance,323,"Hi @pgrosu ; This is a fascinating perspective on how to use variant callers here... actually, we don't ""know"" they reproduce through clonal inheritance. No one has ever seen a male or male organ in 2 centuries. And in the labs, when we see the pattern we have here, we tend to believe indeed they reproduce through clonal inheritance. . But. there are (solid) data and papers that show that when you go outside of the lab do some population genetics, the results are perfectly compatible with sexual reproduction. That doesn't mean it's happening here, of course. But we actually can't assume they are clonal. Or, we can, but we must keep in mind it's absolutely not guaranteed. . Do you think it makes sense to compare with Clair3?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650261666
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:3814,Availability,reliab,reliably,3814,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:3900,Availability,down,downstream,3900,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:298,Energy Efficiency,efficient,efficient,298,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:772,Energy Efficiency,efficient,efficient,772,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1627,Energy Efficiency,adapt,adapts,1627,"rsing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data cond",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1906,Energy Efficiency,adapt,adapt,1906,"n by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:710,Integrability,depend,dependencies,710,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1250,Modifiability,variab,variable,1250,"M is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1627,Modifiability,adapt,adapts,1627,"rsing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data cond",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1856,Modifiability,variab,variability,1856,"n by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1906,Modifiability,adapt,adapt,1906,"n by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2693,Modifiability,variab,variable,2693,"ither DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:3138,Modifiability,variab,variability,3138,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:959,Safety,predict,predict,959,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:3823,Safety,predict,predictable,3823,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2655,Security,validat,validated,2655," for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:3401,Security,validat,validate,3401,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:20,Usability,clear,clearly,20,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1175,Usability,learn,learn,1175,"t might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1196,Usability,learn,learning,1196,"t might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1686,Usability,learn,learning,1686,"stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2026,Usability,simpl,simplified,2026," price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new mode",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2199,Usability,simpl,simple,2199,"y use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2532,Usability,learn,learning,2532," for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832
https://github.com/google/deepvariant/issues/682#issuecomment-1650862825:134,Energy Efficiency,green,green,134,"Hello, very great answer. I have other data including from an artificial evolution experiment. But I can't tell you more here without green light from my PIs. But definitely this thread is one of the most useful ever. Again, thank you so much all. I really really appreciate it. @AndrewCarroll did you get the file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1650862825
https://github.com/google/deepvariant/issues/682#issuecomment-1651199762:1495,Availability,error,error,1495,"> Hi @Axze-rgb,; > ; > Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types  that's the basic hypothesis here.; > ; > So here's a quick way you can fix the multiallelic issue above:; > ; > 1) First split the multiallelic sites into biallelic records like this:; > ; > ```; > bcftools norm -m - multi_allelic.vcf > biallelic.vcf; > ```; > ; > 2) Then parse for the `0/0` and `./.` genotypes  I'm assuming your genotypes are not phased:; > ; > ```; > bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; > ```; > ; > Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data.; > ; > Hope it helps, ~p. For the record there is an issue with your second code, only the first is generated by cut, or it depends on the unix system. ; It's easy to fix by asking bcftools itself to make the new line; > bcftools query -f '[%GT,%GQ,%VAF\n]' biallelic.vcf | grep '\./\.\|0/0' > gq_vaf.csv. I know you probably did it from memory, but in case someone else finds the thread ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762
https://github.com/google/deepvariant/issues/682#issuecomment-1651199762:1732,Integrability,depend,depends,1732,"> Hi @Axze-rgb,; > ; > Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types  that's the basic hypothesis here.; > ; > So here's a quick way you can fix the multiallelic issue above:; > ; > 1) First split the multiallelic sites into biallelic records like this:; > ; > ```; > bcftools norm -m - multi_allelic.vcf > biallelic.vcf; > ```; > ; > 2) Then parse for the `0/0` and `./.` genotypes  I'm assuming your genotypes are not phased:; > ; > ```; > bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; > ```; > ; > Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data.; > ; > Hope it helps, ~p. For the record there is an issue with your second code, only the first is generated by cut, or it depends on the unix system. ; It's easy to fix by asking bcftools itself to make the new line; > bcftools query -f '[%GT,%GQ,%VAF\n]' biallelic.vcf | grep '\./\.\|0/0' > gq_vaf.csv. I know you probably did it from memory, but in case someone else finds the thread ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762
https://github.com/google/deepvariant/issues/682#issuecomment-1651199762:365,Performance,optimiz,optimizes,365,"> Hi @Axze-rgb,; > ; > Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types  that's the basic hypothesis here.; > ; > So here's a quick way you can fix the multiallelic issue above:; > ; > 1) First split the multiallelic sites into biallelic records like this:; > ; > ```; > bcftools norm -m - multi_allelic.vcf > biallelic.vcf; > ```; > ; > 2) Then parse for the `0/0` and `./.` genotypes  I'm assuming your genotypes are not phased:; > ; > ```; > bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; > ```; > ; > Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data.; > ; > Hope it helps, ~p. For the record there is an issue with your second code, only the first is generated by cut, or it depends on the unix system. ; It's easy to fix by asking bcftools itself to make the new line; > bcftools query -f '[%GT,%GQ,%VAF\n]' biallelic.vcf | grep '\./\.\|0/0' > gq_vaf.csv. I know you probably did it from memory, but in case someone else finds the thread ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762
https://github.com/google/deepvariant/issues/682#issuecomment-1651199762:1415,Performance,load,load,1415,"> Hi @Axze-rgb,; > ; > Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types  that's the basic hypothesis here.; > ; > So here's a quick way you can fix the multiallelic issue above:; > ; > 1) First split the multiallelic sites into biallelic records like this:; > ; > ```; > bcftools norm -m - multi_allelic.vcf > biallelic.vcf; > ```; > ; > 2) Then parse for the `0/0` and `./.` genotypes  I'm assuming your genotypes are not phased:; > ; > ```; > bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; > ```; > ; > Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data.; > ; > Hope it helps, ~p. For the record there is an issue with your second code, only the first is generated by cut, or it depends on the unix system. ; It's easy to fix by asking bcftools itself to make the new line; > bcftools query -f '[%GT,%GQ,%VAF\n]' biallelic.vcf | grep '\./\.\|0/0' > gq_vaf.csv. I know you probably did it from memory, but in case someone else finds the thread ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762
https://github.com/google/deepvariant/issues/682#issuecomment-1651199762:799,Testability,test,test,799,"> Hi @Axze-rgb,; > ; > Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types  that's the basic hypothesis here.; > ; > So here's a quick way you can fix the multiallelic issue above:; > ; > 1) First split the multiallelic sites into biallelic records like this:; > ; > ```; > bcftools norm -m - multi_allelic.vcf > biallelic.vcf; > ```; > ; > 2) Then parse for the `0/0` and `./.` genotypes  I'm assuming your genotypes are not phased:; > ; > ```; > bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; > ```; > ; > Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data.; > ; > Hope it helps, ~p. For the record there is an issue with your second code, only the first is generated by cut, or it depends on the unix system. ; It's easy to fix by asking bcftools itself to make the new line; > bcftools query -f '[%GT,%GQ,%VAF\n]' biallelic.vcf | grep '\./\.\|0/0' > gq_vaf.csv. I know you probably did it from memory, but in case someone else finds the thread ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762
https://github.com/google/deepvariant/issues/682#issuecomment-1652518468:82,Availability,down,down,82,"Hi @Axze-rgb and Andrew,. Good catch on the newline character  its hard to slow down at times when things are fun :) . Before having a presentation -- as my bandwidth is a bit tight these days (though I usually love presentations) -- I would be curious to see what you first get from Clair3 and Andrew's candidate region, as well as any additional insight Andrew might get from the VCF. I have this funny feeling of what the outcome might be  and we can have the presentation later among the three of us (and anyone else that's interested) as things emerge more clearly. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1652518468
https://github.com/google/deepvariant/issues/682#issuecomment-1652518468:565,Usability,clear,clearly,565,"Hi @Axze-rgb and Andrew,. Good catch on the newline character  its hard to slow down at times when things are fun :) . Before having a presentation -- as my bandwidth is a bit tight these days (though I usually love presentations) -- I would be curious to see what you first get from Clair3 and Andrew's candidate region, as well as any additional insight Andrew might get from the VCF. I have this funny feeling of what the outcome might be  and we can have the presentation later among the three of us (and anyone else that's interested) as things emerge more clearly. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1652518468
https://github.com/google/deepvariant/issues/682#issuecomment-1655336903:315,Availability,error,error,315,"This is impressive work! Though it looks pretty cool, let's ask ourselves what is happening here. The biological system went through clonal events (let's suppose under some non-random selective conditions), where the sequencer hopefully captured the state of those events as complete as possible (i.e. with minimal error). Then we pass those through two processes (DeepVariant and Clair3), which respond with areas of variation mirroring previous signals it was trained with to maximally respond upon. The key is, are these events biologically meaningful? So if you look in the literature or known databases -- for your organism -- would these changes be reflective of something significant within exomic (gene) regions, or pathways that might be activated? If so why, and what might be those non-random selective conditions that make these genomic changes both significant and predictable?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1655336903
https://github.com/google/deepvariant/issues/682#issuecomment-1655336903:878,Safety,predict,predictable,878,"This is impressive work! Though it looks pretty cool, let's ask ourselves what is happening here. The biological system went through clonal events (let's suppose under some non-random selective conditions), where the sequencer hopefully captured the state of those events as complete as possible (i.e. with minimal error). Then we pass those through two processes (DeepVariant and Clair3), which respond with areas of variation mirroring previous signals it was trained with to maximally respond upon. The key is, are these events biologically meaningful? So if you look in the literature or known databases -- for your organism -- would these changes be reflective of something significant within exomic (gene) regions, or pathways that might be activated? If so why, and what might be those non-random selective conditions that make these genomic changes both significant and predictable?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1655336903
https://github.com/google/deepvariant/issues/682#issuecomment-1655361098:273,Availability,repair,repair,273,"Well, why do you want those events to be non random? . In nature they seem to do this; https://www.nature.com/articles/s41467-020-19614-y. In lab we see them do that ; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9710870/. Random breaks during their meiosis and subsequent repair might have generated sublonces in the petri dish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1655361098
https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:271,Availability,repair,repair,271,"Perfectly reasonable question, and thank you for the references. I mentioned non-random selective conditions that should be present, since you observed the preservation of SNPs in independent lines. Sure random breaks and other DNA damage can occur with their subsequent repair, though the papers have very different goals. If you want to be able to compare them from the point of view of damage-to-repair, it will be a bit difficult. Let me explain why through the papers:. $`\underline{In \; the \; Lab \; (from \; the \; paper)}`$. - This paper showed how the preservation of viability through repair via homologs (preserving consensus) or other DNA repair mechanisms is significant to this model organism. Below are a few excerpts denoting this:. - _*""The signatures of IHR [interhomolog recombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342
https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:399,Availability,repair,repair,399,"Perfectly reasonable question, and thank you for the references. I mentioned non-random selective conditions that should be present, since you observed the preservation of SNPs in independent lines. Sure random breaks and other DNA damage can occur with their subsequent repair, though the papers have very different goals. If you want to be able to compare them from the point of view of damage-to-repair, it will be a bit difficult. Let me explain why through the papers:. $`\underline{In \; the \; Lab \; (from \; the \; paper)}`$. - This paper showed how the preservation of viability through repair via homologs (preserving consensus) or other DNA repair mechanisms is significant to this model organism. Below are a few excerpts denoting this:. - _*""The signatures of IHR [interhomolog recombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342
https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:597,Availability,repair,repair,597,"Perfectly reasonable question, and thank you for the references. I mentioned non-random selective conditions that should be present, since you observed the preservation of SNPs in independent lines. Sure random breaks and other DNA damage can occur with their subsequent repair, though the papers have very different goals. If you want to be able to compare them from the point of view of damage-to-repair, it will be a bit difficult. Let me explain why through the papers:. $`\underline{In \; the \; Lab \; (from \; the \; paper)}`$. - This paper showed how the preservation of viability through repair via homologs (preserving consensus) or other DNA repair mechanisms is significant to this model organism. Below are a few excerpts denoting this:. - _*""The signatures of IHR [interhomolog recombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342
https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:653,Availability,repair,repair,653,"Perfectly reasonable question, and thank you for the references. I mentioned non-random selective conditions that should be present, since you observed the preservation of SNPs in independent lines. Sure random breaks and other DNA damage can occur with their subsequent repair, though the papers have very different goals. If you want to be able to compare them from the point of view of damage-to-repair, it will be a bit difficult. Let me explain why through the papers:. $`\underline{In \; the \; Lab \; (from \; the \; paper)}`$. - This paper showed how the preservation of viability through repair via homologs (preserving consensus) or other DNA repair mechanisms is significant to this model organism. Below are a few excerpts denoting this:. - _*""The signatures of IHR [interhomolog recombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342
https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:1174,Availability,repair,repair,1174,"ks and other DNA damage can occur with their subsequent repair, though the papers have very different goals. If you want to be able to compare them from the point of view of damage-to-repair, it will be a bit difficult. Let me explain why through the papers:. $`\underline{In \; the \; Lab \; (from \; the \; paper)}`$. - This paper showed how the preservation of viability through repair via homologs (preserving consensus) or other DNA repair mechanisms is significant to this model organism. Below are a few excerpts denoting this:. - _*""The signatures of IHR [interhomolog recombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying to show the randomization of variation. This is a very different goal than trying to show the preservation of gene function under variation. Their focus was more on the linkage between SNPs, and chose to carefully lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342
https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:1819,Availability,repair,repair,1819,"ombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying to show the randomization of variation. This is a very different goal than trying to show the preservation of gene function under variation. Their focus was more on the linkage between SNPs, and chose to carefully look at how differences progress across lineages. If you look at one of the gene specific analysis was done for the COX1 gene as a phylogenetic tree -- presented in the Supplementary Materials -- it illustrated high similarity among the lineages:. ![image](https://github.com/google/deepvariant/assets/6555937/a119536e-cdc4-4ea3-a06a-fbf32b2f3d8e). In my experience when nature finds a strong model, it usually continues to fight entropy to preserve itself. To show that, one would have to explore the functional analysis of the lineages, showing that the variation (SNPs) was eith",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342
https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:1556,Energy Efficiency,efficient,efficiently,1556,"d how the preservation of viability through repair via homologs (preserving consensus) or other DNA repair mechanisms is significant to this model organism. Below are a few excerpts denoting this:. - _*""The signatures of IHR [interhomolog recombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying to show the randomization of variation. This is a very different goal than trying to show the preservation of gene function under variation. Their focus was more on the linkage between SNPs, and chose to carefully look at how differences progress across lineages. If you look at one of the gene specific analysis was done for the COX1 gene as a phylogenetic tree -- presented in the Supplementary Materials -- it illustrated high similarity among the lineages:. ![image](https://github.com/google/deepvariant/assets/6555937/a119536e-cdc4-4ea3-a06a-fbf32b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342
https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:1184,Safety,safe,safeguard,1184,"ks and other DNA damage can occur with their subsequent repair, though the papers have very different goals. If you want to be able to compare them from the point of view of damage-to-repair, it will be a bit difficult. Let me explain why through the papers:. $`\underline{In \; the \; Lab \; (from \; the \; paper)}`$. - This paper showed how the preservation of viability through repair via homologs (preserving consensus) or other DNA repair mechanisms is significant to this model organism. Below are a few excerpts denoting this:. - _*""The signatures of IHR [interhomolog recombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying to show the randomization of variation. This is a very different goal than trying to show the preservation of gene function under variation. Their focus was more on the linkage between SNPs, and chose to carefully lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342
https://github.com/google/deepvariant/issues/682#issuecomment-1658655489:321,Availability,error,error,321,"@Axze-rgb That's correct, you have to use AD (allele depth), which counts only informative reads out of the total reads (that IGV shows):. VAF = 15/(75+15) = 0.166667. AD refers to the reference and alternate alleles (in comma-separated order) that pass the filters, IGV shows all of them. The GQ is good with a 0.0631 % error the call is incorrect, or 93.96 % the call is correct. Does that help?. Thanks,; `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658655489
https://github.com/google/deepvariant/issues/682#issuecomment-1658730637:940,Integrability,Message,Message,940,"Hi sorry to jump on this thread but how sure are you of genomic stability; in your organism?. Just thinking about, mutation rates, did you amplify the pacbio sample,; have you checked for methylation?. Joe. On Mon, 31 Jul 2023, 17:17 Axze-rgb, ***@***.***> wrote:. > And that doesn't bother you? We interpret this as low clonal presence,; > while we have actually massive bad mapping of the T at the same place?; > Coincidence? Or are the few ""good"" Ts an artefact?; >; > My advisor once told me if I stopped asking such question I probably would; > have a PhD already, but ... I am warry of coincidence.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658710664>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2UURSWJPUBSPQQJU3LXS7LBBANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you are subscribed to this thread.Message; > ID: ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658730637
https://github.com/google/deepvariant/issues/682#issuecomment-1658739773:640,Integrability,Message,Message,640,"No problem, your variant of interest isn't a genomic region that may be; hyper variable ie a simple sequence repeat (they can occur in coding; regions) or something else that may lead to the variability your seeing?. Joe. On Mon, 31 Jul 2023, 17:30 Axze-rgb, ***@***.***> wrote:. > nothing is amplified no, it's all PCR free; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658733075>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2X446P2BMPITLE5763XS7MRXANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773
https://github.com/google/deepvariant/issues/682#issuecomment-1658739773:79,Modifiability,variab,variable,79,"No problem, your variant of interest isn't a genomic region that may be; hyper variable ie a simple sequence repeat (they can occur in coding; regions) or something else that may lead to the variability your seeing?. Joe. On Mon, 31 Jul 2023, 17:30 Axze-rgb, ***@***.***> wrote:. > nothing is amplified no, it's all PCR free; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658733075>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2X446P2BMPITLE5763XS7MRXANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773
https://github.com/google/deepvariant/issues/682#issuecomment-1658739773:191,Modifiability,variab,variability,191,"No problem, your variant of interest isn't a genomic region that may be; hyper variable ie a simple sequence repeat (they can occur in coding; regions) or something else that may lead to the variability your seeing?. Joe. On Mon, 31 Jul 2023, 17:30 Axze-rgb, ***@***.***> wrote:. > nothing is amplified no, it's all PCR free; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658733075>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2X446P2BMPITLE5763XS7MRXANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773
https://github.com/google/deepvariant/issues/682#issuecomment-1658739773:93,Usability,simpl,simple,93,"No problem, your variant of interest isn't a genomic region that may be; hyper variable ie a simple sequence repeat (they can occur in coding; regions) or something else that may lead to the variability your seeing?. Joe. On Mon, 31 Jul 2023, 17:30 Axze-rgb, ***@***.***> wrote:. > nothing is amplified no, it's all PCR free; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658733075>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2X446P2BMPITLE5763XS7MRXANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773
https://github.com/google/deepvariant/issues/682#issuecomment-1658757046:502,Integrability,Message,Message,502,"Sorry but simple sequence repeats are mutable in prokaryotes too.... On Mon, 31 Jul 2023, 17:45 Axze-rgb, ***@***.***> wrote:. > that's not a feature of this genome we are not in humans.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658755258>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2V75T2P2ATODR6Z6QDXS7OLHANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658757046
https://github.com/google/deepvariant/issues/682#issuecomment-1658757046:10,Usability,simpl,simple,10,"Sorry but simple sequence repeats are mutable in prokaryotes too.... On Mon, 31 Jul 2023, 17:45 Axze-rgb, ***@***.***> wrote:. > that's not a feature of this genome we are not in humans.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658755258>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2V75T2P2ATODR6Z6QDXS7OLHANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658757046
https://github.com/google/deepvariant/issues/682#issuecomment-1658759224:96,Safety,avoid,avoid,96,"Ok so you have no idea what we are talking about, this is not a prokaryote. I would suggest you avoid hopping into conversations for making completely ignorant comments, not even bothering to check about what organisms we are dealing with. I find this kind of behaviour extremely irritating, especially since the answer to your questions are deducible from reading the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658759224
https://github.com/google/deepvariant/issues/682#issuecomment-1658762468:882,Integrability,Message,Message,882,"You said it was random clonal organism, il have a look when the reads are; on the sra database, was just trying to help,. And yes I don't care about SNPs sorry. (Google people) I'll be here to offer random advice to other people if I'm; still allowed,. Joe. On Mon, 31 Jul 2023, 17:48 Axze-rgb, ***@***.***> wrote:. > Ok so you have no idea what we are talking about, this is not a; > prokaryote. I would suggest you avoid hopping into conversations for making; > completely ignorant comments, not even bothering to check about what; > organisms we are dealing with.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658759224>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2TF5UCMO6WOF6KVCJ3XS7OWLANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658762468
https://github.com/google/deepvariant/issues/682#issuecomment-1658762468:417,Safety,avoid,avoid,417,"You said it was random clonal organism, il have a look when the reads are; on the sra database, was just trying to help,. And yes I don't care about SNPs sorry. (Google people) I'll be here to offer random advice to other people if I'm; still allowed,. Joe. On Mon, 31 Jul 2023, 17:48 Axze-rgb, ***@***.***> wrote:. > Ok so you have no idea what we are talking about, this is not a; > prokaryote. I would suggest you avoid hopping into conversations for making; > completely ignorant comments, not even bothering to check about what; > organisms we are dealing with.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658759224>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2TF5UCMO6WOF6KVCJ3XS7OWLANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658762468
https://github.com/google/deepvariant/issues/682#issuecomment-1658765720:1021,Integrability,Message,Message,1021,"And there's bacteria with 4 chromosomes.... On Mon, 31 Jul 2023, 17:50 Joe, ***@***.***> wrote:. > You said it was random clonal organism, il have a look when the reads are; > on the sra database, was just trying to help,; >; > And yes I don't care about SNPs sorry.; >; > (Google people) I'll be here to offer random advice to other people if I'm; > still allowed,; >; > Joe; >; > On Mon, 31 Jul 2023, 17:48 Axze-rgb, ***@***.***> wrote:; >; >> Ok so you have no idea what we are talking about, this is not a; >> prokaryote. I would suggest you avoid hopping into conversations for making; >> completely ignorant comments, not even bothering to check about what; >> organisms we are dealing with.; >>; >> ; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/682#issuecomment-1658759224>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/BAYQV2TF5UCMO6WOF6KVCJ3XS7OWLANCNFSM6AAAAAA2QKAKXQ>; >> .; >> You are receiving this because you commented.Message ID:; >> ***@***.***>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658765720
https://github.com/google/deepvariant/issues/682#issuecomment-1658765720:546,Safety,avoid,avoid,546,"And there's bacteria with 4 chromosomes.... On Mon, 31 Jul 2023, 17:50 Joe, ***@***.***> wrote:. > You said it was random clonal organism, il have a look when the reads are; > on the sra database, was just trying to help,; >; > And yes I don't care about SNPs sorry.; >; > (Google people) I'll be here to offer random advice to other people if I'm; > still allowed,; >; > Joe; >; > On Mon, 31 Jul 2023, 17:48 Axze-rgb, ***@***.***> wrote:; >; >> Ok so you have no idea what we are talking about, this is not a; >> prokaryote. I would suggest you avoid hopping into conversations for making; >> completely ignorant comments, not even bothering to check about what; >> organisms we are dealing with.; >>; >> ; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/682#issuecomment-1658759224>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/BAYQV2TF5UCMO6WOF6KVCJ3XS7OWLANCNFSM6AAAAAA2QKAKXQ>; >> .; >> You are receiving this because you commented.Message ID:; >> ***@***.***>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658765720
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:1834,Availability,error,errors,1834,"EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:1958,Availability,error,error,1958,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:2038,Availability,error,error,2038,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:2649,Availability,down,down,2649,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:2728,Availability,down,down,2728,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:2441,Integrability,depend,depends,2441,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:144,Modifiability,inherit,inherited,144,"@Axze-rgb This has to do with you sequencer detecting the Q-Score (Phred-scaled) read quality, that gets stored in the FASTQ files that and get inherited in converted BAM file when you align the reads. So a SAM/BAM file have read quality scores per base following the sequence, like this:. ```; CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# ; ```. The score is a ASCII encrypted score + the value 33. You can read the Phred value here per ASCII value:. https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm. A full line in a SAM/BAM file would look like this:. ```; readID43GYAX15:7:1:1202:19894/1 256 contig87 540849 1 65M * 0 0 CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:44,Safety,detect,detecting,44,"@Axze-rgb This has to do with you sequencer detecting the Q-Score (Phred-scaled) read quality, that gets stored in the FASTQ files that and get inherited in converted BAM file when you align the reads. So a SAM/BAM file have read quality scores per base following the sequence, like this:. ```; CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# ; ```. The score is a ASCII encrypted score + the value 33. You can read the Phred value here per ASCII value:. https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm. A full line in a SAM/BAM file would look like this:. ```; readID43GYAX15:7:1:1202:19894/1 256 contig87 540849 1 65M * 0 0 CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:455,Security,encrypt,encrypted,455,"@Axze-rgb This has to do with you sequencer detecting the Q-Score (Phred-scaled) read quality, that gets stored in the FASTQ files that and get inherited in converted BAM file when you align the reads. So a SAM/BAM file have read quality scores per base following the sequence, like this:. ```; CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# ; ```. The score is a ASCII encrypted score + the value 33. You can read the Phred value here per ASCII value:. https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm. A full line in a SAM/BAM file would look like this:. ```; readID43GYAX15:7:1:1202:19894/1 256 contig87 540849 1 65M * 0 0 CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:2509,Usability,learn,learning,2509,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491
https://github.com/google/deepvariant/issues/682#issuecomment-1658773430:260,Availability,down,downloaded,260,"Could be the stupidest advice in the universe but are you looking at your; vcf file :). Literally just thought talking through the problem might help, fellow; human, and no I didn't check the organism, you could have told me and I; would gkne the ncbi datanae downloaded the genomes aligned them checked; your region if interest don't worry if I see your name on the email thread; on this public github repository I won't reply and I'll loom forward your; paper on bioarvix hopefully,. Honestly all the best,. And if you don't care about prokaryotes then fair enough,. Joe. On Mon, 31 Jul 2023, 17:54 Axze-rgb, ***@***.***> wrote:. > The name of the organism has been said in this thread, that you are unable; > to find it, and believe we deal with a prokaryote is pathetic, really. Why; > would we bother with your stupid advice when you didn't even take the time; > to read the thread?; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658773430
https://github.com/google/deepvariant/issues/682#issuecomment-1658773430:1203,Integrability,Message,Message,1203,"Could be the stupidest advice in the universe but are you looking at your; vcf file :). Literally just thought talking through the problem might help, fellow; human, and no I didn't check the organism, you could have told me and I; would gkne the ncbi datanae downloaded the genomes aligned them checked; your region if interest don't worry if I see your name on the email thread; on this public github repository I won't reply and I'll loom forward your; paper on bioarvix hopefully,. Honestly all the best,. And if you don't care about prokaryotes then fair enough,. Joe. On Mon, 31 Jul 2023, 17:54 Axze-rgb, ***@***.***> wrote:. > The name of the organism has been said in this thread, that you are unable; > to find it, and believe we deal with a prokaryote is pathetic, really. Why; > would we bother with your stupid advice when you didn't even take the time; > to read the thread?; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658773430
https://github.com/google/deepvariant/issues/682#issuecomment-1658785274:662,Availability,error,errors,662,"Is there a way to ban trolls on github? It is extremely irritating. . @pgrosu I know about sam format (reasonably well) but my point is that I have seen that pop up a lot in my experiment, the same state of character called with high confidence in parts of the reads, and with low confidence in other parts of the reads. There might (might) be something in this genome that throws off the tools we have. My guess is that it's the high heterozygosity distributed all across the chromosomes. It's somehow disappointing to see the problem reappears here with HiFi reads, I was hoping to get rid of it when ditching Illumina, and using the HiFi to model the calling errors of the Illumina (which in itself is immensely ambitious). . I have a plan B, which is assembling genomes de novo, we have good results. Will it have the accuracy needed? That's the big question. EDIT: do you personally trust, based on the evidence presented here, there is a clone of low frequency? I know it's probably not enough data, but to get a feel of to what point you trust the scoring system ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658785274
https://github.com/google/deepvariant/issues/682#issuecomment-1658794438:308,Availability,down,downloaded,308,"> Could be the stupidest advice in the universe but she you looking st your vcf file.or thinking if the necwr comment to send back to me, Literally just thought talking through the problem might help, fellow human, and no I didn't check the organism, you could have told me and I would gkne the ncbi datanae downloaded the genomes aligned them checked your region if interest don't worry if I see your name on the email thread on this public github repository I won't reply and I'll loom forward your paper on bioarvix hopefully, Honestly all the best, And if you don't care about prokaryotes then fair enough, Joe; > [](#); > On Mon, 31 Jul 2023, 17:54 Axze-rgb, ***@***.***> wrote: The name of the organism has been said in this thread, that you are unable to find it, and believe we deal with a prokaryote is pathetic, really. Why would we bother with your stupid advice when you didn't even take the time to read the thread?  Reply to this email directly, view it on GitHub <[#682 (comment)](https://github.com/google/deepvariant/issues/682#issuecomment-1658768123)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ> . You are receiving this because you commented.Message ID: ***@***.***>. There are so many trolls it's difficult to know when someone is just clumsy. I am willing to give you the benefit of the doubt. Buit really you could have read the messages above, or message me to know what we are talking about? If this discussion is in public, it's indeed to attract interest of others. But just read 2 minutes without suggesting the first spontaneous idea you have. Which is not idiot in itself but that's something we though of if ... 2014 in my memory serves me well ^^. Allez, useless to have petty fight and it's not a good look. I might have overreacted to a genuine sympathetic comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658794438
https://github.com/google/deepvariant/issues/682#issuecomment-1658794438:1239,Integrability,Message,Message,1239,"> Could be the stupidest advice in the universe but she you looking st your vcf file.or thinking if the necwr comment to send back to me, Literally just thought talking through the problem might help, fellow human, and no I didn't check the organism, you could have told me and I would gkne the ncbi datanae downloaded the genomes aligned them checked your region if interest don't worry if I see your name on the email thread on this public github repository I won't reply and I'll loom forward your paper on bioarvix hopefully, Honestly all the best, And if you don't care about prokaryotes then fair enough, Joe; > [](#); > On Mon, 31 Jul 2023, 17:54 Axze-rgb, ***@***.***> wrote: The name of the organism has been said in this thread, that you are unable to find it, and believe we deal with a prokaryote is pathetic, really. Why would we bother with your stupid advice when you didn't even take the time to read the thread?  Reply to this email directly, view it on GitHub <[#682 (comment)](https://github.com/google/deepvariant/issues/682#issuecomment-1658768123)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ> . You are receiving this because you commented.Message ID: ***@***.***>. There are so many trolls it's difficult to know when someone is just clumsy. I am willing to give you the benefit of the doubt. Buit really you could have read the messages above, or message me to know what we are talking about? If this discussion is in public, it's indeed to attract interest of others. But just read 2 minutes without suggesting the first spontaneous idea you have. Which is not idiot in itself but that's something we though of if ... 2014 in my memory serves me well ^^. Allez, useless to have petty fight and it's not a good look. I might have overreacted to a genuine sympathetic comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658794438
https://github.com/google/deepvariant/issues/682#issuecomment-1658794438:1429,Integrability,message,messages,1429,"> Could be the stupidest advice in the universe but she you looking st your vcf file.or thinking if the necwr comment to send back to me, Literally just thought talking through the problem might help, fellow human, and no I didn't check the organism, you could have told me and I would gkne the ncbi datanae downloaded the genomes aligned them checked your region if interest don't worry if I see your name on the email thread on this public github repository I won't reply and I'll loom forward your paper on bioarvix hopefully, Honestly all the best, And if you don't care about prokaryotes then fair enough, Joe; > [](#); > On Mon, 31 Jul 2023, 17:54 Axze-rgb, ***@***.***> wrote: The name of the organism has been said in this thread, that you are unable to find it, and believe we deal with a prokaryote is pathetic, really. Why would we bother with your stupid advice when you didn't even take the time to read the thread?  Reply to this email directly, view it on GitHub <[#682 (comment)](https://github.com/google/deepvariant/issues/682#issuecomment-1658768123)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ> . You are receiving this because you commented.Message ID: ***@***.***>. There are so many trolls it's difficult to know when someone is just clumsy. I am willing to give you the benefit of the doubt. Buit really you could have read the messages above, or message me to know what we are talking about? If this discussion is in public, it's indeed to attract interest of others. But just read 2 minutes without suggesting the first spontaneous idea you have. Which is not idiot in itself but that's something we though of if ... 2014 in my memory serves me well ^^. Allez, useless to have petty fight and it's not a good look. I might have overreacted to a genuine sympathetic comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658794438
https://github.com/google/deepvariant/issues/682#issuecomment-1658794438:1448,Integrability,message,message,1448,"> Could be the stupidest advice in the universe but she you looking st your vcf file.or thinking if the necwr comment to send back to me, Literally just thought talking through the problem might help, fellow human, and no I didn't check the organism, you could have told me and I would gkne the ncbi datanae downloaded the genomes aligned them checked your region if interest don't worry if I see your name on the email thread on this public github repository I won't reply and I'll loom forward your paper on bioarvix hopefully, Honestly all the best, And if you don't care about prokaryotes then fair enough, Joe; > [](#); > On Mon, 31 Jul 2023, 17:54 Axze-rgb, ***@***.***> wrote: The name of the organism has been said in this thread, that you are unable to find it, and believe we deal with a prokaryote is pathetic, really. Why would we bother with your stupid advice when you didn't even take the time to read the thread?  Reply to this email directly, view it on GitHub <[#682 (comment)](https://github.com/google/deepvariant/issues/682#issuecomment-1658768123)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ> . You are receiving this because you commented.Message ID: ***@***.***>. There are so many trolls it's difficult to know when someone is just clumsy. I am willing to give you the benefit of the doubt. Buit really you could have read the messages above, or message me to know what we are talking about? If this discussion is in public, it's indeed to attract interest of others. But just read 2 minutes without suggesting the first spontaneous idea you have. Which is not idiot in itself but that's something we though of if ... 2014 in my memory serves me well ^^. Allez, useless to have petty fight and it's not a good look. I might have overreacted to a genuine sympathetic comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658794438
https://github.com/google/deepvariant/issues/682#issuecomment-1658806137:713,Availability,down,downloaded,713,"No worries, but like I have studied quite a lot of dna and rna sequence; data. But i may also be missing the obvious answer. By the way hybrid assembly using both long and short reads can make for a; more complete assembly. On a comletely separate note, you check for methylation in your organism?; Id be happy to do it for you!. Joe. On Mon, 31 Jul 2023, 18:10 Axze-rgb, ***@***.***> wrote:. > Could be the stupidest advice in the universe but she you looking st your; > vcf file.or thinking if the necwr comment to send back to me, Literally; > just thought talking through the problem might help, fellow human, and no I; > didn't check the organism, you could have told me and I would gkne the ncbi; > datanae downloaded the genomes aligned them checked your region if interest; > don't worry if I see your name on the email thread on this public github; > repository I won't reply and I'll loom forward your paper on bioarvix; > hopefully, Honestly all the best, And if you don't care about prokaryotes; > then fair enough, Joe; >  <#m_-2096600892735742938_>; > On Mon, 31 Jul 2023, 17:54 Axze-rgb, *@*.*> wrote: The name of the; > organism has been said in this thread, that you are unable to find it, and; > believe we deal with a prokaryote is pathetic, really. Why would we bother; > with your stupid advice when you didn't even take the time to read the; > thread?  Reply to this email directly, view it on GitHub <#682 (comment); > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>>,; > or unsubscribe; > https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > . You are receiving this because you commented.Message ID: @.*>; >; > There are so many trolls it's difficult to know when someone is just; > clumsy. I am willing to give you the benefit of the doubt. Buit really you; > could have read the messages ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658806137
https://github.com/google/deepvariant/issues/682#issuecomment-1658806137:1799,Integrability,Message,Message,1799,"our name on the email thread on this public github; > repository I won't reply and I'll loom forward your paper on bioarvix; > hopefully, Honestly all the best, And if you don't care about prokaryotes; > then fair enough, Joe; >  <#m_-2096600892735742938_>; > On Mon, 31 Jul 2023, 17:54 Axze-rgb, *@*.*> wrote: The name of the; > organism has been said in this thread, that you are unable to find it, and; > believe we deal with a prokaryote is pathetic, really. Why would we bother; > with your stupid advice when you didn't even take the time to read the; > thread?  Reply to this email directly, view it on GitHub <#682 (comment); > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>>,; > or unsubscribe; > https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > . You are receiving this because you commented.Message ID: @.*>; >; > There are so many trolls it's difficult to know when someone is just; > clumsy. I am willing to give you the benefit of the doubt. Buit really you; > could have read the messages above, or message me to know what we are; > talking about? If this discussion is in public, it's indeed to attract; > interest of others. But just read 2 minutes without suggesting the first; > spontaneous idea you have. Which is not idiot in itself but that's; > something we though of in ... 2014 in my memory serves me well ^^; >; > Allez, useless to have petty fight and it's not a good look. I might have; > overreacted to a genuine sympathetic comment.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658794438>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2WSPRXKHQH7UQOXNVTXS7RHRANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658806137
https://github.com/google/deepvariant/issues/682#issuecomment-1658806137:1992,Integrability,message,messages,1992,"your name on the email thread on this public github; > repository I won't reply and I'll loom forward your paper on bioarvix; > hopefully, Honestly all the best, And if you don't care about prokaryotes; > then fair enough, Joe; >  <#m_-2096600892735742938_>; > On Mon, 31 Jul 2023, 17:54 Axze-rgb, *@*.*> wrote: The name of the; > organism has been said in this thread, that you are unable to find it, and; > believe we deal with a prokaryote is pathetic, really. Why would we bother; > with your stupid advice when you didn't even take the time to read the; > thread?  Reply to this email directly, view it on GitHub <#682 (comment); > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>>,; > or unsubscribe; > https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > . You are receiving this because you commented.Message ID: @.*>; >; > There are so many trolls it's difficult to know when someone is just; > clumsy. I am willing to give you the benefit of the doubt. Buit really you; > could have read the messages above, or message me to know what we are; > talking about? If this discussion is in public, it's indeed to attract; > interest of others. But just read 2 minutes without suggesting the first; > spontaneous idea you have. Which is not idiot in itself but that's; > something we though of in ... 2014 in my memory serves me well ^^; >; > Allez, useless to have petty fight and it's not a good look. I might have; > overreacted to a genuine sympathetic comment.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658794438>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2WSPRXKHQH7UQOXNVTXS7RHRANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658806137
https://github.com/google/deepvariant/issues/682#issuecomment-1658806137:2011,Integrability,message,message,2011,"your name on the email thread on this public github; > repository I won't reply and I'll loom forward your paper on bioarvix; > hopefully, Honestly all the best, And if you don't care about prokaryotes; > then fair enough, Joe; >  <#m_-2096600892735742938_>; > On Mon, 31 Jul 2023, 17:54 Axze-rgb, *@*.*> wrote: The name of the; > organism has been said in this thread, that you are unable to find it, and; > believe we deal with a prokaryote is pathetic, really. Why would we bother; > with your stupid advice when you didn't even take the time to read the; > thread?  Reply to this email directly, view it on GitHub <#682 (comment); > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>>,; > or unsubscribe; > https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > . You are receiving this because you commented.Message ID: @.*>; >; > There are so many trolls it's difficult to know when someone is just; > clumsy. I am willing to give you the benefit of the doubt. Buit really you; > could have read the messages above, or message me to know what we are; > talking about? If this discussion is in public, it's indeed to attract; > interest of others. But just read 2 minutes without suggesting the first; > spontaneous idea you have. Which is not idiot in itself but that's; > something we though of in ... 2014 in my memory serves me well ^^; >; > Allez, useless to have petty fight and it's not a good look. I might have; > overreacted to a genuine sympathetic comment.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658794438>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2WSPRXKHQH7UQOXNVTXS7RHRANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658806137
https://github.com/google/deepvariant/issues/682#issuecomment-1658806137:2775,Integrability,Message,Message,2775,"your name on the email thread on this public github; > repository I won't reply and I'll loom forward your paper on bioarvix; > hopefully, Honestly all the best, And if you don't care about prokaryotes; > then fair enough, Joe; >  <#m_-2096600892735742938_>; > On Mon, 31 Jul 2023, 17:54 Axze-rgb, *@*.*> wrote: The name of the; > organism has been said in this thread, that you are unable to find it, and; > believe we deal with a prokaryote is pathetic, really. Why would we bother; > with your stupid advice when you didn't even take the time to read the; > thread?  Reply to this email directly, view it on GitHub <#682 (comment); > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>>,; > or unsubscribe; > https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > . You are receiving this because you commented.Message ID: @.*>; >; > There are so many trolls it's difficult to know when someone is just; > clumsy. I am willing to give you the benefit of the doubt. Buit really you; > could have read the messages above, or message me to know what we are; > talking about? If this discussion is in public, it's indeed to attract; > interest of others. But just read 2 minutes without suggesting the first; > spontaneous idea you have. Which is not idiot in itself but that's; > something we though of in ... 2014 in my memory serves me well ^^; >; > Allez, useless to have petty fight and it's not a good look. I might have; > overreacted to a genuine sympathetic comment.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658794438>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2WSPRXKHQH7UQOXNVTXS7RHRANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658806137
https://github.com/google/deepvariant/issues/682#issuecomment-1658811104:190,Availability,error,errors,190,"We have done those approaches, except for methylation maybe (not sure about that one). The main issue if we have a diploid genome, with a high heterozygosity, which leads to mapping scoring errors, as well as some callers totally losing their minds.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658811104
https://github.com/google/deepvariant/issues/682#issuecomment-1658889277:541,Availability,down,down,541,"Hi All,. I appreciate that all of you are contributing scientific insights and taking your time to add thoughts on the DeepVariant GitHub issues. as @pichuan said, please remember that we are all scientists trying to work with each other and be respectful of each other. @Axze-rgb I haven't had a chance to look into the data (with what I have I would be looking at the effect in human data just to get a feel for how unusual it is). However, from your results, I'm not sure that this approach is promising and I am sorry I may have led you down a bad path. I was hoping to see sites that look clearly like subclonal SNPs (so look clear on other sources of error). From your pileups posted, it looks like many of the GQ30 VAF 0.15 population are mapping complexities. . It might be possible to further separate out mapping events, for example by increasing the MapQ filter, or excluding sites that are within a short distance of other variants, but I am not sure the approach will be worthwhile. My apologies, I think that in principle this is an interesting concept, but in practice, I am not sure your results indicate a clear win with the approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658889277
https://github.com/google/deepvariant/issues/682#issuecomment-1658889277:657,Availability,error,error,657,"Hi All,. I appreciate that all of you are contributing scientific insights and taking your time to add thoughts on the DeepVariant GitHub issues. as @pichuan said, please remember that we are all scientists trying to work with each other and be respectful of each other. @Axze-rgb I haven't had a chance to look into the data (with what I have I would be looking at the effect in human data just to get a feel for how unusual it is). However, from your results, I'm not sure that this approach is promising and I am sorry I may have led you down a bad path. I was hoping to see sites that look clearly like subclonal SNPs (so look clear on other sources of error). From your pileups posted, it looks like many of the GQ30 VAF 0.15 population are mapping complexities. . It might be possible to further separate out mapping events, for example by increasing the MapQ filter, or excluding sites that are within a short distance of other variants, but I am not sure the approach will be worthwhile. My apologies, I think that in principle this is an interesting concept, but in practice, I am not sure your results indicate a clear win with the approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658889277
https://github.com/google/deepvariant/issues/682#issuecomment-1658889277:594,Usability,clear,clearly,594,"Hi All,. I appreciate that all of you are contributing scientific insights and taking your time to add thoughts on the DeepVariant GitHub issues. as @pichuan said, please remember that we are all scientists trying to work with each other and be respectful of each other. @Axze-rgb I haven't had a chance to look into the data (with what I have I would be looking at the effect in human data just to get a feel for how unusual it is). However, from your results, I'm not sure that this approach is promising and I am sorry I may have led you down a bad path. I was hoping to see sites that look clearly like subclonal SNPs (so look clear on other sources of error). From your pileups posted, it looks like many of the GQ30 VAF 0.15 population are mapping complexities. . It might be possible to further separate out mapping events, for example by increasing the MapQ filter, or excluding sites that are within a short distance of other variants, but I am not sure the approach will be worthwhile. My apologies, I think that in principle this is an interesting concept, but in practice, I am not sure your results indicate a clear win with the approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658889277
https://github.com/google/deepvariant/issues/682#issuecomment-1658889277:631,Usability,clear,clear,631,"Hi All,. I appreciate that all of you are contributing scientific insights and taking your time to add thoughts on the DeepVariant GitHub issues. as @pichuan said, please remember that we are all scientists trying to work with each other and be respectful of each other. @Axze-rgb I haven't had a chance to look into the data (with what I have I would be looking at the effect in human data just to get a feel for how unusual it is). However, from your results, I'm not sure that this approach is promising and I am sorry I may have led you down a bad path. I was hoping to see sites that look clearly like subclonal SNPs (so look clear on other sources of error). From your pileups posted, it looks like many of the GQ30 VAF 0.15 population are mapping complexities. . It might be possible to further separate out mapping events, for example by increasing the MapQ filter, or excluding sites that are within a short distance of other variants, but I am not sure the approach will be worthwhile. My apologies, I think that in principle this is an interesting concept, but in practice, I am not sure your results indicate a clear win with the approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658889277
https://github.com/google/deepvariant/issues/682#issuecomment-1658889277:1123,Usability,clear,clear,1123,"Hi All,. I appreciate that all of you are contributing scientific insights and taking your time to add thoughts on the DeepVariant GitHub issues. as @pichuan said, please remember that we are all scientists trying to work with each other and be respectful of each other. @Axze-rgb I haven't had a chance to look into the data (with what I have I would be looking at the effect in human data just to get a feel for how unusual it is). However, from your results, I'm not sure that this approach is promising and I am sorry I may have led you down a bad path. I was hoping to see sites that look clearly like subclonal SNPs (so look clear on other sources of error). From your pileups posted, it looks like many of the GQ30 VAF 0.15 population are mapping complexities. . It might be possible to further separate out mapping events, for example by increasing the MapQ filter, or excluding sites that are within a short distance of other variants, but I am not sure the approach will be worthwhile. My apologies, I think that in principle this is an interesting concept, but in practice, I am not sure your results indicate a clear win with the approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658889277
https://github.com/google/deepvariant/issues/682#issuecomment-1658896248:86,Availability,failure,failure,86,"I am actually happy we at least can rule something out. Those rotifers have led us to failure on this for years now, there is something that we miss. That we all miss.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658896248
https://github.com/google/deepvariant/issues/682#issuecomment-1658897383:284,Integrability,message,message,284,"No problem, I just wanted to say I did ask google bard some questions I; thought might be important,. You can have a look here: https://g.co/bard/share/4013d7eb8290. Or start a thread of your own :). And I'll hop out of this thread now as I think I'm cluttering it up, but; drop me a message if you ever want to chat. Also I know some clonal organisms that can change there entire allele; frequency and linkage in less than 5 minutes!. Not trying to distact you, all the best,. Joe. On Mon, 31 Jul 2023, 18:34 Pi-Chuan Chang, ***@***.***> wrote:. > Hi everyone on this thread:; > We (the Google team that works on DeepVariant) have really appreciated our; > GitHub community where people post questions, discussions, and help each; > other out.; > Even when our team answer questions, we don't always have the full; > context. We really appreciate community help and discussion from outside; > the team as well.; > For people who post on our GitHub issues, please be respectful to each; > other, and know that people are trying to help out, even when you might not; > feel like the answers are as helpful.; >; > @Axze-rgb <https://github.com/Axze-rgb> we'll take another look at this; > thread and see what answer we can provide. It is possible that some of the; > topics here might be beyond what our team can support, but we'll try our; > best.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658849274>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2U67RCHYD4ET3E22ALXS7UA7ANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658897383
https://github.com/google/deepvariant/issues/682#issuecomment-1658897383:1662,Integrability,Message,Message,1662,"No problem, I just wanted to say I did ask google bard some questions I; thought might be important,. You can have a look here: https://g.co/bard/share/4013d7eb8290. Or start a thread of your own :). And I'll hop out of this thread now as I think I'm cluttering it up, but; drop me a message if you ever want to chat. Also I know some clonal organisms that can change there entire allele; frequency and linkage in less than 5 minutes!. Not trying to distact you, all the best,. Joe. On Mon, 31 Jul 2023, 18:34 Pi-Chuan Chang, ***@***.***> wrote:. > Hi everyone on this thread:; > We (the Google team that works on DeepVariant) have really appreciated our; > GitHub community where people post questions, discussions, and help each; > other out.; > Even when our team answer questions, we don't always have the full; > context. We really appreciate community help and discussion from outside; > the team as well.; > For people who post on our GitHub issues, please be respectful to each; > other, and know that people are trying to help out, even when you might not; > feel like the answers are as helpful.; >; > @Axze-rgb <https://github.com/Axze-rgb> we'll take another look at this; > thread and see what answer we can provide. It is possible that some of the; > topics here might be beyond what our team can support, but we'll try our; > best.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658849274>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2U67RCHYD4ET3E22ALXS7UA7ANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1658897383
https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:1300,Availability,down,down,1300,"you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:1220,Modifiability,variab,variables,1220,"you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:1470,Security,validat,validate,1470,"e second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosity will result in a higher first peak and a lower second peak"",*_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a bett",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:2058,Security,validat,validated,2058,"arting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosity will result in a higher first peak and a lower second peak"",*_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a better question is what exactly are the reads inferring/representing here regarding a point in time of a clone(s) state? This hints at the mapping complexities that Andrew was suggesting. Regarding the engineering approach, here you assume to have a well-established model you rely upon -- or at least backed up thoroughly by prior experiments -- of your organism's behavior under different conditions. It is more goal-driven, as you have stronger expectations of confirmi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:3381,Security,validat,validating,3381,"_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a better question is what exactly are the reads inferring/representing here regarding a point in time of a clone(s) state? This hints at the mapping complexities that Andrew was suggesting. Regarding the engineering approach, here you assume to have a well-established model you rely upon -- or at least backed up thoroughly by prior experiments -- of your organism's behavior under different conditions. It is more goal-driven, as you have stronger expectations of confirming new hypotheses. Given that, you use it to infer how the experiment might behave, or in your case the meaning behind your results. If your model is not well-established for your organism, your organism might respond in a unexpected ways given an experimental setup. I get the feeling we're trying to mix the engineering with the science-based approach, which might cause us to require designing additional experiments for validating previous results -- possibly becoming circular. On another note, I'm sure your already know this regarding IGV colors, you can get complete breakdown of their meaning at the following site: . https://software.broadinstitute.org/software/igv/AlignmentData. Hope it helps,; ~p. #### References. 1. [GenomeScope 2.0 and Smudgeplot for reference-free profiling of polyploid genomes](https://www.nature.com/articles/s41467-020-14998-3); 2. [GenomeScope: fast reference-free genome profiling from short reads](https://academic.oup.com/bioinformatics/article/33/14/2202/3089939?login=false); 3. [Measuring Genome Sizes Using Read-Depth, k-mers, and Flow Cytometry: Methodological Comparisons in Beetles (Coleoptera)](https://academic.oup.com/g3journal/article/10/9/3047/6060154); 4. [Kmer2SNP: reference-free SNP calling from raw reads based on matching](https://www.biorxiv.org/content/10.1101/2020.05.17.100305v1.full)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:718,Testability,test,test,718,"@Axze-rgb A lot of what I'm gonna say I'm sure you already know well. As it feels you might be in a time-crunch, you're a better judge than me on these:. - Do you have a fairly complete story that you and your advisor are happy with?; - If you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:2366,Testability,test,test,2366,"ncy region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosity will result in a higher first peak and a lower second peak"",*_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a better question is what exactly are the reads inferring/representing here regarding a point in time of a clone(s) state? This hints at the mapping complexities that Andrew was suggesting. Regarding the engineering approach, here you assume to have a well-established model you rely upon -- or at least backed up thoroughly by prior experiments -- of your organism's behavior under different conditions. It is more goal-driven, as you have stronger expectations of confirming new hypotheses. Given that, you use it to infer how the experiment might behave, or in your case the meaning behind your results. If your model is not well-established for your organism, your organism might respond in a unexpected ways given an experimental setup. I get the feeling we're trying to mix the engineering with the science-based approach, which might cause us to require designing additional experiment",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:3963,Testability,log,login,3963,"_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a better question is what exactly are the reads inferring/representing here regarding a point in time of a clone(s) state? This hints at the mapping complexities that Andrew was suggesting. Regarding the engineering approach, here you assume to have a well-established model you rely upon -- or at least backed up thoroughly by prior experiments -- of your organism's behavior under different conditions. It is more goal-driven, as you have stronger expectations of confirming new hypotheses. Given that, you use it to infer how the experiment might behave, or in your case the meaning behind your results. If your model is not well-established for your organism, your organism might respond in a unexpected ways given an experimental setup. I get the feeling we're trying to mix the engineering with the science-based approach, which might cause us to require designing additional experiments for validating previous results -- possibly becoming circular. On another note, I'm sure your already know this regarding IGV colors, you can get complete breakdown of their meaning at the following site: . https://software.broadinstitute.org/software/igv/AlignmentData. Hope it helps,; ~p. #### References. 1. [GenomeScope 2.0 and Smudgeplot for reference-free profiling of polyploid genomes](https://www.nature.com/articles/s41467-020-14998-3); 2. [GenomeScope: fast reference-free genome profiling from short reads](https://academic.oup.com/bioinformatics/article/33/14/2202/3089939?login=false); 3. [Measuring Genome Sizes Using Read-Depth, k-mers, and Flow Cytometry: Methodological Comparisons in Beetles (Coleoptera)](https://academic.oup.com/g3journal/article/10/9/3047/6060154); 4. [Kmer2SNP: reference-free SNP calling from raw reads based on matching](https://www.biorxiv.org/content/10.1101/2020.05.17.100305v1.full)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:1333,Usability,simpl,simpler,1333,"you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917
https://github.com/google/deepvariant/issues/682#issuecomment-1662524118:192,Energy Efficiency,power,power,192,"Now I have filtered the HiFi reads based on mapq, to check if we see the same patter of strange peak of GQ 30 and VAF 0,17; I wanted to do the other things as well but we are out of computing power",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1662524118
https://github.com/google/deepvariant/issues/682#issuecomment-1662686740:76,Deployability,update,updates,76,"Hi @Axze-rgb ,; I'll close this issue now. Please feel free to give further updates to this thread. If you have more questions that the DeepVariant team can help support, please feel free to open another issue.; I'm closing this so that it's easier for our people on rotation to track active issues for support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1662686740
https://github.com/google/deepvariant/issues/682#issuecomment-1722449849:761,Usability,learn,learn,761,"Hello, . I write here again to say 2 things; - first I apologize for having be somewhat agressive towards the person who came in the discussion, it seems to me it was just a curious person after all and not a troll. So sorry about that. I apologize @Joe-r-code ; - We are going to do high coverage ONT sequencing, which should allow for a better SNP calling. If it works well, would you be interested to train DeepVariant on my data set? That would make it able to manage high heterozygous genomes. You have been very helpful and I really appreciated it. So, I think we could plan to collaborate on training deepvariant on a highly heterozygous genome with many paralogs? I also ask this, because honestly I would love to understand better Neural Network and I learn better by doing. . The sequencing will take some time though.; Cheers. . EDIT: by ""curious"" I mean someone wanting to understand, I don't mean weird. In French curious can mean weird, so I don't know if this is the case in English.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1722449849
https://github.com/google/deepvariant/issues/682#issuecomment-1722685163:305,Usability,guid,guidance,305,"@Axze-rgb Great to hear from you! How have you been? I only want to hear good news :) Thank you kindly for the opportunity, but unfortunately my time is a bit dominated by a few things these days. A collaboration is a big commitment in order for me to assist properly, so for now I prefer to just provide guidance as necessary. Before you sequence too much, it's best to practice training a model with some data on your own to get a feel for what's happening. You have a great tutorial (including the data) on how to train a model [at the following link](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). Again it has germline diploid assumptions built into it. If your data varies a lot, it will take some time for the weights to shift to your data's behavior in order to achieve good accuracy (when possible). Regarding paralogs, you might need to align to a pangenome graph via giraffe, like in [the following paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365333/). Again you are adding complexities that require careful mastery of nuances you need to recognize when issues come up. Start playing with simple things first to get a good grasp of what's going on. So my humble recommendation is to take baby steps. Try to play with training a model first. Regarding learning about machine learning/deep neural networks (DNNs), find a book/youtube videos you can practice from - which matches your learning style and makes you code - to better understand how Deep Neural Networks work. Ideally sit in a class, so you are forced to do the homework, and begin with the fundamentals before eventually reaching DNNs (which have a prerequisite of some fundamentals of machine learning concepts as background). Most importantly, have fun and keep us posted :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1722685163
https://github.com/google/deepvariant/issues/682#issuecomment-1722685163:1152,Usability,simpl,simple,1152,"@Axze-rgb Great to hear from you! How have you been? I only want to hear good news :) Thank you kindly for the opportunity, but unfortunately my time is a bit dominated by a few things these days. A collaboration is a big commitment in order for me to assist properly, so for now I prefer to just provide guidance as necessary. Before you sequence too much, it's best to practice training a model with some data on your own to get a feel for what's happening. You have a great tutorial (including the data) on how to train a model [at the following link](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). Again it has germline diploid assumptions built into it. If your data varies a lot, it will take some time for the weights to shift to your data's behavior in order to achieve good accuracy (when possible). Regarding paralogs, you might need to align to a pangenome graph via giraffe, like in [the following paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365333/). Again you are adding complexities that require careful mastery of nuances you need to recognize when issues come up. Start playing with simple things first to get a good grasp of what's going on. So my humble recommendation is to take baby steps. Try to play with training a model first. Regarding learning about machine learning/deep neural networks (DNNs), find a book/youtube videos you can practice from - which matches your learning style and makes you code - to better understand how Deep Neural Networks work. Ideally sit in a class, so you are forced to do the homework, and begin with the fundamentals before eventually reaching DNNs (which have a prerequisite of some fundamentals of machine learning concepts as background). Most importantly, have fun and keep us posted :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1722685163
https://github.com/google/deepvariant/issues/682#issuecomment-1722685163:1314,Usability,learn,learning,1314,"@Axze-rgb Great to hear from you! How have you been? I only want to hear good news :) Thank you kindly for the opportunity, but unfortunately my time is a bit dominated by a few things these days. A collaboration is a big commitment in order for me to assist properly, so for now I prefer to just provide guidance as necessary. Before you sequence too much, it's best to practice training a model with some data on your own to get a feel for what's happening. You have a great tutorial (including the data) on how to train a model [at the following link](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). Again it has germline diploid assumptions built into it. If your data varies a lot, it will take some time for the weights to shift to your data's behavior in order to achieve good accuracy (when possible). Regarding paralogs, you might need to align to a pangenome graph via giraffe, like in [the following paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365333/). Again you are adding complexities that require careful mastery of nuances you need to recognize when issues come up. Start playing with simple things first to get a good grasp of what's going on. So my humble recommendation is to take baby steps. Try to play with training a model first. Regarding learning about machine learning/deep neural networks (DNNs), find a book/youtube videos you can practice from - which matches your learning style and makes you code - to better understand how Deep Neural Networks work. Ideally sit in a class, so you are forced to do the homework, and begin with the fundamentals before eventually reaching DNNs (which have a prerequisite of some fundamentals of machine learning concepts as background). Most importantly, have fun and keep us posted :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1722685163
https://github.com/google/deepvariant/issues/682#issuecomment-1722685163:1337,Usability,learn,learning,1337,"@Axze-rgb Great to hear from you! How have you been? I only want to hear good news :) Thank you kindly for the opportunity, but unfortunately my time is a bit dominated by a few things these days. A collaboration is a big commitment in order for me to assist properly, so for now I prefer to just provide guidance as necessary. Before you sequence too much, it's best to practice training a model with some data on your own to get a feel for what's happening. You have a great tutorial (including the data) on how to train a model [at the following link](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). Again it has germline diploid assumptions built into it. If your data varies a lot, it will take some time for the weights to shift to your data's behavior in order to achieve good accuracy (when possible). Regarding paralogs, you might need to align to a pangenome graph via giraffe, like in [the following paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365333/). Again you are adding complexities that require careful mastery of nuances you need to recognize when issues come up. Start playing with simple things first to get a good grasp of what's going on. So my humble recommendation is to take baby steps. Try to play with training a model first. Regarding learning about machine learning/deep neural networks (DNNs), find a book/youtube videos you can practice from - which matches your learning style and makes you code - to better understand how Deep Neural Networks work. Ideally sit in a class, so you are forced to do the homework, and begin with the fundamentals before eventually reaching DNNs (which have a prerequisite of some fundamentals of machine learning concepts as background). Most importantly, have fun and keep us posted :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1722685163
https://github.com/google/deepvariant/issues/682#issuecomment-1722685163:1445,Usability,learn,learning,1445,"@Axze-rgb Great to hear from you! How have you been? I only want to hear good news :) Thank you kindly for the opportunity, but unfortunately my time is a bit dominated by a few things these days. A collaboration is a big commitment in order for me to assist properly, so for now I prefer to just provide guidance as necessary. Before you sequence too much, it's best to practice training a model with some data on your own to get a feel for what's happening. You have a great tutorial (including the data) on how to train a model [at the following link](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). Again it has germline diploid assumptions built into it. If your data varies a lot, it will take some time for the weights to shift to your data's behavior in order to achieve good accuracy (when possible). Regarding paralogs, you might need to align to a pangenome graph via giraffe, like in [the following paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365333/). Again you are adding complexities that require careful mastery of nuances you need to recognize when issues come up. Start playing with simple things first to get a good grasp of what's going on. So my humble recommendation is to take baby steps. Try to play with training a model first. Regarding learning about machine learning/deep neural networks (DNNs), find a book/youtube videos you can practice from - which matches your learning style and makes you code - to better understand how Deep Neural Networks work. Ideally sit in a class, so you are forced to do the homework, and begin with the fundamentals before eventually reaching DNNs (which have a prerequisite of some fundamentals of machine learning concepts as background). Most importantly, have fun and keep us posted :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1722685163
https://github.com/google/deepvariant/issues/682#issuecomment-1722685163:1718,Usability,learn,learning,1718,"@Axze-rgb Great to hear from you! How have you been? I only want to hear good news :) Thank you kindly for the opportunity, but unfortunately my time is a bit dominated by a few things these days. A collaboration is a big commitment in order for me to assist properly, so for now I prefer to just provide guidance as necessary. Before you sequence too much, it's best to practice training a model with some data on your own to get a feel for what's happening. You have a great tutorial (including the data) on how to train a model [at the following link](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). Again it has germline diploid assumptions built into it. If your data varies a lot, it will take some time for the weights to shift to your data's behavior in order to achieve good accuracy (when possible). Regarding paralogs, you might need to align to a pangenome graph via giraffe, like in [the following paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365333/). Again you are adding complexities that require careful mastery of nuances you need to recognize when issues come up. Start playing with simple things first to get a good grasp of what's going on. So my humble recommendation is to take baby steps. Try to play with training a model first. Regarding learning about machine learning/deep neural networks (DNNs), find a book/youtube videos you can practice from - which matches your learning style and makes you code - to better understand how Deep Neural Networks work. Ideally sit in a class, so you are forced to do the homework, and begin with the fundamentals before eventually reaching DNNs (which have a prerequisite of some fundamentals of machine learning concepts as background). Most importantly, have fun and keep us posted :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682#issuecomment-1722685163
https://github.com/google/deepvariant/issues/683#issuecomment-1644260839:1160,Performance,perform,performance,1160,"Hi @tzcoolman,. A few things:. $`1)`$ Yes, it can take a long time, as shown here:. https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#pacbio-hifi. $`2)`$ Yes, `make_examples` is single-threaded and has multiple stages. You can adjust the parallelism distribution indirectly through the number of shards, which can either match the number of chromosomes (or if more then the sub-regions):. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#make_examples. https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md. $`3)`$ Be careful when adjusting the `vsc_min_*` values $`-`$ yes, higher values will make it faster $`-`$ but the reason those were adjusted in post #578 is because the reference was also lower quality, and you might miss some candidates doing so. $`4)`$ One way to make things faster would be to balance the CPU core and thread workloads. CPUs with operating systems have limits at how much they can context switch, before they spend more time resource managing these threads, which is called [thrashing](https://blog.netdata.cloud/understanding-context-switching-and-its-impact-on-system-performance/). A trick you could do is to run DeepVariant with the `--dry_run` parameter, in order to retrieve the individual commands being run. Then you can run the `make_examples` step, adjusting the parameters for `parallel` for either CPU cores or threads, as now the number of jobslots (-j) is its preferred method. `parallel` loves threads and the jobslot (`-j`) argument tries to balance CPU/threads, but the key word here is $`tries`$. In fact, you can force it one way or anther, but you will have to test that empirically in order to see what gets you the best results for your machine. The list of the parameters for parallel are shown on the following page:. https://man.linuxreviews.org/man1/parallel.1.html. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683#issuecomment-1644260839
https://github.com/google/deepvariant/issues/683#issuecomment-1644260839:1671,Testability,test,test,1671,"Hi @tzcoolman,. A few things:. $`1)`$ Yes, it can take a long time, as shown here:. https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#pacbio-hifi. $`2)`$ Yes, `make_examples` is single-threaded and has multiple stages. You can adjust the parallelism distribution indirectly through the number of shards, which can either match the number of chromosomes (or if more then the sub-regions):. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#make_examples. https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md. $`3)`$ Be careful when adjusting the `vsc_min_*` values $`-`$ yes, higher values will make it faster $`-`$ but the reason those were adjusted in post #578 is because the reference was also lower quality, and you might miss some candidates doing so. $`4)`$ One way to make things faster would be to balance the CPU core and thread workloads. CPUs with operating systems have limits at how much they can context switch, before they spend more time resource managing these threads, which is called [thrashing](https://blog.netdata.cloud/understanding-context-switching-and-its-impact-on-system-performance/). A trick you could do is to run DeepVariant with the `--dry_run` parameter, in order to retrieve the individual commands being run. Then you can run the `make_examples` step, adjusting the parameters for `parallel` for either CPU cores or threads, as now the number of jobslots (-j) is its preferred method. `parallel` loves threads and the jobslot (`-j`) argument tries to balance CPU/threads, but the key word here is $`tries`$. In fact, you can force it one way or anther, but you will have to test that empirically in order to see what gets you the best results for your machine. The list of the parameters for parallel are shown on the following page:. https://man.linuxreviews.org/man1/parallel.1.html. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683#issuecomment-1644260839
https://github.com/google/deepvariant/issues/684#issuecomment-1645637328:565,Deployability,update,updated,565,"Hi Amy,. Basically, this all happens when the candidates are selected during the `make_examples` stage:. $`1)`$ DP (read depth) is set from the total allele counts [in this line](https://github.com/google/deepvariant/blob/r1.5/deepvariant/variant_calling.cc#L307):. ```Python; nucleus::SetInfoField(kDPFormatField, TotalAlleleCounts(allele_count), call);; ```. $`2)`$ In the `make_examples` stage, a tensor image (set of matrices) representing information about your reads gets created. That tensor is usually preinitialized with zeros (0), and then the values get updated with the read information for the rows. The first 5 rows are the reference representation anyway, before it gets populated with your read representation starting with row 6. Then this tensor gets processed through the model $`-`$ during the `call_variants` stage $`-`$ generating the genotype probabilities for: homozygous ref, het, and homozygous alt. $`3)`$ Then in `postprocess_variants` $`-`$ based on the most likely genotype (having the maximum genotype probability) from step $`2`$ $`-`$ the [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function assigns the `PASS` or `RefCall` for the `FILTER` column of the VCF file. $`4)`$ If you want to filter out these low depth candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference (notice the the default count is already 2, which you are obvserving):. * vsc_min_count_snps (the default is 2); * vsc_min_count_indels (the default is 2); * vsc_min_fraction_snps (the default is 0.12); * vsc_min_fraction_indels (the default is 0.06); * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```; --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'; ```. You can read more details about these parameters at the following lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/684#issuecomment-1645637328
https://github.com/google/deepvariant/issues/684#issuecomment-1645793952:371,Availability,down,downsampled,371,"Thank you very much @pgrosu for such detailed answer! You are absolutely right. So, @amy-houseman, in summary, if a candidate variant passes all of the VSC's (very sensitive caller) thresholds and then the neural network prediction is confident on the genotype, `post_processing` will assign a PASS to the variant. One more thing to note, we train DeepVariant at several downsampled coverages so the model can capture the coverage variability of regions and different sequencing runs. This also makes DeepVariant robust to different coverages. Hopefully that answers your question. . @pgrosu, again thank you for such detailed and excellent answer. This Q/A is an excellent candidate for our FAQ (https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md). We maintain this as a hub for all common answers. Let us know if it would be OK if we link to your response here in our FAQ.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/684#issuecomment-1645793952
https://github.com/google/deepvariant/issues/684#issuecomment-1645793952:513,Availability,robust,robust,513,"Thank you very much @pgrosu for such detailed answer! You are absolutely right. So, @amy-houseman, in summary, if a candidate variant passes all of the VSC's (very sensitive caller) thresholds and then the neural network prediction is confident on the genotype, `post_processing` will assign a PASS to the variant. One more thing to note, we train DeepVariant at several downsampled coverages so the model can capture the coverage variability of regions and different sequencing runs. This also makes DeepVariant robust to different coverages. Hopefully that answers your question. . @pgrosu, again thank you for such detailed and excellent answer. This Q/A is an excellent candidate for our FAQ (https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md). We maintain this as a hub for all common answers. Let us know if it would be OK if we link to your response here in our FAQ.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/684#issuecomment-1645793952
https://github.com/google/deepvariant/issues/684#issuecomment-1645793952:431,Modifiability,variab,variability,431,"Thank you very much @pgrosu for such detailed answer! You are absolutely right. So, @amy-houseman, in summary, if a candidate variant passes all of the VSC's (very sensitive caller) thresholds and then the neural network prediction is confident on the genotype, `post_processing` will assign a PASS to the variant. One more thing to note, we train DeepVariant at several downsampled coverages so the model can capture the coverage variability of regions and different sequencing runs. This also makes DeepVariant robust to different coverages. Hopefully that answers your question. . @pgrosu, again thank you for such detailed and excellent answer. This Q/A is an excellent candidate for our FAQ (https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md). We maintain this as a hub for all common answers. Let us know if it would be OK if we link to your response here in our FAQ.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/684#issuecomment-1645793952
https://github.com/google/deepvariant/issues/684#issuecomment-1645793952:221,Safety,predict,prediction,221,"Thank you very much @pgrosu for such detailed answer! You are absolutely right. So, @amy-houseman, in summary, if a candidate variant passes all of the VSC's (very sensitive caller) thresholds and then the neural network prediction is confident on the genotype, `post_processing` will assign a PASS to the variant. One more thing to note, we train DeepVariant at several downsampled coverages so the model can capture the coverage variability of regions and different sequencing runs. This also makes DeepVariant robust to different coverages. Hopefully that answers your question. . @pgrosu, again thank you for such detailed and excellent answer. This Q/A is an excellent candidate for our FAQ (https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md). We maintain this as a hub for all common answers. Let us know if it would be OK if we link to your response here in our FAQ.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/684#issuecomment-1645793952
https://github.com/google/deepvariant/issues/685#issuecomment-1646860852:104,Availability,echo,echo,104,"Hi @Axze-rgb ,. instead of setting INPUT_DIR=""${PWD}"" please provide the absolute path. You can also do echo $INPUT_DIR to make sure the variable is set correctly. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646860852
https://github.com/google/deepvariant/issues/685#issuecomment-1646860852:137,Modifiability,variab,variable,137,"Hi @Axze-rgb ,. instead of setting INPUT_DIR=""${PWD}"" please provide the absolute path. You can also do echo $INPUT_DIR to make sure the variable is set correctly. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646860852
https://github.com/google/deepvariant/issues/685#issuecomment-1646862327:227,Modifiability,variab,variable,227,"> Also it seems like you are saying your bam files name is BAM? Please make sure your files name matches the parameters you are setting. Generally the BAM file would be somename.bam. Sorry, I don't understand, I declare a variable BAM=mysorted.bam"" isn't it how it should work? I will try with providing the absolute path then. . Still puzzled why the dry run works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646862327
https://github.com/google/deepvariant/issues/685#issuecomment-1646862969:345,Availability,error,error,345,"Ok that seems to be working with absolute path (at least it's making the examples right now). Also, the index of the bam was corrupted! it seems that you declare the BAM but then deepvariant goes to the index. Which makes sense of course, but maybe it would be a good idea to say somewhere in the doc to pay attention to the index? Or return an error like ""index not found"" rather than ""bam not found""? and why dry run didn't catch this? . Anyway, thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646862969
https://github.com/google/deepvariant/issues/685#issuecomment-1646901356:688,Availability,error,error,688,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```; [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'; ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646901356
https://github.com/google/deepvariant/issues/685#issuecomment-1646901356:1061,Modifiability,variab,variable,1061,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```; [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'; ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646901356
https://github.com/google/deepvariant/issues/685#issuecomment-1646926450:149,Availability,echo,echo,149,"@Axze-rgb,. In bash when you declare a variable as:; ```; BAM=HiFi_vaga.sorted.bam; ```; Then you need to use it as `${BAM}`, for example:; ```bash; echo ${BAM}; # vs.; echo BAM; ```; Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646926450
https://github.com/google/deepvariant/issues/685#issuecomment-1646926450:169,Availability,echo,echo,169,"@Axze-rgb,. In bash when you declare a variable as:; ```; BAM=HiFi_vaga.sorted.bam; ```; Then you need to use it as `${BAM}`, for example:; ```bash; echo ${BAM}; # vs.; echo BAM; ```; Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646926450
https://github.com/google/deepvariant/issues/685#issuecomment-1646926450:394,Availability,error,error,394,"@Axze-rgb,. In bash when you declare a variable as:; ```; BAM=HiFi_vaga.sorted.bam; ```; Then you need to use it as `${BAM}`, for example:; ```bash; echo ${BAM}; # vs.; echo BAM; ```; Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646926450
https://github.com/google/deepvariant/issues/685#issuecomment-1646926450:527,Availability,error,errors,527,"@Axze-rgb,. In bash when you declare a variable as:; ```; BAM=HiFi_vaga.sorted.bam; ```; Then you need to use it as `${BAM}`, for example:; ```bash; echo ${BAM}; # vs.; echo BAM; ```; Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646926450
https://github.com/google/deepvariant/issues/685#issuecomment-1646926450:39,Modifiability,variab,variable,39,"@Axze-rgb,. In bash when you declare a variable as:; ```; BAM=HiFi_vaga.sorted.bam; ```; Then you need to use it as `${BAM}`, for example:; ```bash; echo ${BAM}; # vs.; echo BAM; ```; Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646926450
https://github.com/google/deepvariant/issues/685#issuecomment-1646926450:256,Modifiability,variab,variables,256,"@Axze-rgb,. In bash when you declare a variable as:; ```; BAM=HiFi_vaga.sorted.bam; ```; Then you need to use it as `${BAM}`, for example:; ```bash; echo ${BAM}; # vs.; echo BAM; ```; Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685#issuecomment-1646926450
https://github.com/google/deepvariant/issues/686#issuecomment-1651102049:664,Integrability,Message,Message,664,"Saw this the other day. https://github.com/marcelauliano/MitoHiFi/tree/master. On Wed, 26 Jul 2023, 08:05 crazysummerW, ***@***.***> wrote:. > Hello,; > I would like to know if it is possible to use DeepVariant to analyze; > PacBio mitochondrial data. If not, do you have any suitable tools to; > recommend?; >; > Looking forward to your reply.; > Thanks.; >; > ; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/686>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XGN5AXQVMVB6LKOIDXSC6TTANCNFSM6AAAAAA2YDZTZU>; > .; > You are receiving this because you are subscribed to this thread.Message; > ID: ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686#issuecomment-1651102049
https://github.com/google/deepvariant/issues/686#issuecomment-1652817371:229,Availability,reliab,reliable,229,"Hello, ; Thanks for your reply.; I don't want to perform assembly. I just want to obtain a VCF file for the mitochondria based on the aligned BAM file.; I am unsure if the results for chrM in the VCF generated by DeepVariant are reliable.; PacBio mitochondrial data:; ![1690423792423](https://github.com/google/deepvariant/assets/70870741/f6a18fa3-a432-4d53-9824-20a9e309298c). If DeepVariant is not suitable for calling mitochondrial variants on PacBio mitochondrial data, are there any other software recommendations for calling variants at mitochondrial loci without assembly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686#issuecomment-1652817371
https://github.com/google/deepvariant/issues/686#issuecomment-1652817371:49,Performance,perform,perform,49,"Hello, ; Thanks for your reply.; I don't want to perform assembly. I just want to obtain a VCF file for the mitochondria based on the aligned BAM file.; I am unsure if the results for chrM in the VCF generated by DeepVariant are reliable.; PacBio mitochondrial data:; ![1690423792423](https://github.com/google/deepvariant/assets/70870741/f6a18fa3-a432-4d53-9824-20a9e309298c). If DeepVariant is not suitable for calling mitochondrial variants on PacBio mitochondrial data, are there any other software recommendations for calling variants at mitochondrial loci without assembly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686#issuecomment-1652817371
https://github.com/google/deepvariant/issues/686#issuecomment-1652962088:506,Performance,perform,performs,506,"@crazysummerW mtDNA variant analysis usually requires more specialized steps, as you need to worry about NUMT and heteroplasmy among other things, especially since the number of mitochondria vary for different cell types. DeepVariant I don't believe has the models trained for that, as it is usually geared for autosomal DNA. So, if you just want a VCF file without the large amount of analysis that is required for dealing with mtDNA, you can use the mitochondria mode of Mutect2 in GATK like this, which performs a lot of it for you:; ; ```; gatk Mutect2 \; -R reference.fa \; -L chrM \; --mitochondria-mode \; -I mitochondria.bam \; -O mitochondria.vcf.gz; ```. You can read more about it here:. https://gatk.broadinstitute.org/hc/en-us/articles/13832710384155-Mutect2. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686#issuecomment-1652962088
https://github.com/google/deepvariant/issues/686#issuecomment-1654257037:686,Availability,error,errors,686,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686#issuecomment-1654257037
https://github.com/google/deepvariant/issues/686#issuecomment-1654257037:321,Performance,optimiz,optimized,321,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686#issuecomment-1654257037
https://github.com/google/deepvariant/issues/686#issuecomment-1654257037:515,Performance,optimiz,optimized,515,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686#issuecomment-1654257037
https://github.com/google/deepvariant/issues/687#issuecomment-1651305994:3350,Availability,checkpoint,checkpoint,3350,"s will be written to output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --reads_parent1 ""markduplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994
https://github.com/google/deepvariant/issues/687#issuecomment-1651305994:3647,Availability,checkpoint,checkpoint,3647,"uplicates/S_500061.md.bam"" --reads_parent2 ""markduplicates/S_500062.md.bam"" --reads ""markduplicates/S_500063.md.bam"" --examples ""output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --sample_name ""S_500063"" --sample_name_parent1 ""S_500061"" --sample_name_parent2 ""S_500062"" --channels ""insert_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994
https://github.com/google/deepvariant/issues/687#issuecomment-1651305994:3945,Availability,checkpoint,checkpoint,3945,"t_size"" --gvcf ""output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --pileup_image_height_child ""60"" --pileup_image_height_parent ""40"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_child.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/child/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent1.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output_parent2.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples_parent2.tfrecord@1.gz"" --checkpoint ""/opt/models/deeptrio/wgs/parent/model.ckpt"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_child.tfrecord.gz"" --outfile ""output/S_500063.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_child.tfrecord@1.gz"" --gvcf_outfile ""output/S_500063.g.vcf.gz"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""output/intermediate_results_dir/call_variants_output_parent1.tfrecord.gz"" --outfile ""output/S_500061.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf_parent1.tfrecord@1.gz"" --gvcf_outfile ""output/parent1.g.vcf.gz"". ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"" --infile ""outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994
https://github.com/google/deepvariant/issues/687#issuecomment-1651305994:194,Deployability,update,updated,194,"Hi @hosseinvk,. So just a couple of things:. $`1)`$ The `--output_gvcf_merged` flag is not supported in `run_deeptrio.py`, and GLNexus should be used instead, as indicated by issue #544. I have updated the command with the `--output_gvcf_parent1` and `--output_gvcf_parent2` flags. $`2)`$ If I run it via Docker like this, I get the dry run to work:. ```; docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent2 output/S_500062.g.vcf.gz --output_gvcf_child output/S_500063.g.vcf.gz --output_gvcf_parent1 output/parent1.g.vcf.gz --output_gvcf_parent2 output/parent2.g.vcf.gz --dry_run=true --vcf_stats_report=true; ```. The output I get:. ```; paul$ docker run google/deepvariant:deeptrio-latest /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --reads_parent1=markduplicates/S_500061.md.bam --reads_parent2=markduplicates/S_500062.md.bam --reads_child=markduplicates/S_500063.md.bam --output_vcf_parent1 output/S_500061.output.vcf.gz --output_vcf_parent2 output/S_500062.output.vcf.gz --output_vcf_child output/S_500063.output.vcf.gz --sample_name_parent1 'S_500061' --sample_name_parent2 'S_500062' --sample_name_child 'S_500063' --num_shards $(nproc) --intermediate_results_dir output/intermediate_results_dir --output_gvcf_parent1 output/S_500061.g.vcf.gz --output_gvcf_parent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687#issuecomment-1651305994
https://github.com/google/deepvariant/issues/687#issuecomment-1654874111:525,Deployability,configurat,configuration,525,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687#issuecomment-1654874111
https://github.com/google/deepvariant/issues/687#issuecomment-1654874111:603,Deployability,configurat,configuration,603,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687#issuecomment-1654874111
https://github.com/google/deepvariant/issues/687#issuecomment-1654874111:525,Modifiability,config,configuration,525,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687#issuecomment-1654874111
https://github.com/google/deepvariant/issues/687#issuecomment-1654874111:603,Modifiability,config,configuration,603,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687#issuecomment-1654874111
https://github.com/google/deepvariant/issues/689#issuecomment-1660193654:161,Modifiability,layers,layers,161,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1660193654
https://github.com/google/deepvariant/issues/689#issuecomment-1660193654:343,Modifiability,layers,layers,343,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1660193654
https://github.com/google/deepvariant/issues/689#issuecomment-1660748817:1206,Deployability,release,release,1206,"Hi @linlin-coder ,; Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:; Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1660748817
https://github.com/google/deepvariant/issues/689#issuecomment-1660748817:1438,Deployability,release,release,1438,"Hi @linlin-coder ,; Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:; Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1660748817
https://github.com/google/deepvariant/issues/689#issuecomment-1660748817:1166,Usability,clear,clear,1166,"Hi @linlin-coder ,; Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:; Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1660748817
https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:987,Availability,error,error,987,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240
https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:1531,Availability,error,error,1531,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240
https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:881,Deployability,install,installed,881,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240
https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:175,Modifiability,layers,layers,175,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240
https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:363,Modifiability,layers,layers,363,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240
https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:1122,Performance,optimiz,optimized,1122,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240
https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:1222,Performance,perform,performance-critical,1222,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240
https://github.com/google/deepvariant/issues/689#issuecomment-1661404840:1253,Deployability,release,release,1253,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840
https://github.com/google/deepvariant/issues/689#issuecomment-1661404840:1485,Deployability,release,release,1485,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840
https://github.com/google/deepvariant/issues/689#issuecomment-1661404840:1676,Safety,detect,detection,1676,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840
https://github.com/google/deepvariant/issues/689#issuecomment-1661404840:1775,Safety,detect,detection,1775,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840
https://github.com/google/deepvariant/issues/689#issuecomment-1661404840:1213,Usability,clear,clear,1213,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840
https://github.com/google/deepvariant/issues/689#issuecomment-1661405006:63,Deployability,update,update,63,"Hi @linlin-coder , ; Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in. Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661405006
https://github.com/google/deepvariant/issues/689#issuecomment-1661420506:450,Availability,mainten,maintenance,450,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in.; > ; > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661420506
https://github.com/google/deepvariant/issues/689#issuecomment-1661420506:63,Deployability,update,update,63,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in.; > ; > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661420506
https://github.com/google/deepvariant/issues/689#issuecomment-1661420506:439,Deployability,continuous,continuous,439,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in.; > ; > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661420506
https://github.com/google/deepvariant/issues/689#issuecomment-1661420506:560,Deployability,update,updates,560,"> Hi @linlin-coder , Right, because in v1.5.0 we didn't really update DeepTrio PacBio, we decided to just point our users to v1.4.0, which was the version without the direct read haplotagging built in.; > ; > Like I mention, we expect v1.6.0 (coming out before end of this year) to have a new version of DeepTrio PacBio which won't require an extra step of WhatsHap in between. Thank you to the author and the software developer for their continuous maintenance of open-source tools. As a software user, I will also continue to pay attention to the subsequent updates and article publications of this software. I look forward to more exchanges in the future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689#issuecomment-1661420506
https://github.com/google/deepvariant/issues/690#issuecomment-1660589629:1827,Safety,detect,detected,1827,"e exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```; INPUT_DIR=""${PWD}/YOUR_INPUT_PATH""; OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup --num_records=20. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file.; 2. The reference file used to generate the BAM is different than the one used with DeepVariant.; 3. There might not be many SNPs left supported by informative reads.; 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads?. The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629
https://github.com/google/deepvariant/issues/690#issuecomment-1660589629:606,Usability,Simpl,Simplex,606,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```; INPUT_DIR=""${PWD}/YOUR_INPUT_PATH""; OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup --num_records=20. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file.; 2. The reference file used to generate the BAM is different than the one used with DeepVariant.; 3. There might not be ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629
https://github.com/google/deepvariant/issues/690#issuecomment-1660589629:652,Usability,Simpl,Simplex,652,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```; INPUT_DIR=""${PWD}/YOUR_INPUT_PATH""; OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup --num_records=20. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file.; 2. The reference file used to generate the BAM is different than the one used with DeepVariant.; 3. There might not be ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629
https://github.com/google/deepvariant/issues/690#issuecomment-1660589629:698,Usability,simpl,simplex-case-study,698,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```; INPUT_DIR=""${PWD}/YOUR_INPUT_PATH""; OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup --num_records=20. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file.; 2. The reference file used to generate the BAM is different than the one used with DeepVariant.; 3. There might not be ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629
https://github.com/google/deepvariant/issues/690#issuecomment-1662683748:594,Deployability,release,released,594,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants?. If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1662683748
https://github.com/google/deepvariant/issues/690#issuecomment-1662683748:409,Safety,detect,detect,409,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants?. If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1662683748
https://github.com/google/deepvariant/issues/690#issuecomment-1662683748:501,Safety,detect,detect,501,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants?. If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1662683748
https://github.com/google/deepvariant/issues/690#issuecomment-1663136809:181,Performance,perform,performs,181,"hi @pichuan,. That makes a lot of sense. . I already lowered the threshold but it seem like this is not a way to go. My plan is to retrain the model on our dataset and hopefully it performs better.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1663136809
https://github.com/google/deepvariant/issues/690#issuecomment-1663249541:32,Safety,detect,detecting,32,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1663249541
https://github.com/google/deepvariant/issues/690#issuecomment-1663249541:599,Security,validat,validated,599,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/690#issuecomment-1663249541
https://github.com/google/deepvariant/issues/691#issuecomment-1662692855:313,Availability,Down,Downsample,313,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads""; * Check ""Label indels > label threshold"", and have a value of 0 ; * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1662692855
https://github.com/google/deepvariant/issues/691#issuecomment-1662692855:871,Availability,Down,Downsample,871,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads""; * Check ""Label indels > label threshold"", and have a value of 0 ; * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1662692855
https://github.com/google/deepvariant/issues/691#issuecomment-1662692855:836,Performance,perform,perform,836,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads""; * Check ""Label indels > label threshold"", and have a value of 0 ; * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1662692855
https://github.com/google/deepvariant/issues/691#issuecomment-1662847110:308,Security,validat,validated,308,"Hi Both, thank you so much for your replies! I really appreciate it. I adjusted the settings with your additions Paul but got the same result as I put above. Definitely the same bam and vcf sample, I just double checked. Hopefully just a realignment thing like Pi-Chuan mentioned. We may end up getting them validated just to make sure we're not throwing any potentials away! Hard choice!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1662847110
https://github.com/google/deepvariant/issues/691#issuecomment-1662870870:357,Availability,mask,mask,357,"Hmm, having no doubts is essential. Would you be comfortable with posting a few complete rows from the VCF file, which would include this variant and a few above and below. DeepVariant is base-specific, so it would tell us something that might be captured by a few rows. Also could you post the whole script and command you used to run DeepVariant? You can mask out anything that is sensitive. . Just trying to be thorough we're not missing anything. Thanks,; `p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1662870870
https://github.com/google/deepvariant/issues/691#issuecomment-1667418894:20,Availability,error,error,20,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; ```. I'll let you know when they reply!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894
https://github.com/google/deepvariant/issues/691#issuecomment-1667418894:398,Deployability,install,installed,398,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; ```. I'll let you know when they reply!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894
https://github.com/google/deepvariant/issues/691#issuecomment-1667418894:662,Deployability,install,installed,662,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; ```. I'll let you know when they reply!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894
https://github.com/google/deepvariant/issues/691#issuecomment-1667418894:121,Usability,simpl,simple,121,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; ```. I'll let you know when they reply!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894
https://github.com/google/deepvariant/issues/691#issuecomment-1667663801:720,Deployability,release,release,720,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/; ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667663801
https://github.com/google/deepvariant/issues/691#issuecomment-1667663801:864,Deployability,release,releases,864,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/; ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667663801
https://github.com/google/deepvariant/issues/691#issuecomment-1667663801:934,Deployability,release,releases,934,"Hi Amy,. That's just a warning that you can ignore/fix it via binding `/usr/lib/locale`, at the beginning of your Singularity command like this:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/; ```. By the way, you ran into it last year in a previous issue, that didn't affect your computation in the following post:. [https://github.com/google/deepvariant/issues/542](https://github.com/google/deepvariant/issues/542). Notice the computation continued ignoring your locale settings. Is that the only warning you see, or is the computation stuck? Also, just a minor thing, you're running DeepVariant 1.3.0, and now it is [at version 1.5.0](https://hub.docker.com/r/google/deepvariant/tags), and with the next release the plan is before the end of this year. It probably would not affect your results significantly. You can check on the [changes between releases at the following page](https://github.com/google/deepvariant/releases),. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667663801
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:13,Availability,fault,fault,13,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p htc; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-2; #SBATCH --mem-per-cpu=68GB; #SBATCH --qos=maxjobs500. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file; HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; REGIONS=""chr15:41,132,484-42,007,831""; OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$REGIONS \; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:1537,Availability,error,error,1537,"=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-2; #SBATCH --mem-per-cpu=68GB; #SBATCH --qos=maxjobs500. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file; HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; REGIONS=""chr15:41,132,484-42,007,831""; OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$REGIONS \; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. with the error: . ```; ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:1569,Availability,error,error,1569,"=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-2; #SBATCH --mem-per-cpu=68GB; #SBATCH --qos=maxjobs500. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file; HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; REGIONS=""chr15:41,132,484-42,007,831""; OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$REGIONS \; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. with the error: . ```; ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:2138,Availability,error,error,2138,"TPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$REGIONS \; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads"" \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. with the error: . ```; ***** Intermediate results will be written to /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:3325,Deployability,install,installed,3325,"*** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader; W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs; I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:3589,Deployability,install,installed,3589,"scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader; W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs; I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader; I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:6821,Modifiability,config,config,6821,"py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner; region_processor.initialize(); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize; self._initialize(); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize; self.realigner = realigner.Realigner(; File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__; self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__; self._csv_file = open(self._root_join(self.metrics_filename), 'w'); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join; tf.io.gfile.makedirs(subdir); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2; _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)); tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:693,Performance,load,load,693,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p htc; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-2; #SBATCH --mem-per-cpu=68GB; #SBATCH --qos=maxjobs500. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file; HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; REGIONS=""chr15:41,132,484-42,007,831""; OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$REGIONS \; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:715,Performance,load,load,715,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p htc; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-2; #SBATCH --mem-per-cpu=68GB; #SBATCH --qos=maxjobs500. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file; HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; REGIONS=""chr15:41,132,484-42,007,831""; OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$REGIONS \; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:271,Testability,log,login,271,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p htc; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-2; #SBATCH --mem-per-cpu=68GB; #SBATCH --qos=maxjobs500. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file; HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; REGIONS=""chr15:41,132,484-42,007,831""; OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$REGIONS \; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113
https://github.com/google/deepvariant/issues/691#issuecomment-1667728104:504,Deployability,update,update,504,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104
https://github.com/google/deepvariant/issues/691#issuecomment-1667728104:756,Deployability,update,update,756,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104
https://github.com/google/deepvariant/issues/691#issuecomment-1667728104:110,Security,access,access,110,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104
https://github.com/google/deepvariant/issues/691#issuecomment-1667728104:194,Security,access,access,194,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104
https://github.com/google/deepvariant/issues/691#issuecomment-1667728104:810,Security,access,accessible,810,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104
https://github.com/google/deepvariant/issues/691#issuecomment-1667728104:11,Usability,simpl,simple,11,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104
https://github.com/google/deepvariant/issues/691#issuecomment-1669660043:891,Performance,perform,performed,891,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46; -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time?. Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1669660043
https://github.com/google/deepvariant/issues/691#issuecomment-1669660043:976,Performance,perform,performed,976,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46; -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time?. Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1669660043
https://github.com/google/deepvariant/issues/691#issuecomment-1670038776:275,Testability,log,logic,275,"Looking at @amy-houseman 's latest IGV in https://github.com/google/deepvariant/issues/691#issuecomment-1669163372 , it seems like the realigned BAM would see 1 read containing the alt allele C. If this is the case, I don't expect it to even trigger our candidate generation logic. (From this post, it didn't seem like @amy-houseman has specified different vsc_ thresholds). One possibility is that the realignment BAM was generated with different region compared with when the variant calling was run, like @pgrosu mentioned above.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1670038776
https://github.com/google/deepvariant/issues/691#issuecomment-1670332454:791,Modifiability,variab,variable,791,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1670332454
https://github.com/google/deepvariant/issues/691#issuecomment-1670332454:854,Modifiability,variab,variable,854,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1670332454
https://github.com/google/deepvariant/issues/691#issuecomment-1670332454:1175,Performance,load,loaded,1175,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/691#issuecomment-1670332454
https://github.com/google/deepvariant/issues/692#issuecomment-1670070938:35,Deployability,release,release,35,"Hi Kiran,. We don't have a general release for this. If you are interested, you can email awcarroll@google.com and we can follow up on options.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/692#issuecomment-1670070938
https://github.com/google/deepvariant/issues/695#issuecomment-1677027250:1013,Security,validat,validation,1013,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695#issuecomment-1677027250
https://github.com/google/deepvariant/issues/695#issuecomment-1677027250:1004,Testability,test,test,1004,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/695#issuecomment-1677027250
https://github.com/google/deepvariant/issues/696#issuecomment-1679264044:1266,Availability,down,download,1266,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044
https://github.com/google/deepvariant/issues/696#issuecomment-1679264044:57,Integrability,depend,depends,57,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,; P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044
https://github.com/google/deepvariant/issues/696#issuecomment-1679264044:1467,Integrability,depend,depends,1467,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044
https://github.com/google/deepvariant/issues/696#issuecomment-1679264044:340,Modifiability,layers,layers,340,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,; P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044
https://github.com/google/deepvariant/issues/696#issuecomment-1679264044:537,Modifiability,layers,layers,537,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,; P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044
https://github.com/google/deepvariant/issues/696#issuecomment-1679300379:864,Integrability,depend,depending,864,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679300379
https://github.com/google/deepvariant/issues/696#issuecomment-1679371052:192,Performance,perform,performance,192,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679371052
https://github.com/google/deepvariant/issues/696#issuecomment-1679371052:37,Usability,clear,clear,37,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679371052
https://github.com/google/deepvariant/issues/696#issuecomment-1679436458:111,Performance,optimiz,optimizations,111,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679436458
https://github.com/google/deepvariant/issues/696#issuecomment-1679436458:816,Performance,optimiz,optimizations,816,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679436458
https://github.com/google/deepvariant/issues/696#issuecomment-1679436458:421,Testability,benchmark,benchmark,421,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679436458
https://github.com/google/deepvariant/issues/696#issuecomment-1679436458:776,Testability,log,logarithmic,776,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1679436458
https://github.com/google/deepvariant/issues/696#issuecomment-1681230125:320,Availability,failure,failure,320,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1681230125
https://github.com/google/deepvariant/issues/696#issuecomment-1681230125:465,Integrability,depend,depends,465,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/696#issuecomment-1681230125
https://github.com/google/deepvariant/issues/697#issuecomment-1680213963:311,Availability,down,down-sampling,311,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1680213963
https://github.com/google/deepvariant/issues/697#issuecomment-1680213963:490,Availability,down,down-sample,490,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1680213963
https://github.com/google/deepvariant/issues/697#issuecomment-1680213963:1294,Deployability,release,release,1294,"Hi @observer2735,. So your depth is very high, which either indicates that you are trying to measure something with very low VAF (which this is not the case with a value of 0.4), or you might have duplicated parts of the genome or are working with very small genomic region. Now having said that, you could try down-sampling your region to something along the lines of 80-100, and see if the call changes. Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image. . You seem to have one call that was rejected, meaning a RefCall entry of a proposed candidate but rejected as non-variant by the model, and then uncalled (`./.`) because your GQ is less than 20. Your QUAL is 0, indicating there is no variant there. Your GQ is around 11-12, which this is just a RefCall based on the model. The reason your read depth (DP) is smaller than the number of rows you are counting might be that it locally realigned the reads, and then the allele counter got a different number for the DP given a different number of supporting reads. Probably looking at your BAM in IGV might help to confirm. Also aligning to [GRCh38 without ALT contigs](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz) might also be a better option to try than hg19 for generating the BAM files, and then also using that as a reference in DeepVariant -- as DeepVariant assumes the same reference was used during the mapping when generating the BAM files. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1680213963
https://github.com/google/deepvariant/issues/697#issuecomment-1680308857:1334,Availability,down,down-sample,1334,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data?. Very sorry to bother you, wish you a pleasant work and life,; Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1680308857
https://github.com/google/deepvariant/issues/697#issuecomment-1680308857:499,Safety,detect,detect,499,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data?. Very sorry to bother you, wish you a pleasant work and life,; Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1680308857
https://github.com/google/deepvariant/issues/697#issuecomment-1680308857:1477,Safety,detect,detect,1477,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data?. Very sorry to bother you, wish you a pleasant work and life,; Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1680308857
https://github.com/google/deepvariant/issues/697#issuecomment-1680308857:1532,Safety,detect,detect,1532,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data?. Very sorry to bother you, wish you a pleasant work and life,; Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1680308857
https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:1595,Availability,error,error,1595,"llowing comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script, below your first `mkdir` statement:. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. $`2)`$ Now given that, there would still be a lot of reads in that region for the allele counter to create candidates from. You have below the 1,500 maximum read depth, as noted in the following [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated), but quite a few seem to still remain. You noticed that only 490 appear supporting that position, and DeepVariant is position-specific. If the quality of the allele is low those are not used. For example, your GQ here is 5 to support the variant, and your QUAL 1.5, which tells me there is no variant and the error is 32% of it being that variant. It would also help to check your gVCF as that contains everything. Also if there are multiple deletions at the same location it determines the deletions with the highest read support, and deletes all other deletions from the allele map. $`4)`$ There are additional cutoffs for the type of candidates it selects. If you want to adjust thresholds for specific types of candidates, you can adjust this via the `--make_examples_extra_args=`, by setting some or all of the following parameters to your preference:. * vsc_min_count_snps (the default is 2); * vsc_min_count_indels (the default is 2); * vsc_min_fraction_snps (the default is 0.12); * vsc_min_fraction_indels (the default is 0.06); * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```; --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'; ```. You can read more details about these parameters at th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918
https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:4612,Performance,optimiz,optimization,4612,"T and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918
https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:3240,Safety,predict,prediction,3240,"snps (the default is 2); * vsc_min_count_indels (the default is 2); * vsc_min_fraction_snps (the default is 0.12); * vsc_min_fraction_indels (the default is 0.06); * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```; --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'; ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918
https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:3828,Safety,predict,prediction,3828,"5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918
https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:4529,Safety,predict,prediction,4529,"T and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918
https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:4869,Security,validat,validation,4869,"T and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918
https://github.com/google/deepvariant/issues/697#issuecomment-1681396124:122,Availability,error,errors,122,"@observer2735 Can you say more about what you are trying to do? Is it germline variant calling? Or maybe counting all the errors in the reads?; Please elaborate, thank you!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/697#issuecomment-1681396124
