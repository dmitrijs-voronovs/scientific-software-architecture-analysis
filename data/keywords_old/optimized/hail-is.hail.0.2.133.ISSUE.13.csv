quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,"_</code> (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/274"">#274</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/8685c60dc0e82ee246fbe3d1aa272d1dfe57c24c""><code>8685c60</code></a> Bump mypy from 0.910 to 0.920 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/273"">#273</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/f0a0914345b224448220aeb00d71e6a04a5d24bd""><code>f0a0914</code></a> Bump twine from 3.7.0 to 3.7.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/272"">#272</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/e5c813173b8811b30f4a30eeba56fa8808ab15bb""><code>e5c8131</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/271"">#271</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c7e10db3d0965422122bef263fb36f6fd7572330""><code>c7e10db</code></a> Fix linter configs</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/6af8bf421a799208b27d57c6bf69de0d998fedec""><code>6af8bf4</code></a> Bump pre-commit from 2.15 to 2.16.0 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/269"">#269</a>)</li>; <li><a href=""https://github.com/aio-libs/async-timeout/commit/c71bbb5b5d7330f6dabfde7a1adec30a4611c0be""><code>c71bbb5</code></a> Bump docutils from 0.18 to 0.18.1 (<a href=""https://github-redirect.dependabot.com/aio-libs/async-timeout/issues/266"">#266</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/aio-libs/async-timeout/compare/v3.0.1...v4.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=async-timeout&package-manager=pip&previous-version=3.0.1&new-version=4.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11465:5648,config,configs,5648,https://hail.is,https://github.com/hail-is/hail/pull/11465,1,['config'],['configs']
Modifiability,"_From @cseed on August 26, 2015 14:0_. This might involve patching the Gradle Jacoco plugin. _Copied from original issue: cseed/hail#4_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5:85,plugin,plugin,85,https://hail.is,https://github.com/hail-is/hail/issues/5,1,['plugin'],['plugin']
Modifiability,"_From @jbloom22 on September 29, 2015 17:21_. Once we handle multi-allelic sites, we will need to adapt mendel errors so that, for example, it does not double count errors in multi-allelic trios. _Copied from original issue: cseed/hail#65_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/45:98,adapt,adapt,98,https://hail.is,https://github.com/hail-is/hail/issues/45,1,['adapt'],['adapt']
Modifiability,_main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/process.py'; adding,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:11786,config,config,11786,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['config'],['config']
Modifiability,"_required=False); ```. ---. ### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/checkpoint.20with.20missing.20fields. ```; is.hail.utils.HailException: gs://jn-vcf-cleanup-central1/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626.vcf.gz:offset 1344376382: error while parsing line; chr1	10403	.	ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	A,ACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	.	LowQual	AC=1,1;AF=0.250,0.250;AN=4;AS_QUALapprox=0|23|45;AS_VQSLOD=.,.;AS_YNG=.,.;QUALapprox=45	GT:AD:GQ:RGQ	./.	0/1:23,7,0:20:23	./.	./.	./.	0/2:6,0,4:35:45	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:21); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:21); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1934); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1922); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:1134,adapt,adapted,1134,https://hail.is,https://github.com/hail-is/hail/issues/13346,1,['adapt'],['adapted']
Modifiability,"`. activate NAME` might silently fail if `NAME` does not exist or `conda` is not configured. `. ./loadconda` tries to find conda in a variety of places and configure it (meaning source `conda.sh`). After this, `conda activate NAME` will work correctly. ---. This is already in my batch dag PR, but that's getting bogged down in test issues, and this is blocking @akotlar 's https://github.com/hail-is/hail/pull/5065 PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5066:81,config,configured,81,https://hail.is,https://github.com/hail-is/hail/pull/5066,2,['config'],"['configure', 'configured']"
Modifiability,"`IRSuite.scala` has a class `IRSuite` which has this inheritance sequence:; - `HailSuite`; - `TestNGSuite`; - `Suite`; - `Assertions` (among other interfaces). `Assertions` has [`assertThrows` with one argument](http://doc.scalatest.org/3.0.8/org/scalatest/testng/TestNGSuite.html#assertThrows[T<:AnyRef](f:=>Any)(implicitclassTag:scala.reflect.ClassTag[T],implicitpos:org.scalactic.source.Position):org.scalatest.Assertion). Unfortunately, `IRSuite.scala` also contains `import is.hail.TestUtils._`. This also brings into scope an `assertThrows` with two parameters. I have not bothered to understand Scala's name resolution strategy. SBT 1.3.8 refuses to acknowledge the existence of the `TestUtils.assertThrows` and instead tries to convert the two arguments into a pair and then pass those to `Assertions.assertThrows`. This rightfully raises a warning which we treat as errors. Both gradle and SBT have Scala version set to 2.11.8. I've fixed this by prefixing the assertThrows with `is.hail.TestUtils.assertThrows`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8359:53,inherit,inheritance,53,https://hail.is,https://github.com/hail-is/hail/issues/8359,1,['inherit'],['inheritance']
Modifiability,"`OnDiskBTreeIndexToValue` can index the elements at the base of a BTree. The BGen BTree is a BTree on the byte-offsets of variants in the BGen file. In a following PR, I will use this class to filter the BGen file to a subset of variants, specified by their index. This kind of filtering happens at the level of bytes, it permits me to avoid decoding/decompressing any variants I don't need. Ideally, the index would also include the variant keys themselves. That would be a really nice extension of this work and would enable a more natural file-level filtering user experience. Aside: `IndexBTree` could use some TLC. I'm sort of making the minimal changes to get Caitlin cooking. Last night, I slipped into a rewrite and it's just too big a task to get right before Friday because I don't have any documentation of precisely what our btree file format is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3794:712,rewrite,rewrite,712,https://hail.is,https://github.com/hail-is/hail/pull/3794,1,['rewrite'],['rewrite']
Modifiability,"`Table._select` got way too complicated (mostly my fault) when key changing moved from `TableMapRows` to `TableKeyBy`. Making `_select` a simple wrapper around `TableMapRows`, and moving the key logic to `key_by`, made both way simpler. I then realized the `key_by` code could be even simpler by adding some rules to the optimizer to clean up the case where all new keys are existing fields. I actually think some things had gotten broken in the old `_select` (performance wise). In particular, in `split_multi`, in the `split_rows` function with `rekey=false`, I think it's supposed to extend the key from `['locus']` to `['locus', 'alleles']`, but that wasn't happening. I changed `key_by` to no longer accept `key_by(None)` or `key_by([])`, both of which should now be `key_by()`, which is more consistent with the rest of our interface, but is a breaking change. Is it worth the disruption? Should I add a warning? Or just continue to accept both?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4455:587,extend,extend,587,https://hail.is,https://github.com/hail-is/hail/pull/4455,1,['extend'],['extend']
Modifiability,"``; the dataproc cluster command would be provided the following environment variable through the `--metadata` flag: `VEP_REPLICATE=aus-sydney`. This variable is used within the script `gs://hail-common/hailctl/dataproc/0.2.115/vep-GRCh38.sh` to determine which bucket to pull the VEP cache data from. In more recent versions (tested with 0.2.130), this `VEP_REPLICATE` variable has been changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1233,plugin,plugin,1233,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,"```; + CHANGED=yes; + [[ -e notebook/get-deployed-sha.sh ]]; + [[ yes != no ]]; + cd notebook; + /bin/bash hail-ci-deploy.sh; cat: notebook-image: No such file or directory; sed -e ""s,@sha@,17a365c57d0f,"" \; -e ""s,@image@,,"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""notebook"", Namespace: ""batch-pods""; Object: &{map[""metadata"":map[""labels"":map[""app"":""notebook""] ""name"":""notebook"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""port"":'P' ""protocol"":""TCP"" ""targetPort"":'\u1388']] ""selector"":map[""app"":""notebook""]] ""apiVersion"":""v1"" ""kind"":""Service""]}; from server for: ""deployment.yaml"": services ""notebook"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:4: recipe for target 'deploy' failed; make: *** [deploy] Error 1; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4656:363,config,configuration,363,https://hail.is,https://github.com/hail-is/hail/issues/4656,2,['config'],['configuration']
Modifiability,```; + cd /io; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; ++ mktemp -d; + dir=/tmp/tmp.Txkg8yv5oW; + git clone https://github.com/hail-is/hail.git /tmp/tmp.Txkg8yv5oW; Cloning into '/tmp/tmp.Txkg8yv5oW'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: protocol error: bad pack header; ++ ls -A /tmp/tmp.Txkg8yv5oW. real	1m0.562s; user	0m0.135s; sys	0m0.086s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8170:496,config,config,496,https://hail.is,https://github.com/hail-is/hail/issues/8170,1,['config'],['config']
Modifiability,```; + date; Mon Mar 30 22:11:05 UTC 2020; + rm -rf repo; + mkdir repo; + cd repo; + '[' '!' -d .git ']'; + retry clone; + clone; + set -e; ++ mktemp -d; + dir=/tmp/tmp.3H7wTmq0R2; + git clone https://github.com/danking/hail.git /tmp/tmp.3H7wTmq0R2; Cloning into '/tmp/tmp.3H7wTmq0R2'...; error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.; fatal: The remote end hung up unexpectedly; fatal: early EOF; fatal: index-pack failed; ++ ls -A /tmp/tmp.3H7wTmq0R2. real	0m3.373s; user	0m0.006s; sys	0m0.025s; + git config user.email ci@hail.is; fatal: not in a git directory; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8397:538,config,config,538,https://hail.is,https://github.com/hail-is/hail/issues/8397,1,['config'],['config']
Modifiability,"```; Traceback (most recent call last):; File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/sandbox.py"", line 17, in <module>; pprint(hc.read('%s/variantqc/exacv2_rf.vds' % root, sites_only=True).filter_variants_intervals('gs://exac2/temp').head()); File ""/tmp/09d98b2f-4a41-4652-9eba-e319bfda2ca4/utils.py"", line 201, in head; return json.loads(self.variants_keytable().to_dataframe().toJSON().first()); File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1328, in first; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py"", line 1310, in take; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py"", line 933, in runJob; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; for criterion, pop in criteria_pops:; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 20022, exac-sw-3pdd.c.broad-mpg-gnomad.internal): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.sql.Row; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1311:89,sandbox,sandbox,89,https://hail.is,https://github.com/hail-is/hail/issues/1311,1,['sandbox'],['sandbox']
Modifiability,"```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8201:435,config,config,435,https://hail.is,https://github.com/hail-is/hail/issues/8201,1,['config'],['config']
Modifiability,"```; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=namespaces"", GroupVersionKind: ""/v1, Kind=Namespace""; Name: ""batch-pods"", Namespace: """"; Object: &{map[""apiVersion"":""v1"" ""kind"":""Namespace"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods"" ""namespace"":""""]]}; from server for: ""deployment.yaml"": namespaces ""batch-pods"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get namespaces in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=serviceaccounts"", GroupVersionKind: ""/v1, Kind=ServiceAccount""; Name: ""batch-svc"", Namespace: ""batch-pods""; Object: &{map[""kind"":""ServiceAccount"" ""metadata"":map[""name"":""batch-svc"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""apiVersion"":""v1""]}; from server for: ""deployment.yaml"": serviceaccounts ""batch-svc"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get serviceaccounts in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.author",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:100,config,configuration,100,https://hail.is,https://github.com/hail-is/hail/issues/4609,3,['config'],['configuration']
Modifiability,"```python; >>> hl.eval(hl.min_rep(hl.locus('1', 10000), ['G', hl.null(hl.tstr)])); ```; I don't have any problem with this erroring, but it's mode should be more user friendly. Either that or we allow `NA` in `min_rep` (just return the NA) in the `alleles` array and don't use it for the purposes of actually `min_rep`ping.; ```; java.lang.NullPointerException: null ; at is.hail.codegen.generated.C172.method_2(Unknown Source) ; at is.hail.codegen.generated.C172.method_1(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.codegen.generated.C172.apply(Unknown Source) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:711) ; at is.hail.expr.ir.Interpret$$anonfun$apply$33.apply(Interpret.scala:690) ; at is.hail.utils.package$.using(package.scala:596); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:690); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:91); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:61); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:30); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1$$anonfun$apply$2.apply(FoldConstants.scala:8); at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:7); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:8); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:7); at is.hail.utils.package$.using(package.scala:596); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6889:1253,Rewrite,RewriteBottomUp,1253,https://hail.is,https://github.com/hail-is/hail/issues/6889,6,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,"`aiohttp.ClientOSError` inherits from `OSError`, so we can just use `errno` or `strerror` directly. We should not directly use the `args` because one of the subclasses of `ClientOSError` sets them to *its* arguments after initializing its super classes with the expected arguments:. ```python3; class ClientConnectorError(ClientOSError):; """"""Client connector error. Raised in :class:`aiohttp.connector.TCPConnector` if; a connection can not be established.; """""". def __init__(self, connection_key: ConnectionKey, os_error: OSError) -> None:; self._conn_key = connection_key; self._os_error = os_error; super().__init__(os_error.errno, os_error.strerror); self.args = (connection_key, os_error); ```. I also tried to remove `e.args` from the `ClientPayloadError` case (the one right above this, and the only one still using `e.args`), but neither that class nor any super class sets a field with the error message (in fact, no fields are ever set so we can only use `e.args`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13921:24,inherit,inherits,24,https://hail.is,https://github.com/hail-is/hail/pull/13921,1,['inherit'],['inherits']
Modifiability,"`asyncinit` is unused AFAICT and the `frozenlist` requirement is already inherited from hailtop (though it is not used in `hailtop` only in query code, so am also happy to move it out of hailtop fully and into query).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13988:73,inherit,inherited,73,https://hail.is,https://github.com/hail-is/hail/pull/13988,1,['inherit'],['inherited']
Modifiability,"`cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutable.Set.empty, mutable.Set.empty); extract.analyze(cond, ref.name, Some(rw), trueSet); Some((extract.rewrite(cond, rw), trueSet)); }; }; }; ```; `trueSet` is the set of intervals which contains all rows where `cond` is true. This set is passed back into `analyze` in a second pass, which asks it to rewrite `cond` to something equivalent, under the assumption that all keys are contained in `trueSet`. The abstraction of runtime values tracks two types of information:; * Is this value a reference to / copy of one of the key fields of this row? We need to know this to be able to recognize comparisons with key values, which we want to extract to interval filters.; * For boolean values (including, ultimately, the filter predicate itself), we track three sets of intervals of the key type: overapproximations of when the bool is true, false, and missing. Overapproximation here means, for example, if the boolean evaluates to true in some row with key `k`, then `k` must be contained in the ""tr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:2221,Rewrite,Rewrites,2221,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['Rewrite'],['Rewrites']
Modifiability,`config['domain']` is actually never None in `from_config` so I moved the fallback to the config.ini value of domain to the `from_config_file` function that currently hard-codes `hail.is` when there's no deploy config.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11131:1,config,config,1,https://hail.is,https://github.com/hail-is/hail/pull/11131,3,['config'],['config']
Modifiability,`f` is a thunk so it is currently being evaluated thrice before inserted into the code cache. The `compiledFunction` variable was unused so I think this is what was originally intended.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13796:117,variab,variable,117,https://hail.is,https://github.com/hail-is/hail/pull/13796,1,['variab'],['variable']
Modifiability,`hailctl dataproc connect` and `hailctl dataproc modify` hard-code a default compute zone of us-central1-b. This changes those two commands to use the `compute/zone` value from the user's gcloud configuration if a zone argument is not provided. @johnc1231 [mentioned this in Zulip](https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/Unable.20to.20launch.20notebook) the other day.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8790:195,config,configuration,195,https://hail.is,https://github.com/hail-is/hail/pull/8790,1,['config'],['configuration']
Modifiability,"`hailctl dataproc` can pass through gcloud configurations specified with `--configuration` to the `gcloud dataproc` commands that it runs. However, that argument is often not respected for other `gcloud` commands that `hailctl dataproc` may run. For example, `hailctl dataproc start --configuration some_config` gets the project ID for requester pays configuration from the default gcloud configuration, not the specified configuration. https://github.com/hail-is/hail/blob/9706dd493515bce5aa88c623a83f899e8f8b801b/hail/python/hailtop/hailctl/dataproc/start.py#L262-L265. Hail version: 0.2.57-3f3afaa1d7bd. Zulip chat for context; https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/403.20on.20public.20file",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9587:43,config,configurations,43,https://hail.is,https://github.com/hail-is/hail/issues/9587,6,['config'],"['configuration', 'configurations']"
Modifiability,`hailctl dev config show` is currently inconsistent with the verbiage used; to change those variables. This change ensures `hailctl dev config set k v`; is displayed as `k: v`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10240:13,config,config,13,https://hail.is,https://github.com/hail-is/hail/pull/10240,3,"['config', 'variab']","['config', 'variables']"
Modifiability,"`hailtop.batch.ServiceBackend` uses `get_user_config().get` to read the `HAIL_BATCH_REGIONS` environment variable, when it should use `configuration_of`. This change fixes that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13239:105,variab,variable,105,https://hail.is,https://github.com/hail-is/hail/pull/13239,1,['variab'],['variable']
Modifiability,"`hb.Batch` now supports `default_regions` which completes the natural hierarchy of: config, envvar, backend, batch, job. I went a little hog wild with examples. I think we should have more examples everywhere!. The ServiceBackend doc page also had several basic formatting issues which I addressed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14224:84,config,config,84,https://hail.is,https://github.com/hail-is/hail/pull/14224,1,['config'],['config']
Modifiability,"`hl.balding_nichols_model` generates a MatrixTable representing a genetic dataset randomly drawn according to the Balding Nichols model.; ```; In [5]: hl.balding_nichols_model(2,3,3).show() ; 2019-08-15 10:38:05 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 3 samples, and 3 variants...; +---------------+------------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT |; +---------------+------------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call |; +---------------+------------+------+------+------+; | 1:1 | [""A"",""C""] | 0/0 | 0/0 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 1/1 | 1/1 |; | 1:3 | [""A"",""C""] | 1/1 | 0/1 | 0/0 |; +---------------+------------+------+------+------+. ```; These MatrixTables are useful both as examples and test datasets for genetics-related Hail code. Unfortunately, the loci are chosen sequentially starting with chromosome 1, position 1. This region of chromosome 1 is in the telomere. Many genetic annotations contain no information in this region. As a result, `hl.balding_nichols_model` is not useful when demonstrating the annotation database or third-party genetic annotations. We want to enhance `hl.balding_nichols_model` to select variants (loci-allele-array pairs) that are likely to appear in real genetic datasets. One very simple model would be to draw variants according to their alternate/minor allele frequency in the gnomAD or 1000 Genomes datasets. An additional improvement would be to generate chromosomes roughly proportionally to their true sizes. These changes should not significantly slow down the method. We may want to include a small dataset of allele frequencies with Hail for use when the requested number of variants is small, only loading the full gnomAD or 1000 Genomes allele frequencies when the requested number of variants is in the millions or tens of millions. This functionality should be enabled and disabled by a parameter to `hl.balding_nichols_model`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6880:1187,enhance,enhance,1187,https://hail.is,https://github.com/hail-is/hail/issues/6880,1,['enhance'],['enhance']
Modifiability,"`network=private` is an escape hatch for CI so certain jobs can talk to internal endpoints on our network that we do not permit user jobs to reach. In `main`, all CI jobs are hard-coded to use `private` in `build.py`, but few jobs in the CI pipeline actually require these heightened privileges. The steps in the CI pipeline are of the following types:. - `BuildImageStep`: These do not need to use the private network and have now all been made to use the public network namespaces; - `CreateDatabaseStep`: These *do* need to use `network='private'` because all our DBs only have private IPs on our internal network; - `RunImageStep`: Those steps that contact the DB need the private network. This PR makes the network configurable for these steps but default to public, so steps that need DB access explicitly do `network: private`; - `DeployStep`: These do not need to use the private network because they use the public K8s API server endpoint. Whether they should is perhaps a different question. I'm open to keeping these on the private networks and creating an issue to use the internal API server endpoint instead. We definitely have a static internal IP in GKE but I don't believe we have one for AKS and that would involve some research.; - `CreateNamespaceStep`: I don't believe that this needs the private network because it is functionally the same as a `DeployStep` in that it just talks to K8s, but I am unable to test this step in `test_ci` so I am reluctant to make a change that could brick CI. I instead made it configurable and default to its current value ('private'). We could then make a follow-up PR that tries turning it public.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14294:720,config,configurable,720,https://hail.is,https://github.com/hail-is/hail/pull/14294,2,['config'],['configurable']
Modifiability,`orjson` 3.9.15 fixed the rare segfault that we saw in `3.9.11`. Besides just updating to latest patch and minor versions:. - Removed a redundant requirement of `orjson` in `gear/requirements.txt` -- it inherits `orjson` from hailtop; - Bokeh `3.4` made a breaking change w.r.t. the `circle` method on figures. I have restricted the bounds for `bokeh` to avoid this breaking change but will follow up with a PR that changes our usage of bokeh to follow the deprecation/upgrade advice and undo the bounds restriction,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14471:203,inherit,inherits,203,https://hail.is,https://github.com/hail-is/hail/pull/14471,1,['inherit'],['inherits']
Modifiability,"`pyright` is an alternative to `mypy` for type checking python. It's also a pretty solid language server. While they both adhere to the PEPs for type checking, they [have a few differences](https://github.com/microsoft/pyright/blob/main/docs/mypy-comparison.md#differences-between-pyright-and-mypy). That doc is worth a read, but the parts that I found most compelling are pyright's return-type inference that mypy doesn't do and that pyright differentiates between an `Unknown` type and an `Any` type. `mypy`, for the most part, treats variables as `Any` if it can't figure out what type they are, and as a result lets a lot of behaviors on `Any` variables slide. `pyright` tends to be much stricter (even when not on strict mode!) and while some of the assertions that I had to make here to appease `pyright` are a bit noisy, it's also pointed out a lot of areas where we can be stricter with our typing and then clean up code because some assertions are caught instead by the type system. It also did a good job informing of bad import practices like how `hl.utils` was not actually exported by the `hail` module but incidentally imported by some file in the repo.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13437:537,variab,variables,537,https://hail.is,https://github.com/hail-is/hail/pull/13437,2,['variab'],['variables']
Modifiability,"`rich` stacktraces will by default show the local variables at each level of scope, which is nice for debugging but can leak things like tokens. Best not to have that show up in logs or accidentally pasted into zulip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13212:50,variab,variables,50,https://hail.is,https://github.com/hail-is/hail/pull/13212,1,['variab'],['variables']
Modifiability,`server.py` contains global variables that we really ought not to evaluate when running tests. This moves the minimal set of things out of `server.py` so that the tests do not evaluate `server.py`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5795:28,variab,variables,28,https://hail.is,https://github.com/hail-is/hail/pull/5795,1,['variab'],['variables']
Modifiability,"`tls.py` has many different functions with long names. This change reduces it to three functions:. - internal_server_ssl_context; - internal_client_ssl_context; - external_client_ssl_context. I also added `httpx.py` which contains the HTTPS-related functions that `tls.py` previously; contained. I also simplified the HTTPS-related functions to just:. - client_session; - blocking_client_session. I determine internal vs. external using the deploy config. ---. An [`ssl.SSLContext`](https://docs.python.org/3/library/ssl.html#ssl.SSLContext) defines how a; network library (such as `aiohttp`) should perform SSL/TLS. Let's look at an example:. ```python3; server_ssl_context = ssl.create_default_context(; purpose=Purpose.CLIENT_AUTH,; cafile='/incoming.cacerts'); server_ssl_context.load_cert_chain(ssl_config['cert'],; keyfile=ssl_config['key'],; password=None); server_ssl_context.verify_mode = ssl.CERT_OPTIONAL; server_ssl_context.check_hostname = False; ```. The first function call states that we are a *server* performing *client; authentication* (`Purpose.CLIENT_AUTH`). We also state that anyone who sends requests to us will be; identified by a certificate that is trusted by our certificate database: `/incoming.cacerts` (which; is a file). `load_cert_chain` states where to find the certificates and secret key that prove who we are. The; certificate and secret key together are like a property title that proves someone owns a house. The; `password=None` means that our secret key has no password. Some keys are themselves locked by a; password. `verify_mode` means what do we expect our clients to have. `CERT_OPTIONAL` means anonymous clients; are OK. This is how servers normally operate (https://google.com does not care who you are). `check_hostname` means should we verify that the client certificate matches the client's; hostname. Since we allow anonymous clients, this must be `False`. ---. `test-address.py` is a gross hack. It will disappear in subsequent PRs. For now, I push",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9862:448,config,config,448,https://hail.is,https://github.com/hail-is/hail/pull/9862,1,['config'],['config']
Modifiability,"a few things happening here, most of which was me trying to not have to explicitly list dependencies in the shadowJar/shadowTestJar tasks:; - upgraded gradle to 5.0 and some plugins to be compatible; - split compile dependencies into ""bundled"" and ""unbundled"" to more explicitly separate the things we want in the jars and dependencies that we don't want bundled/are currently depending on the spark installation for. I did it this way because the shadowJar `exclude` filter does not let you exclude transitive dependencies, and I just wanted to exclude the entire spark/scala dependency tree.; - there was a problem where trying to run the tests kept giving me the ""Could not find or load main class org.testng.TestNG"" error, despite the class clearly being findable from the classpath I was providing. I added some excludes per this:; https://stackoverflow.com/questions/51455197/gradle-fatjar-could-not-find-or-load-main-class; (although I believe this is no longer strictly necessary after excluding all the transitive spark dependencies)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248:174,plugin,plugins,174,https://hail.is,https://github.com/hail-is/hail/pull/6248,1,['plugin'],['plugins']
Modifiability,"a host name when no Subject Alternative Names are present is now disabled by default. It can be temporarily re-enabled by adding the value x509ignoreCN=0 to the GODEBUG environment variable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/93264"">kubernetes/kubernetes#93264</a>, <a href=""https://github.com/justaugustus""><code>@​justaugustus</code></a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Release, Scalability, Storage and Testing]</li>; </ul>; </li>; <li>Promote Immutable Secrets/ConfigMaps feature to Beta and enable the feature by default.; This allows to set <code>Immutable</code> field in Secrets or ConfigMap object to mark their contents as immutable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89594"">kubernetes/kubernetes#89594</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Apps and Testing]</li>; <li>Remove <code>BindTimeoutSeconds</code> from schedule configuration <code>KubeSchedulerConfiguration</code> (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91580"">kubernetes/kubernetes#91580</a>, <a href=""https://github.com/cofyc""><code>@​cofyc</code></a>) [SIG Scheduling and Testing]</li>; <li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavinfish""><code>@​gavinfish</code></a>) [SIG Scheduling]</li>; <li>Reserve plugins that fail to reserve will trigger the unreserve extension point (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92391"">kubernetes/kubernetes#92391</a>, <a href=""https://github.com/adtac""><code>@​adtac</code></a>) [SIG Scheduling and Testing]</li>; <li>Resolve regression in <code>metadata.managedFields</code> handling in update/patch requests submitted by older API clients (<a href=""https://github-redirect.d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:11606,config,configuration,11606,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['config'],['configuration']
Modifiability,"a substep of a pod/job, and as docker container. My last renaming proposal got shot down, but we clearly need to improve this in a later PR.; - Heavily reworked worker.py. I believe this fixes https://github.com/hail-is/hail/issues/7350. The main design idea is to having all state creation and cleanup in Pod.run and Container.run.; - worker: Just support pods/status and pods/log, not container level status or logs.; - Pod now writes final status, not containers. Individual containers write their logs.; - I time all the steps of the Pod container (creating, starting, running, uploading log, etc.) with a timing called ""runtime"" which is how long the docker container itself took to start/run. That's usually 4-6 seconds. However, if you log into a machine and run `docker run --rm ubuntu:18.04 echo hi` it takes 1-2 seconds. It would be good to find out where the extra 3-4 seconds are coming from (I feel like @jigold might have some insight into this. Comparing our container config to the docker command line's might be useful here.); - Stop using (value, err) style exception handling. I think we should be able to design this with very little explicit exception handling, mainly in critical blocks to maintain the program invariants.; - Pods can have error status in 1 of 3 ways: the pod itself failed (e.g. couldn't read k8s secrets), one of the pod containers error out (e.g. pull failed due to invalid image), and the docker container finished but the final container status had an ""Error"" field. Next step is to remove pods and merge the pod and job tables. ```; {; ""name"": ""batch-2-job-1"",; ""batch_id"": 2,; ""job_id"": 1,; ""user"": ""test"",; ""state"": ""succeeded"",; ""container_statuses"": {; ""setup"": {; ""name"": ""setup"",; ""state"": ""succeeded"",; ""timing"": {; ""pulling"": 0.038861751556396484,; ""creating"": 0.7245609760284424,; ""starting"": 4.770207166671753,; ""running"": 1.1384251117706299,; ""runtime"": 5.909235715866089,; ""uploading_log"": 0.3659687042236328,; ""deleting"": 0.013197660446166992",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7354:1360,config,config,1360,https://hail.is,https://github.com/hail-is/hail/pull/7354,1,['config'],['config']
Modifiability,"a/pull/983"">python-jsonschema/jsonschema#983</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.3...v4.11.0"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.3...v4.11.0</a></p>; <h2>v4.10.3</h2>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3</a></p>; <h2>v4.10.2</h2>; <ul>; <li>Fix a second place where subclasses may have added attrs attributes (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2</a></p>; <h2>v4.10.1</h2>; <ul>; <li>Fix Validator.evolve (and APIs like <code>iter_errors</code> which call it) for cases; where the validator class has been subclassed. Doing so wasn't intended to be; public API, but given it didn't warn or raise an error it's of course; understandable. The next release however will make it warn (and a future one; will make it error). If you need help migrating usage of inheriting from a; validator class feel free to open a discussion and I'll try to give some; guidance (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-jsonschema/jsonschema/blob/main/CHANGELOG.rst"">jsonschema's changelog</a>.</em></p>; <blockquote>; <h1>v4.15.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12163:3098,evolve,evolve,3098,https://hail.is,https://github.com/hail-is/hail/pull/12163,1,['evolve'],['evolve']
Modifiability,"a2 (see below).; From the error it seems like this is due to Hail's dependency of bokeh using the latest version of jinja2. Downgrading jinja2 to 3.0.0 solves the problem, and it seems like other people have seen this too with the latest release of jinja2:. https://github.com/holoviz/panel/issues/3260. This may be transient and may be solved by bokeh / jinja2 folks but thought I'd let you know in case you hit this issue. ```; ../conda/envs/glow/lib/python3.7/site-packages/bokeh/core/templates.py:43: in <module>; from jinja2 import Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:114",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1105,adapt,adapted,1105,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['adapt'],['adapted']
Modifiability,"a8966fa96859c6672aba986b""><code>25ded57</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ipython/ipython/issues/13804"">#13804</a> from Carreau/sd-x</li>; <li><a href=""https://github.com/ipython/ipython/commit/98e3599e130e253f292679f74982a3a2cd3a7a7a""><code>98e3599</code></a> exclude 3.8</li>; <li><a href=""https://github.com/ipython/ipython/commit/fcdcddd5e528844672688e07bfa5188e48e37521""><code>fcdcddd</code></a> iterate</li>; <li><a href=""https://github.com/ipython/ipython/commit/8ca7b420a29ad781cc6c701dd4a6af0dd21b35c4""><code>8ca7b42</code></a> fix stack-data 0.6 failing tests</li>; <li><a href=""https://github.com/ipython/ipython/commit/93992a7ecd086bb24840ba03cd69960daf41575d""><code>93992a7</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ipython/ipython/issues/13768"">#13768</a> from osherdp/feature/raise-when-opening-standard-st...</li>; <li><a href=""https://github.com/ipython/ipython/commit/f44e27095fd647cc22bf37874f183ec4db85949f""><code>f44e270</code></a> Refactor a bit of uniformity.</li>; <li><a href=""https://github.com/ipython/ipython/commit/1b5674ca8bbac62daa42eb460848173c0542cf2e""><code>1b5674c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ipython/ipython/issues/13778"">#13778</a> from zhizheng1/fix-mpl-webagg</li>; <li>Additional commits viewable in <a href=""https://github.com/ipython/ipython/compare/7.34.0...8.6.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ipython&package-manager=pip&previous-version=7.34.0&new-version=8.6.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-autome",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12442:1771,Refactor,Refactor,1771,https://hail.is,https://github.com/hail-is/hail/pull/12442,1,['Refactor'],['Refactor']
Modifiability,a:1431); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:641); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:8149,rewrite,rewrite,8149,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['rewrite'],['rewrite']
Modifiability,"a> from fabianegli/main</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/7f924b13a50a05b8dc894418fa7faf779201e129""><code>7f924b1</code></a> Fix typo in deprecation documentation</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/4a8f8ada431974f2837260af3ed36299fd382814""><code>4a8f8ad</code></a> build(deps): Bump django from 4.0.2 to 4.0.3 in /testing/plugins_integration ...</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/c0fd2d883940f1292d5e8234803beaacd08315e6""><code>c0fd2d8</code></a> build(deps): Bump pytest-asyncio from 0.18.1 to 0.18.2 in /testing/plugins_in...</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/843e01824c257c3190792a9df430289c3abe349d""><code>843e018</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9732"">#9732</a> from nicoddemus/9730-toml-failure</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/bc43d66b47b917d43a22e0c703ecfe4eea342263""><code>bc43d66</code></a> [automated] Update plugin list (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9733"">#9733</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/e38d1cac489e42f4bdbecbb50f9f25dc9c36c19f""><code>e38d1ca</code></a> Improve error message for malformed pyproject.toml files</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest/compare/6.2.5...7.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest&package-manager=pip&previous-version=6.2.5&new-version=7.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11571:6301,plugin,plugin,6301,https://hail.is,https://github.com/hail-is/hail/pull/11571,2,['plugin'],['plugin']
Modifiability,"ability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, sldsc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64, intercept: float64, intercept_se: float64, ratio: float64, ratio_se: float64}, rhemc_25bin: struct{h2_liability: float64, h2_liability_se: float64, h2_z: float64, h2_observed: float64, h2_observed_se: float64}, rhemc_8bin: struct{h2_liability: float64, h2_liability_se: float64, h2_observed: float64, h2_observed_se: float64, h2_z: float64}, rhemc_25bin_50rv: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}, final: struct{h2_observed: float64, h2_observed_se: float64, h2_liability: float64, h2_liability_se: float64, h2_z: float64}}, qcflags: struct{GWAS_run: bool, defined_h2: bool, significant_z: bool, in_bounds_h2: bool, normal_lambda: bool, normal_ratio: bool, EUR_plus_1: bool, pass_all: bool}, N_ancestry_QC_pass: int32}, saige_version: str, inv_normalized: bool, pop: str, lambda_gc: float64, n_variants: int64, n_sig_variants: int64, saige_heritability: float64}))}; at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:15); at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:15); at is.hail.utils.package$.fatal(package.scala:78); at is.hail.expr.ir.PruneDeadFields$.isSupertype(PruneDeadFields.scala:75); at is.hail.rvd.RVDCoercer.coerce(RVD.scala:31); at is.hail.rvd.RVD$.coerce(RVD.scala:1262); at is.hail.rvd.RVD.changeKey(RVD.scala:143); at is.hail.rvd.RVD.changeKey(RVD.scala:136); [...]; java.util.NoSuchElementException: key not found: 0; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2(PruneDeadFields.scala:62); at is.hail.expr.ir.PruneDeadFields$.$anonfun$isSupertype$2$adapted(PruneDeadFields.scala:61); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10858:4164,adapt,adapted,4164,https://hail.is,https://github.com/hail-is/hail/issues/10858,1,['adapt'],['adapted']
Modifiability,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2282,Rewrite,RewriteBottomUp,2282,https://hail.is,https://github.com/hail-is/hail/issues/9128,6,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3762,Rewrite,RewriteBottomUp,3762,https://hail.is,https://github.com/hail-is/hail/issues/9128,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5153,Rewrite,RewriteBottomUp,5153,https://hail.is,https://github.com/hail-is/hail/issues/6458,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,able.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:4502,Rewrite,RewriteBottomUp,4502,https://hail.is,https://github.com/hail-is/hail/issues/9128,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,able.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2467,Rewrite,RewriteBottomUp,2467,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,able.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3947,Rewrite,RewriteBottomUp,3947,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"abot.com/psf/black/issues/2879"">#2879</a>).</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/black/blob/main/CHANGES.md"">black's changelog</a>.</em></p>; <blockquote>; <h2>22.3.0</h2>; <h3>Preview style</h3>; <ul>; <li>Code cell separators <code>#%%</code> are now standardised to <code># %%</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2919"">#2919</a>)</li>; <li>Remove unnecessary parentheses from <code>except</code> statements (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2939"">#2939</a>)</li>; <li>Remove unnecessary parentheses from tuple unpacking in <code>for</code> loops (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2945"">#2945</a>)</li>; <li>Avoid magic-trailing-comma in single-element subscripts (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2942"">#2942</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not format <code>__pypackages__</code> directories by default (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2836"">#2836</a>)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loadi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:4087,Config,Configuration,4087,https://hail.is,https://github.com/hail-is/hail/pull/11696,1,['Config'],['Configuration']
Modifiability,"abot.com/sphinx-doc/sphinx/issues/10648"">#10648</a>: LaTeX: CSS-named-alike additional :ref:<code>'sphinxsetup' &lt;latexsphinxsetup&gt;</code>; keys allow to configure four separate border-widths, four paddings, four; corner radii, a shadow (possibly inset), colours for border, background, shadow; for each of the code-block, topic, attention, caution, danger, error and warning; directives.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10655"">#10655</a>: LaTeX: Explain non-standard encoding in LatinRules.xdy</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10599"">#10599</a>: HTML Theme: Wrap consecutive footnotes in an <code>&lt;aside&gt;</code> element when; using Docutils 0.18 or later, to allow for easier styling. This matches the; behaviour introduced in Docutils 0.19. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10518"">#10518</a>: config: Add <code>include_patterns</code> as the opposite of <code>exclude_patterns</code>.; Patch by Adam Turner.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/e712eae382d213ce3f4866ad6f5b3c84ce4f4409""><code>e712eae</code></a> Bump to 5.1.1 final</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/0555345ad715b1e5ec83bce2e4a993441ffb8f29""><code>0555345</code></a> Fix ValueError popping out in <code>sphinx.ext.napoleon</code> (<a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10709"">#10709</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/9845500ffa6b75266b1e34701c15eb8e586aa17e""><code>9845500</code></a> Improve support for deprecated builders without env arg (<a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10702"">#10702</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12165:4924,config,config,4924,https://hail.is,https://github.com/hail-is/hail/pull/12165,1,['config'],['config']
Modifiability,"abot.com/tqdm/tqdm/issues/1218"">#1218</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1082"">#1082</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1217"">#1217</a>)</li>; <li>warn on positional CLI arguments</li>; <li>misc build/test framework updates; <ul>; <li>enable <code>py3.10</code> tests</li>; <li>add <code>conda</code> dependencies</li>; <li>update pre-commit hooks</li>; <li>fix <code>pytest</code> config (<code>nbval</code>, <code>asyncio</code>)</li>; <li>fix dependencies &amp; tests</li>; <li>fix site deployment</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.3 stable</h2>; <ul>; <li>fix minor typo (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>minor example fix (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tqdm/tqdm/commit/6791e8c5b3d6c30bdd2060c346996bfb5a6f10d1""><code>6791e8c</code></a> bump version, merge pull request <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1366"">#1366</a> from tqdm/devel</li>; <li><a href=""https://github.com/tqdm/tqdm/commit/754186291e6b4e28ea8b56c9493adc03bf14c404""><code>7541862</code></a> tests: hotfix skip windows errors",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12260:3277,refactor,refactoring,3277,https://hail.is,https://github.com/hail-is/hail/pull/12260,1,['refactor'],['refactoring']
Modifiability,"abot.com/tqdm/tqdm/issues/1218"">#1218</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1082"">#1082</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1217"">#1217</a>)</li>; <li>warn on positional CLI arguments</li>; <li>misc build/test framework updates; <ul>; <li>enable <code>py3.10</code> tests</li>; <li>add <code>conda</code> dependencies</li>; <li>update pre-commit hooks</li>; <li>fix <code>pytest</code> config (<code>nbval</code>, <code>asyncio</code>)</li>; <li>fix dependencies &amp; tests</li>; <li>fix site deployment</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.3 stable</h2>; <ul>; <li>fix minor typo (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>minor example fix (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; <li>fix <code>contrib.concurrent</code> with generators (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1233"">#1233</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1231"">#1231</a>)</li>; </ul>; <h2>tqdm v4.62.1 stable</h2>; <ul>; <li><code>contrib.logging</code>: inherit existing handler output stream (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1191"">#1191</a>)</li>; <li>fix <code>PermissionError</code> by using <code>weakref</code> in <code>DisableOnWriteErro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11587:1487,refactor,refactoring,1487,https://hail.is,https://github.com/hail-is/hail/pull/11587,1,['refactor'],['refactoring']
Modifiability,"ache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:1273,layers,layers,1273,https://hail.is,https://github.com/hail-is/hail/pull/10502,1,['layers'],['layers']
Modifiability,ackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:10); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:37); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:147); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:2806,adapt,adapted,2806,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['adapt'],['adapted']
Modifiability,"acked on #10791~. This PR attempts to allow linear algebra codegen methods, like the LAPACK wrappers and the local whitening methods I'm working on, to defensively assert shape compatibility preconditions, without generating redundant runtime checks. (I always hate when we're pushed to avoid using code generation abstractions (in this case, just factoring code into smaller functions), because they generate worse code.). The method is pretty simple. SNDArray shapes are now arrays of `SizeValue`, which is a sum type with cases `SizeValueDyn(Value[Long])` and `SizeValueStatic(Long)`. I don't think static sizes occur very often, but it was a simple addition. `SizeValue`s can be compared statically with `==`, or at runtime with `ceq`: the former is true only if we can prove statically that the two sizes must be equal, while the latter emits code to check equality at runtime, using static knowledge to elide dynamic checks where possible. The way we encode static knowledge that two sizes are equal is by using the same local variable to store both. The primary interface to introduce that static knowledge (other than using the same set of sizes to construct multiple SNDArrays), is the method `coerceToShape(cb: CodeBuilder, newShape: Seq[SizeValue]): SNDArrayValue`, which emits code to dynamically assert that `this.shape` agrees with `newShape`, then returns `this` with shape replaced by `newShape`. Thus, `a.coerceToShape(cb, newShape).shape == newShape` will always be true, preserving the static knowledge about the shape of `a`. As a simple example, `gemm` verifies its inputs with (simplifying to the case with no transposes); ```; val Seq(m, n) = C.shapes; val k = A.shapes(1); A.assertHasShape(cb, FastIndexedSeq(m, k), errMsg); B.assertHasShape(cb, FastIndexedSeq(k, n), errMsg); ```; If we call this with; ```; val m, n, k = \\ compute expected dim sizes. \\ emit dynamic size checks once; val A_ = A.coerceToShape(cb, IndexedSeq(m, k)); val B_ = B.coerceToShape(cb, IndexedSeq(k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10783:1057,variab,variable,1057,https://hail.is,https://github.com/hail-is/hail/pull/10783,1,['variab'],['variable']
Modifiability,"add array_windows / locus_windows, refactor ld_prune, remove filteredEntriesTable and UpperIndexBounds",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3715:35,refactor,refactor,35,https://hail.is,https://github.com/hail-is/hail/pull/3715,1,['refactor'],['refactor']
Modifiability,"add array_windows / locus_windows, refactor ld_prune, remove filteredEntriesTable and UpperIndexBounds (v2)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873:35,refactor,refactor,35,https://hail.is,https://github.com/hail-is/hail/pull/3873,1,['refactor'],['refactor']
Modifiability,add ci2 build configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5842:14,config,configuration,14,https://hail.is,https://github.com/hail-is/hail/pull/5842,1,['config'],['configuration']
Modifiability,add grouped type variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11558:17,variab,variable,17,https://hail.is,https://github.com/hail-is/hail/pull/11558,1,['variab'],['variable']
Modifiability,add hello rule to router; parameterize wait-for by location; use in test-ci,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7495:26,parameteriz,parameterize,26,https://hail.is,https://github.com/hail-is/hail/pull/7495,1,['parameteriz'],['parameterize']
Modifiability,added RowMatrix and adapted ExportableMatrix functionality,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2661:20,adapt,adapted,20,https://hail.is,https://github.com/hail-is/hail/pull/2661,1,['adapt'],['adapted']
Modifiability,age 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:2893,adapt,adapted,2893,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['adapt'],['adapted']
Modifiability,"age is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port fort HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The firs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:2516,enhance,enhanced,2516,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['enhance'],['enhanced']
Modifiability,"agma: no cover&quot; <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7668"">#7668</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Trim glyph size in ImageFont.getmask() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7669"">#7669</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix loading IPTC images and update test <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7667"">#7667</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Allow uncompressed TIFF images to be saved in chunks <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7650"">#7650</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Concatenate multiple JPEG EXIF markers <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7496"">#7496</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Changed IPTC tile tuple to match other plugins <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7661"">#7661</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Do not assign new fp attribute when exiting context manager <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7566"">#7566</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support arbitrary masks for uncompressed RGB DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7589"">#7589</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support setting ROWSPERSTRIP tag <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7654"">#7654</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Apply ImageFont.MAX_STRING_LENGTH to ImageFont.getmask() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7662"">#7662</a> [<a href=""https://github.com/ra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:1961,plugin,plugins,1961,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['plugin'],['plugins']
Modifiability,"ail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in <module>; 143 ; 144 ; --> 145 _cell_set_template_code = _make_cell_set_template_code(); 146 ; 147 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py in _make_cell_set_template_code(); 124 ); 125 else:; --> 126 return types.CodeType(; 127 co.co_argcount,; 128 co.co_kwonlyargcount,. TypeError: an integer is required (got type bytes). In [2]: . ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:10990,sandbox,sandbox,10990,https://hail.is,https://github.com/hail-is/hail/issues/10197,4,['sandbox'],['sandbox']
Modifiability,"ail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.Asserti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3728:3810,extend,extend,3810,https://hail.is,https://github.com/hail-is/hail/issues/3728,1,['extend'],['extend']
Modifiability,al(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6155,adapt,adapted,6155,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['adapt'],['adapted']
Modifiability,ala:341); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:715); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:985); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:21334,rewrite,rewrite,21334,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['rewrite'],['rewrite']
Modifiability,"all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be executed. If the variable's current value and it's previous value do not differ, no target is generated and thus nothing is executed. #### MAKEFLAGS += --no-builtin-rules. This disables all the automatic rules, making Makefile parsing/interpretation faster. #### .SUFFIXES:. This sets the suffixes to nothing, disabling a bunch of implicit rules. For example, the rule for compiling C files. #### Breeze versions. I found the breeze versions by manually looking at the Spark pom file at the tags for each release, starting with [2.2.0](https://github.com/apache/spark/blob/v2.2.0/pom.xml#L654). ---. [1] A python module is any folder containing a file called `__init__.py`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:3083,variab,variable,3083,https://hail.is,https://github.com/hail-is/hail/pull/5130,2,['variab'],['variable']
Modifiability,"an</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2816"">cbeust/testng#2816</a></li>; <li>Make PackageUtils compliant with JPMS by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2817"">cbeust/testng#2817</a></li>; <li>Ability to retry a data provider during failures by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2820"">cbeust/testng#2820</a></li>; <li>Refactoring by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2821"">cbeust/testng#2821</a></li>; <li>Fixing bug with DataProvider retry by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2822"">cbeust/testng#2822</a></li>; <li>Add config key for callback discrepancy behavior by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2823"">cbeust/testng#2823</a></li>; <li>Upgrading versions by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2824"">cbeust/testng#2824</a></li>; <li>Fix <a href=""https://github-redirect.dependabot.com/cbeust/testng/issues/2770"">#2770</a>: FileAlreadyExistsException on copy by <a href=""https://github.com/melloware""><code>@​melloware</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2827"">cbeust/testng#2827</a></li>; <li>JarFileUtils.delete(File f) throw actual exception (instead of FileNotFound) when file cannot be deleted <a href=""https://github-redirect.dependabot.com/cbeust/testng/issues/2825"">#2825</a> by <a href=""https://github.com/speedythesnail""><code>@​speedythesnail</code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:3942,config,config,3942,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['config'],['config']
Modifiability,"anges actually improves everyone's understanding of this. ---. The goal of this PR is to make this work:. ```; HAIL_QUERY_BACKEND=service \; python3 -c 'import hail as hl; hl.utils.range_table(10).write(""gs://foo/bar.t"")`; ```. In particular, a normal user should not need to know the location of a Hail Query JAR. Currently, you must specify two environment variables: `HAIL_SHA` and `HAIL_JAR_URL`. This PR takes advantage of the well known location of a Hail Query JAR [1]. We use the newly introduced `hl.revision()` to determine the SHA-1 of the currently installed Hail. This PR includes the revision in the driver job spec. The front end has been modified to convert the revision into a cloud storage URL. This PR also provides three escape hatches to the aforementioned default behavior. These escape hatches should more or less only be used by developers. They're specified from highest priority to lowest.; 1. Specify the `jar_url` parameter to `ServiceBackend`.; 2. Specify the `HAIL_JAR_URL` environment variable.; 3. Specify a JAR url in the user config: `hailctl config set query/jar_url gs://...`. While writing this PR, I decided to clean up five bits of cruft I left when I first built the service backend. First, I took the JAR URL out of the ""command"" of the job spec. This ""command"" is just an array of strings. The fact that certain parts of that array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:1085,variab,variable,1085,https://hail.is,https://github.com/hail-is/hail/pull/11645,1,['variab'],['variable']
Modifiability,ants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1727,Rewrite,RewriteBottomUp,1727,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"api server bits into CI (maybe should be prioritized earlier...I prefer to get draft of major functions done first; am new to writing tests for React/Node). ## Near-term goals (<= 6 mo); 1. Upload, download; 2. Launch clusters, pay for them; 3. ?. ## Longer-term goals; 1. Much simpler interface to Hail. I would like steps that can be performed without programming to be done so. I would prefer fasta->variant filtering to be done as in Bystro (at least from the interface standpoint), i.e without opening up a notebook. Common analyses pipelines should also be possible without any interaction with a python notebook: GWAS, rare-variant (SKAT) analyses have, it seems, relatively few permutations. Those should be behind UI primitives. At each stage of a ; 2. Social network bits: users should be able to share job state with other users (requested by Bystro users on 22q consortium project) at the least.; 3. Record job state using something like Merkle tree. Checkout state. Aka ""blockchain""; 4. Cooperative analysis: provide system for people to validate analyses; ; Basic idea: . 1) People donate computational resources for ad-hoc heterogenous clusters. ; 2) People donate intellectual capital. Re-run analyses without the full available code. See if they can replicate (not p-values, but order). Could generate multiple-hypothesis-test corrected aggregate. These users get publication credit as consortia; 3) People donate minor intellectual capital: Re-run analysis with full available code. Report on success. This will catch bugs, and non-deterministic results (for instance, if reported accuracy depends on local minima..similar or better minima may only occur once in a great while). Similar to 2. ## Timetables; 1-3a: 12/10/18; 3b: by 12/15/18; 4a-4b: by 12/12/18; 4c-d: by 12/15/18. This probably shouldn't be merged for a while. Still working on authentication handling for third party APIs. All first party APIs (our stuff) is well controlled, can be extended from existing codebase.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:9778,extend,extended,9778,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['extend'],['extended']
Modifiability,"apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=rolebindings"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=RoleBinding""; Name: ""batch-pods-admin-binding"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""RoleBinding"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods-admin-binding"" ""namespace"":""batch-pods""] ""roleRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""batch-svc"" ""namespace"":""default""]]]}; from server for: ""deployment.yaml"": rolebindings.rbac.authorization.k8s.io ""batch-pods-admin-binding"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get rolebindings.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""hail.is/sha"":""1c6dbf20333a"" ""app"":""batch""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:2654,config,configuration,2654,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4565,Rewrite,RewriteBottomUp,4565,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,ariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11600,Config,Configuration,11600,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,at __C8160Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4(CompileAndEvaluate.scala:61); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2(CompileAndEvaluate.scala:61); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2$adapted(CompileAndEvaluate.scala:59); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:140); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:140); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:59); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:33); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486:3672,rewrite,rewrite,3672,https://hail.is,https://github.com/hail-is/hail/issues/13486,1,['rewrite'],['rewrite']
Modifiability,at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1615,Rewrite,RewriteBottomUp,1615,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:633); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:695); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:461); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:460); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:460); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:12919,adapt,adapted,12919,https://hail.is,https://github.com/hail-is/hail/issues/12982,6,['adapt'],['adapted']
Modifiability,"at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:462); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:461); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:461); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:141); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:141); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:15); 	at is.hail.backend.service.Main.main(Main.scala); 	... 11 more; ```. Which suggests that the service backend experienced an EOF somewhere in the first four bytes of the input file. Unfortunately, we automatically cleanup the input and output files, so I can't investigate further. This PR reads the input and output files and stores them in the error message so that next time this happens we get more information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:6094,adapt,adapted,6094,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['adapt'],['adapted']
Modifiability,at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.Mat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5603,adapt,adapted,5603,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['adapt'],['adapted']
Modifiability,"at(it: Iterable[Doc])`; * `Group(body: Doc)`. Ignoring `Group` for the moment, the first four constructors define formatted documents with only one possible layout, regardless of `width` or `ribbonLength`. `Text` simply prints the string `t`; `Line` prints a newline, followed by the current level of indentation (`ifFlat` is explained when we discuss `Group`); `Indent` increases the indentation of all `Line`s contained in `body` by `i`; and `Concat` simply prints all documents in `it` sequentially. `Group` is the sole source of alternatives which `render` must choose between. `Group(body)` can be rendered in one of two ways:; * replace all `Line`s contained in `body` (including in nested `Group`s) by their `ifFlat` alternative (almost always either "" "" or """"), or, if that would cause the line to exceed `width`; * print `body` normally, as described in the previous paragraph, allowing nested `Group`s to print either flat or normally. This pretty-printer DSL has become fairly standard, with some common enhancements that I don't think we need. It was first described in [A prettier printer](https://homepages.inf.ed.ac.uk/wadler/papers/prettier/prettier.pdf) by Wadler (though my implementation is completely different). This achieves stack safety by `Concat` taking an `Iterable`, so each contained `Doc` can be produced on demand. `render` pulls from these iterators, keeping in memory only things that might print to the current line, but where the format hasn't been decided yet. As soon as the formatting of a group is decided, as much of its body as possible is written to the `java.io.Writer`. A very quick and dirty performance comparison had the new pretty printer about 20% slower. That's paying for both the stack safety and the added smarts. And I think there's still room for optimization if it becomes necessary. Here is a snippet of the IR generated by `test_ld_score_regression`, first on master, then this PR:; ```; (InsertFields; (SelectFields (SNP A1 A2 N Z); (Ref row)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9652:2935,enhance,enhancements,2935,https://hail.is,https://github.com/hail-is/hail/pull/9652,1,['enhance'],['enhancements']
Modifiability,"atch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvlo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2415,config,config,2415,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"atch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2578,config,config,2578,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"ates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_client_ssl_context` which should only be used in; publicly consumable tools (*never* in a service). This function allows the same; tool to be used inside and outside the cluster. It will load the correct certs; for your environment (it will load public certs if you're outside the cluster,; it will load in-cluster-only certs if you're in the cluster). I also added types to `tls.py` and fixed some type errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9120:1817,config,configuration,1817,https://hail.is,https://github.com/hail-is/hail/pull/9120,1,['config'],['configuration']
Modifiability,"ation.k8s.io/v1alpha1</code> API version is removed; use the <code>rbac.authorization.k8s.io/v1</code> API, available since v1.8. The <code>scheduling.k8s.io/v1alpha1</code> API version is removed; use the <code>scheduling.k8s.io/v1</code> API, available since v1.14. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104248"">kubernetes/kubernetes#104248</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-scheduler: support for configuration file version <code>v1beta1</code> is removed. Update configuration files to v1beta2(xref: <a href=""https://github-redirect.dependabot.com/kubernetes/enhancements/issues/2901"">kubernetes/enhancements#2901</a>) or v1beta3 before upgrading to 1.23. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104782"">kubernetes/kubernetes#104782</a>, <a href=""https://github.com/kerthcet""><code>@​kerthcet</code></a>)</li>; <li>KubeSchedulerConfiguration provides a new field <code>MultiPoint</code> which will register a plugin for all valid extension points (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105611"">kubernetes/kubernetes#105611</a>, <a href=""https://github.com/damemi""><code>@​damemi</code></a>) [SIG Scheduling and Testing]</li>; <li>Kubelet should reject pods whose OS doesn't match the node's OS label. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105292"">kubernetes/kubernetes#105292</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>) [SIG Apps and Node]</li>; <li>Kubelet: turn the KubeletConfiguration v1beta1 <code>ResolverConfig</code> field from a <code>string</code> to <code>*string</code>. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104624"">kubernetes/kubernetes#104624</a>, <a href=""https://github.com/Haleygo""><code>@​Haleygo</code></a>)</li>; <li>Kubernetes is now built using go 1.17. (<a href=""https://github-redir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:8932,plugin,plugin,8932,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['plugin'],['plugin']
Modifiability,"aultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15939,Plugin,Plugins,15939,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,aversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2355,Rewrite,RewriteBottomUp,2355,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,aversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3835,Rewrite,RewriteBottomUp,3835,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"aximum to begin with as the attempt start time was always the same. This is probably a place to double check before merging. **Job State Changes:**; - We now support a new state ""Creating"" which represents an instance has been spun up for a job, but it has not been activated yet; - The SQL user resources table and cancellable resources table needed to be changed to add the n_creating_jobs, n_cancellable_creating_jobs, and the n_cancelled_creating_jobs; - Added a new SQL function `mark_job_creating` that is only called by the job private instance creator.; - A lot of the existing SQL functions had to change to account for the fact that an instance in the pending state can have job-specific operations done such as mark_job_complete if the job is cancelled. In addition, the ""Creating"" state is like ""Running"" for some operations in that an attempt has been created and actions are happening on behalf of the user. **Driver Changes:**; - New cancel_creating_jobs event; - Two separate methods to get the pools or job private UI pages and two separate configuration methods. One each for pool and job-private. **JobPrivateInstanceCollection:**; - Has two new loops: an instance creation loop and a scheduling loop; - The instance creation loop does a fair share calculation that is almost identical to the pool one except the resource being allocated is n_ready_jobs compared to total_jobs rather than ready_cores_mcpu. ; - The instance creation loop needs to extract the machine_type, storage_gib, and preemptible from the spec without hitting GCS. Therefore, it is stored in the ""spec"" field in the database which required changing the batch format version a bit.; - We avoid double scheduling by requiring that there are no live instances assigned to attempts for that job before creating an instance.; - We mark a job as creating after creating the instance for the new attempt; - The number of instances that can be created is similar to the pool control loop. The total number of instances",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9972:2180,config,configuration,2180,https://hail.is,https://github.com/hail-is/hail/pull/9972,1,['config'],['configuration']
Modifiability,"b7dc1c</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8481"">#8481</a></li>; <li>fix(dev): avoid FOUC when swapping out link tag (fix <a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7973"">#7973</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8495"">#8495</a>) (<a href=""https://github.com/vitejs/vite/commit/01fa807"">01fa807</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7973"">#7973</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8495"">#8495</a></li>; </ul>; <h2><!-- raw HTML omitted -->2.9.10 (2022-06-06)<!-- raw HTML omitted --></h2>; <ul>; <li>feat: treat Astro file scripts as TS (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8151"">#8151</a>) (<a href=""https://github.com/vitejs/vite/commit/9fdd0a3"">9fdd0a3</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8151"">#8151</a></li>; <li>feat: new hook <code>configurePreviewServer</code> (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/7658"">#7658</a>) (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8437"">#8437</a>) (<a href=""https://github.com/vitejs/vite/commit/7b972bc"">7b972bc</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/7658"">#7658</a> <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8437"">#8437</a></li>; <li>fix: remove empty chunk css imports when using esnext (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8345"">#8345</a>) (<a href=""https://github.com/vitejs/vite/commit/9fbc1a9"">9fbc1a9</a>), closes <a href=""https://github-redirect.dependabot.com/vitejs/vite/issues/8345"">#8345</a></li>; <li>fix: EPERM error on Windows when processing dependencies (<a href=""https://github.com/vitejs/vite/tree/HEAD/packages/vite/issues/8235"">#8235</a>) (<a href=""https://github.com/vitejs/vite/commit/dfe4307"">dfe4307</a>), ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12142:3955,config,configurePreviewServer,3955,https://hail.is,https://github.com/hail-is/hail/pull/12142,2,['config'],['configurePreviewServer']
Modifiability,"base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:20530,Plugin,Plugins,20530,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,584	job.py	schedule_job:443	schedule job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,585	job.py	schedule_job:443	schedule job (93, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-defaul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2090,config,config,2090,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,585	job.py	schedule_job:443	schedule job (93, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2253,config,config,2253,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"benchmarks were kind of their own thing and a little neglected.; This change moves the benchmarks into the `hail/python` folder and updates them to use pytest with a custom plugin/set of pytest hooks.; Now, benchmarks can be run from the command line like any pytest.; This change removes the `benchmark-hail` (or `hailbench`) utility. Benchmarks are marked by `pytest.mark.benchmark` (via the `@benchmark` decorator).; By convention, benchmarks are python tests whose names are prefixed by `benchmark_` and are located in files with the same prefix.; Nothing enforces this, however, so you could name your benchmarks `test_*` and put them in files named `test_*.py`.; Benchmarks may import and use any test code or utilities defined in `test/`.; The results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this so supp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:979,flexible,flexible,979,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['flexible'],['flexible']
Modifiability,binomTest refactored to use integer enum instead of string.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3439:10,refactor,refactored,10,https://hail.is,https://github.com/hail-is/hail/pull/3439,1,['refactor'],['refactored']
Modifiability,"ble. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_cer",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:7040,config,config-site,7040,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config-site']
Modifiability,"bot.com/Azure/azure-sdk-for-java/issues/31389"">#31389</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/a651fcc7c0026641717465f8bbae64de808df187""><code>a651fcc</code></a> Doc consistenty review updates (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31332"">#31332</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/6b1aef8c5c09239d6e4534a116fba195891e3d57""><code>6b1aef8</code></a> target version with fixes (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31310"">#31310</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/b17fed8df6b512691656673a6c5b7e8033ff31c2""><code>b17fed8</code></a> Prepare release for Schema Registry (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31375"">#31375</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/1c2a0d90d42432387794daa3c03469fc0fcd1060""><code>1c2a0d9</code></a> [Perf] Call configureClientBuilder() in DataLake and FileShare tests (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31324"">#31324</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/c85090e334e4ff77a3b3178a57b8b6d24518859d""><code>c85090e</code></a> Create Media Streaming package parser with updated contracts (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31309"">#31309</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/6f7b2986b03ae7c4ab67f52452c5af7f260a3880""><code>6f7b298</code></a> Add Merge-Branch script eng/scripts (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/31222"">#31222</a>)</li>; <li><a href=""https://github.com/Azure/azure-sdk-for-java/commit/02fdc27bc10fa3e1bda08f3125059e95be8a4bb0""><code>02fdc27</code></a> Release/azure communication common/1.2.2 (<a href=""https://github-redirect.dependabot.com/Azure/azure-sdk-for-java/issues/3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12450:1953,config,configureClientBuilder,1953,https://hail.is,https://github.com/hail-is/hail/pull/12450,1,['config'],['configureClientBuilder']
Modifiability,"bot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivial/Internal Changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9521"">#9521</a>: Add test coverage to assertion rewrite path.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest/commit/3f12087fe0d86a319216653b08b66a96d400bee2""><code>3f12087</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/bc3021cdfd76507aa3d9e278bd885da9bc1907b2""><code>bc3021c</code></a> Prepare release version 7.0.1</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/591d476f14e3e83d90fbea75d326a93c5e368708""><code>591d476</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9673"">#9673</a> from nicoddemus/backport-9511</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/6ca733e8f19fa5c4271bf3e5bb295c8b62757e4a""><code>6ca733e</code></a> Enable testing with Python 3.11 (<a href=""https://github-redirect.dependabot.com/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:3834,rewrite,rewrite,3834,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['rewrite'],['rewrite']
Modifiability,"by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does it remove an `order_by(x, ...)` that precedes an `order_by()`. It does, however, remove a `key_by(x, ...)` that precedes an `order_by()` or a `key_by()`. ```; In [47]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().order_by().show() ; ...: ; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 1 |; | 2 |; | 0 |; +---------+. In [48]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx) ; ...: t = t.order_by().show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+. In [49]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.order_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+. In [50]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.key_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+; ```. # Dealing With It. In practice, this following will remove the key on a table without changing the ordering imposed by previous order-changing operations. (NB: ""latent"" ordering inherited from a file [see aforementioned family id, sample id example] is not guaranteed to be preserved by this though, in practice, it often is). ```python; def unkey(t):; if len(t.key) != 0:; t = t.order_by(t.key); return t; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6929:6325,inherit,inherited,6325,https://hail.is,https://github.com/hail-is/hail/issues/6929,1,['inherit'],['inherited']
Modifiability,cala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); E 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:354); E 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerAndExecuteSh,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:1597,Rewrite,RewriteBottomUp,1597,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['Rewrite'],['RewriteBottomUp']
Modifiability,"cala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1957,adapt,adapted,1957,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['adapt'],['adapted']
Modifiability,cala:41); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.expr.ir.analyses.SemanticHash$.go$1(SemanticHash.scala:41); 	at is.hail.expr.ir.analyses.SemanticHash$.$anonfun$apply$4(SemanticHash.scala:54); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.analyses.SemanticHash$.$anonfun$apply$1(SemanticHash.scala:34); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.analyses.SemanticHash$.apply(SemanticHash.scala:26); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:509); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:546); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:542); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:541); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$3(SparkBackend.scala:368); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:364); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:541); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:51); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.AuthFilter.doFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13915:6204,adapt,adapted,6204,https://hail.is,https://github.com/hail-is/hail/issues/13915,1,['adapt'],['adapted']
Modifiability,"can return in two possible ways:; - `Missing` - the stream is actually entirely missing; this usually happens if one of the parameters to a stream is missing (e.g. `ArrayRange(0, NA, 1) = NA`); - `State(s0)` - the stream has started; its initial state is `s0`. `step` can return in three possible ways as well:; - `EOS` - we've reached the End Of Stream, there are no more elements left.; - `Skip(s1)` - this iteration didn't produce an element, you must try stepping again with state `s1` (see ""design notes"").; - `Yield(elt, s1)` - the stream computed an element `elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7228:2067,parameteriz,parameterized,2067,https://hail.is,https://github.com/hail-is/hail/pull/7228,1,['parameteriz'],['parameterized']
Modifiability,"cc @tpoterba . My apologies. I made several changes to lowered logistic regression as well. All the generalized linear model methods share the same fit result. I abstracted this into one datatype at the top of `statgen.py`: `numerical_regression_fit_dtype`. ---. You'll notice I moved the cases such that we check for convergence *before* checking if we are at the maximum iteration. It seemed to me that:; - `max_iter == 0` means do not even attempt to fit.; - `max_iter == 1` means take one gradient step, if you've converged, then return successfully, otherwise fail.; - etc. The `main` branch currently always fails if you set `max_iter == 1`, even if the first step lands on the true maximum likelihood fit. I substantially refactored logistic regression. There were dead code paths (e.g. the covariates array is known to be non-empty). I also found all the function currying and comingling of fitting and testing really confusing. To be fair, the Scala code does this (and its really confusing). I think the current structure is easier to follow:. 1. Fit the null model.; 2. If wald, assume the beta for the genotypes is zero and use the rest of the parameters from the null model fit to compute the score (i.e. the gradient of the likelihood). Recall calculus: gradient near zero => value near the maximum. Return: this is the test.; 3. Otherwise, fit the full model starting at the null fit parameters.; 4. Test the ""goodness"" of this new & full fit. ---. Poisson regression is similar but with a different likelihood function and gradient thereof. Notice that I `key_cols_by()` to indicate to Hail that the order of the cols is irrelevant (the result is a locus-keyed table after all). This is necessary at least until #12753 merges. I think it's generally a good idea though: it indicates to Hail that the ordering of the columns is irrelevant, which is potentially useful information for the optimizer!. ---. Both logistic and Poisson regression can benefit from BLAS3 by running at least t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12793:729,refactor,refactored,729,https://hail.is,https://github.com/hail-is/hail/pull/12793,1,['refactor'],['refactored']
Modifiability,"cc: @akotlar @cseed . I whipped this up today. It modifies the batch api to accept user-configurable cookies and headers. The `BatchClient` looks for a token in `~/.hail/batch/token` or at a user-provided path. There's a whitelist of authorized users. If you can talk directly to batch, you can spoof the `User` header. This makes testing easy. For this approach to be secure, we need to ensure:; - only `gateway` may send HTTP requests to `batch`; - `gateway` does not forward any ""secure"" headers (currently, just `User`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5605:88,config,configurable,88,https://hail.is,https://github.com/hail-is/hail/pull/5605,1,['config'],['configurable']
Modifiability,"cc: @cseed, @patrick-schultz, @catoverdrive . A `ContextRDD[C, T]` is an `RDD[C => Iterator[T]]` and captures the idea that a computation needs a context. I did not inherit from RDD so that `map` and friends can be implemented as if this was an `RDD[T]`. To access the context, one uses `cmap` and friends. The lifetime of a context corresponds roughly to the dynamic extent of an `RDD` computation happening on a single worker node. In particular, a context always ends before a `shuffle` or other network communication happens. One may explicitly end a `ContextRDD`'s context lifetime by calling `ContextRDD.run` which produces an `RDD[T]`. Generally the partitions of a `ContextRDD[C, T]` need only contain one element, but (I believe) I have written `ContextRDD[C, T]` to also handle many `C => Iterator[T]` functions inside a single partition. Most operations on `RVD` do not use the `crdd` yet. I have a growing local stack of branches in which more of these operations are implemented. I am only holding them back because stacked PRs have proven unwieldy. Finally, I added `RVDContext`, which is not fleshed out. It will need to carry at least a region. We might also want it to carry a random seed. Unless you have severe issues with it, I think it's best to treat it as a placeholder for something that will later adopt its name.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3221:165,inherit,inherit,165,https://hail.is,https://github.com/hail-is/hail/pull/3221,1,['inherit'],['inherit']
Modifiability,"cc: @cseed, this explains some weirdness with docker caches. The Docker docs are [misleading at best wrt `--cache-from`](https://github.com/moby/moby/issues/32612). `--cache-from X` means treat `X`'s layers as a cache source *and do not use the local cache*. This is a crucial misfeature for two reasons. First, you [must explicitly specify every image that may have useful cache layers](https://github.com/moby/moby/issues/33002). Second, you cannot include in the cache untagged local images. The latter is particularly an issue for local developers who might run docker builds that fail half-way through. The first issue is not relevant to us because we don't share many layers between images. The second issue is addressed with Makefile conditions that provide a different experience for local versus CI users. Each CI deploy pushes the `latest` tag so that future builds can use it as a cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5016:200,layers,layers,200,https://hail.is,https://github.com/hail-is/hail/pull/5016,3,['layers'],['layers']
Modifiability,"cc: @daniel-goldstein, this is a tricky asyncio situation which you should also keep in mind. OK, there were two problems:. 1. A timeout of 5s appears to be now too short for Google Cloud Storage. I am not sure why but we; timeout substantially more frequently. I have observed this myself on my laptop. Just this; morning I saw it happen to Daniel. 2. When using an `aiohttp.AsyncIterablePayload`, it is *critical* to always check if the coroutine; which actually writes to GCS (which is stashed in the variable `request_task`) is still; alive. In the current `main`, we do not do this which causes hangs (in particular the timeout; exceptions are never thrown ergo we never retry). To understand the second problem, you must first recall how writing works in aiogoogle. There are; two Tasks and an `asyncio.Queue`. The terms ""writer"" and ""reader"" are somewhat confusing, so let's; use left and right. The left Task has the owning reference to both the source ""file"" and the; destination ""file"". In particular, it is the *left* Task which closes both ""files"". Moreover, the; left Task reads chunks from the source file and places those chunks on the `asyncio.Queue`. The; right Task takes chunks off the queue and writes those chunks to the destination file. This situation can go awry in two ways. First, if the right Task encounters any kind of failure, it will stop taking chunks off of the; queue. When the queue (which has a size limit of one) is full, the left Task will hang. The system; is stuck. The left Task will wait forever for the right Task to empty the queue. The second scenario is exactly the same except that the left Task is trying to add the ""stop""; message to the queue rather than a chunk. In either case, it is critical that the left Task waits simultaneously on the queue operation *and*; on the right Task completing. If the right Task has died, no further writes can occur and the left; Task must raise an exception. In the first scenario, we do not observe the right Task'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830:504,variab,variable,504,https://hail.is,https://github.com/hail-is/hail/pull/11830,1,['variab'],['variable']
Modifiability,"cc: @danking . Here is the first major database migration. The goal is to add all attempt resources into the database for any attempt with batch format version less than 3. The node configuration was the same for all attempts back then with standard instances, 16 cores, and a 100GB boot disk. I figured out what the quantity for each resource should be by looking at how we compute quantities for resources in `batch/batch/resources.py`. I checked the inserted quantities are identical to the attempts that already exist in the database right after we converted the billing over to using resources. Once we have all resources for all attempts, the next step (future PR) is to do a scan and repopulate the new aggregated billing tables **by date**. In this PR, I don't try and add the usage to the existing `aggregated_*_resources` tables. I did this to cut down on time and space since we're eventually going to deprecate those tables anyways. Because I don't touch those tables, we don't need to worry about modifying the client code and how the current billing information is calculated. How this migration works is there are 5 phases:; 1. Compute the expected number of attempts to process for format version < 3. ; 2. Divide the search space into chunks of size 100 attempts (empirically determined this was the best chunk size) and randomize the order of the chunks.; 3. Serially process 5000 chunks with only 10 out of the 100 records as a ""burn in period"" to avoid the birthday problem when trying to insert records in parallel.; 4. In parallel, process all the chunks with 10 way parallelism (empirically determined to max CPU for a 4 core db instance); 5. Do an audit of the results to make sure the attempt resources now has the correct number of rows and the billing is within $0.001 per job with the old way and new way of computing the billing. The tolerance of $0.001 was empirically determined. At a threshold of $0.0001, 33/30,000,000 attempts failed. I think this is good enough as t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11990:182,config,configuration,182,https://hail.is,https://github.com/hail-is/hail/pull/11990,1,['config'],['configuration']
Modifiability,"cc: @tpoterba ; ![Screen Shot 2020-03-30 at 6 53 49 PM](https://user-images.githubusercontent.com/106194/77969485-cd780d00-72b7-11ea-96fc-4f529297c830.png). Since my mind was already thinking about CI to address other issues, I thought it prudent to implement this longstanding request. The key idea is that each batch now has a target sha, source sha, and an attempt id. When we refresh state from batch, we look for the oldest batch with the newest attempt_id. We increment attempt_id whenever we `_start_build` on a batch. If the source sha changes, we set the attempt id to zero. It will be incremented to one the next time we build. In this manner, we prevent CI from refreshing from batch and blowing away a retried batch even though it has a higher batch id (and we always prefer lower ids). Retry is implemented in the standard a PR is noted as needing a retry and the watched branch is informed that its state has changed. The heal method of a PR checks for the retry flag, cancels any existing batch, and triggers a new build. Miscellaneous changes/fixes:; - a developers only endpoint to force CI to update right now (convenient for dev deploy where the GitHub triggers are not configured); - don't try to clean up a database that wasn't created; - correct URL for CI in a couple places",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8398:1189,config,configured,1189,https://hail.is,https://github.com/hail-is/hail/pull/8398,1,['config'],['configured']
Modifiability,centralize serializable and broadcasted Hadoop configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5422:47,config,configuration,47,https://hail.is,https://github.com/hail-is/hail/pull/5422,1,['config'],['configuration']
Modifiability,"ch bucket to pull the VEP cache data from. In more recent versions (tested with 0.2.130), this `VEP_REPLICATE` variable has been changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1467,Plugin,Plugins,1467,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Plugin'],['Plugins']
Modifiability,"ch-pods""; Object: &{map[""kind"":""ServiceAccount"" ""metadata"":map[""name"":""batch-svc"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""apiVersion"":""v1""]}; from server for: ""deployment.yaml"": serviceaccounts ""batch-svc"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get serviceaccounts in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=rolebindings"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=RoleBinding""; Name: ""batch-pods-admin-binding"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""RoleBinding"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods-admin-binding"" ""namespace"":""batch-pods""] ""roleRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:1743,config,configuration,1743,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,check that nginx config is valid,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4429:17,config,config,17,https://hail.is,https://github.com/hail-is/hail/pull/4429,1,['config'],['config']
Modifiability,checker configs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4558:8,config,configs,8,https://hail.is,https://github.com/hail-is/hail/pull/4558,1,['config'],['configs']
Modifiability,"ck trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19) (my-first-hail-cluster-w-0.c.open-targets-eu-dev.internal executor 1): java.lang.NoClassDefFoundError: Could not initialize class __C147RGContainer_GRCh38; 	at __C144Compiled.applyregion0_8(Emit.scala); 	at __C144Compiled.apply(Emit.scala); 	at is.hail.expr.ir.TableMapRows.$anonfun$execute$43(TableIR.scala:1938); 	at scala.runtime.java8.JFunction1$mcJJ$sp.apply(JFunction1$mcJJ$sp.java:23); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.next(RichContextRDD.scala:79); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:6919,adapt,adapted,6919,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['adapt'],['adapted']
Modifiability,cleaned/killed a bad import and an unused variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3098:42,variab,variable,42,https://hail.is,https://github.com/hail-is/hail/pull/3098,1,['variab'],['variable']
Modifiability,"closes #7357. * implements `.tail` function on tables:. ```python; def tail(self, n) -> 'Table':; """"""Subset table to last `n` rows. Examples; --------; Subset to the last three rows:. >>> table_result = table1.tail(3); >>> table_result.count(); 3. Notes; -----. The number of partitions in the new table is equal to the number of; partitions containing the last `n` rows. Parameters; ----------; n : int; Number of rows to include. Returns; -------; :class:`.Table`; Table including the last `n` rows.; """""". return Table(TableTail(self._tir, n)); ```. * refactored some of the logic in `TableHead`, because a lot of the behavior is the same. * specifically, moved some partition-counts calculations to `is.hail.utils.PartitionCounts`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7386:554,refactor,refactored,554,https://hail.is,https://github.com/hail-is/hail/pull/7386,1,['refactor'],['refactored']
Modifiability,"com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.3</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.2</code></li>; </ul>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.1</code></li>; </ul>; <h2>1.18.1</h2>; <ul>; <li>Fix mocked module import not working when used as guarded import</li>; </ul>; <h2>1.18.0</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2</code></li>; <li>Handle <code>UnionType</code></li>; </ul>; <h2>1.17.1</h2>; <ul>; <li>Mark it as requiring <code>nptyping&lt;2</code></li>; </ul>; <h2>1.17.0</h2>; <ul>; <li>Add <code>typehints_use_rtype</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11909:2183,config,config,2183,https://hail.is,https://github.com/hail-is/hail/pull/11909,1,['config'],['config']
Modifiability,"com/gaborbernat""><code>@​gaborbernat</code></a> in <a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/pull/232"">tox-dev/sphinx-autodoc-typehints#232</a></li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2"">https://github.com/tox-dev/sphinx-autodoc-typehints/compare/1.18.1...1.18.2</a></p>; <h2>1.18.1</h2>; <p>No release notes provided.</p>; <h2>1.18.0</h2>; <p>No release notes provided.</p>; <h2>1.17.1</h2>; <p>No release notes provided.</p>; <h2>typehints_use_rtype support and handle TypeError</h2>; <p>No release notes provided.</p>; <h2>1.16.0</h2>; <p>No release notes provided.</p>; <h2>1.15.3</h2>; <p>No release notes provided.</p>; <h2>1.15.2</h2>; <p>No release notes provided.</p>; <h2>1.15.1</h2>; <p>No release notes provided.</p>; <h2>1.15.0</h2>; <p>No release notes provided.</p>; <h2>1.14.1</h2>; <p>No release notes provided.</p>; <h2>Added document_defaults config option</h2>; <p>No release notes provided.</p>; <h2>Fix NewType is inserting a reference as first argument</h2>; <p>No release notes provided.</p>; <h2>Python 3.10 support and PEP-563, drop 3.6</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/blob/main/CHANGELOG.md"">sphinx-autodoc-typehints's changelog</a>.</em></p>; <blockquote>; <h2>1.18.2</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2.1.1</code></li>; </ul>; <h2>1.18.1</h2>; <ul>; <li>Fix mocked module import not working when used as guarded import</li>; </ul>; <h2>1.18.0</h2>; <ul>; <li>Support and require <code>nptyping&gt;=2</code></li>; <li>Handle <code>UnionType</code></li>; </ul>; <h2>1.17.1</h2>; <ul>; <li>Mark it as requiring <code>nptyping&lt;2</code></li>; </ul>; <h2>1.17.0</h2>; <ul>; <li>Add <code>typehints_use_rtype</code> option</li>; <li>Handles <code>TypeError</code> wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11893:1680,config,config,1680,https://hail.is,https://github.com/hail-is/hail/pull/11893,1,['config'],['config']
Modifiability,"com/googleapis/java-storage/issues/2197"">#2197</a>) (<a href=""https://github.com/googleapis/java-storage/commit/26552f4b78f77d90df4e3dfb829c3f9c092fc817"">26552f4</a>)</li>; <li>Update dependency info.picocli:picocli to v4.7.4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2177"">#2177</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0c908147375fe58ac280179f5fba10bdd3886003"">0c90814</a>)</li>; <li>Update dependency info.picocli:picocli to v4.7.5 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2183"">#2183</a>) (<a href=""https://github.com/googleapis/java-storage/commit/f2448615ded6d9f43344bf1b9cda7ae3b191223b"">f244861</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.8.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2187"">#2187</a>) (<a href=""https://github.com/googleapis/java-storage/commit/aedbd6a811c4fcfedff68d7d46bb68e93bf9eeee"">aedbd6a</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.26 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2196"">#2196</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5"">4f8bb65</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:10554,plugin,plugin,10554,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,"com/googleapis/java-storage/issues/2197"">#2197</a>) (<a href=""https://github.com/googleapis/java-storage/commit/26552f4b78f77d90df4e3dfb829c3f9c092fc817"">26552f4</a>)</li>; <li>Update dependency info.picocli:picocli to v4.7.4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2177"">#2177</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0c908147375fe58ac280179f5fba10bdd3886003"">0c90814</a>)</li>; <li>Update dependency info.picocli:picocli to v4.7.5 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2183"">#2183</a>) (<a href=""https://github.com/googleapis/java-storage/commit/f2448615ded6d9f43344bf1b9cda7ae3b191223b"">f244861</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.8.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2187"">#2187</a>) (<a href=""https://github.com/googleapis/java-storage/commit/aedbd6a811c4fcfedff68d7d46bb68e93bf9eeee"">aedbd6a</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.26 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2196"">#2196</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5"">4f8bb65</a>)</li>; </ul>; <h2>v2.26.1</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:4592,plugin,plugin,4592,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,"com/prometheus/client_python/issues/730"">#730</a>; [ENHANCEMENT] Begin to add type hints to functions. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/705"">#705</a>; [ENHANCEMENT] Improved go-to-declaration behavior for editors. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/747"">#747</a>; [BUGFIX] Remove trailing slashes from pushgateway URLS. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/722"">#722</a>; [BUGFIX] Catch non-integer bucket/count values. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/726"">#726</a></p>; <h2>0.12.0 / 2021-10-29</h2>; <p>[FEATURE] Exemplar support (excludes multiprocess) <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/669"">#669</a>; [ENHANCEMENT] Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/706"">#706</a>; [ENHANCEMENT] Restricted Registry will handle metrics added after restricting <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/675"">#675</a>, <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/680"">#680</a><br />; [ENHANCEMENT] Raise a more helpful error if a metric is not observable <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/666"">#666</a>; [BUGFIX] Fix instance_ip_grouping_key not working on MacOS <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/687"">#687</a>; [BUGFIX] Fix assertion error from favicion.ico with Python 2.7 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/715"">#715</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/prometheus/client_python/commit/a234283a853238dc73fa22651532590330fd72a1""><code>a234283</code></a> Release 0.13.1</li>; <li><a href=""https://github.com/prometheus/client_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11515:2184,ENHANCE,ENHANCEMENT,2184,https://hail.is,https://github.com/hail-is/hail/pull/11515,1,['ENHANCE'],['ENHANCEMENT']
Modifiability,"com/sveltejs/svelte/issues/7440"">#7440</a>)</li>; <li>Fix handling of void tags in <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7449"">#7449</a>)</li>; <li>Fix handling of boolean attributes in <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7478"">#7478</a>)</li>; <li>Add special style scoping handling of <code>[open]</code> selectors on <code>&lt;dialog&gt;</code> elements (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7494"">#7495</a>)</li>; </ul>; <h2>3.47.0</h2>; <ul>; <li>Add support for dynamic elements through <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/2324"">#2324</a>)</li>; <li>Miscellaneous variable context fixes in <code>{@const}</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7222"">#7222</a>)</li>; <li>Fix <code>{#key}</code> block not being reactive when the key variable is not otherwise used (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7408"">#7408</a>)</li>; <li>Add <code>Symbol</code> as a known global (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7418"">#7418</a>)</li>; </ul>; <h2>3.46.6</h2>; <ul>; <li>Actually include action TypeScript interface in published package (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7407"">#7407</a>)</li>; </ul>; <h2>3.46.5</h2>; <ul>; <li>Add TypeScript interfaces for typing actions (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/6538"">#6538</a>)</li>; <li>Do not generate <code>unused-export-let</code> warning inside <code>&lt;script context=&quot;module&quot;&gt;</code> blocks (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7055"">#7055</a>)</li>; <li>Do not collapse whitespace-only CSS vars (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/71",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12032:4289,variab,variable,4289,https://hail.is,https://github.com/hail-is/hail/pull/12032,3,['variab'],['variable']
Modifiability,configure apache webserver to allow navbar to be loaded from github,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1024:0,config,configure,0,https://hail.is,https://github.com/hail-is/hail/issues/1024,1,['config'],['configure']
Modifiability,"could name your benchmarks `test_*` and put them in files named `test_*.py`.; Benchmarks may import and use any test code or utilities defined in `test/`.; The results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this so support them.; Perhaps an inner loop or something. The process of submitting benchmarks to batch is greatly simplified as the old `Makefile` infrastructure for ; building wheels and docker images etc has been replaced with the script `benchmark_in_batch.py`.; Benchmark images are now based off the `hail-dev` image built in CI (or via the `hail-dev-image` make target). ; Furthermore, you can control the number of ""replicate"" jobs created for each benchmark at the benchmark level using; the `@benchmark(batch_jobs=N)` decotator. Limitations/shortcomings:; - Output is currently jsonl only. So",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:1545,plugin,plugin,1545,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['plugin'],['plugin']
Modifiability,"ct.dependabot.com/PyCQA/pylint/issues/5452"">#5452</a></p>; </li>; <li>; <p>Fix false negative for <code>consider-iterating-dictionary</code> during membership checks encapsulated in iterables; or <code>not in</code> checks</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5323"">#5323</a></p>; </li>; <li>; <p><code>unused-import</code> now check all ancestors for typing guards</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5316"">#5316</a></p>; </li>; </ul>; <h1>What's New in Pylint 2.12.1?</h1>; <p>Release date: 2021-11-25</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pylint/commit/eec287fae66f8fc514d5daa9caee46fd0e0cb6d9""><code>eec287f</code></a> Bump pylint to 2.12.2, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/7def5278afc86224a98cc9d1706fbd9523ddda1b""><code>7def527</code></a> Add Copyrite configuration for Yu Shao</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/608ed329aaee9e457ac51347699d4892d29df802""><code>608ed32</code></a> Require <code>\</code> for asterisks in Sphinx-style parameter docstrings (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5464"">#5464</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/f89a3374ec7d49d2a984c90530758a506eaa4384""><code>f89a337</code></a> Deprecate <code>is_inside_lambda</code> from utils (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5447"">#5447</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/765a0b74bc5f2cface4595661f8832a3aebc68ba""><code>765a0b7</code></a> Add endLine and endColumn keys to JSONReporter (<a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5456"">#5456</a>)</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/28a33ef874cd63b92a32208e844b97f0c6a2f082""><code>28a33ef</code></a> Update outdated class ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11461:5982,config,configuration,5982,https://hail.is,https://github.com/hail-is/hail/pull/11461,2,['config'],['configuration']
Modifiability,"ct.dependabot.com/prometheus/client_python/issues/718"">#718</a>; [FEATURE] Support adding labels when using <code>.time()</code> <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/730"">#730</a>; [ENHANCEMENT] Begin to add type hints to functions. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/705"">#705</a>; [ENHANCEMENT] Improved go-to-declaration behavior for editors. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/747"">#747</a>; [BUGFIX] Remove trailing slashes from pushgateway URLS. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/722"">#722</a>; [BUGFIX] Catch non-integer bucket/count values. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/726"">#726</a></p>; <h2>0.12.0 / 2021-10-29</h2>; <p>[FEATURE] Exemplar support (excludes multiprocess) <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/669"">#669</a>; [ENHANCEMENT] Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/706"">#706</a>; [ENHANCEMENT] Restricted Registry will handle metrics added after restricting <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/675"">#675</a>, <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/680"">#680</a><br />; [ENHANCEMENT] Raise a more helpful error if a metric is not observable <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/666"">#666</a>; [BUGFIX] Fix instance_ip_grouping_key not working on MacOS <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/687"">#687</a>; [BUGFIX] Fix assertion error from favicion.ico with Python 2.7 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/715"">#715</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11515:2047,ENHANCE,ENHANCEMENT,2047,https://hail.is,https://github.com/hail-is/hail/pull/11515,1,['ENHANCE'],['ENHANCEMENT']
Modifiability,ctIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:3779,Rewrite,RewriteBottomUp,3779,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,ctTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2434,Rewrite,RewriteBottomUp,2434,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,ctTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3914,Rewrite,RewriteBottomUp,3914,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,cute$1(EvalRelationalLets.scala:10); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:37); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:147); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:141); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:310); E 	at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:346); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$12(ServiceBackend.scala:698); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withIRFunctionsReadFromInput(ServiceBackend.scala:801); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$11(ServiceBackend.scala:696); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$2(ServiceBackend.scala:654); E 	at is.hail.backend.ExecuteContext$.$anonfu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:4209,adapt,adapted,4209,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['adapt'],['adapted']
Modifiability,cute$1(EvalRelationalLets.scala:12); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:18); E 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:37); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:147); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.apply(LoweringPass.scala:141); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:313); E 	at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:349); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$12(ServiceBackend.scala:702); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withIRFunctionsReadFromInput(ServiceBackend.scala:805); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$11(ServiceBackend.scala:700); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$executeOneCommand$2(ServiceBackend.scala:658); E 	at is.hail.backend.ExecuteContext$.$anonfu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:3923,adapt,adapted,3923,https://hail.is,https://github.com/hail-is/hail/issues/13074,2,['adapt'],['adapted']
Modifiability,"d to hail version. JAR and ZIP:; ```; gs://hail-common/builds/0.2/jars/hail-0.2-3b1cb0772301-Spark-2.2.0.jar; gs://hail-common/builds/0.2/python/hail-0.2-3b1cb0772301.zip; ```. In Google Chrome we see 404s for; ```; GET http://localhost:8123/spark/api/v1/applications; ```; which happened repeatedly if you try to evaluate a cell. On the leader node of the spark cluster, `journalctl -u jupyter` shows:; ```; -- Logs begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505:975,config,configuration,975,https://hail.is,https://github.com/hail-is/hail/issues/5505,1,['config'],['configuration']
Modifiability,"d%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​github-actions</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ajtpio+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​jtpio</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/notebook/blob/@jupyter-notebook/tree@7.0.7/CHANGELOG.md"">notebook's changelog</a>.</em></p>; <blockquote>; <h2>7.0.7</h2>; <p>(<a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/application-extension@7.0.6...089c78c48fd00b2b0d2f33e4463eb42018e86803"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Update to JupyterLab 4.0.11 <a href=""https://redirect.github.com/jupyter/notebook/pull/7215"">#7215</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update ruff config and typing <a href=""https://redirect.github.com/jupyter/notebook/pull/7145"">#7145</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up lint handling <a href=""https://redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:3318,Enhance,Enhancements,3318,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['Enhance'],['Enhancements']
Modifiability,"d. In the case of Caitlin's PRS scoring method, that's about 1% or less of the original variant set. It's a bit janky. The file paths need to be the fully qualified ones that are seen by the hadoop reader. So `file:/full/path/to/file.bgen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3813:1421,layers,layers,1421,https://hail.is,https://github.com/hail-is/hail/pull/3813,1,['layers'],['layers']
Modifiability,"d/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_test.o; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux build/NativeBoot.o -o lib/linux-x86-64/libboot.so; g++ -o build/davies.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -MD -MF build/davies.d -MT build/davies.o -c davies.cpp; davies.cpp: In member function ‘real DaviesAlgo::qf(real*, real*, int*, int, real, real, int, real, real*, int*)’:; davies.cpp:241:8: error: variable ‘qfval’ might be clobbered by ‘longjmp’ or ‘vfork’ [-Werror=clobbered]; real qfval;; ^; cc1plus: all warnings being treated as errors; make: *** [build/davies.o] Error 1; Makefile:63: recipe for target 'build/davies.o' failed; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:7552,variab,variable,7552,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['variab'],['variable']
Modifiability,"d0eb6c2af0e3e98350e24047c4df7d5b8aad89a""><code>fd0eb6c</code></a> Bump pylint to 2.13.0, update changelog</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/1c509edc4ee2dbf1bbe8822e91e0b7df02ce463d""><code>1c509ed</code></a> [cleanup] Remove unused code in pylint.checker.base following refactor</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/1e7d3fa6219934028d2539ad290fe16ce8ea78e2""><code>1e7d3fa</code></a> [refactor] Create a file for the BasicChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/c0b8b32592f8d5d34ff37250adbda6b65269a0af""><code>c0b8b32</code></a> [refactor] Create a file for the BasicErrorChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/3f11fe629a7b89d2a3b92dce09ac5818f3904cee""><code>3f11fe6</code></a> [refactor] Create a package for the NameChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/6940715ba15f81fbd7d9e8685c0a714a8b612f24""><code>6940715</code></a> [refactor] Create a file for the DocstringChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/84d22cf24202bf6006fc179541e1853d145d33e0""><code>84d22cf</code></a> [refactor] Create a file for the PassChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/977b08d160e81aaecebf871d2b8ba2f9a96ef9d6""><code>977b08d</code></a> [refactor] Create files for comparison checker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/ddfca0ca884d677e4eb0e6f53553b16e7a503157""><code>ddfca0c</code></a> [refactor] Create a file for _BasicChecker in pylint.checkers</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/be4699399904654ef4107a228817b4ef176d8999""><code>be46993</code></a> [refactor] Create a package in order to be able to burst base.py</li>; <li>Additional commits viewable in <a href=""https://github.com/PyCQA/pylint/compare/v2.12.2...v2.13.0"">compare view</a></li>; </ul>; </detail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11674:4481,refactor,refactor,4481,https://hail.is,https://github.com/hail-is/hail/pull/11674,1,['refactor'],['refactor']
Modifiability,dEvaluate.scala:60); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2$adapted(CompileAndEvaluate.scala:58); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:58); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:17); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:17); 	at is.hail.expr.ir.TableWriter.apply(TableWriter.scala:51); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:921); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:66); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:11771,rewrite,rewrite,11771,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['rewrite'],['rewrite']
Modifiability,"dPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:20686,Plugin,Plugins,20686,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"d` an `OrderedRVD`, possibly with some non-empty key. This is consistent with the rule that the `rvd` must always have a stronger/longer key than the `TableType`.; * **small tweaks** - Now I start working through the `TableIR` nodes, rewriting them to remove explicit uses of `UnpartitionedRVD`. The general plan is to sandwich the rvd logic between `toOrderedRVD` and `toOldStyleRVD`. The first takes an `UnpartitionedRVD` to an `OrderedRVD` with empty key (and leaves `OrderedRVD`s alone), and the second takes an `OrderedRVD` to an `UnpartitionedRVD` if its key was empty, and leaves it alone otherwise. Once they're all rewritten this way, I redefine `toOldStyleRVD` to always return `OrderedRVD`, and `UnpartitionedRVD` is no longer used.; * **remove `TableUnkey`** - With `UnpartitionedRVD` going away, `TableUnkey` is no longer necessary, it's equivalent to keying by an empty key.; * **small tweaks** - these next two rewrite more `TableIR` nodes; * **Merge master** - the big one; * **tweak MatrixColsTable** - 1) Optimize `coerce` by checking if the requested key is empty, avoiding a scan in that case. 2) Optimize `sortedColsValue` by checking if the column key is empty, avoiding the sort in that case. 3) Simplify `colsRVD`, removing the case on the type of the `RVD`, just calling `coerce` and letting the previous optimizations avoid unnecessary work.; * **`distinctByKey` fix** - While looking over `TableIR` implementations, I noticed a bug in `distinctByKey`: you need to be sure no key is split across multiple partitions. To be sure the empty key edge case still works, I added a test to check that `strictify` on an empty-key partitioner will always collapse everything to one partition.; * **Flipped switch** - redifines `toOldStyleRVD` to just return the `OrderedRVD` unchanged, and asserts that `TableValue.rvd` is always an `OrderedRVD`.; * **rest of the `TableIR` tweaks** - added a factory method `OrderedRVD.unkeyed` to replace `UnpartitionedRVD.apply`.; * the rest are si",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4319:1736,rewrite,rewrite,1736,https://hail.is,https://github.com/hail-is/hail/pull/4319,1,['rewrite'],['rewrite']
Modifiability,dcast(BroadcastManager.scala:75); at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1539); at is.hail.backend.spark.SparkBackend.broadcast(SparkBackend.scala:411); at is.hail.io.plink.MatrixPLINKReader.executeGeneric(LoadPlink.scala:390); at is.hail.io.plink.MatrixPLINKReader.lower(LoadPlink.scala:561); at is.hail.expr.ir.TableReader.lower(TableIR.scala:663); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1062); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:5452,rewrite,rewrite,5452,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['rewrite'],['rewrite']
Modifiability,"dce09ac5818f3904cee""><code>3f11fe6</code></a> [refactor] Create a package for the NameChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/6940715ba15f81fbd7d9e8685c0a714a8b612f24""><code>6940715</code></a> [refactor] Create a file for the DocstringChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/84d22cf24202bf6006fc179541e1853d145d33e0""><code>84d22cf</code></a> [refactor] Create a file for the PassChecker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/977b08d160e81aaecebf871d2b8ba2f9a96ef9d6""><code>977b08d</code></a> [refactor] Create files for comparison checker in pylint.checker.base</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/ddfca0ca884d677e4eb0e6f53553b16e7a503157""><code>ddfca0c</code></a> [refactor] Create a file for _BasicChecker in pylint.checkers</li>; <li><a href=""https://github.com/PyCQA/pylint/commit/be4699399904654ef4107a228817b4ef176d8999""><code>be46993</code></a> [refactor] Create a package in order to be able to burst base.py</li>; <li>Additional commits viewable in <a href=""https://github.com/PyCQA/pylint/compare/v2.12.2...v2.13.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pylint&package-manager=pip&previous-version=2.12.2&new-version=2.13.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11674:5254,refactor,refactor,5254,https://hail.is,https://github.com/hail-is/hail/pull/11674,1,['refactor'],['refactor']
Modifiability,dd.RDD.$anonfun$collect$2(RDD.scala:1021); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2276); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:5266,adapt,adapted,5266,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['adapt'],['adapted']
Modifiability,dding 'hailtop/aiotools/fs/fs.py'; adding 'hailtop/aiotools/fs/stream.py'; adding 'hailtop/auth/__init__.py'; adding 'hailtop/auth/auth.py'; adding 'hailtop/auth/sql_config.py'; adding 'hailtop/auth/tokens.py'; adding 'hailtop/batch/__init__.py'; adding 'hailtop/batch/backend.py'; adding 'hailtop/batch/batch.py'; adding 'hailtop/batch/batch_pool_executor.py'; adding 'hailtop/batch/conftest.py'; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py',MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:10845,config,config,10845,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['config'],['config']
Modifiability,"de it with claims:</p>; <pre><code>claims_options = {; 'iss': {'essential': True, 'values': ['required']}; }; jwt.decode(token, key, claims_options=claims_options); </code></pre>; <p>It didn't raise an error before this fix.</p>; <h2>Version 0.15.3</h2>; <p>Fixed <code>.authorize_access_token</code> for OAuth 1.0 services, via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/308"">lepture/authlib#308</a></p>; <h2>Version 0.15.2</h2>; <p>Fixed httpx authentication bug via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/283"">#283</a></p>; <h2>Version 0.15.1</h2>; <p>Backward compitable fix for using JWKs in JWT, via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/280"">#280</a>.</p>; <h2>Version 0.15</h2>; <p>This is the last release before v1.0. In this release, we added more RFCs; implementations and did some refactors for JOSE:</p>; <ul>; <li>RFC8037: CFRG Elliptic Curve Diffie-Hellman (ECDH) and Signatures in JSON Object Signing and Encryption (JOSE)</li>; <li>RFC7638: JSON Web Key (JWK) Thumbprint</li>; </ul>; <p>We also fixed bugs for integrations:</p>; <ul>; <li>Fixed support for HTTPX&gt;=0.14.3</li>; <li>Added OAuth clients of HTTPX back via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/270"">#270</a></li>; <li>Fixed parallel token refreshes for HTTPX async OAuth 2 client</li>; <li>Raise OAuthError when callback contains errors via <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/275"">#275</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/authlib/blob/master/docs/changelog.rst"">authlib's changelog</a>.</em></p>; <blockquote>; <h2>Version 0.15.5</h2>; <p><strong>Released on Oct 18, 2021.</strong></p>; <ul>; <li>Make Authlib compatible with latest httpx</li>; <li>Make Authlib compatible with latest werkzeug</li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:1544,refactor,refactors,1544,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['refactor'],['refactors']
Modifiability,delint batch2; fix worker boot disk config; internal-gateway: propagate X-Forwarded-Proto for blog,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7492:36,config,config,36,https://hail.is,https://github.com/hail-is/hail/pull/7492,1,['config'],['config']
Modifiability,der$.scopedCode(EmitCodeBuilder.scala:23); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1009); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1062); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1062); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1078); 	at is.hail.expr.ir.Emit.$anonfun$emitI$238(Emit.scala:2361); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:336); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:2341); 	at is.hail.expr.ir.Emit.emitI$3(Emit.scala:2555); 	at is.hail.expr.ir.Emit.$anonfun$emit$22(Emit.scala:2638); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:445); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2637); 	at is.hail.expr.ir.Emit.emit$1(Emit.scala:621); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:657); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1(Emit.scala:579); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1$adapted(Emit.scala:577); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit.emitVoidInSeparateMethod(Emit.scala:577); 	at is.hail.expr.ir.Emit.emitInSeparateMethod(Emit.scala:601); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:793); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:630); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:748); 	at is.hail.expr.ir.RelationalWriter.writeMetadata(TableWriter.scala:463); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:7819,adapt,adapted,7819,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,"dleware type alias <code>aiohttp.typedefs.Middleware</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/5898"">#5898</a>)</p>; </li>; <li>; <p>Exported <code>HTTPMove</code> which can be used to catch any redirection request; that has a location -- :user:<code>dreamsorcerer</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/6594"">#6594</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.9.0 (2023-11-18)</h1>; <h2>Features</h2>; <ul>; <li>; <p>Introduced <code>AppKey</code> for static typing support of <code>Application</code> storage.; See <a href=""https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config"">https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config</a></p>; <p><code>[#5864](https://github.com/aio-libs/aiohttp/issues/5864) &lt;https://github.com/aio-libs/aiohttp/issues/5864&gt;</code>_</p>; </li>; <li>; <p>Added a graceful shutdown period which allows pending tasks to complete before the application's cleanup is called.; The period can be adjusted with the <code>shutdown_timeout</code> parameter. -- by :user:<code>Dreamsorcerer</code>.; See <a href=""https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown"">https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown</a></p>; <p><code>[#7188](https://github.com/aio-libs/aiohttp/issues/7188) &lt;https://github.com/aio-libs/aiohttp/issues/7188&gt;</code>_</p>; </li>; <li>; <p>Added <code>handler_cancellation &lt;https://docs.aiohttp.org/en/stable/web_advanced.html#web-handler-cancellation&gt;</code>_ parameter to cancel web handler on client disconnection. -- by :user:<code>mosquito</code>; This (optionally) reintroduces a feature removed in a previous release.; Recom",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14027:3764,config,config,3764,https://hail.is,https://github.com/hail-is/hail/pull/14027,6,['config'],['config']
Modifiability,"docker_prefix is not exactly the ""registry name"" in azure's definition, but it is `<registry_name>.azurecr.io` which `az acr login` accepts alternatively to just the registry name. Didn't seem worth adding a mostly redundant config field.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11301:225,config,config,225,https://hail.is,https://github.com/hail-is/hail/pull/11301,1,['config'],['config']
Modifiability,"doesn't include the ability to use user-provided expr to generate double matrixes for PCA. @cseed @tpoterba I think you had feelings on whether or not PCA should keep the current behavior where it returns an annotated vsm. . Currently, I've kept VariantSampleMatrix.pca as something that annotates and returns a vsm (using annotateVariantsTable for loadings), and then there's another method (VariantSampleMatrix.pcaResults) that just returns a tuple of (scores, loadings, eigenvalues). I could refactor them such that pca() => pcaAsAnnotations() (or something) and pcaResults() => pca(), and updating the python interface to match?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2432:495,refactor,refactor,495,https://hail.is,https://github.com/hail-is/hail/pull/2432,1,['refactor'],['refactor']
Modifiability,"doop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:3781,Config,Configuration,3781,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"e HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) serve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3795,config,configuration,3795,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"e all async fixtures and tests; automatically by asyncio; provide <em>strict</em> mode if a test suite; should work with different async frameworks simultaneously, e.g.; <code>asyncio</code> and <code>trio</code>.</li>; </ul>; <h1>Installation</h1>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest-asyncio/blob/master/CHANGELOG.rst"">pytest-asyncio's changelog</a>.</em></p>; <blockquote>; <h1>0.20.1 (22-10-21)</h1>; <ul>; <li>Fixes an issue that warned about using an old version of pytest, even though the most recent version was installed. <code>[#430](https://github.com/pytest-dev/pytest-asyncio/issues/430) &lt;https://github.com/pytest-dev/pytest-asyncio/issues/430&gt;</code>_</li>; </ul>; <h1>0.20.0 (22-10-21)</h1>; <ul>; <li>BREAKING: Removed <em>legacy</em> mode. If you're upgrading from v0.19 and you haven't configured <code>asyncio_mode = legacy</code>, you can upgrade without taking any additional action. If you're upgrading from an earlier version or you have explicitly enabled <em>legacy</em> mode, you need to switch to <em>auto</em> or <em>strict</em> mode before upgrading to this version.</li>; <li>Deprecate use of pytest v6.</li>; <li>Fixed an issue which prevented fixture setup from being cached. <code>[#404](https://github.com/pytest-dev/pytest-asyncio/issues/404) &lt;https://github.com/pytest-dev/pytest-asyncio/pull/404&gt;</code>_</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/c8d017407d39dd81d6864fa9a58ba1240d54be9f""><code>c8d0174</code></a> fix: Do not warn about outdated pytest version when pytest&gt;=7 is installed. (...</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/6450ddbe974f5359d56317ba8bdda8b2ab48655a""><code>6450ddb</code></a> Prepare release of v0.20.0. (<a href=""https://github-redir",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12390:3214,config,configured,3214,https://hail.is,https://github.com/hail-is/hail/pull/12390,1,['config'],['configured']
Modifiability,"e handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do we configure the database? How do we create new tables? Should this be done in the applications themselves or during deployment?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193:3694,config,configure,3694,https://hail.is,https://github.com/hail-is/hail/issues/5193,1,['config'],['configure']
Modifiability,"e in one <code>read</code> call. :issue:<code>2558</code></li>; <li>A cookie header that starts with <code>=</code> is treated as an empty key and discarded,; rather than stripping the leading <code>==</code>.</li>; <li>Specify a maximum number of multipart parts, default 1000, after which a; <code>RequestEntityTooLarge</code> exception is raised on parsing. This mitigates a DoS; attack where a larger number of form/file parts would result in disproportionate; resource use.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/werkzeug/commit/22a254fca2ad0130adbbcbd11d3de51bcb04a08b""><code>22a254f</code></a> release version 2.2.3</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/517cac5a804e8c4dc4ed038bb20dacd038e7a9f1""><code>517cac5</code></a> Merge pull request from GHSA-xg9f-g7g7-2323</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/babc8d9e8c9fa995ef26050698bc9b5a92803664""><code>babc8d9</code></a> rewrite docs about request data limits</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/09449ee77934a0c883f5959785864ecae6aaa2c9""><code>09449ee</code></a> clean up docs</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/fe899d0cdf767a7289a8bf746b7f72c2907a1b4b""><code>fe899d0</code></a> limit the maximum number of multipart form parts</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/cf275f42acad1b5950c50ffe8ef58fe62cdce028""><code>cf275f4</code></a> Merge pull request from GHSA-px8h-6qxv-m22q</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/8c2b4b82d0cade0d37e6a88e2cd2413878e8ebd4""><code>8c2b4b8</code></a> don't strip leading = when parsing cookie</li>; <li><a href=""https://github.com/pallets/werkzeug/commit/7c7ce5cb73f3f7d3b9c09340e4f322aeb583dbc5""><code>7c7ce5c</code></a> [pre-commit.ci] pre-commit autoupdate (<a href=""https://github-redirect.dependabot.com/pallets/werkzeug/issues/2585"">#2585</a>)</li>; <li><a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12703:3450,rewrite,rewrite,3450,https://hail.is,https://github.com/hail-is/hail/pull/12703,1,['rewrite'],['rewrite']
Modifiability,"e key, which we really shouldn't; do; Tim is working on it), which was broken (presumably) when we changed Scala versions. The; `var` modifier ensures the name is compiled as a JVM field. 4. Correctly convert from a `Byte` to an `Int`. By default `Byte` to `Int` conversion (which is done; automatically when you return a `Byte` from a function whose return type is `Int`) is; sign-preserving. That means that the byte `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule. - Only apply nest_asyncio when an event loop is already started.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10314:3533,config,config,3533,https://hail.is,https://github.com/hail-is/hail/pull/10314,1,['config'],['config']
Modifiability,"e requester pays bucket in the region specified by the user, available regions are `'us'` and `'eu'`. . `db = hl.experimental.DB(region='us')`; `mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics')`. Entries in the `annotation_db.json` file were modified to the following format:. ```; ""dataset_name"": { ""description"": ""some description here"",; ""key_properties"": [],; ""url"": ""https://www.someurlhere.com"",; ""versions"": [{""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""one_version""},; {""url"": {""eu"": ""gs://hail-datasets-eu/dataset"",; ""us"": ""gs://hail-datasets-us/dataset""},; ""version"": ""another_version""}]}; ```. The `annotation_db.json` file is now used by the `load_dataset()` function in `datasets.py` as well, any dataset in the JSON file should now be able to be loaded this way. Made changes to the following:; - `DB` class now requires a `region` parameter.; - `Dataset.from_name_and_json()` has had a `custom_config` parameter added that indicates whether or not the user has supplied their own `config` or `url`. `Dataset.from_name_and_json()` now calls `DatasetVersion.get_region()` method to retrieve the dataset from the bucket in the selected region if `custom_config` is `False`. ; - The `DatasetVersion.get_region()` method takes the dataset `name`, a list of `DatasetVersion` objects, and a `region`, and returns a list of the versions that are available for that region. This method calls the instance method `in_region()` to check if the dataset is available in the requested region.; - If `in_region()` determines the desired region is not available for some dataset that otherwise is available in another region, it will raise a warning. If user still tries to call `db.annotate_rows_db()` using a dataset unavailable in their region, then it will get caught by the `_check_availability` instance method in the `DB` class and raise a `ValueError`.; - Started to add documentation to the classes and methods, still a work in progr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9496:1156,config,config,1156,https://hail.is,https://github.com/hail-is/hail/pull/9496,1,['config'],['config']
Modifiability,"e should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`.; - The `query` user was missing from bootstrap-create-accounts.; - `hail-ubuntu-stmp` was missing from `docker/Makefile`'s `clean` rule; - Use a dummy `WorkerBackend` when we're on the worker. The worker isn't allowed to call these; methods anyway, that would amount to Hail Queries inside of Hail Queries, madness!; - New transient error in Java.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:2170,config,config,2170,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['config'],['config']
Modifiability,"e to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:4427,Plugin,Plugins,4427,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"e with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.10.x branch, and on adding new features on the main branch.</p>; <p>This release requires Python <code>3.8+</code> and NumPy <code>1.19.5</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>A new dedicated datasets submodule (<code>scipy.datasets</code>) has been added, and is; now preferred over usage of <code>scipy.misc</code> for dataset retrieval.</li>; <li>A new <code>scipy.interpolate.make_smoothing_spline</code> function was added. This; function constructs a smoothing cubic spline from noisy data, using the; generalized cross-validation (GCV) criterion to find the tradeoff between; smoothness and proximity to data points.</li>; <li><code>scipy.stats</code> has three new distributions, two new hypothesis tests, three; new sample statistics, a class for greater control over calculations; involving covariance matrices, and many other enhancements.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.datasets</code> introduction</h1>; <ul>; <li>A new dedicated <code>datasets</code> submodule has been added. The submodules; is meant for datasets that are relevant to other SciPy submodules ands; content (tutorials, examples, tests), as well as contain a curated; set of datasets that are of wider interest. As of this release, all; the datasets from <code>scipy.misc</code> have been added to <code>scipy.datasets</code>; (and deprecated in <code>scipy.misc</code>).</li>; <li>The submodule is based on <a href=""https://www.fatiando.org/pooch/latest/"">Pooch</a>; (a new optional dependency for SciPy), a Python package to simplify fetching; data files. This move will, in a subsequent release, facilitate SciPy; to trim down the sdist/wheel sizes, by decoupling the data files and; moving them out of the SciPy repository, hosting them externa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13227:1900,enhance,enhancements,1900,https://hail.is,https://github.com/hail-is/hail/pull/13227,1,['enhance'],['enhancements']
Modifiability,"e.apis:google-api-services-storage to v1-rev...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/5e76f1963db18c9d081133755b5572e186cd1b34""><code>5e76f19</code></a> chore: Update the Java code generator (gapic-generator-java) to 2.25.0 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2198"">#2198</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/26552f4b78f77d90df4e3dfb829c3f9c092fc817""><code>26552f4</code></a> deps: update dependency com.google.cloud:google-cloud-shared-dependencies to ...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/bffb397730d39f4c1c9f8fa80e316a26c39534ce""><code>bffb397</code></a> chore: add SyncingFileChannel (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2157"">#2157</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5""><code>4f8bb65</code></a> deps: update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.26...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/67badabaa5126ec6d011879c3983e6b69880c900""><code>67badab</code></a> chore(benchmarking): Remove default for temp directory and read from java.io....</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/45e66e89373ef016eff9b7deb30dbdfa818770d2""><code>45e66e8</code></a> deps: update actions/checkout action to v4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2190"">#2190</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/5c048c499eef224dade8f4409dfae732cb5a7017""><code>5c048c4</code></a> deps: update actions/checkout action to v4 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2189"">#2189</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v2.17.1...v2.27.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-bad",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:14287,plugin,plugin,14287,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,e.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.126-ee77707f4fab; Error summary: HailException: cannot set missing field for required type +PFloat64; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:20176,adapt,adapted,20176,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,"e.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def broadcast_f(left, data, jt):; 1464 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1465 return process_joins(self, exprs, broadcast_f); 1466 ; 1467 def cache(self):. ~/projects/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 365 all_uids.extend(list(t)); 366 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); --> 367 data_json = t._to_json(data); 368 left = broadcast_f(left, data_json, t._jtype); 369 . ~/projects/hail/python/hail/expr/types.py in _to_json(self, x); 176 def _to_json(self, x):; 177 converted = self._convert_to_json_na(x); --> 178 return json.dumps(converted); 179 ; 180 def _convert_to_json_na(self, x):. ~/anaconda2/envs/hail/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3708:3341,extend,extend,3341,https://hail.is,https://github.com/hail-is/hail/issues/3708,1,['extend'],['extend']
Modifiability,"e/userFiles-7053b92c-9117-46b6-8c52-752fee2701e9/fetchFileTemp5171194947676284646.tmp; 2018-10-09 14:46:41 Executor: INFO: Adding file:/private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-02128b51-f37e-4798-84bb-d3e3819e51be/userFiles-7053b92c-9117-46b6-8c52-752fee2701e9/sparklyr-2.2-2.11.jar to class loader; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 142.013327 ms; 2018-10-09 14:46:41 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 972 bytes result sent to driver; 2018-10-09 14:46:41 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 406 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:41 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:41 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.420 s; 2018-10-09 14:46:41 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.669318 s; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: table7e606a8b83f4; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: CACHE TABLE `table7e606a8b83f4`; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: `table7e606a8b83f4`; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 17.141549 ms; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 10.417049 ms; 2018-10-09 14:46:41 SparkContext: INFO: Starting job: sql at NativeMethodAccessorImpl.java:0; 2018-10-09 14:46:41 DAGScheduler: INFO: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 14:46:41 DAGScheduler: INFO: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions; 2018-10-09 14:46:41 DAGScheduler: INFO: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:35506,config,configuration,35506,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,"eRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""batch-svc"" ""namespace"":""default""]]]}; from server for: ""deployment.yaml"": rolebindings.rbac.authorization.k8s.io ""batch-pods-admin-binding"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get rolebindings.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""hail.is/sha"":""1c6dbf20333a"" ""app"":""batch""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""1c6dbf20333a""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:4b4139c73fe9be3bee6c2895aa74059e157eb861d2bdac7d2304ba44b5421f88"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""labels"":map[""app"":""batch""] ""name"":""batch""] ""spec""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:3665,config,configuration,3665,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,"e_pool.py code has been split amongst zone_monitor.py, pool.py, gce.py, create_instance.py, and instance_collection.py. The scheduler code is now in PoolScheduler in pool.py. The SQL code has vectorized user_resources by instance_collection as well as batch_cancellable_resources and batches_staging. There are also two new tables: inst_colls and pools. Each job and instance must belong to an instance collection noted by the field `inst_coll`. The job_update trigger had to be updated to insert into user_resources to the correct pool. The cancel_batch and close_batch functions changed to vectorize by instance collection. I deleted the global `ready_cores` table. The front end code does not change except looking for a `worker_type` field in the resources field of the job spec (default if undefined is standard). I added a PoolSelector class which is overkill for now, but will be used in the future for more complicated scenarios. There was an issue with our existing code for converting between memory in bytes to memory in MB in the worker_config.py code for the `resources()` function. For the highcpu case, it is impossible for the memory in bytes to be divisible by 1024**2. The utils.py code now rounds up bytes using math.ceil. The hailtop.batch library adds a `worker_type` method on Job. I didn't change the interface significantly at this time as I think this is fine for now. More significant changes will come when we change how cpu and memory and storage are interpreted by the worker. I added 3 new tests: we spin up a highmem and highcpu machine in test_batch.py. In the hailtop.batch tests, I check the `worker_type` interface. The driver UI page has changed such that there's a list of pools with links to a pools.html page, which gives information on the instances in the pool, the configuration updates, and the current user resources for that pool. Things to double check:; - SQL code especially cancel_batch where I had to use temporary variables and recompute_incremental",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9832:2301,config,configuration,2301,https://hail.is,https://github.com/hail-is/hail/pull/9832,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,"ebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services #this was causing the error, and of course the create-services role is superseded by the the create-services-and-pods role; apiGroup: """"; ---; ```. After:; ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: create-services-and-pods; rules:; - apiGroups: [""""]; resources: [""services""]; verbs: [""*""]; - apiGroups: [""""]; resources: [""pods""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: default; name: notebook-create-services-and-pods; subjects:; - kind: ServiceAccount; name: notebook; namespace: default; roleRef:; kind: Role; name: create-services-and-pods; apiGroup: """"; ---; ```. ### Results of test runs. Before:. ```sh; kubectl apply -f k8s-config.yaml; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. namespace/batch-pods unchanged; ...; The RoleBinding ""notebook-create-services-and-pods"" is invalid: roleRef: Invalid value: rbac.RoleRef{APIGroup:""rbac.authorization.k8s.io"", Kind:""Role"", Name:""create-services""}: cannot change roleRef; make: *** [k8s-config] Error 1; ```. After:; ```sh; ERROR: (gcloud.compute.addresses.describe) Could not fetch resource:; - Required 'compute.addresses.get' permission for 'projects/hail-vdc-staging/regions/us-central1/addresses/site'. ...; role.rbac.authorization.k8s.io/create-services-and-pods unchanged; rolebinding.rbac.authorization.k8s.io/notebook-create-services-and-pods configured; role.rbac.authorization.k8s.io/read-get-user-secret unchanged; rolebinding.rbac.authorization.k8s.io/notebook-read-get-users-secret configured; ```. I think the error just reflects my not having hail-vdc-staging permissions, that is unaffected by this PR. cc @cseed, @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5746:2845,config,config,2845,https://hail.is,https://github.com/hail-is/hail/pull/5746,3,['config'],"['config', 'configured']"
Modifiability,"ecovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for updating an existing named query.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for new AMI property 'lastLaunchedTime'</li>; <li>api-change:<code>servicecatalog-appregistry</code>: [<code>botocore</code>] AppRegistry is deprecating Application and Attribute-Group Name update feature. In this release, we are marking the name attributes for Update APIs as deprecated to give a heads up to our customers.</li>; </ul>; <h1>1.21.8</h1>; <ul>; <li>api-change:<code>elasticache</code>: [<code>botocore</code>] Doc only update for ElastiCache</li>; <li>api-change:<code>panorama</code>: [<code>botocore</code>] Added NTP server configuration parameter to ProvisionDevice operation. Added alternate software fields to DescribeDevice response</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/261b0f2ffe079b6940d683657fcad358195f882e""><code>261b0f2</code></a> Merge branch 'release-1.21.12'</li>; <li><a href=""https://github.com/boto/boto3/commit/44f4f5ef0b66d1a508685b62388f1e4a7d60dace""><code>44f4f5e</code></a> Bumping version to 1.21.12</li>; <li><a href=""https://github.com/boto/boto3/commit/bb003d02bd7afefede0ab4678abaea99fe1662ce""><code>bb003d0</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/ad9a92e8fe3d5bc90b2980cdb6839c713b56fbda""><code>ad9a92e</code></a> Merge branch 'release-1.21.11'</li>; <li><a href=""https://github.com/boto/boto3/commit/42b3e0d3c1d02",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:4758,config,configuration,4758,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['config'],['configuration']
Modifiability,"ecrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; test-gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: test-gsa-key; Optional: false; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: ci-gsa-key; Optional: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 13m default-scheduler Successfully assigned batch-pods/batch-3-job-41-39d17b to gke-vdc-preemptible-pool-9c7148b2-1f89; Warning FailedCreatePodSandBox 13m kubelet, gke-vdc-preemptible-pool-9c7148b2-1f89 Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container ""99ac9edad98221dbfaf4ab8eb443bc6d3fdc6df84164594469900813652fd913"" network for pod ""batch-3-job-41-39d17b"": NetworkPlugin kubenet failed to set up pod ""batch-3-job-41-39d17b_batch-pods"" network: Error adding container to network: failed to set bridge addr: could not add IP address to ""cbr0"": file exists; ```. ```; $ kubectl -n batch-pods get pods -o yaml batch-3-job-41-39d17b; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-07-12T17:17:15Z""; labels:; app: batch-job; batch_id: ""3""; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; job_id: ""41""; task: main; user: ci; uuid: f53f127847864f1cbf7d4bdc911a6646; name: batch-3-job-41-39d17b; namespace: batch-pods; resourceVersion: ""87247110""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-3-job-41-39d17b; uid: e4d87ac3-a4c8-11e9-a4bb-42010a8000af; spec:; containers:; - command:; - bash; - -c; - |-; set -e; gcloud -q auth activate-service-account --key-fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6625:2283,sandbox,sandbox,2283,https://hail.is,https://github.com/hail-is/hail/issues/6625,2,['sandbox'],['sandbox']
Modifiability,"ect.github.com/aio-libs/aiohttp/issues/5704"">#5704</a>)</p>; </li>; <li>; <p>Added a middleware type alias <code>aiohttp.typedefs.Middleware</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/5898"">#5898</a>)</p>; </li>; <li>; <p>Exported <code>HTTPMove</code> which can be used to catch any redirection request; that has a location -- :user:<code>dreamsorcerer</code>.</p>; <p>(<a href=""https://redirect.github.com/aio-libs/aiohttp/issues/6594"">#6594</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.9.0 (2023-11-18)</h1>; <h2>Features</h2>; <ul>; <li>; <p>Introduced <code>AppKey</code> for static typing support of <code>Application</code> storage.; See <a href=""https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config"">https://docs.aiohttp.org/en/stable/web_advanced.html#application-s-config</a></p>; <p><code>[#5864](https://github.com/aio-libs/aiohttp/issues/5864) &lt;https://github.com/aio-libs/aiohttp/issues/5864&gt;</code>_</p>; </li>; <li>; <p>Added a graceful shutdown period which allows pending tasks to complete before the application's cleanup is called.; The period can be adjusted with the <code>shutdown_timeout</code> parameter. -- by :user:<code>Dreamsorcerer</code>.; See <a href=""https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown"">https://docs.aiohttp.org/en/latest/web_advanced.html#graceful-shutdown</a></p>; <p><code>[#7188](https://github.com/aio-libs/aiohttp/issues/7188) &lt;https://github.com/aio-libs/aiohttp/issues/7188&gt;</code>_</p>; </li>; <li>; <p>Added <code>handler_cancellation &lt;https://docs.aiohttp.org/en/stable/web_advanced.html#web-handler-cancellation&gt;</code>_ parameter to cancel web handler on client disconnection. -- by :user:<code>mosqui",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14027:3689,config,config,3689,https://hail.is,https://github.com/hail-is/hail/pull/14027,6,['config'],['config']
Modifiability,ection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:5008,adapt,adapted,5008,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['adapt'],['adapted']
Modifiability,"ed arguments. `-o`, which starts with a dash, is an option. This PR makes the following changes:. - For dataproc commands taking extra gcloud parameters, all parameters after a double-dash (--) are passed to gcloud.; - The actual rule is slightly more complicated, but I think the above rule is the right take away. In detail, extra parameters are passed to gcloud. Unknown options (starting with a dash) before `--` are reported as an error. So arguments (not options) before `--` and all parameters after are passed to gcloud. ; - Short options don't need a `=` when specifying a value. It is now `-p2`, not `-p=2`.; - While I was making breaking changes, I changed `dataproc submit` `--gcloud_configuration` to `--gcloud-configuration`. I am happy to undo this one.; - Group arguments must go before the next command. Write `hailctl dataproc --beta start ...` not `hailctl dataproc start --beta ...`, which is an error since `start` has no option `--beta`. This PR rewrites argument parsing to use click instead of argparse: https://click.palletsprojects.com/en/7.x/. Things you need to know about click:; - A group is a group of commands or subgroups, like `hailctl dataproc`, `hailctl batch`, etc. Groups are defined like this:; ; ```; @hailctl.group(; help=""Manage the Hail Batch service.""); def batch():; pass; ```; - A command in a group is defined like this:. ```; @batch.command(; help=""Get a particular batch's info.""); @click.argument('batch_id', type=int); @click.option('--output-format', '-o',; type=click.Choice(['yaml', 'json']),; default='yaml', show_default=True,; help=""Specify output format"",); def get(batch_id, output_format):; ...; ```. The command decorator replaces the function with one that takes a `List[str]` of command line parameters, parses them, and calls the original function. The option options are pretty self-explanatory. - To access an argument to a group (like `dataproc --beta`) in a (sub)command, use the decorator `click.pass_context` to pass the click cont",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:1316,rewrite,rewrites,1316,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['rewrite'],['rewrites']
Modifiability,"ed the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3299,config,config-http,3299,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['config-http']
Modifiability,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2249,Rewrite,RewriteBottomUp,2249,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3729,Rewrite,RewriteBottomUp,3729,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(Nativ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5120,Rewrite,RewriteBottomUp,5120,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,ed.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$an,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:4469,Rewrite,RewriteBottomUp,4469,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"edirect.dependabot.com/kubernetes/kubernetes/pull/108129"">kubernetes/kubernetes#108129</a>, <a href=""https://github.com/ahg-g""><code>@​ahg-g</code></a>)</li>; <li>The AnyVolumeDataSource feature is now beta, and the feature gate is enabled by default. In order to provide user feedback on PVCs with data sources, deployers must install the VolumePopulators CRD and the data-source-validator controller. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108736"">kubernetes/kubernetes#108736</a>, <a href=""https://github.com/bswartz""><code>@​bswartz</code></a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/9ecc1143a8f7b34264b48dea12edd4d66230476f""><code>9ecc114</code></a> [chore] update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/ae5fd81a0fbea7d7fdac8d418876acaa288bcc0f""><code>ae5fd81</code></a> fix: config reader handles bool types (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/218"">#218</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/7bbb327cee0d4ed908a5deb28aaf5a9ccc1f4602""><code>7bbb327</code></a> [chore] update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/4d52e04e03c0195eef581919f9efa4b6d0bd35ea""><code>4d52e04</code></a> [chore] update changelog</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/078dde081f8be6b190bbe9271d9f8d3839b617c7""><code>078dde0</code></a> fixed watch.stream bug of not working with apis with follow kwarg (<a href=""https://github-redirect.dependabot.com/tomplus/kubernetes_asyncio/issues/216"">#216</a>)</li>; <li><a href=""https://github.com/tomplus/kubernetes_asyncio/commit/3ecb956c6af223b343ea5695d377982e98d4341c""><code>3ecb956</code></a> [chore] update version</li>; <li><a href=""https://github.com/tomplus/kubernetes_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:14342,config,config,14342,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['config'],['config']
Modifiability,"edirect.dependabot.com/samtools/htsjdk/issues/1621"">#1621</a>); 9fd0ecf21 Disable codecov until we can fix the uploader (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1622"">#1622</a>); 347c0ac57 Fix EdgeReadIterator (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>); d15a5bacb Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</p>; <h2>3.0.0</h2>; <p>Htsjdk 3.0.0: Revenge of the Simple Allele</p>; <p>This is the first htsjdk with a major version increase in a long time. We bumped it to indicate there are some breaking changes that will potentially require downstream code changes. Notably, <code>Allele</code> became an interface instead of a concrete class. <code>SimpleAllele</code> may be used as a replacement if you have classes which previously subclassed allele.</p>; <p>New Plugin Infrastructure:; 6a60de7c2 Move API marker annotations into new annotation package. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1558"">#1558</a>); 7ac95d5f7 Plugin framework and interfaces for versioned file format codecs (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1525"">#1525</a>); d40fe5412 Beta implementation of Bundles. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1546"">#1546</a>)</p>; <p>CRAM; 489c4192d Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>); 22aec6782 Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>); 6507249a4 Make the CRAM MD5 failure message more user friendly. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1607"">#1607</a>); b5af659e6 Fix restoration of read base feature code. <a href=""https://github-redirect.dependabot.co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:1606,Plugin,Plugin,1606,https://hail.is,https://github.com/hail-is/hail/pull/12229,2,['Plugin'],['Plugin']
Modifiability,"een changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1656,Plugin,Plugins,1656,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"ef=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1106"">#1106</a>)</li>; </ul>; <h2>Fixes</h2>; <ul>; <li>Remove del from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>) (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li>fix socket.error raises (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li>Fix buffer is closed error when using PythonParser class (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; </ul>; <h2>Version v2.0.0</h2>; <p>Version 2.0 is a complete rewrite of aioredis. Starting with this version, aioredis now follows the API of <a href=""https://github.com/andymccurdy/redis-py"">redis-py</a>, so you can easily adapt synchronous code that uses redis-py for async applications with aioredis-py.</p>; <p><strong>NOTE:</strong> This version is <em>not</em> compatible with earlier versions of aioredis. If you upgrade, you will need to make code changes.</p>; <p>For more details, read our <a href=""https://aioredis.readthedocs.io/en/latest/migration/"">documentation on migrating to version 2.0</a>.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aioredis-py/blob/master/CHANGELOG.md"">aioredis's changelog</a>.</em></p>; <blockquote>; <h2>2.0.1 - (2021-12-20)</h2>; <h3>Features</h3>; <ul>; <li>Added Python 3.10 to CI &amp; Updated the Docs; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:1377,rewrite,rewrite,1377,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['rewrite'],['rewrite']
Modifiability,"ef=""https://github.com/microsoft/debugpy/commit/6e247fb17bc15488a2cbefb7eed80c87179b147c""><code>6e247fb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1009"">#1009</a> from rchiodo/rchiodo/missed_thread_for_self_trace</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/cccfb954bd7321fbb25f234f0c9a2e8372646297""><code>cccfb95</code></a> Missed a thread to allow debugging</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/6c19aba462d2a9b6ea79d5cbce2886bd961823eb""><code>6c19aba</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1007"">#1007</a> from rchiodo/rchiodo/allow_adapter_debugging</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/10d8839a4cac4bdaf99cfe44ca2ee5479c95c003""><code>10d8839</code></a> Review feedback</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/83ff28006d2f85cfbd03fcdc2650eea6831ab2b1""><code>83ff280</code></a> Add DEBUGPY_TRACE_DEBUGPY variable to allow debugpy to debug itself</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/4f6638b0a6bbdec598987f7c2b62107a57edd6a6""><code>4f6638b</code></a> Fix <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1001"">#1001</a>: Enable controlling shell expansion via &quot;argsCanBeInterpretedByShell&quot;</li>; <li><a href=""https://github.com/microsoft/debugpy/commit/6b276e339cd850c5f8c93ff4bdbd305dd963d7bb""><code>6b276e3</code></a> Step in/step over support for IPython. Fixes <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/869"">#869</a></li>; <li><a href=""https://github.com/microsoft/debugpy/commit/a294092d9c6d8459126ecb8f537b6012fb7e7d28""><code>a294092</code></a> Properly stop at line 1 in frame eval mode. Fixes <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/995"">#995</a></li>; <li>Additional commits viewable in <a href=""https://github.com/microsoft/debugpy/compare/v1.6.0..",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12103:4383,variab,variable,4383,https://hail.is,https://github.com/hail-is/hail/pull/12103,2,['variab'],['variable']
Modifiability,"ef=""https://redirect.github.com/fonttools/fonttools/issues/3006"">#3006</a>).</li>; <li>[featureVars] Fixed bug in <code>overlayBox</code> (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3003"">#3003</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/3005"">#3005</a>).</li>; <li>[glyf] Added experimental support for cubic bezier curves in TrueType glyf table, as outlined in glyf v1 proposal (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2988"">#2988</a>):<br />; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-cubicOutlines.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-cubicOutlines.md</a></li>; <li>Added new qu2cu module and related qu2cuPen, the reverse of cu2qu for converting TrueType quadratic splines to cubic bezier curves (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2993"">#2993</a>).</li>; <li>[glyf] Added experimental support for reading and writing Variable Composites/Components as defined in glyf v1 spec proposal (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2958"">#2958</a>):<br />; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-varComposites.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-varComposites.md</a>.</li>; <li>[pens]: Added <code>addVarComponent</code> method to pen protocols' base classes, which pens can implement to handle varcomponents (by default they get decomposed).</li>; <li>[misc.transform] Added DecomposedTransform class which implements an affine transformation with separate translate, rotation, scale, skew, and transformation-center components (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2598"">#2598</a>)</li>; <li>[sbix] Ensure Glyph.referenceGlyphName is set; fixes error after dumping and re-compiling sbix table with 'dupe' glyphs (<a href=""https://redirect.github.com/fonttools/fonttools/issues/2984"">#2984</a>).</li>; <li>[feaLib] ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:6175,Variab,Variable,6175,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['Variab'],['Variable']
Modifiability,"elt`; the following stream state will be `s1`. **Design Notes**. - The `Skip` return is very useful for simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple functions. Instead of `init` ""returning"" `Missing`/`Start(s0)` (which is impossible, since these are compile time data structures; they won't exist during JVM runtime), init takes a continuation `k: Init[S] => Code[Ctrl]`, which it must call with one of these values. To use `init`, you can simply provide a pattern matching lambda, which closely resembles the syntax you would normally use to pattern match on a returned value, e.g. (from `contMap`):; ```scala; self.init(mb, jb, param) {; case Missing => k(Missing); case Start(s) => Code(setup, k(Start(s))); };",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7228:2626,Parameteriz,Parameterized,2626,https://hail.is,https://github.com/hail-is/hail/pull/7228,1,['Parameteriz'],['Parameterized']
Modifiability,"em)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3352,config,configuration,3352,https://hail.is,https://github.com/hail-is/hail/pull/8513,2,['config'],"['configuration', 'configure']"
Modifiability,"emoving batch workers' reliance on the docker daemon and docker in general, in favor of a lower level of abstraction that gives us finer control over resources on the worker like overlays and network namespaces, allowing us to shortcut and pre-configure some of the overhead that goes into running a job. ## What this does differently; Currently, the high-level process for running a job involves communicating with the docker daemon to:; 1. Pull an image for a job; 2. Start a container from that image; 3. Run the container; 4. Delete the container and its associated resources. We offload some of these responsibilities into the worker code and onto [crun](https://github.com/containers/crun), a lightweight low-level runtime with the same API as `runc`, what docker uses to run containers. Once docker has retrieved an image, if we see that the pulled image has a new digest from one we currently have cached on the worker, we extract the image's filesystem into a directory on the worker's disk. We then:. - use `mount` to create an overlay on top of the image that the container will use as its rootfs; - use `xfs_quota` to limit the container's storage in the overlay; - invoke `crun` to run a container with the overlay as its root filesystem and an appropriate network namespace that we set up at worker-start time. Since we control the overlay, we can set the XFS quota before creating the container. So what was separate calls to docker create/start/run/delete is just a single `crun run`. Fewer steps, less back and forth with a single daemon, and pre-configuring the networks gives some sizable performance gains reliable, as well as reliable and consistent performance. ## What this doesn't solve; - Docker is still running the worker container. I don't see any real challenge to this it's just a matter of translating the docker parameters; - Still using docker to pull images and extract filesystems / environment variables from them. I don't have a substitute for this at the moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10376:1591,config,configuring,1591,https://hail.is,https://github.com/hail-is/hail/pull/10376,2,"['config', 'variab']","['configuring', 'variables']"
Modifiability,"en deprecated for several years.; :meth:<code>~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey.public_bytes</code>; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey.from_encoded_point</code>; should be used instead.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Support for using MD5 or SHA1 in; :class:<code>~cryptography.x509.CertificateBuilder</code>, other X.509 builders, and; PKCS7 has been removed.</li>; <li><strong>BACKWARDS INCOMPATIBLE:</strong> Dropped support for macOS 10.10 and 10.11, macOS; users must upgrade to 10.12 or newer.</li>; <li><strong>ANNOUNCEMENT:</strong> The next version of <code>cryptography</code> (40.0) will change; the way we link OpenSSL. This will only impact users who build; <code>cryptography</code> from source (i.e., not from a <code>wheel</code>), and specify their; own version of OpenSSL. For those users, the <code>CFLAGS</code>, <code>LDFLAGS</code>,; <code>INCLUDE</code>, <code>LIB</code>, and <code>CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS</code> environment; variables will no longer be respected. Instead, users will need to; configure their builds <code>as documented here</code>_.</li>; <li>Added support for; :ref:<code>disabling the legacy provider in OpenSSL 3.0.x&lt;legacy-provider&gt;</code>.</li>; <li>Added support for disabling RSA key validation checks when loading RSA; keys via; :func:<code>~cryptography.hazmat.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:2354,variab,variables,2354,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['variab'],['variables']
Modifiability,"endabot.com/sveltejs/svelte/pull/7297"">#7297</a>)</li>; <li>Fix value of <code>let:</code> bindings not updating in certain cases (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7440"">#7440</a>)</li>; <li>Fix handling of void tags in <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7449"">#7449</a>)</li>; <li>Fix handling of boolean attributes in <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7478"">#7478</a>)</li>; <li>Add special style scoping handling of <code>[open]</code> selectors on <code>&lt;dialog&gt;</code> elements (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7494"">#7495</a>)</li>; </ul>; <h2>3.47.0</h2>; <ul>; <li>Add support for dynamic elements through <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/2324"">#2324</a>)</li>; <li>Miscellaneous variable context fixes in <code>{@const}</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7222"">#7222</a>)</li>; <li>Fix <code>{#key}</code> block not being reactive when the key variable is not otherwise used (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7408"">#7408</a>)</li>; <li>Add <code>Symbol</code> as a known global (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7418"">#7418</a>)</li>; </ul>; <h2>3.46.6</h2>; <ul>; <li>Actually include action TypeScript interface in published package (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7407"">#7407</a>)</li>; </ul>; <h2>3.46.5</h2>; <ul>; <li>Add TypeScript interfaces for typing actions (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/6538"">#6538</a>)</li>; <li>Do not generate <code>unused-export-let</code> warning inside <code>&lt;script context=&quot;module&quot;&gt;</code> blocks (<a href=""https://github-redirect.depe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12032:4082,variab,variable,4082,https://hail.is,https://github.com/hail-is/hail/pull/12032,3,['variab'],['variable']
Modifiability,"endabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; <li>fix <code>contrib.concurrent</code> with generators (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1233"">#1233</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1231"">#1231</a>)</li>; </ul>; <h2>tqdm v4.62.1 stable</h2>; <ul>; <li><code>contrib.logging</code>: inherit existing handler output stream (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1191"">#1191</a>)</li>; <li>fix <code>PermissionError</code> by using <code>weakref</code> in <code>DisableOnWriteError</code> (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1207"">#1207</a>)</li>; <li>fix <code>contrib.telegram</code> creation rate limit handling (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1223"">#1223</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1221"">#1221</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1220"">#1220</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1076"">#1076</a>)</li>; <li>tests: fix py27 <code>keras</code> dependencies (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1222"">#1222</a>)</li>; <li>misc tidy: use relative imports (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1222"">#1222</a>)</li>; <li>minor documentation updates (<a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11587:2384,inherit,inherit,2384,https://hail.is,https://github.com/hail-is/hail/pull/11587,1,['inherit'],['inherit']
Modifiability,"ensure we always return to the original working directory. I also fixed the path. - Use pytest [recommended directory structure](https://docs.pytest.org/en/latest/goodpractices.html) when your tests directory is a python module [1]. In particular, we now use:. ```; python/; - setup.py; - src/; - hail/; - __init__.py; - ...; - tests/; - __init__.py; - ...; ```. - Calculate number of cores using python's multiprocessing and use that as a default PARALLELISM parameter. - Move non-java/scala specific functionality out of `build.gradle` and into a `Makefile`. - The resulting rules are more succinct and correctly rely on file-system modification dependencies. - No use of `SPARK_HOME` and `PYTHONPATH`, and limited use of `PYSPARK_SUBMIT_ARGS`. Python tests now rely on the python package directly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:1750,config,configuration,1750,https://hail.is,https://github.com/hail-is/hail/pull/5130,1,['config'],['configuration']
Modifiability,"er --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2107,Plugin,Plugins,2107,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['Plugin'],['Plugins']
Modifiability,"er.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:16017,Plugin,Plugins,16017,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,er.scala:8); at is.hail.expr.ir.lowering.LowerBlockMatrixIR$.apply(LowerBlockMatrixIR.scala:1067); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:33); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.apply(LoweringPass.scala:86); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:345); at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:389); at is.hail.backend.service.ServiceBackendAPI.$anonfun$doAction$3(ServiceBackend.scala:610); at is.hail.backend.service.ServiceBackendAPI.withIRFunctionsReadFromInput(ServiceBackend.scala:655); at is.hail.backend.service.ServiceBackendAPI.$anonfun$doAction$2(ServiceBackend.scala:609); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); at is.hail.utils.package$.using(package.scala:664); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); at is.hail.uti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:9040,adapt,adapted,9040,https://hail.is,https://github.com/hail-is/hail/issues/14537,1,['adapt'],['adapted']
Modifiability,erEventProcessLoop.onReceive(DAGScheduler.scala:2792); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2257); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2289); 	at is.hail.sparkextras.ContextRDD.crunJobWithIndex(ContextRDD.scala:238); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1029); 	at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1104); 	at is.hail.rvd.RVD$.coerce(RVD.scala:1060); 	at is.hail.rvd.RVD.changeKey(RVD.scala:142); 	at is.hail.rvd.RVD.changeKey(RVD.scala:135); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:716); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:143); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:17); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:12044,Rewrite,RewriteBottomUp,12044,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Rewrite'],['RewriteBottomUp']
Modifiability,erOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpser,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:13488,adapt,adapted,13488,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['adapt'],['adapted']
Modifiability,ers.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4598,Rewrite,RewriteBottomUp,4598,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"es added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10260"">#10260</a>: Enable <code>FORCE_COLOR</code> and <code>NO_COLOR</code> for terminal colouring</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10234"">#10234</a>: autosummary: Add &quot;autosummary&quot; CSS class to summary tables</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10125"">#10125</a>: extlinks: Improve suggestion message for a reference having title</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10112"">#10112</a>: extlinks: Add :confval:<code>extlinks_detect_hardcoded_links</code> to enable; hardcoded links detector feature</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9494"">#9494</a>, <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9456"">#9456</a>: html search: Add a config variable; :confval:<code>html_show_search_summary</code> to enable/disable the search summaries</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9337"">#9337</a>: HTML theme, add option <code>enable_search_shortcuts</code> that enables :kbd:'/' as; a Quick search shortcut and :kbd:<code>Esc</code> shortcut that; removes search highlighting.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10107"">#10107</a>: i18n: Allow to suppress translation warnings by adding <code>#noqa</code>; comment to the tail of each translation message</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10252"">#10252</a>: C++, support attributes on classes, unions, and enums.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10253"">#10253</a>: :rst:dir:<code>pep</code> role now generates URLs based on peps.python.org</li>; </ul>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11714:2408,config,config,2408,https://hail.is,https://github.com/hail-is/hail/pull/11714,4,"['config', 'variab']","['config', 'variable']"
Modifiability,"es using multiple keys and logical operators.</li>; <li>api-change:<code>lakeformation</code>: [<code>botocore</code>] This release adds a new parameter &quot;Parameters&quot; in the DataLakeSettings.</li>; <li>api-change:<code>managedblockchain</code>: [<code>botocore</code>] Updating the API docs data type: NetworkEthereumAttributes, and the operations DeleteNode, and CreateNode to also include the supported Goerli network.</li>; <li>api-change:<code>proton</code>: [<code>botocore</code>] Add support for CodeBuild Provisioning</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] This release adds support for restoring an RDS Multi-AZ DB cluster snapshot to a Single-AZ deployment or a Multi-AZ DB instance deployment.</li>; <li>api-change:<code>workdocs</code>: [<code>botocore</code>] Added 2 new document related operations, DeleteDocumentVersion and RestoreDocumentVersions.</li>; <li>api-change:<code>xray</code>: [<code>botocore</code>] This release enhances GetServiceGraph API to support new type of edge to represent links between SQS and Lambda in event-driven applications.</li>; </ul>; <h1>1.26.8</h1>; <ul>; <li>api-change:<code>glue</code>: [<code>botocore</code>] Added links related to enabling job bookmarks.</li>; <li>api-change:<code>iot</code>: [<code>botocore</code>] This release add new api listRelatedResourcesForAuditFinding and new member type IssuerCertificates for Iot device device defender Audit.</li>; <li>api-change:<code>license-manager</code>: [<code>botocore</code>] AWS License Manager now supports onboarded Management Accounts or Delegated Admins to view granted licenses aggregated from all accounts in the organization.</li>; <li>api-change:<code>marketplace-catalog</code>: [<code>botocore</code>] Added three new APIs to support tagging and tag-based authorization: TagResource, UntagResource, and ListTagsForResource. Added optional parameters to the StartChangeSet API to support tagging a resource while making a request to create it.</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12466:1405,enhance,enhances,1405,https://hail.is,https://github.com/hail-is/hail/pull/12466,1,['enhance'],['enhances']
Modifiability,"es who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site mak",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:6546,config,config,6546,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config']
Modifiability,"et the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1887,Plugin,Plugins,1887,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"et.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:544); at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:509); at java.lang.Thread.run(Thread.java:750); ```. Interesting thing is, when I tried to convert the exactly same data in local computer using singularity instead of docker, it worked. Also, for the the other chromosomes with less variants but same samples, such as chr13, it worked well in Terra. Since we will convert multiple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9648,sandbox,sandbox,9648,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,"et.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:16095,Plugin,Plugins,16095,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,et.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:7636,Config,Configuration,7636,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"etails>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">com.google.cloud:google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2158"">#2158</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <ul>; <li>Implement BufferToDiskThenUpload BlobWriteSessionConfig (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2139"">#2139</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4dad2d5c3a81eda7190ad4f95316471e7fa30f66"">4dad2d5</a>)</li>; <li>Introduce new BlobWriteSession (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2123"">#2123</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e0191b518e50a49fae0691894b50f0c5f33fc6af"">e0191b5</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li><strong>grpc:</strong> Return error if credentials are detected to be null (<a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13605:5598,plugin,plugin,5598,https://hail.is,https://github.com/hail-is/hail/pull/13605,1,['plugin'],['plugin']
Modifiability,"eted` as it; has very low cardinality. In some forms of this query, the planner tries to use; it to its peril. A problem in query 0 with #14629 (see below) was that fewer filters on batches; made the optimiser consider joins in a suboptimal order - it did a table scan ; on `job_groups` first then sorted the results by to `batches.id DESC` instead; of doing an index scan on `batches` in reverse. Using `STRAIGHT_JOIN`s instead of `INNER JOIN` mades the optimiser start from; `batches` and read its index in reverse before considering other tables in ; subsequent joins. From the [documentation](https://dev.mysql.com/doc/refman/8.4/en/join.html):. > STRAIGHT_JOIN is similar to JOIN, except that the left table is always read; before the right table. This can be used for those (few) cases for which the; join optimizer processes the tables in a suboptimal order. This is advantageous for a couple of reasons:; - We want to list newer batches first; - For this query, the `batches` table has more applicables indexes; - We want the variable to order by to be in the primary key of the first; table so we can read the index in reverse. Before and after timings, collected by running the query 5 times, then using; profiles gathered by MySQL.; ```; +-------+---------------------------------------------------*; | query | description | ; +-------+---------------------------------------------------+; | 0 | All batches accessible to user `ci` |; | 1 | All batches accessible to user `ci` owned by `ci` |; +-------+---------------------------------------------------*. +-------+--------+--------------------------------------------------------+------------+------------+; | query | branch | timings | mean | stdev | ; +-------+--------+--------------------------------------------------------+------------+------------+; | 0 | main | 0.05894400,0.05207850,0.07067875,0.06281800,0.060250 | 0.06095385 | 0.00602255 |; | 1 | main | 14.1106150,12.2619323,13.8442850,12.0749633,14.0297822 | 13.2643156 | 0.9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14649:1319,variab,variable,1319,https://hail.is,https://github.com/hail-is/hail/pull/14649,1,['variab'],['variable']
Modifiability,"eters have been removed.; <code>download_name</code> replaces <code>attachment_filename</code>, <code>max_age</code>; replaces <code>cache_timeout</code>, and <code>etag</code> replaces <code>add_etags</code>.; Additionally, <code>path</code> replaces <code>filename</code> in; <code>send_from_directory</code>.</li>; <li>The <code>RequestContext.g</code> property returning <code>AppContext.g</code> is; removed.</li>; </ul>; </li>; <li>; <p>Update Werkzeug dependency to &gt;= 2.2.</p>; </li>; <li>; <p>The app and request contexts are managed using Python context vars; directly rather than Werkzeug's <code>LocalStack</code>. This should result; in better performance and memory use. :pr:<code>4682</code></p>; <ul>; <li>Extension maintainers, be aware that <code>_app_ctx_stack.top</code>; and <code>_request_ctx_stack.top</code> are deprecated. Store data on; <code>g</code> instead using a unique prefix, like; <code>g._extension_name_attr</code>.</li>; </ul>; </li>; <li>; <p>The <code>FLASK_ENV</code> environment variable and <code>app.env</code> attribute are; deprecated, removing the distinction between development and debug; mode. Debug mode should be controlled directly using the <code>--debug</code>; option or <code>app.run(debug=True)</code>. :issue:<code>4714</code></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/flask/commit/a1c478bc93d3dc018a6e7a1ba3cf5409553c9df3""><code>a1c478b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/flask/issues/4755"">#4755</a> from pallets/release-2.2.2</li>; <li><a href=""https://github.com/pallets/flask/commit/43d2fff317aec64a000604a764b8ab2dc751c753""><code>43d2fff</code></a> release version 2.2.2</li>; <li><a href=""https://github.com/pallets/flask/commit/e9af7c23ae19fcc50781b7711f4672c113636892""><code>e9af7c2</code></a> Merge pull request <a href=""https://github-re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12206:6969,variab,variable,6969,https://hail.is,https://github.com/hail-is/hail/pull/12206,1,['variab'],['variable']
Modifiability,"evan); Fixed: GITHUB-893: TestNG should provide an Api which allow to find all dependent of a specific test (Krishnan Mahadevan); New: Added .yml file extension for yaml suite files, previously only .yaml was allowed for yaml (Steven Jubb); Fixed: GITHUB-141: regular expression in &quot;dependsOnMethods&quot; does not work (Krishnan Mahadevan); Fixed: GITHUB-2770: FileAlreadyExistsException when report is generated (melloware); Fixed: GITHUB-2825: Programmatically Loading TestNG Suite from JAR File Fails to Delete Temporary Copy of Suite File (Steven Jubb); Fixed: GITHUB-2818: Add configuration key for callback discrepancy behavior (Krishnan Mahadevan); Fixed: GITHUB-2819: Ability to retry a data provider in case of failures (Krishnan Mahadevan); Fixed: GITHUB-2308: StringIndexOutOfBoundsException in findClassesInPackage - Surefire/Maven - JDK 11 fails (Krishnan Mahadevan); Fixed: GITHUB:2788: TestResult.isSuccess() is TRUE when test fails due to expectedExceptions (Krishnan Mahadevan); Fixed: GITHUB-2800: Running Test Classes with Inherited <a href=""https://github.com/Factory""><code>@​Factory</code></a> and <a href=""https://github.com/DataProvider""><code>@​DataProvider</code></a> Annotated Non-Static Methods Fail (Krishnan Mahadevan); New: Ability to provide custom error message for assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: GITHUB-2830: TestHTMLReporter parameter toString should be fail-safe (Sergey Chernov); Fixed: GITHUB-2798: Parallel executions coupled with retry analyzer results in duplicate retry analyzer instances being created (Krishnan Mahadevan)</p>; <p>7.6.1; Fixed: GITHUB-2761: Exception: ERROR java.nio.file.NoSuchFileException: /tmp/testngXmlPathInJar-15086412835569336174 (Krishnan Mahadevan); 7.6.0; Fixed: GITHUB-2741: Show fully qualified name",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:11567,Inherit,Inherited,11567,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['Inherit'],['Inherited']
Modifiability,"eve breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initializ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:2232,refactor,refactoring,2232,https://hail.is,https://github.com/hail-is/hail/pull/3727,1,['refactor'],['refactoring']
Modifiability,export_elasticsearch does not accept config argument,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4063:37,config,config,37,https://hail.is,https://github.com/hail-is/hail/issues/4063,1,['config'],['config']
Modifiability,"ext.autosummary.import_by_name()</code> now raises; <code>ImportExceptionGroup</code> instead of <code>ImportError</code> when it failed to import; target object. Please handle the exception if your extension uses the; function to import Python object. As a workaround, you can disable the; behavior via <code>grouped_exception=False</code> keyword argument until v7.0.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9962"">#9962</a>: texinfo: Customizing styles of emphasized text via <code>@definfoenclose</code>; command was not supported because the command was deprecated since texinfo 6.8</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/2068"">#2068</a>: :confval:<code>intersphinx_disabled_reftypes</code> has changed default value; from an empty list to <code>['std:doc']</code> as avoid too surprising silent; intersphinx resolutions.; To migrate: either add an explicit inventory name to the references; intersphinx should resolve, or explicitly set the value of this configuration; variable to an empty list.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10197"">#10197</a>: html theme: Reduce <code>body_min_width</code> setting in basic theme to 360px</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9999"">#9999</a>: LaTeX: separate terms from their definitions by a CR (refs: <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9985"">#9985</a>)</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10062"">#10062</a>: Change the default language to <code>'en'</code> if any language is not set in; <code>conf.py</code></li>; </ul>; <p>5.0.0 final</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10474"">#10474</a>: :confval:<code>language</code> does not accept <code>None</code> as it value. The default; value of <code>language</code> becomes to <code>'en",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11871:2623,config,configuration,2623,https://hail.is,https://github.com/hail-is/hail/pull/11871,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"ext.autosummary.import_by_name()</code> now raises; <code>ImportExceptionGroup</code> instead of <code>ImportError</code> when it failed to import; target object. Please handle the exception if your extension uses the; function to import Python object. As a workaround, you can disable the; behavior via <code>grouped_exception=False</code> keyword argument until v7.0.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9962"">#9962</a>: texinfo: Customizing styles of emphasized text via <code>@definfoenclose</code>; command was not supported because the command was deprecated since texinfo 6.8</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/2068"">#2068</a>: :confval:<code>intersphinx_disabled_reftypes</code> has changed default value; from an empty list to <code>['std:doc']</code> as avoid too surprising silent; intersphinx resolutions.; To migrate: either add an explicit inventory name to the references; intersphinx should resolve, or explicitly set the value of this configuration; variable to an empty list.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10197"">#10197</a>: html theme: Reduce <code>body_min_width</code> setting in basic theme to 360px</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9999"">#9999</a>: LaTeX: separate terms from their definitions by a CR (refs: <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9985"">#9985</a>)</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10062"">#10062</a>: Change the default language to <code>'en'</code> if any language is not set in; <code>conf.py</code></li>; </ul>; <p>5.0.0 final</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10474"">#10474</a>: :confval:<code>language</code> does not accept <code>None</code> as it value. The default</li>; </ul>; <!-- raw HTML omitted -->; </blockquote",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11885:3422,config,configuration,3422,https://hail.is,https://github.com/hail-is/hail/pull/11885,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,extend FilterVariants to extract an arbitrary subset of positions or variants as listed in a file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/114:0,extend,extend,0,https://hail.is,https://github.com/hail-is/hail/issues/114,1,['extend'],['extend']
Modifiability,extend `missing` argument in import_table to accept a list of missing values,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5612:0,extend,extend,0,https://hail.is,https://github.com/hail-is/hail/issues/5612,1,['extend'],['extend']
Modifiability,"external_data_disk_size_gb` in the PoolConfig; - Added {Azure,GCP}UserCredentials to the worker to abstract away the names of environment variables and the mount paths of credentials in containers. ## Auth; - Added new fields in the auth database for `azure service principal name` and `azure_credentials_secret_name`; - Made `auth` only create `GSAResource` if CLOUD == 'gcp'. ## Gear; - Added `azure-vm` to the location options for `DeployConfig`. # Assumptions:; - Mapped `{'lowmen': 'F', 'standard': 'D', 'highmem': 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10970:1360,config,config,1360,https://hail.is,https://github.com/hail-is/hail/pull/10970,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"f0d9453391f1f13""><code>213e006</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1653"">#1653</a> from asottile/lower-bound-importlib-metadata</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/e94ee2b5f1801354b940cfe830b9160852915aec""><code>e94ee2b</code></a> require sufficiently new importlib-metadata</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/318a86a4a12d28cdfc41030ead547332d3461f45""><code>318a86a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1646"">#1646</a> from televi/main</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/7b8b374c9bc1a141ca7cf6670c8cee6708398490""><code>7b8b374</code></a> Clarify entry point naming</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/7160561028ee5db09ea58b8555db08c66ee092b3""><code>7160561</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1649"">#1649</a> from PyCQA/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/84d56a8c25106b5e0a41cdf63b5de261f8da5c99""><code>84d56a8</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/ff6569b87db8ae28c41b548071454de620ad14d5""><code>ff6569b</code></a> Release 5.0.3</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/e76b59ae44f46f7958d13b28bd2d7d9bdc0f5962""><code>e76b59a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1648"">#1648</a> from PyCQA/invalid-syntax-partial-parse</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/25e8ff18b30b58f1dabc1d20546ebc20fd775560""><code>25e8ff1</code></a> ignore config files that partially parse as flake8 configs</li>; <li>Additional commits viewable in <a href=""https://github.com/pycqa/flake8/compare/4.0.1...5.0.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12105:1347,config,config,1347,https://hail.is,https://github.com/hail-is/hail/pull/12105,1,['config'],['config']
Modifiability,f2e99865&quot;&gt;&lt;code&gt;a1639ef&lt;/code&gt;&lt;/a&gt; Update CHANGES.rst (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/411&quot;&gt;#411&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/727b305a5707a937b427894360eba11c402b1755&quot;&gt;&lt;code&gt;727b305&lt;/code&gt;&lt;/a&gt; Enable camelcase eslint rule (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/410&quot;&gt;#410&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/dade11a7a1281ca3060bf1149fdc1d6d0763c97e&quot;&gt;&lt;code&gt;dade11a&lt;/code&gt;&lt;/a&gt; fixed css sort tringles (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/409&quot;&gt;#409&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/e00532d9c8a598fb848d16b0ce23665789e3517a&quot;&gt;&lt;code&gt;e00532d&lt;/code&gt;&lt;/a&gt; Use scss nesting &amp;amp; variables (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/393&quot;&gt;#393&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/9bd4907682f10849dde1fe866b5a71402c74e551&quot;&gt;&lt;code&gt;9bd4907&lt;/code&gt;&lt;/a&gt; remove all read the doc documentation from the repo (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/405&quot;&gt;#405&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/21fafe40b19e7d1c8b4b8bceb7fe1410e2cbdc2a&quot;&gt;&lt;code&gt;21fafe4&lt;/code&gt;&lt;/a&gt; Document how to modify the environment section after tests are finished (&lt;a href=&quot;https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/400&quot;&gt;#400&lt;/a&gt;)&lt;/li&gt;; &lt;li&gt;&lt;a href=&quot;https://github.com/pytest-dev/pytest-html/commit/8b7bdc1fc5f21e946,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11524:14582,variab,variables,14582,https://hail.is,https://github.com/hail-is/hail/pull/11524,1,['variab'],['variables']
Modifiability,"f=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/316"">#316</a>) (<a href=""https://github.com/googleapis/python-logging/commit/5267152574b2ee96eb6f5c536a762f58bd2f886e"">5267152</a>)</li>; <li>support string-encoded json (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/339"">#339</a>) (<a href=""https://github.com/googleapis/python-logging/commit/6fa17735fe3edb45483ec5e3abd1f53c24ffa881"">6fa1773</a>)</li>; <li>trace improvements (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/450"">#450</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e0c5fc02160ae87faf4ba5c2b62be86de6b02cf3"">e0c5fc0</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>allow reading logs from non-project paths (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/444"">#444</a>) (<a href=""https://github.com/googleapis/python-logging/commit/97e32b67603553fe350b6327455fc9f80b8aa6ce"">97e32b6</a>)</li>; <li>api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e1506fa9030776353878048ce562c53bf6ccf7bf"">e1506fa</a>)</li>; </ul>; <h3>Miscellaneous Chores</h3>; <ul>; <li>deprecate AppEngineHandler and ContainerEngineHandler (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/310"">#310</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e3cac888d40bf67af11e57b74615b0c3b8e8aa3e"">e3cac88</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>update usage guide for v3.0.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/456"">#456</a>) (<a href=""https://github.com/googleapis/python-logging/commit/8a67b73cdfcb9da545671be6cf59c724360b1544"">8a67b73</a>)</li>; </ul>; <h2><a href=""https://www.github.com/googleapis/python-logging/compare/v2.6.0...v2.7.0"">2.7.0</a> (2021-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:9170,layers,layers,9170,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['layers'],['layers']
Modifiability,"f=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/316"">#316</a>) (<a href=""https://github.com/googleapis/python-logging/commit/5267152574b2ee96eb6f5c536a762f58bd2f886e"">5267152</a>)</li>; <li>support string-encoded json (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/339"">#339</a>) (<a href=""https://github.com/googleapis/python-logging/commit/6fa17735fe3edb45483ec5e3abd1f53c24ffa881"">6fa1773</a>)</li>; <li>trace improvements (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/450"">#450</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e0c5fc02160ae87faf4ba5c2b62be86de6b02cf3"">e0c5fc0</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>allow reading logs from non-project paths (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/444"">#444</a>) (<a href=""https://github.com/googleapis/python-logging/commit/97e32b67603553fe350b6327455fc9f80b8aa6ce"">97e32b6</a>)</li>; <li>api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e1506fa9030776353878048ce562c53bf6ccf7bf"">e1506fa</a>)</li>; </ul>; <h3>Miscellaneous Chores</h3>; <ul>; <li>deprecate AppEngineHandler and ContainerEngineHandler (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/310"">#310</a>) (<a href=""https://github.com/googleapis/python-logging/commit/e3cac888d40bf67af11e57b74615b0c3b8e8aa3e"">e3cac88</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>update usage guide for v3.0.0 (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/456"">#456</a>) (<a href=""https://github.com/googleapis/python-logging/commit/8a67b73cdfcb9da545671be6cf59c724360b1544"">8a67b73</a>)</li>; </ul>; <h2>v2.7.0</h2>; <h3>Features</h3>; <ul>; <li>add context manager support in client (<a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:3737,layers,layers,3737,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['layers'],['layers']
Modifiability,"f=""https://github-redirect.dependabot.com/prometheus/client_python/issues/747"">#747</a>; [BUGFIX] Remove trailing slashes from pushgateway URLS. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/722"">#722</a>; [BUGFIX] Catch non-integer bucket/count values. <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/726"">#726</a></p>; <h2>0.12.0 / 2021-10-29</h2>; <p>[FEATURE] Exemplar support (excludes multiprocess) <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/669"">#669</a>; [ENHANCEMENT] Add support for Python 3.10 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/706"">#706</a>; [ENHANCEMENT] Restricted Registry will handle metrics added after restricting <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/675"">#675</a>, <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/680"">#680</a><br />; [ENHANCEMENT] Raise a more helpful error if a metric is not observable <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/666"">#666</a>; [BUGFIX] Fix instance_ip_grouping_key not working on MacOS <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/687"">#687</a>; [BUGFIX] Fix assertion error from favicion.ico with Python 2.7 <a href=""https://github-redirect.dependabot.com/prometheus/client_python/issues/715"">#715</a></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/prometheus/client_python/commit/a234283a853238dc73fa22651532590330fd72a1""><code>a234283</code></a> Release 0.13.1</li>; <li><a href=""https://github.com/prometheus/client_python/commit/557d123b349f3881cd6475a29ff4c79088a85a26""><code>557d123</code></a> Relax type constraints Timestamp</li>; <li><a href=""https://github.com/prometheus/client_python/commit/b44b63e59b168c6a8498ca31ddcce3ea5e46dcdc""><code>b44b63e</code></a> Declare <code>reg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11515:2458,ENHANCE,ENHANCEMENT,2458,https://hail.is,https://github.com/hail-is/hail/pull/11515,1,['ENHANCE'],['ENHANCEMENT']
Modifiability,"f=""https://github.com/afrouzMashaykhi""><code>@​afrouzMashaykhi</code></a>) [SIG API Machinery, Cluster Lifecycle, Instrumentation and Node]</li>; <li>Kubernetes is now built with golang 1.15.0-rc.1.; <ul>; <li>The deprecated, legacy behavior of treating the CommonName field on X.509 serving certificates as a host name when no Subject Alternative Names are present is now disabled by default. It can be temporarily re-enabled by adding the value x509ignoreCN=0 to the GODEBUG environment variable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/93264"">kubernetes/kubernetes#93264</a>, <a href=""https://github.com/justaugustus""><code>@​justaugustus</code></a>) [SIG API Machinery, Auth, CLI, Cloud Provider, Cluster Lifecycle, Instrumentation, Network, Node, Release, Scalability, Storage and Testing]</li>; </ul>; </li>; <li>Promote Immutable Secrets/ConfigMaps feature to Beta and enable the feature by default.; This allows to set <code>Immutable</code> field in Secrets or ConfigMap object to mark their contents as immutable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89594"">kubernetes/kubernetes#89594</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Apps and Testing]</li>; <li>Remove <code>BindTimeoutSeconds</code> from schedule configuration <code>KubeSchedulerConfiguration</code> (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91580"">kubernetes/kubernetes#91580</a>, <a href=""https://github.com/cofyc""><code>@​cofyc</code></a>) [SIG Scheduling and Testing]</li>; <li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavinfish""><code>@​gavinfish</code></a>) [SIG Scheduling]</li>; <li>Reserve plugins that fail to reserve will trigger the unreserve extension point (<a href=""https://github-redirect.dependabot.com/kubernetes/kubern",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:11283,Config,ConfigMap,11283,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['Config'],['ConfigMap']
Modifiability,"f=""https://redirect.github.com/googleapis/java-storage/issues/2196"">#2196</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5"">4f8bb65</a>)</li>; </ul>; <h2>v2.26.1</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2158"">#2158</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2>v2.26.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">com.google.cloud:google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.1...v2.27.0"">2.27.0</a> (2023-09-12)</h2>; <h3>Features</h3>; <ul>; <li>Add new JournalingBlobWriteSessionConfig usable with gRPC transport (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2194"">#2194</a>) (<a href=""https://githu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:5623,plugin,plugin,5623,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,"fe to delete it all and start clean), `build.gradle`, `gradle/`, `gradlew`, `gradlew.bat`, `pgradle`, `settings.gradle`; * run `mill mill.bsp.BSP/install` to generate the `.bsp` config directory (bsp is the Build Server Protocol); * In IntelliJ, go to File->Open, and choose the hail root directory; * When the project is open, go to File->Project Structure; * in the Project pane, set an sdk (8 or 11), and set the language level to 8; * in the Modules pane, delete the existing root module, click the plus sign -> Import Module, choose the `hail/` subdirectory, and choose ""Import module from external model"" and `BSP`; * you should see a progress bar at the bottom as it imports the project; * when it's done, quit and reopen IntelliJ. There should now be a bsp icon (two bars with two arrows between them) on the right, where the gradle elephant used to be. Just like before, sometimes you'll need to click the ""reload"" icon in there if things get wonky.; * if it says ""scalafmt configuration detected"", go ahead and enable the formatter. ## Metals setup. * delete any `.metals` directories; * open the hail repo in VSCode (even if you won't use VSCode, this seems to be the best way to get metals set up initially); * it should ask you to import a Mill build; * when that finishes, at the bottom it should say it's connected to a Bloop build server. In general, I think using Mill as the BSP directly will work best, but I don't have much experience to say for sure. To switch, run `Metals: switch build server` from the command palette. ## Debug and release builds. As before, debug mode adds some (fairly expensive) checking to our native memory system. But now there are a few other differences:; * treat warnings as errors only in release mode, so you can still compile, run tests, etc. during development without fixing all warnings; * enable optimization in scalac only in release mode. The intention is that we use debug mode during development, and release mode ony for published artifac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:4582,config,configuration,4582,https://hail.is,https://github.com/hail-is/hail/pull/14147,1,['config'],['configuration']
Modifiability,fer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at is.hail.lir.Classx.asBytes(X.scala:120); 	at is.hail.asm4s.ClassBuilder.classBytes(ClassBuilder.scala:488); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1(ClassBuilder.scala:166); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1$adapted(ClassBuilder.scala:166); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:9046,adapt,adapted,9046,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['adapt'],['adapted']
Modifiability,fer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at is.hail.lir.Classx.asBytes(X.scala:121); 	at is.hail.asm4s.ClassBuilder.classBytes(ClassBuilder.scala:394); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1(ClassBuilder.scala:151); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1$adapted(ClassBuilder.scala:151); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:3969,adapt,adapted,3969,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['adapt'],['adapted']
Modifiability,fer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at is.hail.lir.Classx.asBytes(X.scala:123); 	at is.hail.asm4s.ClassBuilder.classBytes(ClassBuilder.scala:351); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1(ClassBuilder.scala:151); 	at is.hail.asm4s.ModuleBuilder.$anonfun$classesBytes$1$adapted(ClassBuilder.scala:151); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49); 	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366); 	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364); 	at scala.collection.AbstractIterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:17420,adapt,adapted,17420,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,"fix encode/decode of non-wrapped values, some other refactoring changes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6759:52,refactor,refactoring,52,https://hail.is,https://github.com/hail-is/hail/pull/6759,1,['refactor'],['refactoring']
Modifiability,"fix-threaded-client</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/30ce7539778e2a25ff5e6eba4ccb6c08b8a0fe20""><code>30ce753</code></a> fix sphinx 5.0 support</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/a2e90574645052320de861bb84ba1752e25ef2dd""><code>a2e9057</code></a> ignore type error</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/3c6fc38e8dda754aba4a1217733eb1a0146b4c57""><code>3c6fc38</code></a> Run qtconsole test suite as a another downstream project</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/dcb45960b337fb089e04b0c3dde880e8f0f10ae5""><code>dcb4596</code></a> Revert changes related to _handle_recv in ThreadedZMQSocketChannel</li>; <li><a href=""https://github.com/jupyter/jupyter_client/commit/01bfdd18c2eb8ea34cbb9915cb2bc7d9806f81a4""><code>01bfdd1</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/799"">#799</a> from jupyter/pre-commit-ci-update-config</li>; <li>Additional commits viewable in <a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.1...v7.3.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jupyter-client&package-manager=pip&previous-version=7.3.1&new-version=7.3.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:9689,config,config,9689,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['config'],['config']
Modifiability,"fixed numeric aggregations behavior on empty arrays and sets; - modified min, max, mean, and median functions in FunctionRegistry; - added regression tests for empty sets and empty arrays to ExprSuite; - updated and refactored ExpressionLanguage docs to reflect changes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1289:216,refactor,refactored,216,https://hail.is,https://github.com/hail-is/hail/pull/1289,1,['refactor'],['refactored']
Modifiability,fixed sphinx input variable to not run tutorial,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1544:19,variab,variable,19,https://hail.is,https://github.com/hail-is/hail/pull/1544,1,['variab'],['variable']
Modifiability,"fixes #7418. The main change here is fixing #7418 by making nodes which perform aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overri",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7479:203,variab,variable,203,https://hail.is,https://github.com/hail-is/hail/pull/7479,5,['variab'],"['variable', 'variables']"
Modifiability,"for better or worse. I believe the behavior is the same and the code and user ergonomics seem better. The one known issue I haven't figured out to fix is the usage output on unknown flags:. ```; $ hailctl --fleep auth login; usage: hailctl [-h] {version,dataproc,auth,dev,batch,curl,config} ...; hailctl: error: unrecognized arguments: --fleep. $ hailctl auth --fleep login; usage: hailctl [-h] {version,dataproc,auth,dev,batch,curl,config} ...; hailctl: error: unrecognized arguments: --fleep; ```. This implies argparse should allow hailctl flags to appear after auth, but that isn't the case. For example:. ```; $ hailctl dataproc connect --beta foo nb; usage: hailctl [-h] {version,dataproc,auth,dev,batch,curl,config} ...; hailctl: error: unrecognized arguments: --beta; ```. I will keep investigating a fix for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9836:283,config,config,283,https://hail.is,https://github.com/hail-is/hail/pull/9836,3,['config'],['config']
Modifiability,"format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4269,config,configuration,4269,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"full control canned ACL to be set when Athena writes query results to S3 buckets.</li>; <li>api-change:<code>keyspaces</code>: [<code>botocore</code>] This release adds support for data definition language (DDL) operations</li>; <li>api-change:<code>ecr</code>: [<code>botocore</code>] This release adds support for tracking images lastRecordedPullTime.</li>; </ul>; <h1>1.21.10</h1>; <ul>; <li>api-change:<code>mediapackage</code>: [<code>botocore</code>] This release adds Hybridcast as an available profile option for Dash Origin Endpoints.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] Documentation updates for Multi-AZ DB clusters.</li>; <li>api-change:<code>mgn</code>: [<code>botocore</code>] Add support for GP3 and IO2 volume types. Add bootMode to LaunchConfiguration object (and as a parameter to UpdateLaunchConfigurationRequest).</li>; <li>api-change:<code>kafkaconnect</code>: [<code>botocore</code>] Adds operation for custom plugin deletion (DeleteCustomPlugin) and adds new StateDescription field to DescribeCustomPlugin and DescribeConnector responses to return errors from asynchronous resource creation.</li>; </ul>; <h1>1.21.9</h1>; <ul>; <li>api-change:<code>finspace-data</code>: [<code>botocore</code>] Add new APIs for managing Users and Permission Groups.</li>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Add repositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:2503,plugin,plugin,2503,https://hail.is,https://github.com/hail-is/hail/pull/11486,2,['plugin'],['plugin']
Modifiability,"fused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this glob",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3256,config,configuration,3256,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"fy entry point naming</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/7160561028ee5db09ea58b8555db08c66ee092b3""><code>7160561</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1649"">#1649</a> from PyCQA/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/84d56a8c25106b5e0a41cdf63b5de261f8da5c99""><code>84d56a8</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/ff6569b87db8ae28c41b548071454de620ad14d5""><code>ff6569b</code></a> Release 5.0.3</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/e76b59ae44f46f7958d13b28bd2d7d9bdc0f5962""><code>e76b59a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pycqa/flake8/issues/1648"">#1648</a> from PyCQA/invalid-syntax-partial-parse</li>; <li><a href=""https://github.com/PyCQA/flake8/commit/25e8ff18b30b58f1dabc1d20546ebc20fd775560""><code>25e8ff1</code></a> ignore config files that partially parse as flake8 configs</li>; <li>Additional commits viewable in <a href=""https://github.com/pycqa/flake8/compare/4.0.1...5.0.4"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=flake8&package-manager=pip&previous-version=4.0.1&new-version=5.0.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12105:2056,config,config,2056,https://hail.is,https://github.com/hail-is/hail/pull/12105,2,['config'],"['config', 'configs']"
Modifiability,"fyi @cseed . All of the auxilliary methods have the same arguments as the original emitted function, to preserve the behavior of In and aggregator stuff without change. Any IR that binds a new environment variable (`Let`, `Map`, etc.) now stores those values on a class field so that other methods don't have to worry about whether or not there exist such values in scope. That's basically the only change I made to the structure of Emit; `MakeArray`, `MakeStruct`, and `MakeTuple` now check how big their fields/elements are. I'm a little worried we'll start hitting the class size limit; turning up one of the tests really far already hits it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3308:205,variab,variable,205,https://hail.is,https://github.com/hail-is/hail/pull/3308,1,['variab'],['variable']
Modifiability,g 'hailtop/aiotools/fs/exceptions.py'; adding 'hailtop/aiotools/fs/fs.py'; adding 'hailtop/aiotools/fs/stream.py'; adding 'hailtop/auth/__init__.py'; adding 'hailtop/auth/auth.py'; adding 'hailtop/auth/sql_config.py'; adding 'hailtop/auth/tokens.py'; adding 'hailtop/batch/__init__.py'; adding 'hailtop/batch/backend.py'; adding 'hailtop/batch/batch.py'; adding 'hailtop/batch/batch_pool_executor.py'; adding 'hailtop/batch/conftest.py'; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py';,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:10808,config,config,10808,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['config'],['config']
Modifiability,"g JUnit 4 Tests using TestNG (Krishnan Mahadevan); Fixed: GITHUB-2847: Deprecate support for running JUnit tests (Krishnan Mahadevan); Fixed: GITHUB-2844: Deprecate support for running Spock Tests (Krishnan Mahadevan); Fixed: GITHUB-550: Weird <a href=""https://github.com/BeforeMethod""><code>@​BeforeMethod</code></a> and <a href=""https://github.com/AfterMethod""><code>@​AfterMethod</code></a> behaviour with dependsOnMethods (Krishnan Mahadevan); Fixed: GITHUB-893: TestNG should provide an Api which allow to find all dependent of a specific test (Krishnan Mahadevan); New: Added .yml file extension for yaml suite files, previously only .yaml was allowed for yaml (Steven Jubb); Fixed: GITHUB-141: regular expression in &quot;dependsOnMethods&quot; does not work (Krishnan Mahadevan); Fixed: GITHUB-2770: FileAlreadyExistsException when report is generated (melloware); Fixed: GITHUB-2825: Programmatically Loading TestNG Suite from JAR File Fails to Delete Temporary Copy of Suite File (Steven Jubb); Fixed: GITHUB-2818: Add configuration key for callback discrepancy behavior (Krishnan Mahadevan); Fixed: GITHUB-2819: Ability to retry a data provider in case of failures (Krishnan Mahadevan); Fixed: GITHUB-2308: StringIndexOutOfBoundsException in findClassesInPackage - Surefire/Maven - JDK 11 fails (Krishnan Mahadevan); Fixed: GITHUB:2788: TestResult.isSuccess() is TRUE when test fails due to expectedExceptions (Krishnan Mahadevan); Fixed: GITHUB-2800: Running Test Classes with Inherited <a href=""https://github.com/Factory""><code>@​Factory</code></a> and <a href=""https://github.com/DataProvider""><code>@​DataProvider</code></a> Annotated Non-Static Methods Fail (Krishnan Mahadevan); New: Ability to provide custom error message for assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: G",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:11107,config,configuration,11107,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['config'],['configuration']
Modifiability,g.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annota,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:17855,adapt,adapted,17855,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,g.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:8301,adapt,adapted,8301,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,gcloud uses this variable to find the tokens file. We should set it so; that users do not need to configure anything. I also fixed some; missing arguments in the non-async get_userinfo.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9737:17,variab,variable,17,https://hail.is,https://github.com/hail-is/hail/pull/9737,2,"['config', 'variab']","['configure', 'variable']"
Modifiability,"ge:. ```; Traceback (most recent call last):; File ""…/borkscript.py"", line 8, in <module>; my_job, my_output = make_job(batch); File ""…/site-packages/hailtop/batch/job.py"", line 125, in __getitem__; return self._get_resource(item); File ""…/site-packages/hailtop/batch/job.py"", line 118, in _get_resource; r = self._batch._new_job_resource_file(self, value=item); File ""…/site-packages/hailtop/batch/batch.py"", line 405, in _new_job_resource_file; jrf = _resource.JobResourceFile(value, source); File ""…/site-packages/hailtop/batch/resource.py"", line 128, in __init__; super().__init__(value); File ""…/site-packages/hailtop/batch/resource.py"", line 48, in __init__; assert value is None or isinstance(value, str); AssertionError; ```. Of course, in a 400-line script it took a long while to figure out what the traceback that seemed to have little to do with any dubious code of ours was trying to tell us, and to notice that the actual problem was the `return` 200 lines away!. The problem is that these classes define `__getitem__()` so their resources can be accessed as if via a dict. The assignment into multiple variables causes Python to try to interpret the RHS as something iterable, and as `__getitem__` is defined, it will use `__getitem__(0)`, `__getitem__(1)`,... to implement that iteration. These classes are not really iterable, so define a no-op `__iter__()` to prevent this. With this, we get:. ```; Traceback (most recent call last):; File ""…/borkscript.py"", line 8, in <module>; my_job, my_output = make_job(batch); File ""…/site-packages/hailtop/batch/job.py"", line 127, in __iter__; raise TypeError(f'{type(self).__name__!r} object is not iterable'); TypeError: 'BashJob' object is not iterable; ```. Which, while still not pointing directly at the problem, is much clearer. Especially for ResourceGroup, it may be worth defining iteration for these classes in future. But at the moment `__getitem__`-based iteration fails, so this ensures it fails with a clear TypeError message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14390:1449,variab,variables,1449,https://hail.is,https://github.com/hail-is/hail/pull/14390,1,['variab'],['variables']
Modifiability,"gen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach `BgenInputFormat` to produce splits that describe which variants to keep; - modify `BgenRecord` to use variant filtering information in file splits; - add some groady code to ship variant filter lists through the hadoop configuration",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3813:2617,config,configuration,2617,https://hail.is,https://github.com/hail-is/hail/pull/3813,1,['config'],['configuration']
Modifiability,generic intervals 2: add extended ordering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2675:25,extend,extended,25,https://hail.is,https://github.com/hail-is/hail/pull/2675,1,['extend'],['extended']
Modifiability,"github.com/pyasn1/pyasn1-modules/commit/7d8e520aa7d0e71ef7144ce381c8a41464e687dc""><code>7d8e520</code></a> Modernize build and test infra (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/2"">#2</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/51f5bfe83178871fe2ee80df6b8e13ed54a2d897""><code>51f5bfe</code></a> Add GitHub Actions CI, test with 3.9 to 3.11 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/1"">#1</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/bdbcc5d9650a8e8382979f089df3307dd4121b49""><code>bdbcc5d</code></a> Bump up coverage percentage cut at tox</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/7c7e4add6cb9f1a47a2303f819c8472491f6ebbb""><code>7c7e4ad</code></a> Add support for RFC 8769 (<a href=""https://redirect.github.com/pyasn1/pyasn1-modules/issues/136"">#136</a>)</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/13ca0da0cc4d0703ca42113f607bde95cf0bfd9c""><code>13ca0da</code></a> Fix tox deps inheritance</li>; <li><a href=""https://github.com/pyasn1/pyasn1-modules/commit/00fa3b9d15c783389afee7887f5ba3738a005545""><code>00fa3b9</code></a> Run unittests across many Python versions</li>; <li>Additional commits viewable in <a href=""https://github.com/pyasn1/pyasn1-modules/compare/v0.2.8...v0.3.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyasn1-modules&package-manager=pip&previous-version=0.2.8&new-version=0.3.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12928:8623,inherit,inheritance,8623,https://hail.is,https://github.com/hail-is/hail/pull/12928,1,['inherit'],['inheritance']
Modifiability,"gitt""><code>@​liggitt</code></a>) [SIG API Machinery]</li>; <li>EnvVarSource api doc bug fixes (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91194"">kubernetes/kubernetes#91194</a>, <a href=""https://github.com/wawa0210""><code>@​wawa0210</code></a>) [SIG Apps]</li>; <li>Fix bug in reflector that couldn't recover from &quot;Too large resource version&quot; errors (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92537"">kubernetes/kubernetes#92537</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG API Machinery]</li>; <li>Fixed: log timestamps now include trailing zeros to maintain a fixed width (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91207"">kubernetes/kubernetes#91207</a>, <a href=""https://github.com/iamchuckss""><code>@​iamchuckss</code></a>) [SIG Apps and Node]</li>; <li>Generic ephemeral volumes, a new alpha feature under the <code>GenericEphemeralVolume</code> feature gate, provide a more flexible alternative to <code>EmptyDir</code> volumes: as with <code>EmptyDir</code>, volumes are created and deleted for each pod automatically by Kubernetes. But because the normal provisioning process is used (<code>PersistentVolumeClaim</code>), storage can be provided by third-party storage vendors and all of the usual volume features work. Volumes don't need to be empt; for example, restoring from snapshot is supported. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92784"">kubernetes/kubernetes#92784</a>, <a href=""https://github.com/pohly""><code>@​pohly</code></a>) [SIG API Machinery, Apps, Auth, CLI, Instrumentation, Node, Scheduling, Storage and Testing]</li>; <li>Go1.14.4 is now the minimum version required for building Kubernetes (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92438"">kubernetes/kubernetes#92438</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG API Machinery, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:7314,flexible,flexible,7314,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['flexible'],['flexible']
Modifiability,"goes in ```configureAndCreateSparkContext```. ```sc.uiWebUrl.foreach { s => info(s""SparkUI started at $s"") }```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1142:11,config,configureAndCreateSparkContext,11,https://hail.is,https://github.com/hail-is/hail/issues/1142,1,['config'],['configureAndCreateSparkContext']
Modifiability,grafana and prometheus jobs for rendering their nginx configs were producing same name outputs so one would overwrite the other.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10181:54,config,configs,54,https://hail.is,https://github.com/hail-is/hail/pull/10181,1,['config'],['configs']
Modifiability,"h in dev namespaces and default. Currently, the grafana and prometheus pods have two containers: the app itself (grafana or prometheus) and an nginx container that sits in front of it. The flow is as follows, and since this works the exact same for both prometheus and grafana I will just talk about grafana as the example and the same thing should apply to both:. 1. User sends request to grafana.hail.is; 2. Gateway sees an HTTP request going to a production service and forwards that request to the grafana k8s Service port 443; 3. The grafana K8s Service forwards that request to the grafana pod port 443; 4. Nginx is listening on port 443 in the grafana pod and receives that request. It makes an authorization check to auth to make sure that the request is coming from a developer; 5. Nginx forwards that request to 127.0.0.1:3000, which is where grafana is listening. This PR does not change any behavior, just replaces Nginx with Envoy. Currently, building the nginx container involves running jinja on its config files and building a docker image. With envoy, we can just use the `envoyproxy/envoy` image from DockerHub (which I have copied into our container registries) and feed it a single configmap. The big mess of yaml that is the new configmap for envoy has a lot of boilerplate, but it comprises of the following sections which hopefully on their own are not too bad. ### Envoy config; 1. The top of the `listeners` section shows that Envoy is listening on port 8443 (which is the port that the k8s `Service` will now forward traffic to); 2. The `virtual_hosts` section shows that Envoy will send all paths (prefix ""/"") to the cluster `grafana`; 3. The `http_filters` section says that Envoy will first send an authorization request to the `auth` cluster before allowing the request to pass to grafana; 4. The `clusters` section says that there are two other services Envoy knows about and can send traffic to, one named `auth` that can be found at address `auth` (same as `https://a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12364:1282,config,config,1282,https://hail.is,https://github.com/hail-is/hail/pull/12364,1,['config'],['config']
Modifiability,"hadoop_open has a somewhat strange behavior, when the global fs is; HadoopFS, BGzip and Gzip files are handled transparently by file; extension, so python reads and writes uncompressed data. This is not the; case if the global fs is LocalFS or GoogleCloudStorageFS. We'd; eventually like to move away from this behavior for HadoopFS altogether,; but we cannot change the behavior of hadoop_open without breaking user; code. To that end, rewrite HadoopFS.open to ignore codecs, and add legacy_open; to preserve the old behavior. As a result of this, we also implement the seekable interface in python; for HadoopFS opened files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10812:437,rewrite,rewrite,437,https://hail.is,https://github.com/hail-is/hail/pull/10812,1,['rewrite'],['rewrite']
Modifiability,"hail % gsutil cp ./src/test/resources/ldprune2.vcf gs://danking/chr1.vcf; Copying file://./src/test/resources/ldprune2.vcf [Content-Type=text/x-vcard]...; / [1 files][ 11.5 KiB/ 11.5 KiB] ; Operation completed over 1 objects/11.5 KiB. ; (base) dking@wm28c-761 hail % gsutil cp ./src/test/resources/ldprune2.vcf gs://danking/chr2.vcf; Copying file://./src/test/resources/ldprune2.vcf [Content-Type=text/x-vcard]...; / [1 files][ 11.5 KiB/ 11.5 KiB] ; Operation completed over 1 objects/11.5 KiB. ; (base) dking@wm28c-761 hail % ipython ; Python 3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.16.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl; ...: hl.import_vcf('gs://danking/chr*.vcf').count(); Initializing Hail with default parameters...; /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:29: UserWarning: You have specified the GCS requester pays configuration in both your spark-defaults.conf (/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/conf/spark-defaults.conf) and either an explicit argument or through `hailctl config`. For GCS requester pays configuration, Hail first checks explicit arguments, then `hailctl config`, then spark-defaults.conf.; warnings.warn(; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13915:1297,config,configuration,1297,https://hail.is,https://github.com/hail-is/hail/issues/13915,1,['config'],['configuration']
Modifiability,hail.expr.ir.TableTextFinalizer.writeMetadata(TableWriter.scala:552); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit.emitVoid$1(Emit.scala:627); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5$adapted(Emit.scala:644); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4$adapted(Emit.scala:643); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3(Emit.scala:643); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3$adapted(Emit.scala:641); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:641); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:6966,adapt,adapted,6966,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['adapt'],['adapted']
Modifiability,hailctl config is too user unfriendly; it should: error or warn on invalid names and document the list of valid names,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13195:8,config,config,8,https://hail.is,https://github.com/hail-is/hail/issues/13195,1,['config'],['config']
Modifiability,hailctl dataproc does not use specified gcloud configuration,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9587:47,config,configuration,47,https://hail.is,https://github.com/hail-is/hail/issues/9587,1,['config'],['configuration']
Modifiability,"he results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this so support them.; Perhaps an inner loop or something. The process of submitting benchmarks to batch is greatly simplified as the old `Makefile` infrastructure for ; building wheels and docker images etc has been replaced with the script `benchmark_in_batch.py`.; Benchmark images are now based off the `hail-dev` image built in CI (or via the `hail-dev-image` make target). ; Furthermore, you can control the number of ""replicate"" jobs created for each benchmark at the benchmark level using; the `@benchmark(batch_jobs=N)` decotator. Limitations/shortcomings:; - Output is currently jsonl only. Some more human friendly output might be nice on a per iteration basis.; - Old `benchmark-hail` utilities are broken. I'll restore these in subsequent changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:1683,plugin,plugin,1683,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['plugin'],['plugin']
Modifiability,"hetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:9445,sandbox,sandbox,9445,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['sandbox'],['sandbox']
Modifiability,https:// is considered invalid by hailctl config set batch/remote_tmpdir,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13049:42,config,config,42,https://hail.is,https://github.com/hail-is/hail/issues/13049,1,['config'],['config']
Modifiability,"https://github.com/chardet/chardet/commit/eca9558cf7569c1f7689bd66e5aaf965a56e903c""><code>eca9558</code></a> Fix missing black formatting</li>; <li><a href=""https://github.com/chardet/chardet/commit/f1f9d4280e11fb3a9b2d9eaf1827dac9263cb1cb""><code>f1f9d42</code></a> slight increase in performance (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/252"">#252</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/f9ef56cfd6c9b24b9c865eae6dc2285c67ffb75c""><code>f9ef56c</code></a> Use Python-3 super() syntax in Latin1Prober (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/240"">#240</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/c5e5d5a8f1b6e135a8bffd8d60b2f726bb168339""><code>c5e5d5a</code></a> Simple maintenance improvements (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/244"">#244</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/49b8341f507bed68f7d3ff7138bb97047a0e04f0""><code>49b8341</code></a> Configure setuptools using the declarative syntax in setup.cfg (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/239"">#239</a>)</li>; <li><a href=""https://github.com/chardet/chardet/commit/5c73bfcdf819251d1a1d0de672e34480ebafbe1f""><code>5c73bfc</code></a> Run all pre-commit hooks on pull requests (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/236"">#236</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/chardet/chardet/compare/4.0.0...5.0.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=chardet&package-manager=pip&previous-version=4.0.0&new-version=5.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12107:4994,Config,Configure,4994,https://hail.is,https://github.com/hail-is/hail/pull/12107,1,['Config'],['Configure']
Modifiability,"https://github.com/hail-is/hail/pull/13610 added the setup for bokeh and plotly into `hailtop/__init__.py`, which causes `IPython` to be imported when invoking `hailctl`, adding unnecessary startup time to it (observed between 0.210s and 0.400s). This change moves the setup into the `__init__.py` files for the plotting modules, with the resulting state stored in a global variable in `hailtop/__init__.py`. The plotting setup is still run upon invoking `import hail`, as this imports all submodules of hail as well, but the `IPython` import no longer happens when invoking `hailctl`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13648:374,variab,variable,374,https://hail.is,https://github.com/hail-is/hail/pull/13648,1,['variab'],['variable']
Modifiability,"https://github.com/hail-is/hail/pull/5196 introduced a bug where `hl.get_reference('GRCh37')` only works *after* a call to `hl.init()`. ```; (hail) dking@wmb16-359 # ipython; import Python 3.6.7 | packaged by conda-forge | (default, Nov 20 2018, 18:37:09) ; Type 'copyright', 'credits' or 'license' for more information; IPython 6.3.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl; In [2]: hl.get_reference('GRCh37'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-2-3880f3d97a41> in <module>(); ----> 1 hl.get_reference('GRCh37'). ~/anaconda2/envs/hail/lib/python3.6/site-packages/hail/context.py in get_reference(name); 308 return default_reference(); 309 else:; --> 310 return ReferenceGenome._references[name]; 311 ; 312 . KeyError: 'GRCh37'; ```. This issue is considered fixed when:; - [ ] there is a test that would fail against current master `dcf43490c732`; - [ ] there is a fix for the issue. First reported by Claudia Dastmalchi [on Zulip](https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/KeyError.20for.20get_reference).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5252:341,enhance,enhanced,341,https://hail.is,https://github.com/hail-is/hail/issues/5252,1,['enhance'],['enhanced']
Modifiability,"https://numpydoc.readthedocs.io/en/latest/format.html. ```; def foo(a, b):; """"""One-line summary. Deprecation warning, if any. Extended summary (a few sentences, clarify functionality). Parameters. Returns. Other parameters, for infrequently used parameters on functions with large parameter lists. Warnings. See also (reference related functions that may be what the user actually wants or needs). Notes section, a possibly lengthy discussion of the algorithm and other gotchas. References for the notes section. Examples; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5304:126,Extend,Extended,126,https://hail.is,https://github.com/hail-is/hail/issues/5304,1,['Extend'],['Extended']
Modifiability,"hub.com/psf/requests/commit/b639e66c816514e40604d46f0088fbceec1a5149""><code>b639e66</code></a> test on py3.12 (<a href=""https://redirect.github.com/psf/requests/issues/6448"">#6448</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/d3d504436ef0c2ac7ec8af13738b04dcc8c694be""><code>d3d5044</code></a> Fixed a small typo (<a href=""https://redirect.github.com/psf/requests/issues/6452"">#6452</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/2ad18e0e10e7d7ecd5384c378f25ec8821a10a29""><code>2ad18e0</code></a> v2.30.0</li>; <li><a href=""https://github.com/psf/requests/commit/f2629e9e3c7ce3c3c8c025bcd8db551101cbc773""><code>f2629e9</code></a> Remove strict parameter (<a href=""https://redirect.github.com/psf/requests/issues/6434"">#6434</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/87d63de8739263bbe17034fba2285c79780da7e8""><code>87d63de</code></a> v2.29.0</li>; <li><a href=""https://github.com/psf/requests/commit/51716c4ef390136b0d4b800ec7665dd5503e64fc""><code>51716c4</code></a> enable the warnings plugin (<a href=""https://redirect.github.com/psf/requests/issues/6416"">#6416</a>)</li>; <li><a href=""https://github.com/psf/requests/commit/a7da1ab3498b10ec3a3582244c94b2845f8a8e71""><code>a7da1ab</code></a> try on ubuntu 22.04 (<a href=""https://redirect.github.com/psf/requests/issues/6418"">#6418</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/psf/requests/compare/v2.28.2...v2.31.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=requests&package-manager=pip&previous-version=2.28.2&new-version=2.31.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13091:6850,plugin,plugin,6850,https://hail.is,https://github.com/hail-is/hail/pull/13091,6,['plugin'],['plugin']
Modifiability,"i>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/b92a5febd4da3d7097a3d2b8d7cac6f5d57ea20c""><code>b92a5fe</code></a> Version bump 2.0.4</li>; <li><a href=""https://github.com/lepture/mistune/commit/98a1c0afc51d4be719cb17401a35e62f46206915""><code>98a1c0a</code></a> Fix url plugin render, <a href=""https://github-redirect.dependabot.com/lepture/mistune/issues/308"">#308</a></li>; <li><a href=""https://github.com/lepture/mistune/commit/979d6d3bfc7d6159f38deb8e751611e4205033f6""><code>979d6d3</code></a> Fix * parsing, <a href=""https://github-redirect.dependabot.com/lepture/mistune/issues/312"">#312</a></li>; <li><a href=""https://github.com/lepture/mistune/commit/f857f048ebb2f6f2bb7ab97dcb7a159172a20649""><code>f857f04</code></a> Trigger GitHub dependency graph</li>; <li><a href=""https://github.com/lepture/mistune/commit/3f422f1e84edae0f39756c45be453ecde534b755""><code>3f422f1</code></a> Version bump 2.0.3</li>; <li><a href=""https://github.com/lepture/mistune/commit/a6d43215132fe4f3d93f8d7e90ba83b16a0838b2""><code>a6d4321</code></a> Fix asteris emphasis regex CVE-2022-34749</li>; <li><a href=""https://github.com/lepture/mistune/commit/5638e460459cb59ceb20e4ce4716c802d4d73c53""><code>5638e46</code></a> Merge pull request <a href=""https://gith",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12066:2263,plugin,plugin,2263,https://hail.is,https://github.com/hail-is/hail/pull/12066,2,['plugin'],['plugin']
Modifiability,"i>; <li><a href=""https://github.com/googleapis/java-storage/commit/cf900f4139f30f89e3c0784467ddc12cc00cf81c""><code>cf900f4</code></a> deps: update dependency org.apache.httpcomponents:httpclient to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1795"">#1795</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/099a6165722464b46d37206af274a637d3f0461a""><code>099a616</code></a> test(deps): update cross product test dependencies (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1792"">#1792</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3184d65cce1368c2f39ff85a6ed02cf536902244""><code>3184d65</code></a> deps: update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.19...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/7d6742115bcea6b848a289fdf5c4e4bbafc4cf18""><code>7d67421</code></a> build(deps): update dependency com.google.cloud:google-cloud-shared-config to...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3bf403e94c035e6cf936e062a1ced2b5221b3912""><code>3bf403e</code></a> deps: update dependency org.apache.httpcomponents:httpcore to v4.4.16 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1786"">#1786</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.16.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-version=1.106.0&new-version=2.16.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:12318,config,config,12318,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['config'],['config']
Modifiability,"i>; <li><a href=""https://github.com/pytest-dev/pytest/commit/a65c47a1a40dad1bb5ce0beb83657d492011a425""><code>a65c47a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9783"">#9783</a> from pytest-dev/backport-9780-to-7.1.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/30d995ed25e6d76e85da140663e6253fa5b41935""><code>30d995e</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/10a14d13181fd69dd0eaf48bf5b3d389de896713""><code>10a14d1</code></a> [7.1.x] testing: fix tests when run under <code>-v</code> or <code>-vv</code></li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/f4cfc596c6574abf68ed49503fd1b8ef1484125d""><code>f4cfc59</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/f1df8074b3d9185313752cbc29b88d889a1879d9""><code>f1df807</code></a> [7.1.x] config: restore pre-pytest 7.1.0 confcutdir exclusion behavior</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/7d4d1ecde6cdc3feae9ee076ee5aab4e05393fa6""><code>7d4d1ec</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9758"">#9758</a> from pytest-dev/release-7.1.0</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/1dbffcc0b4d822b87ad9f90595ffab6d9beee769""><code>1dbffcc</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/d53a5fb37194faf63ee5d74606cc883138879bc4""><code>d53a5fb</code></a> Prepare release version 7.1.0</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest/compare/6.2.5...7.1.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest&package-manager=pip&previous-version=6.2.5&new-version",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11619:5589,config,config,5589,https://hail.is,https://github.com/hail-is/hail/pull/11619,1,['config'],['config']
Modifiability,"iases.; The main breaking change from v5.x is that ""mapping_types"" have been removed, so the ; kt.export_elasticsearch `mapping_type` arg isn't needed anymore (https://www.elastic.co/guide/en/elasticsearch/reference/6.0/breaking-changes-6.0.html). ### Hail version:; 0.1, 0.2. ### What you did:. kt.export_elasticsearch. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/hail-elasticsearch-pipelines/hail_scripts/v01/load_clinvar_to_es_pipeline.py"", line 112, in <module>; export_globals_to_index_meta=True,; File ""/hail-elasticsearch-pipelines/hail_scripts/v01/utils/elasticsearch_client.py"", line 142, in export_vds_to_elasticsearch; verbose=verbose); File ""/hail-elasticsearch-pipelines/hail_scripts/v01/utils/elasticsearch_client.py"", line 287, in export_kt_to_elasticsearch; kt.export_elasticsearch(self._host, int(self._port), index_name, index_type_name, block_size, config=elasticsearch_config); File ""<decorator-gen-143>"", line 2, in export_elasticsearch; File ""/hail/build/distributions/hail-python.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 20050, localhost): org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:1111,config,config,1111,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['config'],['config']
Modifiability,"ice] slightly more useful error message when socket dies; - [ ] (@tpoterba) f79c4023cf [shuffler] if we have an ExecuteContext, use it; - [x] (@daniel-goldstein,fyi: @tpoterba) 259f70dd25 [query-service] JSON Logging; - [ ] (@catoverdrive) f5c3ffcbd1 [query-service] pervasively retry all idempotent operations; - [ ] (@tpoterba) 507db4b468 [hail] fix using; - [x] (@jigold) c32a253bb9 [query] when testing, ensure our thread has an event loop; - [ ] (@tpoterba) 110469c2da [query][lir] avoid dumping massive classes onto stderr; - [ ] (@tpoterba) e4aa1c15fe [query] do not print misleading log in RegionPool.finalizer; - [x] (trivial) 33eab9a80e [query-service] better logging information; - [ ] (@catoverdrive) e358e8feeb [query-service] remove race conditions in user management; - [ ] (@tpoterba) b60cb2bae5 [lir] make LIR genName thread-safe; - [ ] (@catoverdrive) 2d82e5faf5 [query-service] send a token for job identifiability; - [x] (@daniel-goldstein) fd78caedcb [query-service] reduce image size by ~2GB; - [ ] (@catoverdrive) 00d1840421 [query-service] retry CLOSE, CLOSED (i.e. connection dropped); - [ ] (@catoverdrive) c985d3e3de [query-service] remove old test code; - [ ] (@catoverdrive) 0a5dc8c651 [query-service] all operations are idempotent; - [ ] (@cseed) 6d02d173fa [make] fix config.mk; - [x] (@daniel-goldstein) d21df54e63 [devbin] teach devbin/functions.py about multiple containers; - [x] (@jigold) 38878f7874 [batch] remove batch_worker_image false dependency on service_base_image; - [x] (@daniel-goldstein) f03defab3d [java-services] avoid NPEs in isTransientError; - [x] (@jigold) e535bdc00d [dependencies] upgrade gcsfs to 0.7.2 to fix GoogleFS rmtree issue; - [x] (@cseed) 743b5ba62f [query-service] enable auto-scaling for PR and dev deploy; - [ ] (@cseed) 6a52d45f6f [query-service] retry EndOfStream errors from java; - [ ] (@jigold) 5853a0bec4 [batch] remove restrictions on PR and dev batch pools; - [ ] (@cseed) 035b19642a [query-service] resolve last two issues",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10100:3597,config,config,3597,https://hail.is,https://github.com/hail-is/hail/pull/10100,1,['config'],['config']
Modifiability,"iguration system. Changes in this PR:. 1. Include JVM output in error logs when the JVM crashes. This should help debugging of JVM crashing in production until the JVM logs are shown on a per-worker page. 2. JVMEntryway is now a real gradle project. I need to compile against log4j, and I didn't want to do that by hand with `javac`. Ignore gradlew, gradlew.bat, and gradle/wrapper, they're programmatically generated by gradle. 3. Add logging to JVMEntryway. JVMEntryway now logs its arguments into the QoB job log. I also log exceptions from the main thread or the cancel thread into the job log. We also flush the logs after the main thread completes, the cancel thread completes, and when the try-catch exits. This should ensure that regardless of what goes wrong (even if both threads fail to start) we at least see the arguments that the JVMEntryway received. 4. Use log4j2 programmatic reconfiguration after every job. This restores log4j2 to well enough working order that, *if you do not try to reconfigure it using log4j1 programmatic configuration*, logs will work. All old versions of Hail use log4j1 programmatic configuration. As a result, **all old versions of Hail will still have no logs**. However, new versions of Hail will log correctly even if an old version of Hail used the JVM before it. 5. `QoBAppender`. This is how we always should have done logging. A custom appender which we can flush and then redirect to a new file at our whim. I followed the log4j2 best practices for creating a new appender. All these annotations, factory methods, and managers are The Right Way, for better or worse. If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without any runtime log configuration (thanks to `QoBAppender`). I would like to eliminate reconfiguration because log4j2 reconfiguration leaves around oprhaned appenders and appender managers. Maybe I'm implementing the Appender or Appender ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941:1277,config,configuration,1277,https://hail.is,https://github.com/hail-is/hail/pull/12941,1,['config'],['configuration']
Modifiability,il.expr.ir.EmitFunctionBuilder.resultWithIndex(EmitClassBuilder.scala:1078); 	at is.hail.expr.ir.Emit.$anonfun$emitI$238(Emit.scala:2400); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:336); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:2341); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:630); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:748); 	at is.hail.expr.ir.TableTextFinalizer.writeMetadata(TableWriter.scala:552); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit.emitVoid$1(Emit.scala:627); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5$adapted(Emit.scala:644); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4$adapted(Emit.scala:643); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3(Emit.scala:643); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3$adapted(Emit.scala:641); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:641); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:6604,adapt,adapted,6604,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['adapt'],['adapted']
Modifiability,il.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4413,Rewrite,RewriteBottomUp,4413,https://hail.is,https://github.com/hail-is/hail/issues/6458,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,ilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.Comp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4486,Rewrite,RewriteBottomUp,4486,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Logs + .Statuses + .Vertexes) | add | {Timestamp, ID, Name}'`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:1763,layers,layers,1763,https://hail.is,https://github.com/hail-is/hail/pull/10502,1,['layers'],['layers']
Modifiability,improve variable names in VariantPackage object,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1326:8,variab,variable,8,https://hail.is,https://github.com/hail-is/hail/pull/1326,1,['variab'],['variable']
Modifiability,"in.doFilter(Filter.java:79); 	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82); 	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:822); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:794); 	at sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:199); 	at sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:544); 	at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:509); 	at java.lang.Thread.run(Thread.java:750). java.util.NoSuchElementException: Ref with name __iruid_1834 could not be resolved in env BindingEnv((__iruid_1832 -> struct{},__iruid_2157 -> struct{}),None,None,()); 	at is.hail.expr.ir.TypeCheck$.checkSingleNode(TypeCheck.scala:110); 	at is.hail.expr.ir.TypeCheck$.$anonfun$check$4(TypeCheck.scala:37); 	at is.hail.expr.ir.TypeCheck$.$anonfun$check$4$adapted(TypeCheck.scala:29); 	at is.hail.utils.StackSafe$StackFrame.$anonfun$map$1(StackSafe.scala:30); 	at is.hail.utils.StackSafe$StackFrame.flatMap(StackSafe.scala:21); 	at is.hail.utils.StackSafe$StackFrame.map(StackSafe.scala:30); 	at is.hail.expr.ir.TypeCheck$.check(TypeCheck.scala:29); 	at is.hail.expr.ir.TypeCheck$.$anonfun$check$2(TypeCheck.scala:31); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:29); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(Arra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:18667,adapt,adapted,18667,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['adapt'],['adapted']
Modifiability,"ince 1.20+. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/107527"">kubernetes/kubernetes#107527</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>)</li>; <li>Kubelet external Credential Provider feature is moved to Beta. Credential Provider Plugin and Credential Provider Config API's updated from v1alpha1 to v1beta1 with no API changes. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108847"">kubernetes/kubernetes#108847</a>, <a href=""https://github.com/adisky""><code>@​adisky</code></a>)</li>; <li>Make STS available replicas optional again. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/109241"">kubernetes/kubernetes#109241</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>MaxUnavailable for StatefulSets, allows faster RollingUpdate by taking down more than 1 pod at a time. The number of pods you want to take down during a RollingUpdate is configurable using maxUnavailable parameter. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/82162"">kubernetes/kubernetes#82162</a>, <a href=""https://github.com/krmayankk""><code>@​krmayankk</code></a>)</li>; <li>Non-graceful node shutdown handling is enabled for stateful workload failovers (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/108486"">kubernetes/kubernetes#108486</a>, <a href=""https://github.com/sonasingh46""><code>@​sonasingh46</code></a>)</li>; <li>Omit enum declarations from the static openapi file captured at <a href=""https://git.k8s.io/kubernetes/api/openapi-spec"">https://git.k8s.io/kubernetes/api/openapi-spec</a>. This file is used to generate API clients, and use of enums in those generated clients (rather than strings) can break forward compatibility with additional future values in those fields. See <a href=""https://issue.k8s.io/109177"">https://issue.k8s.io/109177</a> for details. (<a href=""https://gi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12196:9903,config,configurable,9903,https://hail.is,https://github.com/hail-is/hail/pull/12196,1,['config'],['configurable']
Modifiability,"ing.google.com/sre/books/; - Distributed Systems Observability https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/; - ""Learning to Build Distributed Systems"" http://brooker.co.za/blog/2019/04/03/learning.html; - Increment's On-Call issue https://increment.com/on-call/; # SWE; - ""Designing Data-Intensive Systems"" by Kleppman https://www.amazon.com/gp/product/1449373321/; # SEC; - ""The Confused Deputy"" http://zoo.cs.yale.edu/classes/cs422/2010/bib/hardy88confused.pdf; - ""Blueprint fo a science of cybersecurity"" http://www.cs.cornell.edu/fbs/publications/SoS.blueprint.pdf; - ""Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud"" https://ai.google/research/pubs/pub41892; - ""Native Client: A Sandbox for Portable, Untrusted x86 Native Code"" https://ai.google/research/pubs/pub34913; - What is CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF); - What is XSS https://www.owasp.org/index.php/Cross-site_Scripting_(XSS); ## Containers; - gVisor Architecture Guide https://gvisor.dev/docs/architecture_guide/; - ""cgroups"" https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt; - ""cgroups v2"" https://github.com/torvalds/linux/blob/master/Documentation/admin-guide/cgroup-v2.rst; - ""Docker Security"" https://docs.docker.com/engine/security/security/; - ""On the security of containers"" https://medium.com/@ewindisch/on-the-security-of-containers-2c60ffe25a9e; - ""User namespaces might not be enough"" https://medium.com/@ewindisch/linux-user-namespaces-might-not-be-secure-enough-a-k-a-subverting-posix-capabilities-f1c4ae19cad; - ""OS-level virtualization"" https://en.wikipedia.org/wiki/OS-level_virtualisation; - ""Sandbox (computer security)"" https://en.wikipedia.org/wiki/Sandbox_(computer_security); - ""Making Containers More Isolated: An Overview of Sandboxed Container Technologies"" https://unit42.paloaltonetworks.com/making-containers-more-isolated-an-overview-of-sandboxed-container-technologies/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6720:1753,Sandbox,Sandbox,1753,https://hail.is,https://github.com/hail-is/hail/issues/6720,3,"['Sandbox', 'sandbox']","['Sandbox', 'Sandboxed', 'sandboxed-container-technologies']"
Modifiability,"ingleton subscriptions; e.g., <code>nptyping.Float[64]</code></li>; <li>Resolve forward references</li>; <li>Expand and better handle <code>TypeVar</code></li>; <li>Add intershpinx reference link for <code>...</code> to <code>Ellipsis</code> (as is just an alias)</li>; </ul>; <h2>1.15.3</h2>; <ul>; <li>Prevents reaching inner blocks that contains <code>if TYPE_CHECKING</code></li>; </ul>; <h2>1.15.2</h2>; <ul>; <li>Log a warning instead of crashing when a type guard import fails to resolve</li>; <li>When resolving type guard imports if the target module does not have source code (such is the case for C-extension; modules) do nothing instead of crashing</li>; </ul>; <h2>1.15.1</h2>; <ul>; <li>Fix <code>fully_qualified</code> should be <code>typehints_fully_qualified</code></li>; </ul>; <h2>1.15.0</h2>; <ul>; <li>Resolve type guard imports before evaluating annotations for objects</li>; <li>Remove <code>set_type_checking_flag</code> flag as this is now done by default</li>; <li>Fix crash when the <code>inspect</code> module returns an invalid python syntax source</li>; <li>Made formatting function configurable using the option <code>typehints_formatter</code></li>; </ul>; <h2>1.14.1</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/73aa9b6aea40720ca270b1107c1980b909943cb3""><code>73aa9b6</code></a> Fix mock imports on guarded imports (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/225"">#225</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/4d5867d5a235040b3e7d3373a56c5b2b580db7b7""><code>4d5867d</code></a> Handle UnionType (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/221"">#221</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/13ca2b458b0ee9c8d1c980b6a5e97a6ee78f46c7""><code>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11773:2157,config,configurable,2157,https://hail.is,https://github.com/hail-is/hail/pull/11773,1,['config'],['configurable']
Modifiability,"instead we get:; ```; Python 3.7.5 (default, Nov 1 2019, 02:16:32) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: %%time ; ...: import os ; ...: import sys ; ...: ; ...: from hailtop import pipeline as pl ; ...: ; ...: BENCHMARK_IMAGE = 'ubuntu:18.04' ; ...: p = pl.Pipeline(name='download_data_fail', ; ...: backend=pl.BatchBackend(billing_project='hail'), ; ...: default_image=BENCHMARK_IMAGE, ; ...: default_cpu=1) ; ...: ; ...: for i in range(1): ; ...: t = p.new_task(f'replicate_{i}') ; ...: t.command('echo ' + 'a' * 1000) ; ...: p.run(wait=False) ; aenter; aexit; submit jobs timing {'create': 264, 'total': 265}; Traceback (most recent call last):; File ""<timed exec>"", line 15, in <module>; File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/pipeline.py"", line 395, in run; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); File ""/Users/dking/projects/hail/hail/python/hailtop/pipeline/backend.py"", line 329, in _run; batch = batch.submit(); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 164, in submit; async_batch = async_to_blocking(self._async_builder.submit()); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/client.py"", line 7, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py"", line 579, in run_until_complete; return future.result(); File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/futures.py"", line 178, in result; raise self._exception; File ""/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/Users/dking/projects/hail/hail/python/hailtop/batch_client/aioclient.py"", line 500, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7839:152,enhance,enhanced,152,https://hail.is,https://github.com/hail-is/hail/issues/7839,1,['enhance'],['enhanced']
Modifiability,"invar_ht = clinvar_ht.key_by(clinvar_ht.locus, clinvar_ht.alleles); clinvar_mt = hl.MatrixTable.from_rows_table(clinvar_ht); clinvar_mt = split_multi_dynamic(clinvar_mt, left_aligned = False); clinvar_mt = clinvar_mt.repartition(100); clinvar_mt = clinvar_mt.key_rows_by(clinvar_mt.locus, clinvar_mt.alleles); clinvar_vep = hl.vep(clinvar_mt, vep_config); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; In [6]: clinvar_vep = hl.vep(clinvar_mt, vep_config); 2018-03-08 02:46:03 Hail: WARN: property `hail.vep.assembly' not specified. Setting to GRCh37; [Stage 22:======================================================>(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_ar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3099:1528,config,config,1528,https://hail.is,https://github.com/hail-is/hail/issues/3099,1,['config'],['config']
Modifiability,"inx-doc/sphinx/issues/10535"">#10535</a> from AA-Turner/css-nav-contents</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/709602437df850d5538a4fe899a50625c01a0f80""><code>7096024</code></a> Update CHANGES for PR <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10539"">#10539</a></li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/d0452276689bfb5b97ca7a3469e1afb505895cdd""><code>d045227</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10539"">#10539</a> from AA-Turner/fix-inherited-attrs</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/29edce9243046962f5f024d510315133448dd3e1""><code>29edce9</code></a> test: Add testcase for autodoc_inherit_docstring and attributes (refs: <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10539"">#10539</a>)</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/3956cf2249d27ed63e8381c07dfde36f6c96f78f""><code>3956cf2</code></a> Fix documenting inherited attributes</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/27f05328d0369ad0db85c27935d52fdadf020f6b""><code>27f0532</code></a> Move <code>aside.topic</code> into the conditional blocks</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/5806f0af2788db40661d62e5e88c2c1560ae46b6""><code>5806f0a</code></a> Add <code>nav.contents</code> everywhere that <code>div.topic</code> is used</li>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/8da2efb1d71ab2d384ddc90cf4fdebe5d18e91cd""><code>8da2efb</code></a> Rename CSS files to CSS template files</li>; <li>Additional commits viewable in <a href=""https://github.com/sphinx-doc/sphinx/compare/v3.5.4...v5.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sphinx&package-manager=pip&previous-version=3.5.4&new-version=5.0.2)](https://docs.github.com/en/github/man",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:5552,inherit,inherited,5552,https://hail.is,https://github.com/hail-is/hail/pull/11925,1,['inherit'],['inherited']
Modifiability,"ion: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project-.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.gbsc-project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:2373,adapt,adapted,2373,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['adapt'],['adapted']
Modifiability,ions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11623,Config,Configuration,11623,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"irect.dependabot.com/pytest-dev/pytest/issues/9355"">#9355</a>: Fixed error message prints function decorators when using assert in Python 3.8 and above.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivial/Internal Changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9521"">#9521</a>: Add test coverage to assertion rewrite path.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest/commit/3f12087fe0d86a319216653b08b66a96d400bee2""><code>3f12087</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/bc3021cdfd76507aa3d9e278bd885da9bc1907b2""><code>bc3021c</code></a> Prepare release version 7.0.1</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/591d476f14e3e83d90fbea75d326a93c5e368708""><code>591d476</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9673"">#9673</a> from nicoddemus/backport-9511</li>; <li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:3594,config,configuration,3594,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['config'],['configuration']
Modifiability,"irst commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. Among those we *may* want to care about, [IE11 has ~2% global use (more if only desktop browsers)](https://caniuse.com/#feat=flexbox); ### NodeJS; We use 10.15. [This is the latest LTS release](https://nodejs.org/en/download/). ### Versioning / dependency management; TL;DR: `npm`; ```sh; npm init # creates a package.json file, which tracks dependencies; npm install next react react-dom # install 3 packages and save them to the dependencies property; ```. #### package.json; The file that tracks dependencies, and their semantic versioning numbers. Shape:; ```json; {; ""name"": ""hail-web-client"",; ""version"": ""0.2.0"",; ""scripts"": {; ""dev"": ""next"",; ""build"": ""next build"",; ""start"": ""NODE_ENV=production SSL=true next start""; },; ""author"": ""Hail Team"",; ""license"": ""MIT"",; ""dependencies"": {; ""next"": ""^7.0.2-canary.50"",;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:1856,config,configurable,1856,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['config'],['configurable']
Modifiability,"is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3990,config,configuration,3990,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['config'],['configuration']
Modifiability,is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 		at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.utils.package$.using(package.scala:637) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:23918,adapt,adapted,23918,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['adapt'],['adapted']
Modifiability,is.hail.relocated.com.google.cloud.storage.StorageImpl.writer(StorageImpl.java:674) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.relocated.com.google.cloud.storage.StorageImpl.writer(StorageImpl.java:95) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$doHandlingRequesterPays$2(GoogleStorageFS.scala:300) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$doHandlingRequesterPays$1(GoogleStorageFS.scala:300) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$doHandlingRequesterPays$1$adapted(GoogleStorageFS.scala:299) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS.is$hail$io$fs$GoogleStorageFS$$handleRequesterPays(GoogleStorageFS.scala:181) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:304) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$close$1(GoogleStorageFS.scala:326) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 		at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:20824,adapt,adapted,20824,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['adapt'],['adapted']
Modifiability,"issues/3217"">#3217</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Black now uses the presence of debug f-strings to detect target version (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3215"">#3215</a>)</li>; <li>Fix misdetection of project root and verbose logging of sources in cases involving <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Immediate <code>.gitignore</code> files in source directories given on the command line are now also respected, previously only <code>.gitignore</code> files in the project root and automatically discovered directories were respected (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3237"">#3237</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Recommend using BlackConnect in IntelliJ IDEs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3150"">#3150</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Vim plugin: prefix messages with <code>Black: </code> so it's clear they come from Black (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3194"">#3194</a>)</li>; <li>Docker: changed to a /opt/venv installation + added to PATH to be available to non-root users (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3202"">#3202</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Change from deprecated <code>asyncio.get_event_loop()</code> to create our event loop which removes DeprecationWarning (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3164"">#3164</a>)</li>; <li>Remove logging from internal <code>blib2to3</code> library since it regularly emits error logs about failed caching that can and should be ignored (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3193"">#3193</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Type comments are now included in the AST equivalence check consistently so accidental deletion raises an error. Though type comments can't",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:3785,plugin,plugin,3785,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['plugin'],['plugin']
Modifiability,"ithout a palette <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7797"">#7797</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use palette when loading ICO images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7798"">#7798</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use consistent arguments for load_read and load_seek <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7713"">#7713</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Turn off nullability warnings for macOS SDK <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7827"">#7827</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix shift-sign issue in Convert.c <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7838"">#7838</a> [<a href=""https://github.com/r-barnes""><code>@​r-barnes</code></a>]</li>; <li>winbuild: Refactor dependency versions into constants <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7843"">#7843</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Build macOS arm64 wheels natively <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7852"">#7852</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed typo <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7855"">#7855</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Open 16-bit grayscale PNGs as I;16 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7849"">#7849</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Handle truncated chunks at the end of PNG images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7709"">#7709</a> [<a href=""https://github.com/lajiyuan""><code>@​lajiyuan</code></a>]</li>; <li>Match mask size to pasted image si",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:7650,Refactor,Refactor,7650,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['Refactor'],['Refactor']
Modifiability,"ithub-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/205"">#205</a>)</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/1b5d69760d19cb7f88cbc837ee46456c494c0696""><code>1b5d697</code></a> Bump up version number to 5.2.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/7d6de83037ca41cd2f2f31830b43e43720e45b3a""><code>7d6de83</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:2402,plugin,plugin,2402,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['plugin'],['plugin']
Modifiability,"ithub-redirect.dependabot.com/pytest-dev/pytest-metadata/issues/50"">#50</a> from BeyondEvil/release-v2.0.2</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/f0b5503452f922a84faa213224a4970c57d8a654""><code>f0b5503</code></a> Release v2.0.2</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/ff493afce81b1bbe7a2f866cd27510e5d9b8feef""><code>ff493af</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-metadata/issues/47"">#47</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/4591db1fa5546ff372ae95155caed99ce8dc4842""><code>4591db1</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/0414bb9f81cc1856ea021504eecd22d202462f1d""><code>0414bb9</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-metadata/issues/46"">#46</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/025be8999a22ae395b0e2b8ae4e7c9fa2334f874""><code>025be89</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/429840f4de26276560961929f21aab79ed305875""><code>429840f</code></a> Avoid running nightly on forks</li>; <li><a href=""https://github.com/pytest-dev/pytest-metadata/commit/c1968f39609978ec9c6a4bcf91c37c6164483f04""><code>c1968f3</code></a> Fix nightly</li>; <li>See full diff in <a href=""https://github.com/pytest-dev/pytest-metadata/compare/v2.0.1...v2.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pytest-metadata&package-manager=pip&previous-version=2.0.1&new-version=2.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12188:1644,config,config,1644,https://hail.is,https://github.com/hail-is/hail/pull/12188,1,['config'],['config']
Modifiability,"ithub.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Akrassowski+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​krassowski</code></a> | <a href=""https://github.com/search?q=repo%3Ajupyter%2Fnotebook+involves%3Ameeseeksmachine+updated%3A2023-10-17..2024-01-19&amp;type=Issues""><code>@​meeseeksmachine</code></a></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/notebook/blob/@jupyter-notebook/tree@7.0.7/CHANGELOG.md"">notebook's changelog</a>.</em></p>; <blockquote>; <h2>7.0.7</h2>; <p>(<a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/application-extension@7.0.6...089c78c48fd00b2b0d2f33e4463eb42018e86803"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Update to JupyterLab 4.0.11 <a href=""https://redirect.github.com/jupyter/notebook/pull/7215"">#7215</a> (<a href=""https://github.com/krassowski""><code>@​krassowski</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>Update ruff config and typing <a href=""https://redirect.github.com/jupyter/notebook/pull/7145"">#7145</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Clean up lint handling <a href=""https://redirect.github.com/jupyter/notebook/pull/7142"">#7142</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>Adopt ruff format <a href=""https://redirect.github.com/jupyter/notebook/pull/7132"">#7132</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[7.0.x] Install stable JupyterLab 4.0 in the releaser hook <a href=""https://redirect.github.com/jupyter/notebook/pull/7183"">#7183</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; <li>Update publish-release workflow for PyPI trusted publisher <a href=""https://redirect.github.com/jupyter/notebook/pull/7176"">#7176</a> (<a href=""https://github.com/jtpio""><code>@​jtpio</code></a>)</li>; </ul>; <h3>Contributors to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:3608,config,config,3608,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['config'],['config']
Modifiability,ive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 			at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 			at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 			at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 			at is.hail.rvd.RVD.combine(RVD.scala:688); 			at is.hail.expr.ir.Interpret$.run(Interpret.scala:804); 			at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 			at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:39); 			at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 			at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 			at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 			at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 			at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:12546,rewrite,rewrite,12546,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['rewrite'],['rewrite']
Modifiability,ive(DAGScheduler.scala:2792); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2257); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2289); 	at is.hail.sparkextras.ContextRDD.crunJobWithIndex(ContextRDD.scala:238); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1029); 	at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1104); 	at is.hail.rvd.RVD$.coerce(RVD.scala:1060); 	at is.hail.rvd.RVD.changeKey(RVD.scala:142); 	at is.hail.rvd.RVD.changeKey(RVD.scala:135); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:716); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:143); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:17); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:14); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:167); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:161); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:12078,Rewrite,RewriteBottomUp,12078,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"java-storage/compare/v2.14.0...v2.15.0"">2.15.0</a> (2022-11-07)</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.1...v2.16.0"">2.16.0</a> (2022-12-06)</h2>; <h3>Features</h3>; <ul>; <li>Add {Compose,Rewrite,StartResumableWrite}Request.object_checksums and Bucket.RetentionPolicy.retention_duration (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1790"">#1790</a>) (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Added a new retention_duration field of Duration type (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Added object_checksums for compose/rewrite/startResumableWrite request (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>Removed WriteObject routing annotations (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Clarified relative resource names in gRPC IAM RPCs (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Clarified the object can be deleted via DeleteObject (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; <li>Updated the document link for <code>Naming Guidelines</code> (<a href=""https://github.com/googleapis/java-storage/commit/31c1b18acc3c118e39eb613a82ee292f3e246b8f"">31c1b18</a>)</li>; </ul>; <h3>Dependencies</h3>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:5845,rewrite,rewrite,5845,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['rewrite'],['rewrite']
Modifiability,java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2441); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2383); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2372); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:362); 	at is.hail.rvd.RVD.$anonfun$head$1(RVD.scala:526); 	at is.hail.utils.PartitionCounts$.incrementalPCSubsetOffset(PartitionCoun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:8621,adapt,adapted,8621,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['adapt'],['adapted']
Modifiability,java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2257); 	at is.hail.sparkextras.ContextRDD.crunJobWithIndex(ContextRDD.scala:238); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1233); 	at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1308); 	at is.hail.rvd.R,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:6489,adapt,adapted,6489,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['adapt'],['adapted']
Modifiability,"kages/_pytest/runner.py"", line 341 in from_call; File ""/usr/local/lib/python3.9/dist-packages/_pytest/runner.py"", line 261 in call_runtest_hook; File ""/usr/local/lib/python3.9/dist-packages/_pytest/runner.py"", line 222 in call_and_report; File ""/usr/local/lib/python3.9/dist-packages/_pytest/runner.py"", line 133 in runtestprotocol; File ""/usr/local/lib/python3.9/dist-packages/_pytest/runner.py"", line 114 in pytest_runtest_protocol; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/usr/local/lib/python3.9/dist-packages/_pytest/main.py"", line 350 in pytest_runtestloop; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/usr/local/lib/python3.9/dist-packages/_pytest/main.py"", line 325 in _main; File ""/usr/local/lib/python3.9/dist-packages/_pytest/main.py"", line 271 in wrap_session; File ""/usr/local/lib/python3.9/dist-packages/_pytest/main.py"", line 318 in pytest_cmdline_main; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/usr/local/lib/python3.9/dist-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/usr/local/lib/python3.9/dist-packages/_pytest/config/__init__.py"", line 169 in main; File ""/usr/local/lib/python3.9/dist-packages/_pytest/config/__init__.py"", line 192 in console_main; File ""/usr/local/lib/python3.9/dist-packages/pytest/__main__.py"", line 5 in <module>; File ""/usr/lib/python3.9/runpy.py"", line 87 in _run_code; File ""/usr/lib/python3.9/runpy.py"", line 197 in _run_module_as_main; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14299:7381,config,config,7381,https://hail.is,https://github.com/hail-is/hail/issues/14299,2,['config'],['config']
Modifiability,"key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.11 (2022-07-25)</h1>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would; raise an <code>OverflowError</code> on Python 3.9 and earlier.</li>; </ul>; <h1>1.26.10 (2022-07-07)</h1>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP; instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/aa3def7d242525e6e854991247c4b68583d15135""><code>aa3def7</code></a> Release 1.26.11</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/6f93b8f450b18b4c9f4c6333d759a911a63d15ae""><code>6f93b8f</code></a> Fix <code>OverflowError</code> when TLS is used on some Python versions</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/0a5f34d2c2ee6457e8365543243eccd3d1dc9430""><code>0a5f34d</code></a> Set GHA token permissions to be read-only</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/ac61b73da703df53707c31030b4ea51aab22d43c""><code>ac61b73</code></a> Backport publish workflow and process to 1.26.x</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/1fd77edc1a1373c9a7e762de148f19f1e2edd418""><code>1fd77ed</code></a> Release 1.26.1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12104:2128,config,configuring,2128,https://hail.is,https://github.com/hail-is/hail/pull/12104,2,['config'],"['configured', 'configuring']"
Modifiability,"kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a depend",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3869,config,configuration,3869,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['config'],['configuration']
Modifiability,"l use the setup/close hooks to implement region management in the stream emitter. A `Stream` takes two pieces of information from its consumer: a `Code` to run when at the end of the stream, and a `Code` to run when the stream produces a value. (When I say ""run when..."", I really mean ""inline at the point in the control flow at which...""). It also takes an `EmitStreamContext`, which is just a bundle of a `MethodBuilder` and a `JoinPointBuilder`. It then produces six pieces of information for its consumer to use, packaged into a `Source` object. A `Stream` must satisfy the following invariants:; * `close` and `close0` must be okay to emit multiple times, i.e. they should be function calls, or a small number of function calls. (This is because I don't see a way to unify the control paths of the cases where a producer ends the stream, and where the consumer ends it.); * `eos` and `push` must be emitted at most once.; * Any local variables created by the stream must be initialized to dummy values in `setup0`, and initialized to correct initial values in `setup`. The former is needed to satisfy the bytecode verifier (otherwise it can't prove that the local always has a value in the basic blocks in which it's read).; * Any resources created in `setup0` must be freed in `close0`, and similarly for `setup`/`close`. The former will be run exactly once in a stream pipeline, while the latter will be run once per usage of the stream, which can be many times if the stream is used inside a flatMap.; * Both `firstPull` and `pull` must ultimately give control to either `eos` or `push`.; * On any control path leading to `eos`, `close` must have been run. The consumer of a `Stream` must satisfy the invariants:; * All pieces of code besides `setup0` and `setup` must be emitted at most once. This can always be arranged by emitting the code in a `JoinPoint` (label), then jumping to the label at each call site.; * If `firstPull` is defined, it must always be executed before `pull` on any",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8129:1754,variab,variables,1754,https://hail.is,https://github.com/hail-is/hail/pull/8129,1,['variab'],['variables']
Modifiability,l$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:45); 	at is.hail.expr.ir.FoldConstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1542,Rewrite,RewriteBottomUp,1542,https://hail.is,https://github.com/hail-is/hail/issues/9128,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,l.expr.ir.Emit.emit$1(Emit.scala:591); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:624); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1(Emit.scala:549); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1$adapted(Emit.scala:547); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:985); 	at is.hail.expr.ir.Emit.emitVoidInSeparateMethod(Emit.scala:547); 	at is.hail.expr.ir.Emit.emitInSeparateMethod(Emit.scala:571); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:760); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:600); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:715); 	at is.hail.expr.ir.RelationalWriter.writeMetadata(TableWriter.scala:341); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:715); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:985); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:77); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.Low,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:20517,adapt,adapted,20517,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,l.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2861); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2803); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2792); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2257); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2289); 	at is.hail.sparkextras.ContextRDD.crunJobWithIndex(ContextRDD.scala:238); 	at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1029); 	at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1104); 	at is.hail.rvd.R,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:10649,adapt,adapted,10649,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,l.rvd.RVD$.coerce(RVD.scala:1264); 	at is.hail.rvd.RVD.changeKey(RVD.scala:144); 	at is.hail.rvd.RVD.changeKey(RVD.scala:137); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:722); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:875); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:731); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1216); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:493); 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:717); 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:73); 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:8457,rewrite,rewrite,8457,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['rewrite'],['rewrite']
Modifiability,la:1141); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1141); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1157); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:91); 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:74); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:125); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:121); 	at is.hail.backend.spark.SparkBackend.lookupOrCompileCachedFunction(SparkBackend.scala:273); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:40); 	at is.hail.expr.ir.lowering.TableStageToRVD$.apply(RVDToTableStage.scala:112); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:689); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:110); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:8828,Rewrite,RewriteBottomUp,8828,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Rewrite'],['RewriteBottomUp']
Modifiability,la:463); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:68); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$1(CompileAndEvaluate.scala:50); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:9411,rewrite,rewrite,9411,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['rewrite'],['rewrite']
Modifiability,"lass org.apache.hadoop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:3758,Config,Configuration,3758,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,lass.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5249,Rewrite,RewriteBottomUp,5249,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,lass.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:4598,Rewrite,RewriteBottomUp,4598,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,le$.$anonfun$apply$4(Compile.scala:74); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:125); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:121); 	at is.hail.backend.spark.SparkBackend.lookupOrCompileCachedFunction(SparkBackend.scala:273); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:40); 	at is.hail.expr.ir.lowering.TableStageToRVD$.apply(RVDToTableStage.scala:112); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:689); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:110); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(Res,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:9089,Rewrite,RewriteBottomUp,9089,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"leapis/java-storage/commit/b1d026608a5e3772e8bf77f25f1daf68b007427a"">b1d0266</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpclient to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1795"">#1795</a>) (<a href=""https://github.com/googleapis/java-storage/commit/cf900f4139f30f89e3c0784467ddc12cc00cf81c"">cf900f4</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpcore to v4.4.16 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1786"">#1786</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3bf403e94c035e6cf936e062a1ced2b5221b3912"">3bf403e</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpmime to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1796"">#1796</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c9ee3ca8820531cd709bb8f8a58a736813346861"">c9ee3ca</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.18 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1782"">#1782</a>) (<a href=""https://github.com/googleapis/java-storage/commit/5bc517623ef04bdb9a71a51666754b9f753f4c69"">5bc5176</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.19 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1791"">#1791</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3184d65cce1368c2f39ff85a6ed02cf536902244"">3184d65</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.0...v2.15.1"">2.15.1</a> (2022-11-17)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Disable REGAPIC transport in storage v2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1762"">#1762</a>) (<a href=""https://github.com/googleapis/java-storage/commit/13d630e7ce89273c292acca7a7e048218ece4182"">13d630e</a>)</li>; <li>Update GrpcStorageImpl#get(BlobId) to return null on ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:8031,plugin,plugin,8031,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['plugin'],['plugin']
Modifiability,"leapis/java-storage/commit/b1d026608a5e3772e8bf77f25f1daf68b007427a"">b1d0266</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpclient to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1795"">#1795</a>) (<a href=""https://github.com/googleapis/java-storage/commit/cf900f4139f30f89e3c0784467ddc12cc00cf81c"">cf900f4</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpcore to v4.4.16 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1786"">#1786</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3bf403e94c035e6cf936e062a1ced2b5221b3912"">3bf403e</a>)</li>; <li>Update dependency org.apache.httpcomponents:httpmime to v4.5.14 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1796"">#1796</a>) (<a href=""https://github.com/googleapis/java-storage/commit/c9ee3ca8820531cd709bb8f8a58a736813346861"">c9ee3ca</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.18 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1782"">#1782</a>) (<a href=""https://github.com/googleapis/java-storage/commit/5bc517623ef04bdb9a71a51666754b9f753f4c69"">5bc5176</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.19 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1791"">#1791</a>) (<a href=""https://github.com/googleapis/java-storage/commit/3184d65cce1368c2f39ff85a6ed02cf536902244"">3184d65</a>)</li>; </ul>; <h2>v2.15.1</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.0...v2.15.1"">2.15.1</a> (2022-11-17)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Disable REGAPIC transport in storage v2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1762"">#1762</a>) (<a href=""https://github.com/googleapis/java-storage/commit/13d630e7ce89273c292acca7a7e048218ece4182"">13d630e</a>)</li>; <li>Update GrpcStorageImpl#get(BlobId) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12545:3180,plugin,plugin,3180,https://hail.is,https://github.com/hail-is/hail/pull/12545,1,['plugin'],['plugin']
Modifiability,lect$1(RDD.scala:1021); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1020); 	at is.hail.backend.spark.SparkBackend.parallelizeAndComputeWithIndex(SparkBackend.scala:429); 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:82); 	at __C1286Compiled.__m1290split_Block_region18_70(Emit.scala); 	at __C1286Compiled.__m1290split_Block(Emit.scala); 	at __C1286Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4(CompileAndEvaluate.scala:60); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2(CompileAndEvaluate.scala:60); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2$adapted(CompileAndEvaluate.scala:58); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:58); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:17); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:17); 	at is.hail.expr.ir.TableWriter.apply(TableWriter.scala:51); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:921); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:66); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:10870,adapt,adapted,10870,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['adapt'],['adapted']
Modifiability,"lemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview support for Accessing GCS via gRPC</p>; <ol>; <li>Set the environment variable <code>GOOGLE_CLOUD_ENABLE_DIRECT_PATH_XDS=true</code>, then run your program.</li>; </ol>; </li>; </ol>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/java-storage/blob/main/CHANGELOG.md"">google-cloud-storage's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.15.0...v2.15.1"">2.15.1</a> (2022-11-17)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Disable REGAPIC transport in storage v2 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1762"">#1762</a>) (<a href=""https://github.com/googleapis/java-storage/commit/13d630e7ce89273c292acca7a7e048218ece4182"">13d630e</a>)</li>; <li>Update GrpcStorageImpl#get(BlobId) to return null on 404 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/iss",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12529:5313,variab,variable,5313,https://hail.is,https://github.com/hail-is/hail/pull/12529,1,['variab'],['variable']
Modifiability,"lemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview support for Accessing GCS via gRPC</p>; <ol>; <li>Set the environment variable <code>GOOGLE_CLOUD_ENABLE_DIRECT_PATH_XDS=true</code>, then run your program.</li>; <li>When configuring your <code>StorageOptions</code> mimic the following:; <pre><code> StorageOptions.grpc(); .setAttemptDirectPath(true); .build(); </code></pre>; </li>; <li>Internally the default host endpoint <code>https://storage.googleapis.com:443</code> will be transformed to the applicable <code>google-c2p-experimental:///storage.googleapis.com</code></li>; </ol>; </li>; <li>; <p>Support for <code>java.time</code> types on model classes</p>; <ol>; <li>Points in time are now represented with <code>java.time.OffsetDateTime</code>, while durations are represented with <code>java.time.Duration</code></li>; <li>All existing <code>Long</code> centric methods are still present, but have been deprecated in favor of their corresponding <code>java.time</code> variant</li>; <li>At the next major version, these deprecated method",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:4283,variab,variable,4283,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['variab'],['variable']
Modifiability,"lemented for gRPC will result in a runtime error. For those operations which are not yet implemented, please continue to use the existing HTTP transport.</p>; <p>Special thanks (in alphabetical order) to <a href=""https://github.com/BenWhitehead""><code>@​BenWhitehead</code></a>, <a href=""https://github.com/frankyn""><code>@​frankyn</code></a>, <a href=""https://github.com/jesselovelace""><code>@​jesselovelace</code></a> and <a href=""https://github.com/sydney-munro""><code>@​sydney-munro</code></a> for their hard work on this effort.</p>; <h4>Notable Improvements</h4>; <ol>; <li>; <p>For all gRPC media related operations (upload/download) we are now more resource courteous then the corresponding HTTP counterpart. Buffers are fixed to their specified size (can't arbitrarily grow without bounds), are allocated lazily and only if necessary.</p>; <ol>; <li>Investigation into the possibility of backporting these improvements to the HTTP counterparts is ongoing</li>; </ol>; </li>; <li>; <p>Preview support for Accessing GCS via gRPC</p>; <ol>; <li>Set the environment variable <code>GOOGLE_CLOUD_ENABLE_DIRECT_PATH_XDS=true</code>, then run your program.</li>; <li>When configuring your <code>StorageOptions</code> mimic the following:; <pre><code> StorageOptions.grpc(); </code></pre>; </li>; </ol>; </li>; </ol>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/googleapis/java-storage/commit/bfd48a1b5542ff28ffa337eba883c4ca6c3b0aad""><code>bfd48a1</code></a> chore(main): release 2.15.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1765"">#1765</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3b8d137a113376d7dac9010b9207d435df2622f7""><code>3b8d137</code></a> docs: annotate all Option factory methods with their Nullability bounds (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1775"">#1775</a>)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12529:10677,variab,variable,10677,https://hail.is,https://github.com/hail-is/hail/pull/12529,1,['variab'],['variable']
Modifiability,"lete binary tree with `k` leaves. Conceptually, the leaves hold the current `k` elements (the heads of each of the `k` input streams), while each internal node records the result of the comparison between the ""winners"" of the two subtrees, where in this case the winner is the least element. Thus we can find the smallest of all `k` elements by looking at the root node. If we remove the smallest element, and replace it with the next value from that stream, we change the value in the corresponding leaf node, then we just need to replay the comparisons at all internal nodes on the path to the root. Note that to replay a comparison, we only need to know what element *lost* at this node previously. It must have lost to the previous overall winner, the element we just replaced. Using that observation, we only need to store the `k` current values in the `k` leaves, and in each of the `k-1` internal nodes we store the index of the element which lost the comparison at that node. That just leaves the overall winner, which we can store in a separate variable. Note that each element besides the overall winner loses exactly one ""match"", so the internal nodes store a permutation of the indices 0 to (k-1), minus the overall winner. This is a so-called ""loser tree"". In the implementation, I store the `k` leaves in a `Array[Long]`, where each element is a pointer to the head of the corresponding stream, and I store the `k-1` internal nodes in a `Array[Int]`, in the usual breadth-first order, where each element is the index of the stream which lost the comparison at that node (had the larger value). I use an index of `-1` to represent an imaginary element smaller than all real elements, and similarly an index of `k` is larger than all real elements. The tournament tree begins filled with `-1`, and each stream is advanced once, as their values push out all the `-1`s. When a steam ends, it's leaf is given the value `k`, and once the overall winner is `k`, we know all streams have ended.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9033:1718,variab,variable,1718,https://hail.is,https://github.com/hail-is/hail/pull/9033,1,['variab'],['variable']
Modifiability,"li>; <li><a href=""https://github.com/jupyter/notebook/commit/d2ef92f0b385b7ecd11cbf0f3af181ee8e494623""><code>d2ef92f</code></a> Backport PR <a href=""https://redirect.github.com/jupyter/notebook/issues/7142"">#7142</a>: Clean up lint handling (<a href=""https://redirect.github.com/jupyter/notebook/issues/7185"">#7185</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/8e9390d9af903f34bb1c8414c7e9b49d2fdec32f""><code>8e9390d</code></a> Backport PR <a href=""https://redirect.github.com/jupyter/notebook/issues/7132"">#7132</a>: Adopt ruff format (<a href=""https://redirect.github.com/jupyter/notebook/issues/7184"">#7184</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/4d07f1ee9b6d3dca2736e2bf3a1254451add8259""><code>4d07f1e</code></a> Install stable JupyterLab 4.0 in the releaser hook (<a href=""https://redirect.github.com/jupyter/notebook/issues/7183"">#7183</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/e73d410074e6dbf97273f761d1513ff61db2965c""><code>e73d410</code></a> Updated ui-tests Configuration in Contributing.md (<a href=""https://redirect.github.com/jupyter/notebook/issues/7124"">#7124</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/ea1a1538ef56084654b7486e1d0b96f06b33acbe""><code>ea1a153</code></a> Set <code>navigation_with_keys</code> to <code>False</code> (<a href=""https://redirect.github.com/jupyter/notebook/issues/7129"">#7129</a>)</li>; <li><a href=""https://github.com/jupyter/notebook/commit/d717c6b3613f3609139bc2b9fe8d0a126aaeeae2""><code>d717c6b</code></a> Add Python 3.12 classifier (<a href=""https://redirect.github.com/jupyter/notebook/issues/7111"">#7111</a>)</li>; <li>See full diff in <a href=""https://github.com/jupyter/notebook/compare/@jupyter-notebook/tree@7.0.6...@jupyter-notebook/tree@7.0.7"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=notebook&package-manager=pip&previo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14182:7871,Config,Configuration,7871,https://hail.is,https://github.com/hail-is/hail/pull/14182,1,['Config'],['Configuration']
Modifiability,"li>; <li>Fix conversions for custom metrics. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/94654"">kubernetes/kubernetes#94654</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Instrumentation]</li>; <li>A new alpha-level field, <code>SupportsFsGroup</code>, has been introduced for CSIDrivers to allow them to specify whether they support volume ownership and permission modifications. The <code>CSIVolumeSupportFSGroup</code> feature gate must be enabled to allow this field to be used. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92001"">kubernetes/kubernetes#92001</a>, <a href=""https://github.com/huffmanca""><code>@​huffmanca</code></a>) [SIG API Machinery, CLI and Storage]</li>; <li>Added pod version skew strategy for seccomp profile to synchronize the deprecated annotations with the new API Server fields. Please see the corresponding section <a href=""https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/135-seccomp/README.md#version-skew-strategy"">in the KEP</a> for more detailed explanations. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91408"">kubernetes/kubernetes#91408</a>, <a href=""https://github.com/saschagrunert""><code>@​saschagrunert</code></a>) [SIG Apps, Auth, CLI and Node]</li>; <li>Adds the ability to disable Accelerator/GPU metrics collected by Kubelet (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91930"">kubernetes/kubernetes#91930</a>, <a href=""https://github.com/RenaudWasTaken""><code>@​RenaudWasTaken</code></a>) [SIG Node]</li>; <li>Admission webhooks can now return warning messages that are surfaced to API clients, using the <code>.response.warnings</code> field in the admission review response. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92667"">kubernetes/kubernetes#92667</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>) [SIG API Machinery a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:3007,enhance,enhancements,3007,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['enhance'],['enhancements']
Modifiability,"libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1106"">#1106</a>)</li>; </ul>; <h2>Fixes</h2>; <ul>; <li>Remove del from Redis (Fixes <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1115"">#1115</a>) (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1227"">#1227</a>)</li>; <li>fix socket.error raises (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1129"">#1129</a>)</li>; <li>Fix buffer is closed error when using PythonParser class (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1213"">#1213</a>)</li>; </ul>; <h2>Version v2.0.0</h2>; <p>Version 2.0 is a complete rewrite of aioredis. Starting with this version, aioredis now follows the API of <a href=""https://github.com/andymccurdy/redis-py"">redis-py</a>, so you can easily adapt synchronous code that uses redis-py for async applications with aioredis-py.</p>; <p><strong>NOTE:</strong> This version is <em>not</em> compatible with earlier versions of aioredis. If you upgrade, you will need to make code changes.</p>; <p>For more details, read our <a href=""https://aioredis.readthedocs.io/en/latest/migration/"">documentation on migrating to version 2.0</a>.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aioredis-py/blob/master/CHANGELOG.md"">aioredis's changelog</a>.</em></p>; <blockquote>; <h2>2.0.1 - (2021-12-20)</h2>; <h3>Features</h3>; <ul>; <li>Added Python 3.10 to CI &amp; Updated the Docs; (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1160"">#1160</a>)</li>; <li>Enable mypy in CI (see <a href=""https://github-redirect.dependabot.com/aio-libs/aioredis-py/issues/1101"">#1101</a>)</li>; <li>Synchronized reading the responses from a connection; (see <a href",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11569:1540,adapt,adapt,1540,https://hail.is,https://github.com/hail-is/hail/pull/11569,1,['adapt'],['adapt']
Modifiability,"lier PR are responded to there and addressed in the code for this one. This PR is to enable `hail-az/https` Azure file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; - Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; - Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new azure-mgmt-storage package requirement.; - Updated `AzureAsyncFS` to use `(account, container, credential)` tuple as internal `BlobServiceClient` cache key; - Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token; - Update `RouterFS.ls` function and associated listfiles function to allow for trailing query strings during path traversal; - Update `AsyncFS.open_from` function to handle query-string urls in zero-length case; - Change to existing behavior: `LocalAsyncFSURL.__str__` no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; - Updated `InputResource` to not include the SAS token as part of the destination file name; - Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions to respect the new model, where it is no longer safe to extend URLs by just appending new segments with `+ ""/""` because there may be a query string, and added `'sas/azure-https'` test case to the fixture. Running tests for the SAS case requires some new test variables to allow the test code to generate SAS tokens (`build.yaml/test_hail_python_fs`):; ```; # Required for SAS testing on Azure; export HAIL_TEST_AZURE_RESGRP=haildev; export HAIL_TEST_AZURE_SUBID=12ab51c6-da79-4a99-8dec-3d2decc97343; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140:1745,extend,extend,1745,https://hail.is,https://github.com/hail-is/hail/pull/13140,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"localhost, executor driver, partition 0, ANY, 4726 bytes); 2018-10-09 14:46:42 Executor: INFO: Running task 0.0 in stage 4.0 (TID 4); 2018-10-09 14:46:42 ShuffleBlockFetcherIterator: INFO: Getting 1 non-empty blocks out of 1 blocks; 2018-10-09 14:46:42 ShuffleBlockFetcherIterator: INFO: Started 0 remote fetches in 0 ms; 2018-10-09 14:46:42 Executor: INFO: Finished task 0.0 in stage 4.0 (TID 4). 1539 bytes result sent to driver; 2018-10-09 14:46:42 TaskSetManager: INFO: Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:42 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:42 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.007 s; 2018-10-09 14:46:42 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.053572 s; 2018-10-09 14:46:42 CodeGenerator: INFO: Code generated in 5.955541 ms; 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4` AS `zzz1`; WHERE (0 = 1); 2018-10-09 14:46:42 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:42 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table7e606a8b83f4`; 2018-10-09 14:46:43 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 14:46:43 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 14:46:43 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 14:46:43 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 14:46:43 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 14:46:43 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 14:46:43 DAGScheduler: INFO: Missing parents: List(); 2018-10",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:45951,config,configuration,45951,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,"localhost, executor driver, partition 0, ANY, 4726 bytes); 2018-10-09 15:04:37 Executor: INFO: Running task 0.0 in stage 4.0 (TID 4); 2018-10-09 15:04:37 ShuffleBlockFetcherIterator: INFO: Getting 1 non-empty blocks out of 1 blocks; 2018-10-09 15:04:37 ShuffleBlockFetcherIterator: INFO: Started 0 remote fetches in 0 ms; 2018-10-09 15:04:37 Executor: INFO: Finished task 0.0 in stage 4.0 (TID 4). 1539 bytes result sent to driver; 2018-10-09 15:04:37 TaskSetManager: INFO: Finished task 0.0 in stage 4.0 (TID 4) in 7 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:37 TaskSchedulerImpl: INFO: Removed TaskSet 4.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:37 DAGScheduler: INFO: ResultStage 4 (collect at utils.scala:197) finished in 0.008 s; 2018-10-09 15:04:37 DAGScheduler: INFO: Job 2 finished: collect at utils.scala:197, took 0.051042 s; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 5.011153 ms; 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074` AS `zzz1`; WHERE (0 = 1); 2018-10-09 15:04:37 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:37 SparkSqlParser: INFO: Parsing command: SELECT *; FROM `table8508c46074`; 2018-10-09 15:04:38 root: INFO: optimize: before:; (TableCount; (TableKeyBy () False; (TableLiteral))); 2018-10-09 15:04:38 root: INFO: optimize: after:; (TableCount; (TableLiteral)); 2018-10-09 15:04:38 SparkContext: INFO: Starting job: fold at RVD.scala:361; 2018-10-09 15:04:38 DAGScheduler: INFO: Got job 3 (fold at RVD.scala:361) with 1 output partitions; 2018-10-09 15:04:38 DAGScheduler: INFO: Final stage: ResultStage 5 (fold at RVD.scala:361); 2018-10-09 15:04:38 DAGScheduler: INFO: Parents of final stage: List(); 2018-10-09 15:04:38 DAGScheduler: INFO: Missing parents: List(); 2018-10-09 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:28428,config,configuration,28428,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['configuration']
Modifiability,lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); at is.hail.utils.package$.using(package.scala:664); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); at is.hail.utils.package$.using(package.scala:664); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630); at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:5886,adapt,adapted,5886,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['adapt'],['adapted']
Modifiability,lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:601); at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:637); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:632); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:631); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:77); at is.hail.utils.package$.using(package.scala:665); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:77); at is.hail.utils.package$.using(package.scala:665); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:64); at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:631); at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:89); at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:7148,adapt,adapted,7148,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['adapt'],['adapted']
Modifiability,"ls/htsjdk/issues/1622"">#1622</a>); 347c0ac57 Fix EdgeReadIterator (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>); d15a5bacb Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</p>; <h2>3.0.0</h2>; <p>Htsjdk 3.0.0: Revenge of the Simple Allele</p>; <p>This is the first htsjdk with a major version increase in a long time. We bumped it to indicate there are some breaking changes that will potentially require downstream code changes. Notably, <code>Allele</code> became an interface instead of a concrete class. <code>SimpleAllele</code> may be used as a replacement if you have classes which previously subclassed allele.</p>; <p>New Plugin Infrastructure:; 6a60de7c2 Move API marker annotations into new annotation package. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1558"">#1558</a>); 7ac95d5f7 Plugin framework and interfaces for versioned file format codecs (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1525"">#1525</a>); d40fe5412 Beta implementation of Bundles. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1546"">#1546</a>)</p>; <p>CRAM; 489c4192d Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>); 22aec6782 Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>); 6507249a4 Make the CRAM MD5 failure message more user friendly. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1607"">#1607</a>); b5af659e6 Fix restoration of read base feature code. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1379"">#1379</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1590"">#1590</a>); e63c34a92 Ignore TC, TN on CRAM read (<a hr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:1797,Plugin,Plugin,1797,https://hail.is,https://github.com/hail-is/hail/pull/12229,2,['Plugin'],['Plugin']
Modifiability,"lt even though a DesignSpace v5 did contain 'STAT' definitions (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3045"">#3045</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/3046"">#3046</a>).</li>; </ul>; <p><strong>NOTE</strong>: The 4.39.1 distribution was &quot;yanked&quot; from PyPI to prevent users from accidentally upgrading to it.</p>; <h2>4.39.1</h2>; <ul>; <li>[avar2] Added experimental support for reading/writing avar version 2 as specified in this draft proposal:; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/avar2.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/avar2.md</a></li>; <li>[glifLib] Wrap underlying XML library exceptions with GlifLibError when parsing GLIFs, and also print the name and path of the glyph that fails to be parsed (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3029"">fonttools/fonttools#3029</a>).</li>; <li>[feaLib] Consult avar for normalizing user-space values in ConditionSets and in VariableScalars (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3042"">fonttools/fonttools#3042</a>, <a href=""https://redirect.github.com/fonttools/fonttools/pull/3043"">fonttools/fonttools#3043</a>).</li>; <li>[ttProgram] Handle string input to Program.fromAssembly() (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3038"">fonttools/fonttools#3038</a>).</li>; <li>[otlLib] Added a config option to emit GPOS 7 lookups, currently disabled by default because of a macOS bug (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3034"">fonttools/fonttools#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3027"">fonttools/fonttools#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3032"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:1935,Variab,VariableScalars,1935,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['Variab'],['VariableScalars']
Modifiability,"lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1177"">#1177</a>)</li>; <li>misc speed improvements/optimisations</li>; </ul>; <h2>tqdm v4.63.0 stable</h2>; <ul>; <li>add <code>__reversed__()</code></li>; <li>add efficient <code>__contains__()</code></li>; <li>improve CLI startup time (replace <code>pkg_resources</code> =&gt; <code>importlib</code>)</li>; <li><code>tqdm.autonotebook</code> warning &amp; <code>std</code> fallback on missing <code>ipywidgets</code> (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1218"">#1218</a> &lt;- <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1082"">#1082</a>, <a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1217"">#1217</a>)</li>; <li>warn on positional CLI arguments</li>; <li>misc build/test framework updates; <ul>; <li>enable <code>py3.10</code> tests</li>; <li>add <code>conda</code> dependencies</li>; <li>update pre-commit hooks</li>; <li>fix <code>pytest</code> config (<code>nbval</code>, <code>asyncio</code>)</li>; <li>fix dependencies &amp; tests</li>; <li>fix site deployment</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.3 stable</h2>; <ul>; <li>fix minor typo (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>minor example fix (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1246"">#1246</a>)</li>; <li>misc tidying &amp; refactoring</li>; <li>misc build/dev framework updates; <ul>; <li>update dependencies</li>; <li>update linters</li>; <li>update docs deployment branches</li>; </ul>; </li>; <li>misc test/ci updates; <ul>; <li>test forks</li>; <li>tidy OS &amp; Python version tests</li>; <li>bump primary python version 3.7 =&gt; 3.8</li>; <li>beta py3.10 testing</li>; <li>fix py2.7 tests</li>; <li>better timeout handling</li>; </ul>; </li>; </ul>; <h2>tqdm v4.62.2 stable</h2>; <ul>; <li>fix notebook memory leak (<a href=""https://github-redirect.dependabot.com/tqdm/tqdm/issues/1216"">#1216</a>)</li>; </",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12260:2853,config,config,2853,https://hail.is,https://github.com/hail-is/hail/pull/12260,1,['config'],['config']
Modifiability,"lucene_grammar.py raised when run...</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/e4f3ce2a0805bf1561b96b8f13210f07fe3651ab""><code>e4f3ce2</code></a> Prep for 3.1.0 release</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/40babe02aa8a7905c5b46d010888516036be02ab""><code>40babe0</code></a> More example updates, PEP-8 names, f-strings.</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/85c2ef19a3e11faad0cb405a5ab66bdca7e49f45""><code>85c2ef1</code></a> Minor formatting change, bug-fix on 0000 time</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/2fc41a02f32b4f7a769f036daec58ba4f233b106""><code>2fc41a0</code></a> Update ci.yml</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/08e7cfdb94fecf2d99c324856c37ec66fac0eeed""><code>08e7cfd</code></a> Minor changes in examples, conversion to PEP8 names, etc.</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/a8b05ccbe380117dac40b4cf6d9ffe08266fd7ed""><code>a8b05cc</code></a> Slight perf enhancement in Empty</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/801863aa4582a8ce5e6a7408d4966afcd247ea90""><code>801863a</code></a> Make htmlStripper.py and html_table_parser examples use PEP-8 names, add comm...</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/7d4da80b2bca8a2767134f4a181ea9aac4bbb230""><code>7d4da80</code></a> Prep for release</li>; <li><a href=""https://github.com/pyparsing/pyparsing/commit/be0310a83436bb4893d0068bb5da3059199e4c0b""><code>be0310a</code></a> Add bf parser/executor example</li>; <li>Additional commits viewable in <a href=""https://github.com/pyparsing/pyparsing/compare/pyparsing_3.0.9...3.1.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyparsing&package-manager=pip&previous-version=3.0.9&new-version=3.1.0)](https://docs.github.com/en/github/managing-security-vulnerabilit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13334:8159,enhance,enhancement,8159,https://hail.is,https://github.com/hail-is/hail/pull/13334,1,['enhance'],['enhancement']
Modifiability,"ly which handles correctly handles dependencies like `pyspark`. - There are also some phony targets for convenience: `jar`, `zip`, `pip-install`, `docs`, and `docs-no-test`. - Fix configuration of Spark version for the python package. The version is written by make into `python/spark_version` and read by `python/setup.py`. Many of the tests pass against 2.3.0, but there's some floating point value changes. - add breezeVersions for all currently released Spark versions greater than 2.2.0. - For developers, require python package `py` version 1.7.0 or later to allow `pytest` to test an installed package while loading the doctest expressions from the source code. (We could also determine where hail was installed and pass that path to pytest instead of `python/src`, but using the environment variable `PY_IGNORE_IMPORTMISMATCH` seems simple and safe enough). ---. ### Explainers. #### env_var.mk. This is a Makefile that is intended to be `include`d by other Makefiles. It defines a [multi-line variable](https://www.gnu.org/software/make/manual/html_node/Multi_002dLine.html) that [takes arguments](https://www.gnu.org/software/make/manual/html_node/Call-Function.html#Call-Function) (known in any reasonable language as a ""function""). It is intended to be used like this:. ```; VERSION = 30; $(eval $(call ENV_VAR,VERSION)). build: env/VERSION; build:; ... $(VERSION) ...; ```. Each time this Makefile is executed, at Makefile parse-time, `make` evaluates the `ifneq` to compare the current value of the variable to the previously used value (if any). If they differ, a phony (ergo always needs to be rebuilt) target is dynamically generated. That target will force a execution of any dependent targets, in the example above, it will force `build` to be executed. If the variable's current value and it's previous value do not differ, no target is generated and thus nothing is executed. #### MAKEFLAGS += --no-builtin-rules. This disables all the automatic rules, making Makefile parsing/in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130:2572,variab,variable,2572,https://hail.is,https://github.com/hail-is/hail/pull/5130,1,['variab'],['variable']
Modifiability,"m/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Other.20genome/near/397467764). The following is seemingly correct code but it doesn't work:; ```python3; import hail as hl. rgwheat = hl.ReferenceGenome('Wheat', ...). hl.init(default_reference=rgwheat); ```; The first problem is that the `@typecheck` on `hl.init`, `hl.init_spark`, etc. only allows a built-in reference genome. . Even if we relax that requirement, we encounter a deeper problem: creating the reference genome initializes Hail. In particular, [we call `Env.backend()`](https://github.com/hail-is/hail/blob/main/hail/python/hail/genetics/reference_genome.py#L117-L118) (which calls `Env.hc()`, which forces initialization) so that we can call `add_reference`. What does initialization mean? Historically, it meant connection to or starting a JVM/Spark process. In QoB/ServiceBackend, initialization just loads configurations, it doesn't really do anything irreversible. Regardless of what it does, we only allow initialization *once*. OK, so, there's two possible routes to fix this problem:; 1. Rewrite `ReferenceGenome.__init__` such that it does not initialize Hail. You have to decide how reference genomes are ultimately communicated to the backend. Do you hang a list of all created reference genomes off of the `ReferenceGenome` class? Do you require explicit registering a la `hl.register_reference`? The latter seems a bit silly. The former seems OK, but you could also ...; 2. Allow modification of the default reference after initialization. The default reference genome is just a field on the HailContext: `_default_ref` which is accessed through `hl.default_reference()`. Just modify `hl.default_reference` to *return* the reference with no arguments and *set* the reference with one argument. Now this works:. ```python3; import hail as hl; rgwheat = hl.ReferenceGenome('Wheat', ...); hl.default_reference(rgwheat); mt = hl.import_vcf('wheat.vcf'); ```. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13856:1160,Rewrite,Rewrite,1160,https://hail.is,https://github.com/hail-is/hail/issues/13856,1,['Rewrite'],['Rewrite']
Modifiability,"m/python-pillow/Pillow/issues/7928"">#7928</a>; [radarhere, hugovk]</p>; </li>; <li>; <p>Deprecate <code>eval()</code>, replacing it with <code>lambda_eval()</code> and <code>unsafe_eval()</code> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7927"">#7927</a>; [radarhere, hugovk]</p>; </li>; <li>; <p>Raise <code>ValueError</code> if seeking to greater than offset-sized integer in TIFF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7883"">#7883</a>; [radarhere]</p>; </li>; <li>; <p>Add <code>--report</code> argument to <code>__main__.py</code> to omit supported formats <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7818"">#7818</a>; [nulano, radarhere, hugovk]</p>; </li>; <li>; <p>Added RGB to I;16, I;16L, I;16B and I;16N conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7918"">#7918</a>, <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7920"">#7920</a>; [radarhere]</p>; </li>; <li>; <p>Fix editable installation with custom build backend and configuration options <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7658"">#7658</a>; [nulano, radarhere]</p>; </li>; <li>; <p>Fix putdata() for I;16N on big-endian <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7209"">#7209</a>; [Yay295, hugovk, radarhere]</p>; </li>; <li>; <p>Determine MPO size from markers, not EXIF data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7884"">#7884</a>; [radarhere]</p>; </li>; <li>; <p>Improved conversion from RGB to RGBa, LA and La <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7888"">#7888</a>; [radarhere]</p>; </li>; <li>; <p>Support FITS images with GZIP_1 compression <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7894"">#7894</a>; [radarhere]</p>; </li>; <li>; <p>Use I;16 mode for 9-bit JPEG 2000 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7900"">#7900</a>; [scaramallion, radarhere]</p>; </l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:11303,config,configuration,11303,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['config'],['configuration']
Modifiability,"mance</h3>; <ul>; <li>Speed-up the new backtracking parser about 4X in general (enabled when <code>--target-version</code> is set to 3.10 and higher). (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2728"">#2728</a>)</li>; <li>Black is now compiled with mypyc for an overall 2x speed-up. 64-bit Windows, MacOS, and Linux (not including musl) are supported. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/1009"">#1009</a>, <a href=""https://github-redirect.dependabot.com/psf/black/issues/2431"">#2431</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not accept bare carriage return line endings in pyproject.toml (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2408"">#2408</a>)</li>; <li>Add configuration option (<code>python-cell-magics</code>) to format cells with custom magics in Jupyter Notebooks (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2744"">#2744</a>)</li>; <li>Allow setting custom cache directory on all platforms with environment variable <code>BLACK_CACHE_DIR</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2739"">#2739</a>).</li>; <li>Enable Python 3.10+ by default, without any extra need to specify -<code>-target-version=py310</code>. (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2758"">#2758</a>)</li>; <li>Make passing <code>SRC</code> or <code>--code</code> mandatory and mutually exclusive (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2804"">#2804</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>Improve error message for invalid regular expression (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2678"">#2678</a>)</li>; <li>Improve error message when parsing fails during AST safety check by embedding the underlying SyntaxError (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2693"">#2693</a>)</li>; <li>No longer color diff headers white as it's unreadable in light themed terminals (<a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11468:5006,variab,variable,5006,https://hail.is,https://github.com/hail-is/hail/pull/11468,1,['variab'],['variable']
Modifiability,"mature product from Intel. Production users include JD.com. ### User-level access control ; An orthogonal issue that still needs to be addressed. [RBAC Authorization - Kubernetes](https://kubernetes.io/docs/reference/access-authn-authz/rbac/). *TODO*. ### Related: Firecracker; Interesting project, similar to Kata and gVisor in its isolation properties. Doesn’t work with Kubernetes, replicates some Kube functionality.; * [Announcing the Firecracker Open Source Technology: Secure and Fast microVM for Serverless Computing | AWS Open Source Blog](https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/); * Potentially lower runtime cost that Kata; * Written in Rust :). ### Alternatives; [Nabla containers: a new approach to container isolation · Nabla Containers](https://nabla-containers.github.io); * Unclear how good containment is. Worth exploring. ### Performance; [Runtime performance benchmark result. containerd vs CRI-containerd vs CRI-O · GitHub](https://gist.github.com/kunalkushwaha/66629a90e0f8f5cc5dc512ef1c346f2f). [Measuring the Horizontal Attack Profile of Nabla Containers](https://outlookseries.com/A0784/Infrastructure/3868.htm); * 10-30% cost for networking-heavy operations. ### Example implementations of sandboxed containers; https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md. [CRI installation - Kubernetes](https://kubernetes.io/docs/setup/cri/). ### References:; [Kata Containers - Why Kata Containers doesn’t replace Kubernetes: A Kata Containers explainer](https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/). [Kubernetes Container Runtimes - kubedex.com](https://kubedex.com/kubernetes-container-runtimes/). [GitHub - containerd/cri: Containerd Plugin for Kubernetes Container Runtime Interface](https://github.com/containerd/cri). https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5111:2271,sandbox,sandboxed,2271,https://hail.is,https://github.com/hail-is/hail/issues/5111,2,"['Plugin', 'sandbox']","['Plugin', 'sandboxed']"
Modifiability,"ment>;; const good3 = () => <span><div>OK</div><span>GOOD!</span></span>;; ```. #### JSX naming conventions; 1. Lowercase components are just built-in html elements. i.e `<span>` is a an HTML `<span>` on output.; 2. Uppercase components are javascript functions. This makes composing components really simple. ```jsx; const CoolComponent = () => <span>Hello World</span>;. export default () => <CoolComponent>;; ```. You can pass state to these user-defined components, much like you would in HTML, using attributes. These attributes can have arbitrary names, except they must start with a lowercase letter, and follow camel-case convention. These attributes are called `props`. ```jsx; const CoolComponent = (props) => <span>Hello {props.name}!</span>;. export default () => <CoolComponent name=""Alex"">; #mounts <span>Hello Alex!</span> in DOM; ```. #### PureComponent / shallow watch; React's reconciler is triggered whenever this.setState is called, resulting in a walk down the descendent nodes, based on either the presence of that state variable as a ""prop"" (i.e `<MyComponent name={this.state.name}/>`), or its use directly within the component (i.e `{this.state.name === 'Alex' ? <div>Do stuff</div> : <div>Do other stuff</div>). To give the reconciler less work to do, when accepting objects as props, use a `<PureComponent>`. This will tell React to check the reference for diff, rather than deep value compare. Obviously much faster to do the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:9213,variab,variable,9213,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['variab'],['variable']
Modifiability,mer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); E 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:354); E 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.mutable.Re,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:1866,Rewrite,RewriteBottomUp,1866,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['Rewrite'],['RewriteBottomUp']
Modifiability,"method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3027"">fonttools/fonttools#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3032"">fonttools/fonttools#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3031"">fonttools/fonttools#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built with feaLib from FEA feature file. Also, added support for building multiple VFs defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3024"">fonttools/fonttools#3024</a>).</li>; <li>[mtiLib] Only add <code>Debg</code> table with lookup names when <code>FONTTOOLS_LOOKUP_DEBUGGING</code> env variable is set (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3023"">fonttools/fonttools#3023</a>).</li>; </ul>; <h2>4.39.0</h2>; <ul>; <li>[mtiLib] Optionally add <code>Debg</code> debug info for MTI feature builds (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3018"">#3018</a>).</li>; <li>[ttx] Support reading input file from standard input using special <code>-</code> character, similar to existing <code>-o -</code> option to write output to standard output (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3020"">#3020</a>).</li>; <li>[cython] Prevent <code>cython.compiled</code> raise AttributeError if cython not installed properly (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3017"">#3017</a>).</li>; <li>[OS/2] Guard against ZeroDivisionError when calculating xAvgCharWidth in the unlikely scenario no glyph has non-zero advance (<a href=""https://redirect.github.com/fonttools/fontt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:3599,variab,variable,3599,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['variab'],['variable']
Modifiability,"methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2507,variab,variables,2507,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['variab'],['variables']
Modifiability,"mitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at mapPartitions at ContextRDD.scala:137) (first 15 tasks are for partitions Vector(0)); 2018-10-09 15:04:38 TaskSchedulerImpl: INFO: Adding task set 5.0 with 1 tasks; 2018-10-09 15:04:38 TaskSetManager: INFO: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4777 bytes); 2018-10-09 15:04:38 Executor: INFO: Running task 0.0 in stage 5.0 (TID 5); 2018-10-09 15:04:38 BlockManager: INFO: Found block rdd_9_0 locally; 2018-10-09 15:04:38 CodeGenerator: INFO: Code generated in 14.135243 ms; 2018-10-09 15:04:38 CodeGenerator: INFO: Code generated in 8.306294 ms; 2018-10-09 15:04:38 Executor: INFO: Finished task 0.0 in stage 5.0 (TID 5). 1119 bytes result sent to driver; ```; </details>. <details>; <summary>Broken hail.log</summary>. ```; 2018-10-09 14:46:38 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 14:46:38 Hail: INFO: Running Hail version devel-e7552fd55a9d; 2018-10-09 14:46:38 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 14:46:38 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 14:46:38 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@28f0ac7{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49a30f89{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4495af6e{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6baf9f3b{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 14:46:38 ContextHan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:31125,config,config,31125,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['config'],['config']
Modifiability,"mmands or subgroups, like `hailctl dataproc`, `hailctl batch`, etc. Groups are defined like this:; ; ```; @hailctl.group(; help=""Manage the Hail Batch service.""); def batch():; pass; ```; - A command in a group is defined like this:. ```; @batch.command(; help=""Get a particular batch's info.""); @click.argument('batch_id', type=int); @click.option('--output-format', '-o',; type=click.Choice(['yaml', 'json']),; default='yaml', show_default=True,; help=""Specify output format"",); def get(batch_id, output_format):; ...; ```. The command decorator replaces the function with one that takes a `List[str]` of command line parameters, parses them, and calls the original function. The option options are pretty self-explanatory. - To access an argument to a group (like `dataproc --beta`) in a (sub)command, use the decorator `click.pass_context` to pass the click context which allows you to access parent group parameters. `dataproc start` is an example:. ```; @dataproc.command(; help=""Start a Dataproc cluster configured for Hail.""); @click.argument('cluster_name'); ...; @click.pass_context; def start(ctx, cluster_name, ...):; beta = ctx.parent.params['beta']; ```. The help output for a group looks like this:. ```; $ hailctl dataproc --help; Usage: hailctl dataproc [OPTIONS] COMMAND [ARGS]... Manage and monitor Hail deployments. Options:; --beta Force use of `beta` in gcloud commands; --help Show this message and exit. Commands:; connect Connect to a running Dataproc cluster; describe Gather information about a Hail (Table or MatrixTable) file...; diagnose Diagnose problems in a Dataproc cluster.; list List Dataproc clusters.; modify; start Start a Dataproc cluster configured for Hail.; stop Shut down a Dataproc cluster.; submit Submit a Python script to a running Dataproc cluster.; ```. The help output for a command looks like:. ```; $ hailctl batch get --help; Usage: hailctl batch get [OPTIONS] BATCH_ID. Get a particular batch's info. Options:; -o, --output-format [yaml|json]; Sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:2494,config,configured,2494,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['config'],['configured']
Modifiability,"mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5010,config,configuration,5010,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"mples by one but only D will receive ""own time"" (I believe). Sorting by samples sort of reveals the call stack. You have to be careful to focus on the own time as you scan down the stack though!. My take on the relatively expensive operations:. 1. Struct decoding (the fourth generated code one, with 399 own time and 229 samples) is pretty branchy: it checks a bit for each field. I'm not sure how to speed this up. Consider a struct of 8 optional fields. There are 2^8 possible missingness pattern. Each pattern corresponds to a different sequence of field-decoders. I suppose we could generate 256 different patterns and jump to them? That seems excessive. We could maybe generate 16 patterns but that only saves 3/4 of the branches. Maybe that's enough for a substantial speedup?; 3. `LEB128InputBuffer.readByte` (365, 139). `readByte` just calls its child `InputBuffer`'s `readByte`. My best explanation: we call `readByte` *a lot*. That kinda makes sense: `LEB128InputBuffer` issues a `readByte` for each byte in the variable length encoding of the integer. ; 4. Optional Array of Optional Int32 (450, 138). These arrays will have a bunch of LEB128 encoded integers, which, as stated, has a bunch of branches making this fairly expensive.; 5. `BlockingInputBuffer.ensure` is expensive. (705, 130). The blocking happens *after* Zstd. This dataset is about 16 GiB when uncompressed. The block size is 65kB (ergo 244k blocks the uncompressed dataset) When reading a large numbers of bytes, `ensure` is called infrequently compared to actually reading the bytes. However, LEB128 relies heavily on `readByte` and thus issues O(N_BYTES) calls to `ensure`.; 6. `Zstd.decompressByteArray` (249, 111). This is eliminated in main by commit 507744f2d7. Patrick's branch (on which I ran these experiments) is out of date.; 7. `BlockingInputBuffer.readBytes` (798, 88). I think this is a bit unavoidable: we have to copy bytes from input to the region. We could avoid the intermediary buffer: copy as much a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792:4971,variab,variable,4971,https://hail.is,https://github.com/hail-is/hail/issues/13792,1,['variab'],['variable']
Modifiability,"much detail as possible. -----------------------------------------------------------------------------. Package Info:; Name: hail; Version: 0.2.93; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages; Requires: dill, bokeh, scipy, azure-storage-blob, janus, parsimonious, botocore, google-cloud-storage, tabulate, Jinja2, python-json-logger, plotly, avro, azure-identity, PyJWT, orjson, tqdm, aiohttp-session, google-auth, nest-asyncio, uvloop, humanize, hurry.filesize, decorator, requests, Deprecated, aiohttp, asyncinit, numpy, pyspark, sortedcontainers, boto3, pandas. -----------------------------------------------------------------------------. Importing hail via the IPython console in Spyder causes the following error:. Python 3.8.12 (default, Oct 12 2021, 06:23:56) ; IPython 8.2.0 -- An enhanced Interactive Python. In [1]: `import hail`. > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/comms/frontendcomm.py"", line 164, in poll_one; > asyncio.run(handler(out_stream, ident, msg)); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/nest_asyncio.py"", line 36, in run; > task = asyncio.ensure_future(main); > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/asyncio/tasks.py"", line 684, in ensure_future; > raise TypeError('An asyncio.Future, a coroutine or an awaitable is '; > TypeError: An asyncio.Future, a coroutine or an awaitable is required; > [SpyderKernelApp] ERROR | Exception in message handler:; > Traceback (most recent call last):; > File ""/Users/jacobbayer/opt/anaconda3/lib/python3.8/site-packages/spyder_kernels/comms/frontendcomm.py"", line 164, in poll_one; > asyncio.run(handler(out_stream, ident, msg)); > File ""/Users/jacobb",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758:1179,enhance,enhanced,1179,https://hail.is,https://github.com/hail-is/hail/issues/11758,1,['enhance'],['enhanced']
Modifiability,"n local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:3078,variab,variable,3078,https://hail.is,https://github.com/hail-is/hail/pull/11328,2,['variab'],['variable']
Modifiability,"n the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2975,variab,variable,2975,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['variab'],['variable']
Modifiability,"n, payload); 208 path = action_routes[action]; 209 port = self._backend_server_port; --> 210 resp = self._requests_session.post(f'http://localhost:{port}{path}', data=data); 211 if resp.status_code >= 400:; 212 error_json = orjson.loads(resp.content). File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:635, in Session.post(self, url, data, json, **kwargs); 624 def post(self, url, data=None, json=None, **kwargs):; 625 r""""""Sends a POST request. Returns :class:`Response` object.; 626 ; 627 :param url: URL for the new :class:`Request` object.; (...); 632 :rtype: requests.Response; 633 """"""; --> 635 return self.request(""POST"", url, data=data, json=json, **kwargs). File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:587, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 582 send_kwargs = {; 583 ""timeout"": timeout,; 584 ""allow_redirects"": allow_redirects,; 585 }; 586 send_kwargs.update(settings); --> 587 resp = self.send(prep, **send_kwargs); 589 return resp. File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:701, in Session.send(self, request, **kwargs); 698 start = preferred_clock(); 700 # Send the request; --> 701 r = adapter.send(request, **kwargs); 703 # Total elapsed time of the request (approximately); 704 elapsed = preferred_clock() - start. File /opt/conda/lib/python3.10/site-packages/requests/adapters.py:502, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 487 resp = conn.urlopen(; 488 method=request.method,; 489 url=url,; (...); 498 chunked=chunked,; 499 ); 501 except (ProtocolError, OSError) as err:; --> 502 raise ConnectionError(err, request=request); 504 except MaxRetryError as e:; 505 if isinstance(e.reason, ConnectTimeoutError):; 506 # TODO: Remove this in 3.0.0: see #2811. ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:12774,adapt,adapter,12774,https://hail.is,https://github.com/hail-is/hail/issues/13960,2,['adapt'],"['adapter', 'adapters']"
Modifiability,n.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.lambda$getPropsWithPrefix$3(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getLookupKey(HadoopConfigurationProperty.java:120); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.HadoopConfigurationProperty.getPropsWithPrefix(HadoopConfigurationProperty.java:106); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:421); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemConfiguration.getGcsFsOptionsBuilder(GoogleHadoopFileSystemConfiguration.java:383); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1516); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494); at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669); at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94); at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703); at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685); at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373); at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); at is.hail.io.fs.HadoopFS.is$hail$io$fs$HadoopFS$$_fileSystem(HadoopFS.scala:157); at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:244); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:226); at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:225); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:3432,config,configure,3432,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['config'],['configure']
Modifiability,"n3.10/site-packages/notebook/notebook/handlers.py"", line 95, in get; May 16 14:17:07 mw116-m python[8309]: self.write(self.render_template('notebook.html',; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/base/handlers.py"", line 507, in render_template; May 16 14:17:07 mw116-m python[8309]: return template.render(**ns); May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/jinja2/environment.py"", line 1301, in render; May 16 14:17:07 mw116-m python[8309]: self.environment.handle_exception(); May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/jinja2/environment.py"", line 936, in handle_exception; May 16 14:17:07 mw116-m python[8309]: raise rewrite_traceback_stack(source=source); May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/templates/notebook.html"", line 1, in top-level template code; May 16 14:17:07 mw116-m python[8309]: {% extends ""page.html"" %}; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/templates/page.html"", line 154, in top-level template code; May 16 14:17:07 mw116-m python[8309]: {% block header %}; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/templates/notebook.html"", line 114, in block 'header'; May 16 14:17:07 mw116-m python[8309]: {% for exporter in get_frontend_exporters() %}; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/notebook/notebook/handlers.py"", line 23, in get_frontend_exporters; May 16 14:17:07 mw116-m python[8309]: from nbconvert.exporters.base import get_export_names, get_exporter; May 16 14:17:07 mw116-m python[8309]: File ""/opt/conda/miniconda3/lib/python3.10/site-packages/nbconvert/__init__.py"", line 4, in <module>; May 16 14:17:07 mw116-m python[8309]: from .exporters import *; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13059:2482,extend,extends,2482,https://hail.is,https://github.com/hail-is/hail/issues/13059,1,['extend'],['extends']
Modifiability,"n; Python 3.9.18; ```; I activate java 11.0.20.1; ```sh; $ java -version; openjdk version ""11.0.20.1"" 2023-08-22 LTS; OpenJDK Runtime Environment Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS); OpenJDK 64-Bit Server VM Corretto-11.0.20.9.1 (build 11.0.20.1+9-LTS, mixed mode); ```; * I clone hail; ```sh; $ cd /tmp; $ git clone --branch 0.2.124 --depth 1 https://github.com/broadinstitute/hail.git; ```; * I build hail; ```sh; $ cd hail/hail/; $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; [...]; Successfully installed hail-0.2.124; hailctl config set query/backend spark; ```; * At this point Hail seems correcly installed; ```sh; $ pip show hail; Name: hail; Version: 0.2.124; Summary: Scalable library for exploring and analyzing genomic data.; Home-page: https://hail.is; Author: Hail Team; Author-email: hail@broadinstitute.org; License: UNKNOWN; Location: /home/hadoop/.local/lib/python3.9/site-packages; ```; * For sake of configuration I create a symlink of the hail backend; ```sh; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * Confident of the. installation I try to run spark shell; ```sh; $ spark-shell; [...]; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings ; ```. I am out of idea on how to solve the current situation. ; Thanks. ### Version. 0.2.124. ### Relevant log output. ```shell; $ spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adju",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837:1268,config,configuration,1268,https://hail.is,https://github.com/hail-is/hail/issues/13837,1,['config'],['configuration']
Modifiability,nCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); at is.hail.utils.package$.using(package.scala:664); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:5113,adapt,adapted,5113,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['adapt'],['adapted']
Modifiability,nCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:601); at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:637); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:632); at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:631); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:77); at is.hail.utils.package$.using(package.scala:665); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:6375,adapt,adapted,6375,https://hail.is,https://github.com/hail-is/hail/issues/14168,3,['adapt'],['adapted']
Modifiability,nCompilable.scala:67); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:416); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:452); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:646); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:646); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); at is.hail.backend.spark.Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:4207,adapt,adapted,4207,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['adapt'],['adapted']
Modifiability,"namespaces and services are active in our cluster at a given point in time. TL;DR Switching from NGINX to Envoy with CI acting as the ""control plane"" for our internal networking allows us to more easily dynamically configure our Kubernetes networking and achieve proper connection pooling/load-balancing over TLS, which translates to less resource consumption and lower request latencies. ## Motivation; This is primarily a performance-motivated change, and one largely based on our (ab)use of NGINX in order to work with our dynamically-generated Kubernetes test namespaces. Currently, we configure NGINX by creating server blocks that dynamically resolve and dispatch requests based on matching regular expressions on the host and path headers. This is in large part due that at gateway deploy time we do not statically know all of the namespaces and namespace-service combinations that will exist in the cluster in the future. This is true for `default`, but not test namespaces, and NGINX will refuse to start with statically-configured clusters that it cannot reach. Making the server blocks make the routing decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:1214,config,configured,1214,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['config'],['configured']
Modifiability,"nd Functionality:</p>; <ul>; <li>Updating axios requests to be delayed by pre-emptive promise creation (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Adding &quot;synchronous&quot; and &quot;runWhen&quot; options to interceptors api (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2702"">#2702</a>)</li>; <li>Updating of transformResponse (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3377"">#3377</a>)</li>; <li>Adding ability to omit User-Agent header (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3703"">#3703</a>)</li>; <li>Adding multiple JSON improvements (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3688"">#3688</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3763"">#3763</a>)</li>; <li>Fixing quadratic runtime and extra memory usage when setting a maxContentLength (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3738"">#3738</a>)</li>; <li>Adding parseInt to config.timeout (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3781"">#3781</a>)</li>; <li>Adding custom return type support to interceptor (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3783"">#3783</a>)</li>; <li>Adding security fix for ReDoS vulnerability (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3980"">#3980</a>)</li>; </ul>; <p>Internal and Tests:</p>; <ul>; <li>Updating build dev dependancies (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3401"">#3401</a>)</li>; <li>Fixing builds running on Travis CI (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3538"">#3538</a>)</li>; <li>Updating follow rediect version (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3694"">#3694</a>, <a href=""https://github-redirect.dependabot.com/axios/axios/pull/3771"">#3771</a>)</li>; <li>Updating karma sauce launcher to fix failing sauce tests (<a href=""https://github-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11080:1326,config,config,1326,https://hail.is,https://github.com/hail-is/hail/pull/11080,4,['config'],['config']
Modifiability,"nddistrict/allow-passing-in-additionl-jwt-headers</li>; <li><a href=""https://github.com/lepture/authlib/commit/38c6444bf4ae193c55c96a4b0d849ea9800d85f8""><code>38c6444</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/333"">#333</a> from jeffsawatzky/maintain-0.15</li>; <li><a href=""https://github.com/lepture/authlib/commit/2ea533c4f658206ec0f09265bd3b3f0c48844667""><code>2ea533c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/388"">#388</a> from minddistrict/backport-httpx-oauth2-client-fixes-...</li>; <li><a href=""https://github.com/lepture/authlib/commit/fca7f8523042cd9eadc68a1be091af9bc4ecab8b""><code>fca7f85</code></a> fix assertion client for httpx</li>; <li><a href=""https://github.com/lepture/authlib/commit/4d8a6ef7775dbe7033bb6d3efbc1919ff13579aa""><code>4d8a6ef</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/390"">#390</a> from minddistrict/parameterize-signing-algoritm-for-r...</li>; <li><a href=""https://github.com/lepture/authlib/commit/1e511edf07afcf14b4ad6704a7333eb5e78ef99a""><code>1e511ed</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/lepture/authlib/issues/393"">#393</a> from nam3less/maintain-0.15-bugfix-377</li>; <li>Additional commits viewable in <a href=""https://github.com/lepture/authlib/compare/v0.11...v0.15.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=authlib&package-manager=pip&previous-version=0.11&new-version=0.15.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-sta",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:5788,parameteriz,parameterize-signing-algoritm-for-r,5788,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['parameteriz'],['parameterize-signing-algoritm-for-r']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not includ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13651:2398,extend,extend,2398,https://hail.is,https://github.com/hail-is/hail/pull/13651,2,['extend'],['extend']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/e9f75c9912ed25b9777bc0257853370951220b17""><code>e9f75c9</code></a> Merge branch 'py312'</li>; <li><a href=""https://github.com/Textualize/rich/commit/35b64f1237f1c64326329b4668b116809a7fc596""><code>35b64f1</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3139"">#3139</a> from Textualize/py312</li>; <li><a href=""https://github.com/Textualize/rich/commit/c8ff546416b086c92d1ab9a1a0bb24447da12826""><code>c8ff546</code></a> version bump</li>; <li><a href=""https://github.com/Textualize/rich/commit/3f8c4af45133590590257de90fcbdc40834c0775""><code>3f8c4af</code></a> tests for ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13758:4703,extend,extend,4703,https://hail.is,https://github.com/hail-is/hail/pull/13758,2,['extend'],['extend']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <ul>; <li>Relaxed markdown-it-py dependency</li>; </ul>; <h2>[13.4.1] - 2023-05-31</h2>; <h3>Fixed</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/ec91917deb47b43188312e0e3f03bbab7e4e2e7e""><code>ec91917</code></a> changelog</li>; <li><a href=""https://github.com/Textualize/rich/commit/5360fe6fe4f582e5a5bec591cf7433ed85e6863d""><code>5360fe6</code></a> version bump</li>; <li><a href=""https://github.com/Textualize/rich/commit/e0d3aee1eccd424c98d05b94910f9d5ddb821a40""><code>e0d3aee</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3132"">#3132</a> from Textualize/fix-markdown-on-light</li>; <li><a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13651:4633,extend,extend,4633,https://hail.is,https://github.com/hail-is/hail/pull/13651,2,['extend'],['extend']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <ul>; <li>Relaxed markdown-it-py dependency</li>; </ul>; <h2>[13.4.1] - 2023-05-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed typing extensions import in markdown <a href=""https://redirect.github.com/Textualize/rich/issues/2979"">Textualize/rich#2979</a></li>; </ul>; <h2>[13.4.0] - 2023-05-31</h2>; <h3>Added</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/Textualize/rich/commit/720800e6930d85ad027b1e9bd0cbb96b5e994ce3""><code>720800e</code></a> fix tab size issue</li>; <li><a href=""https://github.com/Textualize/rich/commit/4037906f2be2d07f864687ef324da2abdf2028b9""><code>4037906</code></a> Merge pull request <a href=""https://redirect.github.com/Textualize/rich/issues/3071"">#30",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13380:4037,extend,extend,4037,https://hail.is,https://github.com/hail-is/hail/pull/13380,2,['extend'],['extend']
Modifiability,"nding spans.</li>; <li>Fixed TimeElapsedColumn from showing negative.</li>; <li>Fix for escaping strings with a trailing backslash <a href=""https://redirect.github.com/Textualize/rich/issues/2987"">Textualize/rich#2987</a></li>; <li>Fixed exception in Markdown with partial table <a href=""https://redirect.github.com/Textualize/rich/issues/3053"">Textualize/rich#3053</a></li>; <li>Fixed the HTML export template so that the <code>&lt;html&gt;</code> tag comes before the <code>&lt;head&gt;</code> tag <a href=""https://redirect.github.com/Textualize/rich/issues/3021"">Textualize/rich#3021</a></li>; <li>Fixed issue with custom classes overwriting <code>__eq__</code> <a href=""https://redirect.github.com/Textualize/rich/issues/2875"">Textualize/rich#2875</a></li>; <li>Fix rich.pretty.install breakage in iPython <a href=""https://redirect.github.com/Textualize/rich/issues/3013"">Textualize/rich#3013</a></li>; </ul>; <h3>Added</h3>; <ul>; <li>Added Text.extend_style method.</li>; <li>Added Span.extend method.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Text.tab_size now defaults to <code>None</code> to indicate that Console.tab_size should be used.</li>; </ul>; <h2>v13.4.2</h2>; <h2>[13.4.2] - 2023-06-12</h2>; <h3>Changed</h3>; <ul>; <li>Relaxed markdown-it-py dependency</li>; </ul>; <h2>Hot fix for typing extension issue</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tab assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>[13.5.0] - 2023-07-29</h2>; <h3>Fixed</h3>; <ul>; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13380:2003,extend,extend,2003,https://hail.is,https://github.com/hail-is/hail/pull/13380,2,['extend'],['extend']
Modifiability,"ng. All artifacts, such as HTML reports are served through the batch outputs mechanism described above. Job file dependencies are handled exactly as described in input/output dependencies above.; - [ ] allow ""finalizer"" jobs. A finalizer job executes when its parents are all complete or cancelled. It is not cancelled when its parents are cancelled.; - [ ] add namespace dependencies. CI allocates anonymous namespaces as requested by the build process. All `exec`s are, by default, run in an anonymous namespace. CI adds a finalizer job that deletes namespaces when all relevant `exec`s are finished; - [ ] add image dependencies. CI can create a batch job that builds a docker image with an anonymous name and pushes it to the project's GCR. CI adds a finalizer job that deletes the image when all relevant `exec`s are finished.; - [ ] batch and notebook are parameterized by their worker namespace so they can use the namespaces described above; - [ ] hail's build steps are parameterized in a way that permits them to use a jar not built locally on this machine (hopefully the Make PR makes this easy, otherwise we have to fool gradle into not rebuilding the jar). To reliably handle clean up, we *must* persist batch jobs, so I think that should be either higher priority or at least happening in parallel to the above (i.e. two developers working in parallel). - [ ] persist batch jobs in a durable store with all of the fields in the beginning of `Job.__init__`. When batch starts up, before serving any requests, it restores its state from the durable store and then refreshes from k8s. The k8s label `hail.is/batch-instance` is retired. Instead, pods have `hail.is/batch-version` which is a monotonically increasing natural number. It is only incremented if batch is backwards incompatible with the pod specs. Probably batch should destroy any pods that are alive from an out-of-date version of batch.; - [ ] persist CI information in a durable store [this needs more detail]; - [ ] How do ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193:2553,parameteriz,parameterized,2553,https://hail.is,https://github.com/hail-is/hail/issues/5193,2,['parameteriz'],['parameterized']
Modifiability,ng.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:161); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:517); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:546); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:542); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:541); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$3(SparkBackend.scala:368); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:364); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:541); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:81); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:14006,adapt,adapted,14006,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,nsion(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:7613,Config,Configuration,7613,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,nstants$$anonfun$is$hail$expr$ir$FoldConstants$$foldConstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1648,Rewrite,RewriteBottomUp,1648,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,nstants$1.apply(FoldConstants.scala:13); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:1694,Rewrite,RewriteBottomUp,1694,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"nt over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:6497,config,config,6497,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config']
Modifiability,"nt.py\"", line 1285, in _send_request\n self.endheaders(body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1234, in endheaders\n self._send_output(message_body, encode_chunked=encode_chunked)\n File \""/usr/lib/python3.6/http/client.py\"", line 1026, in _send_output\n self.send(msg)\n File \""/usr/lib/python3.6/http/client.py\"", line 964, in send\n self.connect()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 181, in connect\n conn = self._new_conn()\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\"", line 168, in _new_conn\n self, \""Failed to establish a new connection: %s\"" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/requests/adapters.py\"", line 449, in send\n timeout=timeout\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\"", line 638, in urlopen\n _stacktrace=sys.exc_info()[2])\n File \""/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\"", line 399, in increment\n raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='10.32.16.16', port=5001): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff2413b8470>: Failed to establish a new connection: [Errno 113] No route to host',))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1341, in polling_event_loop\n await refresh_k8s_state()\n File \""/usr/local/lib/python3.6/dist-packages/batch/batch.py\"", line 1332, in refresh_k8s_state\n await refresh_k8s_pods()\n Fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6754:2488,adapt,adapters,2488,https://hail.is,https://github.com/hail-is/hail/issues/6754,1,['adapt'],['adapters']
Modifiability,"nt/compare/v7.3.2...37ca37d865db260e7da6fa85339be450d6fd3c3c"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Add local-provisioner entry point to pyproject.toml Fixes <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/800"">#800</a> <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/801"">#801</a> (<a href=""https://github.com/utkonos""><code>@​utkonos</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-06-06&amp;to=2022-06-07&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Autkonos+updated%3A2022-06-06..2022-06-07&amp;type=Issues""><code>@​utkonos</code></a></p>; <h2>v7.3.2</h2>; <h2>7.3.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.1...c81771416d9e09e0e92be799f3e8549d0db57e43"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Correct <code>Any</code> type annotations. <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/791"">#791</a> (<a href=""https://github.com/joouha""><code>@​joouha</code></a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/main/CHANGELOG.md"">jupyter-client's changelog</a>.</em></p>; <blockquote>; <h2>7.3.4</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.3...ca4cb2d6a4b95a6925de85a47b323d2235032c74"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Revert latest changes to <code>ThreadedZMQSocketChannel</code> because they break Qtconsole <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/803"">#803</a> (<a href=""https://github.com/ccordoba12""><code>@​ccordoba12</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improveme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:3031,Enhance,Enhancements,3031,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['Enhance'],['Enhancements']
Modifiability,"ntaining the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` obey the aforementioned matrix?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:5929,config,config,5929,https://hail.is,https://github.com/hail-is/hail/pull/8513,2,['config'],"['config', 'configurations']"
Modifiability,ntext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2278) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2274) at scala.Option.foreach(Option.scala:257) at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2274) at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2353) at org.apache.spark.SparkContext.<init>(SparkContext.scala:85) at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) at is.hail.HailContext$.apply(HailContext.scala:164) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:280) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:214) at java.lang.Thread.run(Thread.java:745). Thank you so much!!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:6337,config,configureAndCreateSparkContext,6337,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['config'],['configureAndCreateSparkContext']
Modifiability,"ntrol flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact that a single field's missingness was flipped was a red herring -- all higher bits are flipped to 0 (defined)! Here's a look at the LIR looks like, though it was ultimately the JVM class file printout that tipped me off to the problem:. ```code. # I2B is stored as a class field on spills. The Z at the end of the next line indicates this field is a Boolean, not a byte. 31017 (PutFieldX PUTFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 31018 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:3369,variab,variable,3369,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['variab'],['variable']
Modifiability,"numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.acc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:9880,sandbox,sandbox,9880,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['sandbox'],['sandbox']
Modifiability,"oal of this PR is to make this work:. ```; HAIL_QUERY_BACKEND=service \; python3 -c 'import hail as hl; hl.utils.range_table(10).write(""gs://foo/bar.t"")`; ```. In particular, a normal user should not need to know the location of a Hail Query JAR. Currently, you must specify two environment variables: `HAIL_SHA` and `HAIL_JAR_URL`. This PR takes advantage of the well known location of a Hail Query JAR [1]. We use the newly introduced `hl.revision()` to determine the SHA-1 of the currently installed Hail. This PR includes the revision in the driver job spec. The front end has been modified to convert the revision into a cloud storage URL. This PR also provides three escape hatches to the aforementioned default behavior. These escape hatches should more or less only be used by developers. They're specified from highest priority to lowest.; 1. Specify the `jar_url` parameter to `ServiceBackend`.; 2. Specify the `HAIL_JAR_URL` environment variable.; 3. Specify a JAR url in the user config: `hailctl config set query/jar_url gs://...`. While writing this PR, I decided to clean up five bits of cruft I left when I first built the service backend. First, I took the JAR URL out of the ""command"" of the job spec. This ""command"" is just an array of strings. The fact that certain parts of that array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAIL_SHA/revision from the Worker and Hail Query Java code. This was on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:1129,config,config,1129,https://hail.is,https://github.com/hail-is/hail/pull/11645,2,['config'],['config']
Modifiability,"ocal computer using singularity instead of docker, it worked. Also, for the the other chromosomes with less variants but same samples, such as chr13, it worked well in Terra. Since we will convert multiple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9783,sandbox,sandbox,9783,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,"ockquote>; <h2>[2.0.2] - 2021-07-27</h2>; <h3>Added</h3>; <ul>; <li>Officially supporting 3.9 - <a href=""https://github.com/felixonmars""><code>@​felixonmars</code></a>.</li>; <li>You can now add static fields to log objects - <a href=""https://github.com/cosimomeli""><code>@​cosimomeli</code></a>.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Dropped 3.4 support.</li>; <li>Dropped Travis CI for Github Actions.</li>; <li>Wheel should build for python 3 instead of just 3.4 now.</li>; </ul>; <h2>[2.0.1] - 2020-10-12</h2>; <h3>Added</h3>; <ul>; <li>Support Pypi long descripton - <a href=""https://github.com/ereli-cb""><code>@​ereli-cb</code></a></li>; </ul>; <h3>Changed</h3>; <ul>; <li>You can now rename output fields - <a href=""https://github.com/schlitzered""><code>@​schlitzered</code></a></li>; </ul>; <h2>[2.0.0] - 2020-09-26</h2>; <h3>Added</h3>; <ul>; <li>New Changelog</li>; <li>Added timezone support to timestamps - <a href=""https://github.com/lalten""><code>@​lalten</code></a></li>; <li>Refactored log record to function - <a href=""https://github.com/georgysavva""><code>@​georgysavva</code></a></li>; <li>Add python 3.8 support - <a href=""https://github.com/tommilligan""><code>@​tommilligan</code></a></li>; </ul>; <h3>Removed</h3>; <ul>; <li>Support for Python 2.7</li>; <li>Debian directory</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/madzak/python-json-logger/commit/076b407aa7f34bc64a729cc77da336fb159d7597""><code>076b407</code></a> Release 2.0.2</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/f51d8fe76154380cac2fe6a30a944d67dc09df2d""><code>f51d8fe</code></a> added test/build requirements to ci file</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/b07b580670c6c4e340c372c73d0e76cdddc8b456""><code>b07b580</code></a> moved release out of test workflow. setup.cfg specifies a proper wheel now</li>; <li><a href=""https://github.com/madzak/python-json-logger/commit/4df12f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11467:2310,Refactor,Refactored,2310,https://hail.is,https://github.com/hail-is/hail/pull/11467,1,['Refactor'],['Refactored']
Modifiability,"ode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. `create_certs.py` includes the previous deploy's certificates in the trust; chain, so we can always accept clients from one deploy backwards. Old services; will not trust the new clients, but the `build.yaml` ensures things are deployed; in dependency order. Deploy would never work if a client could depend on; a not-yet-deployed server. ---. Things for you to verify:; - does every s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4886,config,configuration,4886,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"ode></a> Streamline overloaded assertion methods for Groovy</li>; <li><a href=""https://github.com/cbeust/testng/commit/5ac0021d14f7eb00804fe235aaefc5c2fbce57d1""><code>5ac0021</code></a> Adding release notes</li>; <li><a href=""https://github.com/cbeust/testng/commit/c0e1e772f1fc0ab2142f3a4114a2b8cfe60fa7e1""><code>c0e1e77</code></a> Adjust version reference in deprecation msgs.</li>; <li><a href=""https://github.com/cbeust/testng/commit/011527d9bf0f91a40539f5e5467cc106888810d9""><code>011527d</code></a> Bump version to 7.7.0 for release</li>; <li><a href=""https://github.com/cbeust/testng/commit/7846c444a411647f7e401a097224702188c93835""><code>7846c44</code></a> Deprecate support for running JUnit tests</li>; <li><a href=""https://github.com/cbeust/testng/commit/8630a7e8fe12985d71c00212f9362fd38fb0cb9e""><code>8630a7e</code></a> Ensure ITestContext available for JUnit4 tests</li>; <li><a href=""https://github.com/cbeust/testng/commit/7070b020def0089d0d9dc695a5762ad16e974ce6""><code>7070b02</code></a> Streamline dependsOnMethods for configurations</li>; <li><a href=""https://github.com/cbeust/testng/commit/d7e0bb1cbcd7933d34d704678e75cbaf42704505""><code>d7e0bb1</code></a> Deprecate support for running Spock Tests</li>; <li><a href=""https://github.com/cbeust/testng/commit/ca7a3a293008389096be75fea4936af8e5f79650""><code>ca7a3a2</code></a> Ensure All tests run all the time</li>; <li>Additional commits viewable in <a href=""https://github.com/cbeust/testng/compare/testng-6.8.21...7.7.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.testng:testng&package-manager=gradle&previous-version=6.8.21&new-version=7.7.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:16082,config,configurations,16082,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['config'],['configurations']
Modifiability,"ode>build_encrypted_persistence()</code> API. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/87"">#87</a>, <a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Remove: Old TokenCache API which has been deprecated for 2 years. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Enhancement: Make all platform-dependent parameters optional (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/103"">#103</a>)</li>; <li>Enhancement: Provide <code>PersistenceEncryptError</code> and <code>PersistenceDecryptError</code>, currently raised when encryption on Windows fails. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/108"">#108</a>)</li>; <li>Enhancement: The data file will be created with <code>600</code> permission when running in Unix-like systems. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/107"">#107</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/a88fa673af3602fe7c8c922314599b0c245e7add""><code>a88fa67</code></a> Merge branch 'release-1.0.0'</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/bd5b4074dbb7d03c9d91ce6a75378851be92552a""><code>bd5b407</code></a> Update README to reflect the new APIs</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/6f77b1e70be086aae752dcf7e08d7f06bcabdcd7""><code>6f77b1e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11992:1486,Enhance,Enhancement,1486,https://hail.is,https://github.com/hail-is/hail/pull/11992,1,['Enhance'],['Enhancement']
Modifiability,"ode>html_style</code> to an iterable of strings.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10366"">#10366</a>: std domain: Add support for emphasising placeholders in :rst:dir:<code>option</code>; directives through a new :confval:<code>option_emphasise_placeholders</code> configuration; option.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10439"">#10439</a>: std domain: Use the repr of some variables when displaying warnings,; making whitespace issues easier to identify.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10571"">#10571</a>: quickstart: Reduce content in the generated <code>conf.py</code> file. Patch by; Pradyun Gedam.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10648"">#10648</a>: LaTeX: CSS-named-alike additional :ref:<code>'sphinxsetup' &lt;latexsphinxsetup&gt;</code>; keys allow to configure four separate border-widths, four paddings, four; corner radii, a shadow (possibly inset), colours for border, background, shadow; for each of the code-block, topic, attention, caution, danger, error and warning; directives.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10655"">#10655</a>: LaTeX: Explain non-standard encoding in LatinRules.xdy</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10599"">#10599</a>: HTML Theme: Wrap consecutive footnotes in an <code>&lt;aside&gt;</code> element when; using Docutils 0.18 or later, to allow for easier styling. This matches the; behaviour introduced in Docutils 0.19. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10518"">#10518</a>: config: Add <code>include_patterns</code> as the opposite of <code>exclude_patterns</code>.; Patch by Adam Turner.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <su",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12165:4104,config,configure,4104,https://hail.is,https://github.com/hail-is/hail/pull/12165,1,['config'],['configure']
Modifiability,"ode>sphinx.util.stemmer</code> in favour of <code>snowballstemmer</code>.; Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9856"">#9856</a>: Deprecated <code>sphinx.ext.napoleon.iterators</code>.</li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10444"">#10444</a>: html theme: Allow specifying multiple CSS files through the <code>stylesheet</code>; setting in <code>theme.conf</code> or by setting <code>html_style</code> to an iterable of strings.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10366"">#10366</a>: std domain: Add support for emphasising placeholders in :rst:dir:<code>option</code>; directives through a new :confval:<code>option_emphasise_placeholders</code> configuration; option.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10439"">#10439</a>: std domain: Use the repr of some variables when displaying warnings,; making whitespace issues easier to identify.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10571"">#10571</a>: quickstart: Reduce content in the generated <code>conf.py</code> file. Patch by; Pradyun Gedam.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10648"">#10648</a>: LaTeX: CSS-named-alike additional :ref:<code>'sphinxsetup' &lt;latexsphinxsetup&gt;</code>; keys allow to configure four separate border-widths, four paddings, four; corner radii, a shadow (possibly inset), colours for border, background, shadow; for each of the code-block, topic, attention, caution, danger, error and warning; directives.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10655"">#10655</a>: LaTeX: Explain non-standard encoding in LatinRules.xdy</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10599"">#10599</a>: HTML Theme: Wrap c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12165:3616,variab,variables,3616,https://hail.is,https://github.com/hail-is/hail/pull/12165,1,['variab'],['variables']
Modifiability,"ods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get roles.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=rolebindings"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=RoleBinding""; Name: ""batch-pods-admin-binding"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""RoleBinding"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods-admin-binding"" ""namespace"":""batch-pods""] ""roleRef"":map[""apiGroup"":"""" ""kind"":""Role"" ""name"":""batch-pods-admin""] ""subjects"":[map[""kind"":""ServiceAccount"" ""name"":""batch-svc"" ""namespace"":""default""]]]}; from server for: ""deployment.yaml"": rolebindings.rbac.authorization.k8s.io ""batch-pods-admin-binding"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get rolebindings.rbac.authorization.k8s.io in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:2295,config,configuration,2295,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,"oduleBuilder. So MethodBuilder has the ClassBuilder interface, but is not actually a ClassBuilder. I tried a bunch of variants for the design of this, and while I don't think this is quite perfect, it seems workable.; - EmitMethodBuilder extends WrappedMethodBuilder, etc. Rather than overloading, the two interfaces are distinct: genMethod vs genEmitMethod, etc.; - Pushed ""new vs gen"" into more places e.g. newMethod vs genMethod. newMethod takes a name and creates a method of that name (e.g. apply). genMethod takes a baseName and creates a unique name based on the baseName.; - MethodBuilder newField => genFieldThisRef to distinguish it from ClassBuilder.newField. The former returns a Settable[T] referencing `this.<field>`, the latter just returns a Field.; - All methods supporting code generation for IR take EmitMethodBuilder rather than MethodBuilder (PType routines, aggregators, etc.). Summarizing the new class structure:. ```; class ModuleBuilder; trait WrappedModuleBuilder; def modb: ModuleBuilder; class ClassBuilder[C] extends WrappedModuleBuilder; trait WrappedClassBuilder[C]; def cb: ClassBuilder[C]; class MethodBuilder[C] extends WrappedClassBuilder[C]; trait WrappedMethodBuilder[C]; def mb: MethodBuilder; class FunctionBuilder[F] extends WrappedMethodBuilder[F]; def apply_method: MethodBuilder[F]. class EmitModuleBuilder extends WrappedModuleBuilder; trait WrappedEmitModuleBuilder extends WrappedModuleBuilder; def emodb: EmitModuleBuilder; class EmitClassBuilder[C] extends WrappedEmitModuleBuilder with WrappedClassBuilder[C]; trait WrappedEmitClassBuilder[C] extends WrappedModuleBuilder with WrappedClassBuilder[C]; def ecb: EmitClassBuilder; class EmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; trait WrappedEmitMethodBuilder[C] extends WrappedClassBuilder[C] with WrappedMethodBuilder[C]; def emb: EmitMethodBuilder; class EmitFunctionBuilder[F] extends WrappedEmitMethodBuilder[F]; def apply_method: EmitFunctionBuilder[F]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8335:2061,extend,extends,2061,https://hail.is,https://github.com/hail-is/hail/pull/8335,10,['extend'],['extends']
Modifiability,"of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services; will be unable to talk to not-yet-deployed serv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:7117,config,configuration,7117,https://hail.is,https://github.com/hail-is/hail/pull/8561,2,['config'],"['configuration', 'configures']"
Modifiability,"oft-authentication-extensions-for-python/releases"">msal-extensions's releases</a>.</em></p>; <blockquote>; <h2>MSAL Extensions for Python, 1.0.0</h2>; <p>This package is now considered stable and production-ready.</p>; <ul>; <li>New: Add a new platform-independent <code>build_encrypted_persistence()</code> API. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/87"">#87</a>, <a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Remove: Old TokenCache API which has been deprecated for 2 years. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Enhancement: Make all platform-dependent parameters optional (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/103"">#103</a>)</li>; <li>Enhancement: Provide <code>PersistenceEncryptError</code> and <code>PersistenceDecryptError</code>, currently raised when encryption on Windows fails. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/108"">#108</a>)</li>; <li>Enhancement: The data file will be created with <code>600</code> permission when running in Unix-like systems. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/107"">#107</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/a88fa673af3602fe7c8c922314599b0c245e7add""><code>a88fa67</code></a> Merge branch 'release-1.0.0'</li>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/bd5b4074dbb7d03c9d91ce6a75378851be92552a""><code>bd5b407</code></a> Update README to reflect the new APIs</li>; <li><a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11992:1199,Enhance,Enhancement,1199,https://hail.is,https://github.com/hail-is/hail/pull/11992,1,['Enhance'],['Enhancement']
Modifiability,"ole <a href=""https://redirect.github.com/Textualize/rich/issues/3057"">Textualize/rich#3057</a></li>; </ul>; <h2>Mostly cake, one or two puppies</h2>; <p><a href=""https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/"">https://textual.textualize.io/blog/2023/07/29/pull-requests-are-cake-or-puppies/</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.7.0] - 2023-11-15</h2>; <h3>Added</h3>; <ul>; <li>Adds missing parameters to Panel.fit <a href=""https://redirect.github.com/Textualize/rich/issues/3142"">Textualize/rich#3142</a></li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Some text goes missing during wrapping when it contains double width characters <a href=""https://redirect.github.com/Textualize/rich/issues/3176"">Textualize/rich#3176</a></li>; <li>Ensure font is correctly inherited in exported HTML <a href=""https://redirect.github.com/Textualize/rich/issues/3104"">Textualize/rich#3104</a></li>; <li>Fixed typing for <code>FloatPrompt</code>.</li>; </ul>; <h2>[13.6.0] - 2023-09-30</h2>; <h3>Added</h3>; <ul>; <li>Added Python 3.12 to classifiers.</li>; </ul>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14012:2874,inherit,inherited,2874,https://hail.is,https://github.com/hail-is/hail/pull/14012,2,['inherit'],['inherited']
Modifiability,"ols/fonttools/issues/3034"">#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3027"">#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning; None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3032"">#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3031"">#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built; with feaLib from FEA feature file. Also, added support for building multiple VFs; defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3024"">#3024</a>).</li>; <li>[mtiLib] Only add <code>Debg</code> table with lookup names when <code>FONTTOOLS_LOOKUP_DEBUGGING</code>; env variable is set (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3023"">#3023</a>).</li>; </ul>; <h2>4.39.0 (released 2023-03-06)</h2>; <ul>; <li>[mtiLib] Optionally add <code>Debg</code> debug info for MTI feature builds (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3018"">#3018</a>).</li>; <li>[ttx] Support reading input file from standard input using special <code>-</code> character,; similar to existing <code>-o -</code> option to write output to standard output (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3020"">#3020</a>).</li>; <li>[cython] Prevent <code>cython.compiled</code> raise AttributeError if cython not installed; properly (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3017"">#3017</a>).</li>; <li>[OS/2] Guard against ZeroDivisionError when calculating xAvgCharWidth in the unlikely; scenario no glyph has non-zero advance (<a href=""https://redirect.github.com/fontto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:13433,variab,variable,13433,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['variab'],['variable']
Modifiability,"ols/fonttools/pull/3042"">fonttools/fonttools#3042</a>, <a href=""https://redirect.github.com/fonttools/fonttools/pull/3043"">fonttools/fonttools#3043</a>).</li>; <li>[ttProgram] Handle string input to Program.fromAssembly() (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3038"">fonttools/fonttools#3038</a>).</li>; <li>[otlLib] Added a config option to emit GPOS 7 lookups, currently disabled by default because of a macOS bug (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3034"">fonttools/fonttools#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3027"">fonttools/fonttools#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3032"">fonttools/fonttools#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3031"">fonttools/fonttools#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built with feaLib from FEA feature file. Also, added support for building multiple VFs defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3024"">fonttools/fonttools#3024</a>).</li>; <li>[mtiLib] Only add <code>Debg</code> table with lookup names when <code>FONTTOOLS_LOOKUP_DEBUGGING</code> env variable is set (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3023"">fonttools/fonttools#3023</a>).</li>; </ul>; <h2>4.39.0</h2>; <ul>; <li>[mtiLib] Optionally add <code>Debg</code> debug info for MTI feature builds (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3018"">#3018</a>).</li>; <li>[ttx] Support reading input file from standard input using special <code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:3004,variab,variable,3004,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['variab'],['variable']
Modifiability,"om the exception chain in; 447 # Python 3 (including for exceptions like SystemExit).; 448 # Otherwise it looks like a bug in the code. File /opt/conda/lib/python3.10/http/client.py:1375, in HTTPConnection.getresponse(self); 1374 try:; -> 1375 response.begin(); 1376 except ConnectionError:. File /opt/conda/lib/python3.10/http/client.py:318, in HTTPResponse.begin(self); 317 while True:; --> 318 version, status, reason = self._read_status(); 319 if status != CONTINUE:. File /opt/conda/lib/python3.10/http/client.py:287, in HTTPResponse._read_status(self); 284 if not line:; 285 # Presumably, the server closed the connection before; 286 # sending a valid response.; --> 287 raise RemoteDisconnected(""Remote end closed connection without""; 288 "" response""); 289 try:. RemoteDisconnected: Remote end closed connection without response. During handling of the above exception, another exception occurred:. ProtocolError Traceback (most recent call last); File /opt/conda/lib/python3.10/site-packages/requests/adapters.py:487, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 486 try:; --> 487 resp = conn.urlopen(; 488 method=request.method,; 489 url=url,; 490 body=request.body,; 491 headers=request.headers,; 492 redirect=False,; 493 assert_same_host=False,; 494 preload_content=False,; 495 decode_content=False,; 496 retries=self.max_retries,; 497 timeout=timeout,; 498 chunked=chunked,; 499 ); 501 except (ProtocolError, OSError) as err:. File /opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:787, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw); 785 e = ProtocolError(""Connection aborted."", e); --> 787 retries = retries.increment(; 788 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]; 789 ); 790 retries.sleep(). File /opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py:550, in Retry.increment(self, m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:6273,adapt,adapters,6273,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['adapt'],['adapters']
Modifiability,"om/danielduhh""><code>@​danielduhh</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/133"">#133</a> <a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/134"">#134</a>)</li>; </ul>; <p>Version 0.8.0 (released 2019-03-17)</p>; <ul>; <li>Respect <code>dot_notation</code> flag in ignore argument (<a href=""https://github.com/yoyonel""><code>@​yoyonel</code></a>) (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/107"">#107</a>)</li>; <li>Adds argument for toggling dot notation in diff. (<a href=""https://github.com/robinchew""><code>@​robinchew</code></a>)</li>; </ul>; <p>Version 0.7.2 (released 2019-02-22)</p>; <ul>; <li>Two NaN values are considered the same, hence they are not shown in <code>diff</code>; output. (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/114"">#114</a>) (<a href=""https://github.com/t-b""><code>@​t-b</code></a>)</li>; <li>Refactors <code>diff</code> method to reduce recursive call stack size. (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/112"">#112</a>); (<a href=""https://github.com/yoyonel""><code>@​yoyonel</code></a>)</li>; <li>Python porting best practice use feature detection instead; of version detection to save an import and pass both PyLint; and Flake8 tests with neither 'pragma' nor 'noqa'. (<a href=""https://github.com/cclauss""><code>@​cclauss</code></a>)</li>; </ul>; <p>Version 0.7.1 (released 2018-05-04)</p>; <ul>; <li>Resolves issue with keys containing dots. (<a href=""https://github-redirect.dependabot.com/inveniosoftware/dictdiffer/issues/101"">#101</a>)</li>; </ul>; <p>Version 0.7.0 (released 2017-10-16)</p>; <ul>; <li>Fixes problem with diff results that reference the original structure by; introduction of <code>deepcopy</code> for all possibly unhashable items. Thus the diff; does not change later when the diffed structures change.</li>; <li>Adds new option for patchi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11485:3929,Refactor,Refactors,3929,https://hail.is,https://github.com/hail-is/hail/pull/11485,1,['Refactor'],['Refactors']
Modifiability,"om/hail-is/hail/pull/12883/commits/ae51e0a9af12e4c89a44e7ce3235f3f665ff4830). ---. [VPC Flow Logs](https://cloud.google.com/vpc/docs/flow-logs):. > VPC Flow Logs records a sample of network flows sent from and received by VM instances, including; > instances used as Google Kubernetes Engine nodes. These logs can be used for network monitoring,; > forensics, real-time security analysis, and expense optimization. I found the collection process the most elucidating part of the documentation. My summary of that; process follows:. 1. Packets are sampled on the network interface of a VM. Google claims an average sampling rate of; 1/30. This rate reduces if the VM is under load. This rate is immutable to us. 2. Within an ""aggregation interval"", packets are aggregated into ""records"" which are keyed (my term); by source & destination. There are currently six choices for aggregation interval: 5s, 30s, 1m,; 5m, 10m, and 15m. 3. Records are sampled. The sampling rate is a user configured floating point number (precision; unclear) between 0 and 1. 4. Metadata is optionally added to the records. The metadata captures information about the source; and destination VM such as project id, VM name, zone, region, GKE pod, GKE service, and geographic; information of external parties. The user may elect to receive all metadata, no metadata, or a; specific set of metadata fields. 5. The records are written to Google Cloud Logging. The pricing of VPC Flow Logs is described at the [network pricing page](https://cloud.google.com/vpc/network-pricing#network-telemetry). Notice that, if logs are only sent to Cloud Logging (not to BigQuery, Pub/Sub, or Cloud Storage):. > If you store your logs in Cloud Logging, logs generation charges are waived, and only Logging charges apply. I believe in this phrase ""logs generation charges"" refers to *VPC Flow logs* generation charges. The Google Cloud Logging [pricing page](https://cloud.google.com/stackdriver/pricing#google-clouds-operations-suite-pricing) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12883:1066,config,configured,1066,https://hail.is,https://github.com/hail-is/hail/pull/12883,1,['config'],['configured']
Modifiability,"omize RFC7523 <code>alg</code> value</li>; </ul>; <h2>Version 0.15.4</h2>; <p><strong>Released on Jul 17, 2021.</strong></p>; <ul>; <li>Security fix when JWT claims is None.</li>; </ul>; <h2>Version 0.15.3</h2>; <p><strong>Released on Jan 15, 2021.</strong></p>; <ul>; <li>Fixed <code>.authorize_access_token</code> for OAuth 1.0 services, via :gh:<code>issue#308</code>.</li>; </ul>; <h2>Version 0.15.2</h2>; <p><strong>Released on Oct 18, 2020.</strong></p>; <ul>; <li>Fixed HTTPX authentication bug, via :gh:<code>issue#283</code>.</li>; </ul>; <h2>Version 0.15.1</h2>; <p><strong>Released on Oct 14, 2020.</strong></p>; <ul>; <li>Backward compitable fix for using JWKs in JWT, via :gh:<code>issue#280</code>.</li>; </ul>; <h2>Version 0.15</h2>; <p><strong>Released on Oct 10, 2020.</strong></p>; <p>This is the last release before v1.0. In this release, we added more RFCs; implementations and did some refactors for JOSE:</p>; <ul>; <li>RFC8037: CFRG Elliptic Curve Diffie-Hellman (ECDH) and Signatures in JSON Object Signing and Encryption (JOSE)</li>; <li>RFC7638: JSON Web Key (JWK) Thumbprint</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/authlib/commit/d8e428c9350c792fc3d25dbaaffa3bfefaabd8e3""><code>d8e428c</code></a> Version bump 0.15.5</li>; <li><a href=""https://github.com/lepture/authlib/commit/f24962835fd0725349cb1b368ee69ba0cc8670f9""><code>f249628</code></a> Add changelog</li>; <li><a href=""https://github.com/lepture/authlib/commit/38ac0d22837a2a5400205bdbd11bb63e5aa7660f""><code>38ac0d2</code></a> Improve rfc7523 parameters compatibility.</li>; <li><a href=""https://github.com/lepture/authlib/commit/e880f1640168683d2d568fd88d1dd0ec0b29f0a6""><code>e880f16</code></a> Rename parameters of InsufficientScopeError</li>; <li><a href=""https://github.com/lepture/authlib/commit/afaeaf98c88baa3ea2fa9f9ac3e65f9a55c8773f""><code>afaeaf9</code></a> Merge pul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11483:3580,refactor,refactors,3580,https://hail.is,https://github.com/hail-is/hail/pull/11483,1,['refactor'],['refactors']
Modifiability,ommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5814,adapt,adapted,5814,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['adapt'],['adapted']
Modifiability,"on Linux <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7200"">#7200</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>If the clipboard fails to open on Windows, wait and try again <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7141"">#7141</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed saving multiple 1 mode frames to GIF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7181"">#7181</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Replaced absolute PIL import with relative import <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7173"">#7173</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed files and types override <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7194"">#7194</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed duplicate config <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7193"">#7193</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Replaced deprecated Py_FileSystemDefaultEncoding for Python &gt;= 3.12 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7192"">#7192</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improved wl-paste mimetype handling in ImageGrab <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7094"">#7094</a> [<a href=""https://github.com/rrcgat""><code>@​rrcgat</code></a>]</li>; <li>Updated redirected URLs <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7178"">#7178</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added <em>repr_jpeg</em>() for IPython display_jpeg <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7135"">#7135</a> [<a href=""https://github.com/n3011""><code>@​n3011</code></",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:7758,config,config,7758,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['config'],['config']
Modifiability,"on't quite match (e.g. an input is let bound, or the fold implementing ""locus is contained in a set of intervals"" is written slightly differently), this uses a standard abstract interpretation framework, which is almost completely insensitive to the form of the IR, only depending on the semantics. It also correctly handles missing key fields, where the previous implementation often produced an unsound transformation of the IR. Also adds a much more thorough test suite than we had before. At the top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:1220,rewrite,rewrites,1220,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['rewrite'],['rewrites']
Modifiability,on.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5226,Rewrite,RewriteBottomUp,5226,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,on.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optim,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:4575,Rewrite,RewriteBottomUp,4575,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"onda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 63 tpl = Env.jutils().handleForPython(e.java_exception); 64 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 65 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 66 except pyspark.sql.utils.CapturedException as e:; 67 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist. Java stack trace:; is.hail.utils.HailException: Chain file 'grch37_to_grch38.over.chain.gz' does not exist.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.variant.ReferenceGenome.addLiftover(ReferenceGenome.scala:407); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2(SparkBackend.scala:613); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$2$adapted(SparkBackend.scala:612); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:347); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1(SparkBackend.scala:612); 	at is.hail.backend.spark.SparkBackend.$anonfun$pyAddLiftover$1$adapted(SparkBackend.scala:611); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.pyAddLiftover(SparkBackend.scala:611); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993:3808,adapt,adapted,3808,https://hail.is,https://github.com/hail-is/hail/issues/13993,1,['adapt'],['adapted']
Modifiability,"oogleapis/java-storage/commit/82aacd7922573d6f4779f21cdc83de10616d7a08"">82aacd7</a>)</li>; <li>Update retries for Notifications (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1734"">#1734</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0fb2f1823f9eff8534f15240321003f120fed3f4"">0fb2f18</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.0.6 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1761"">#1761</a>) (<a href=""https://github.com/googleapis/java-storage/commit/803a90b7747b8972f51d1407616c51084d97c589"">803a90b</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.7.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1758"">#1758</a>) (<a href=""https://github.com/googleapis/java-storage/commit/140e90911229c876de7b674dd1e61b278e8b07fd"">140e909</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.17 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1759"">#1759</a>) (<a href=""https://github.com/googleapis/java-storage/commit/7e3175a56a06dac0aa0841f221a486bb69b5c9bf"">7e3175a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.13.1...v2.14.0"">2.14.0</a> (2022-10-26)</h2>; <h3>Google Cloud Storage gRPC API Preview</h3>; <p>The first release of <code>google-cloud-storage</code> with support for a subset of the Google Cloud Storage gRPC API which is in private preview. The most common operations have all been implemented and are available for experimentation.</p>; <p>Given not all public api surface of <code>google-cloud-storage</code> classes are supported for gRPC a new annotation <code>@TransportCompatibility</code> has been added to various classes, methods and fields/enum values to signal where that thing can be expected to work. As we implement more of the operations these annotations will be upd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:7124,plugin,plugin,7124,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['plugin'],['plugin']
Modifiability,"oogleapis/java-storage/commit/82aacd7922573d6f4779f21cdc83de10616d7a08"">82aacd7</a>)</li>; <li>Update retries for Notifications (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1734"">#1734</a>) (<a href=""https://github.com/googleapis/java-storage/commit/0fb2f1823f9eff8534f15240321003f120fed3f4"">0fb2f18</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency com.google.cloud:google-cloud-shared-dependencies to v3.0.6 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1761"">#1761</a>) (<a href=""https://github.com/googleapis/java-storage/commit/803a90b7747b8972f51d1407616c51084d97c589"">803a90b</a>)</li>; <li>Update dependency net.jqwik:jqwik to v1.7.1 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1758"">#1758</a>) (<a href=""https://github.com/googleapis/java-storage/commit/140e90911229c876de7b674dd1e61b278e8b07fd"">140e909</a>)</li>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.17 (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1759"">#1759</a>) (<a href=""https://github.com/googleapis/java-storage/commit/7e3175a56a06dac0aa0841f221a486bb69b5c9bf"">7e3175a</a>)</li>; </ul>; <h2>v2.14.0</h2>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.13.1...v2.14.0"">2.14.0</a> (2022-10-26)</h2>; <h3>Google Cloud Storage gRPC API Preview</h3>; <p>The first release of <code>google-cloud-storage</code> with support for a subset of the Google Cloud Storage gRPC API which is in private preview. The most common operations have all been implemented and are available for experimentation.</p>; <p>Given not all public api surface of <code>google-cloud-storage</code> classes are supported for gRPC a new annotation <code>@TransportCompatibility</code> has been added to various classes, methods and fields/enum values to signal where that thing can be expected to work. As we implement more of the operations these annot",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12456:1612,plugin,plugin,1612,https://hail.is,https://github.com/hail-is/hail/pull/12456,2,['plugin'],['plugin']
Modifiability,"ool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:310); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:449); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:448); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.98-f8833c1ae16b; Error summary: SparkException: Job aborted due to stage failure: Task 582 in stage 10.0 failed 20 times, most recent failure: Lost task 582.19 in stage 10.0 (TID 461381) (cluster-w-144.c.project.internal executor 3568): ExecutorLostFailure (executor 3568 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 128936 ms; Driver stacktrace:; ```; We also tried using a similar cluster configuration with dynamic scaling and received the same error. Do you have a recommended cluster configuration to run king() on this many samples?. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:8238,config,configuration,8238,https://hail.is,https://github.com/hail-is/hail/issues/12290,2,['config'],['configuration']
Modifiability,op/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.120.dist-info/METADATA'; adding 'hail-0.2.120.dist-info/WHEEL'; adding 'hail-0.2.120.dist-info/entry_points.txt'; adding 'hail-0.2.120.dist-info/top_level.txt'; adding 'hail-0.2.120.dist-info/RECORD'; removing build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Requirement already satisfied: pip-tools==6.13.0 in /usr/loca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445:12509,config,config,12509,https://hail.is,https://github.com/hail-is/hail/issues/13445,1,['config'],['config']
Modifiability,"or assertThrows\expectThrows methods (Anatolii Yuzhakov); Fixed: GITHUB-2780: Use SpotBugs instead of abandoned FindBugs; Fixed: GITHUB-2801: JUnitReportReporter is too slow; Fixed: GITHUB-2807: buildStackTrace should be fail-safe (Sergey Chernov); Fixed: GITHUB-2830: TestHTMLReporter parameter toString should be fail-safe (Sergey Chernov); Fixed: GITHUB-2798: Parallel executions coupled with retry analyzer results in duplicate retry analyzer instances being created (Krishnan Mahadevan)</p>; <p>7.6.1; Fixed: GITHUB-2761: Exception: ERROR java.nio.file.NoSuchFileException: /tmp/testngXmlPathInJar-15086412835569336174 (Krishnan Mahadevan); 7.6.0; Fixed: GITHUB-2741: Show fully qualified name of the test instead of just the function name for better readability of test output.(Krishnan Mahadevan); Fixed: GITHUB-2725: Honour custom attribute values in TestNG default reports (Krishnan Mahadevan); Fixed: GITHUB-2726: <a href=""https://github.com/AfterClass""><code>@​AfterClass</code></a> config method is executed for EACH <a href=""https://github.com/Test""><code>@​Test</code></a> method when parallel == methods (Krishnan Mahadevan); Fixed: GITHUB-2752: TestListener is being lost when implenting both IClassListener and ITestListener (Krishnan Mahadevan); New: GITHUB-2724: DataProvider: possibility to unload dataprovider class, when done with it (Dzmitry Sankouski); Fixed: GITHUB-217: Configure TestNG to fail when there's a failure in data provider (Krishnan Mahadevan); Fixed: GITHUB-2743: SuiteRunner could not be initial by default Configuration (Nan Liang); Fixed: GITHUB-2729: beforeConfiguration() listener method should be invoked for skipped configurations as well(Nan Liang); Fixed: assertEqualsNoOrder for Collection and Iterators size check was missing (Adam Kaczmarek); Fixed: GITHUB-2709: Testnames not working together with suites in suite (Martin Aldrin); Fixed: GITHUB-2704: IHookable and IConfigurable callback discrepancy (Krishnan Mahadevan); Fixed: GITHUB-2637: Upgrade",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:12815,config,config,12815,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['config'],['config']
Modifiability,or domain specified. The key bug here was a check for `'domain' not in config` instead of `config['domain'] is None`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13585:71,config,config,71,https://hail.is,https://github.com/hail-is/hail/pull/13585,2,['config'],['config']
Modifiability,orImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11700,Config,Configuration,11700,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"orcing the global uniqueness of names as a basic invariant of the IR (typecheck could also check this invariant); * keep a string in the `Name`, but no longer require it to be unique. Instead it's just a suggestion for how to show the name in printouts, adding a uniqueifying suffix as needed. With `NormalizeNames` gone, this would let us preserve meaningful variable names further in the lowering pipeline.; * possibly keep other state in the `Name`, for example to allow a more efficient implementation of environments, similar to the `mark` state on `BaseIR`. This is obviously a large change, but there are only a few conceptual pieces (appologies for not managing to separate these out):; * attempt to minimize the number of locations in which the `Name` constructor is called, to make future refactorings easier; * add `freshName()`, which just wraps `genUID()`, returning a `Name`; * convert IR construction to use the convenience methods in `ir.package`, which take scala lambdas to represent blocks with bound variables, instead of manually creating new variable names; * replace uses of the magic constant variable names (`row`, `va`, `sa`, `g`, `global`) with constants (`TableIR.{rowName, globalName}`, `MatrixIR.{rowName, colName, entryName, globalName}`); * the above changes modified the names we use for bound variables in many places. That shouldn't matter, but it cought a couple bugs where it did.; * `NormalizeNames` optionally allows the IR to contain free variables. But it didn't do anything to ensure the newly generated variable names are distinct from any contained free variables. Thus it was possible to rename a bound variable to mistakenly capture a contained free variable. I've fixed that.; * `SimplifySuite` compared simplified IR with the pre-constructed expected IR, carefully controlling the `genUID` global state to make simplify generate exactly the names expected. I've replaced that by just comparing with the expected IR using a alpha-equivalence comparison.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14547:1593,variab,variables,1593,https://hail.is,https://github.com/hail-is/hail/pull/14547,9,['variab'],"['variable', 'variables']"
Modifiability,"orker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,662	job.py	schedule_job:443	schedule job (100, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,663	job.py	schedule_job:443	schedule job (98, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (101, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:32,455	job.py	schedule_job:443	schedule job (102, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:33,456	hail_logging.py	log:40	https POST /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/activate done in 3.2369999999998527s: 200; ERROR	2022-03-02 19:06:33,492	job.py	schedule_job:473	error while scheduling job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q	Traceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/aiohttp/connector.py"", line 969, in _wrap_create_connection\n return await self._loop.create_connection(*args, **kwargs) # type: ignore # noqa\n File ""uvloop/loop.pyx"", line 1974, in create_connection\n File ""uvloop/loop.pyx"", line 1951, in uvloop.loop.Loop.create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 449, in schedule_job\n time",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:2741,config,config,2741,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"orm in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd40949b1fc822d2b7e6517dc0ac993b62badaff8 /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; 24/01/17 20:59:51 WARN NativeCodeLoader: Unable to ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:10024,sandbox,sandbox,10024,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,"ormat):; ...; ```. The command decorator replaces the function with one that takes a `List[str]` of command line parameters, parses them, and calls the original function. The option options are pretty self-explanatory. - To access an argument to a group (like `dataproc --beta`) in a (sub)command, use the decorator `click.pass_context` to pass the click context which allows you to access parent group parameters. `dataproc start` is an example:. ```; @dataproc.command(; help=""Start a Dataproc cluster configured for Hail.""); @click.argument('cluster_name'); ...; @click.pass_context; def start(ctx, cluster_name, ...):; beta = ctx.parent.params['beta']; ```. The help output for a group looks like this:. ```; $ hailctl dataproc --help; Usage: hailctl dataproc [OPTIONS] COMMAND [ARGS]... Manage and monitor Hail deployments. Options:; --beta Force use of `beta` in gcloud commands; --help Show this message and exit. Commands:; connect Connect to a running Dataproc cluster; describe Gather information about a Hail (Table or MatrixTable) file...; diagnose Diagnose problems in a Dataproc cluster.; list List Dataproc clusters.; modify; start Start a Dataproc cluster configured for Hail.; stop Shut down a Dataproc cluster.; submit Submit a Python script to a running Dataproc cluster.; ```. The help output for a command looks like:. ```; $ hailctl batch get --help; Usage: hailctl batch get [OPTIONS] BATCH_ID. Get a particular batch's info. Options:; -o, --output-format [yaml|json]; Specify output format [default: yaml]; --help Show this message and exit.; ```. I also made `BatchClient` a context manager and made the default limit unbounded in `BatchClient.list_batches`. I have marked this WIP until we are happy with the interface changes and how we're going to communicate them to the users. CHANGELOG: Rewrote the hailctl argument parsing code, which made some incompatible changes to the way hailctl handles arguments. For more details, see [the URL of this PR]. Spice level: medium.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:3162,config,configured,3162,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['config'],['configured']
Modifiability,"ositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/67b84e02c185294c54a8e49510d4cb962e89cee2""><code>67b84e0</code></a> Merge branch 'release-1.21.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/99acd545b20fe30ffa2f589a674c5a7ad74c266b""><code>99acd54</code></a> Bumping version to 1.21.13</li>; <li><a href=""https://github.com/boto/boto3/commit/83a8f662655bada44d442df7f33cb20d71ead257""><code>83a8f66</code></a> Add changelog entries from botocore</li>; <li><a href=""https://github.com/boto/boto3/commit/261b0f2ffe079b6940d683657fcad358195f882e""><code>261b0f2</code></a> Merge branch 'release-1.21.12'</li>; <li><a href=""https://github.com/boto/boto3/commit/a972b1bed4caacf0c97f1056cabdfe4b5ccc2681""><code>a972b1b</code></a> Merge branch 'release-1.21.12' into develop</li>; <li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:5395,config,configure,5395,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['config'],['configure']
Modifiability,"ositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for updating an existing named query.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for new AMI property 'lastLaunchedTime'</li>; <li>api-change:<code>servicecatalog-appregistry</code>: [<code>botocore</code>] AppRegistry is deprecating Application and Attribute-Group Name update feature. In this release, we are marking the name attributes for Update APIs as deprecated to give a heads up to our customers.</li>; </ul>; <h1>1.21.8</h1>; <ul>; <li>api-change:<code>elasticache</code>: [<code>botocore</code>] Doc only update for ElastiCache</li>; <li>api-change:<code>panorama</code>: [<code>botocore</code>] Added NTP server configuration parameter to ProvisionDevice operation. Added alternate software fields to DescribeDevice response</li>; </ul>; <!-- raw HTML omitted -->; </blockq",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:3948,config,configure,3948,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['config'],['configure']
Modifiability,"ough using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point 2 is possible before we make any changes to our networking, so that comes first in #12093. Point 3 is taken care of in #12094, and the rest of Point 2 and Point 1, everything to do with Envoy, is in this PR. ### Additional QoL improvements; - Envoy by default exposes Prometheus metrics that we can use to easily monitor things like rate-li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:4578,Config,ConfigMap,4578,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['Config'],['ConfigMap']
Modifiability,"ound. In the world of low-level container runtimes there exists the term ""bundle"", which basically means the pair of a root filesystem and a `config.json` file containing all of the other necessary information to run the container. If you invoke `crun` or `runc` with `--bundle /path/to/bundle`, the runtime assumes the following:. - The configuration file for the container is located at `/path/to/bundle/config.json`; - That `config.json` contains a field [`root.path`](https://github.com/opencontainers/runtime-spec/blob/main/config.md#root) that specifies the location of the root filesystem, most commonly as a path relative to `/path/to/bundle`. `crun` offers a way to explicitly reference the location of `config.json` through its `--config` flag. This seems fairly innocuous, but specifying a custom `--config` path can have some unfortunate unintended consequences because it invalidates the assumption in the specification that the configuration resides at `/path/to/bundle/config.json`. Specifically, it breaks [Hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#posix-platform-hooks). When a hook is run, the runtime (crun) feeds it the [container state](https://github.com/opencontainers/runtime-spec/blob/main/runtime.md#state), a JSON of information about the container including the `bundle` path. Any hook that attempts to load the `config.json`, like for example, the `nvidia-container-runtime-hook`, will crash. ### Change. This change stops using the `--config` flag for crun and instead does the following to create a well-formed bundle:. - Instead of the bundle being the merged directory of the container overlay, it is the container's scratch directory; - `root.path` is adjusted inside of `config.json` to now point to the merged directory of the container overlay. I've opted to use an absolute path here because why use a relative path.; - Move `config.json` into the container scratch directory so that it is inside the root of the bundle directory.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13438:1097,config,config,1097,https://hail.is,https://github.com/hail-is/hail/pull/13438,5,['config'],['config']
Modifiability,"our AWS organization in a decentralized way. You can now allow member accounts to manage Organizations policies.</li>; <li>api-change:<code>rds</code>: [<code>botocore</code>] This release enables new Aurora and RDS feature called Blue/Green Deployments that makes updates to databases safer, simpler and faster.</li>; <li>api-change:<code>textract</code>: [<code>botocore</code>] This release adds support for classifying and splitting lending documents by type, and extracting information by using the Analyze Lending APIs. This release also includes support for summarized information of the processed lending document package, in addition to per document results.</li>; <li>api-change:<code>transcribe</code>: [<code>botocore</code>] This release adds support for 'inputType' for post-call and real-time (streaming) Call Analytics within Amazon Transcribe.</li>; </ul>; <h1>1.26.16</h1>; <ul>; <li>api-change:<code>grafana</code>: [<code>botocore</code>] This release includes support for configuring a Grafana workspace to connect to a datasource within a VPC as well as new APIs for configuring Grafana settings.</li>; <li>api-change:<code>rbin</code>: [<code>botocore</code>] This release adds support for Rule Lock for Recycle Bin, which allows you to lock retention rules so that they can no longer be modified or deleted.</li>; </ul>; <h1>1.26.15</h1>; <ul>; <li>bugfix:Endpoints: [<code>botocore</code>] Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botoco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12507:4207,config,configuring,4207,https://hail.is,https://github.com/hail-is/hail/pull/12507,2,['config'],['configuring']
Modifiability,"owerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3492,config,config,3492,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['config']
Modifiability,owering.EvalRelationalLetsPass.apply(LoweringPass.scala:162); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:630); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79); 	at sun.net.httpserver.AuthFilter.doFilter(AuthFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14529:11232,adapt,adapted,11232,https://hail.is,https://github.com/hail-is/hail/issues/14529,1,['adapt'],['adapted']
Modifiability,owering.EvalRelationalLetsPass.apply(LoweringPass.scala:164); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:601); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:637); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:632); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:631); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:77); 	at is.hail.utils.package$.using(package.scala:665); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:77); 	at is.hail.utils.package$.using(package.scala:665); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:64); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); 	at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:631); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:89); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpser,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:17994,adapt,adapted,17994,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['adapt'],['adapted']
Modifiability,"owever, I am; pretty certain I understand the bug. I have seen this happen in GCP; and in Azure. Take a look at an interval of driver logs:; ```; INFO	2022-03-02 19:06:30,198	main.py	get_credentials_1:226	returning azure credentials to activating instance instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q; INFO	2022-03-02 19:06:30,199	hail_logging.py	log:40	https GET /pr-11438-default-g6cibyji6520/batch-driver/api/v1alpha/instances/credentials done in 0.005999999999858119s: 200; INFO	2022-03-02 19:06:30,226	main.py	activate_instance_1:237	activating instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q; INFO	2022-03-02 19:06:30,991	base.py	check:335	checking on instance batch-worker-pr-11438-default-g6cibyji6520-highcpu-z0idl, last updated 60.151s ago; INFO	2022-03-02 19:06:31,526	pool.py	schedule_loop_body:371	schedule pool standard: starting; INFO	2022-03-02 19:06:31,583	job.py	schedule_job:443	schedule job (94, 2) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,584	job.py	schedule_job:443	schedule job (95, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,585	job.py	schedule_job:443	schedule job (93, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (90, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,586	job.py	schedule_job:443	schedule job (94, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,598	job.py	schedule_job:443	schedule job (97, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2q: made job config; INFO	2022-03-02 19:06:31,656	job.py	schedule_job:443	schedule job (99, 1) on instance batch-worker-pr-11438-default-g6cibyji6520-standard-9xy2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11451:1118,config,config,1118,https://hail.is,https://github.com/hail-is/hail/pull/11451,1,['config'],['config']
Modifiability,"p itself (grafana or prometheus) and an nginx container that sits in front of it. The flow is as follows, and since this works the exact same for both prometheus and grafana I will just talk about grafana as the example and the same thing should apply to both:. 1. User sends request to grafana.hail.is; 2. Gateway sees an HTTP request going to a production service and forwards that request to the grafana k8s Service port 443; 3. The grafana K8s Service forwards that request to the grafana pod port 443; 4. Nginx is listening on port 443 in the grafana pod and receives that request. It makes an authorization check to auth to make sure that the request is coming from a developer; 5. Nginx forwards that request to 127.0.0.1:3000, which is where grafana is listening. This PR does not change any behavior, just replaces Nginx with Envoy. Currently, building the nginx container involves running jinja on its config files and building a docker image. With envoy, we can just use the `envoyproxy/envoy` image from DockerHub (which I have copied into our container registries) and feed it a single configmap. The big mess of yaml that is the new configmap for envoy has a lot of boilerplate, but it comprises of the following sections which hopefully on their own are not too bad. ### Envoy config; 1. The top of the `listeners` section shows that Envoy is listening on port 8443 (which is the port that the k8s `Service` will now forward traffic to); 2. The `virtual_hosts` section shows that Envoy will send all paths (prefix ""/"") to the cluster `grafana`; 3. The `http_filters` section says that Envoy will first send an authorization request to the `auth` cluster before allowing the request to pass to grafana; 4. The `clusters` section says that there are two other services Envoy knows about and can send traffic to, one named `auth` that can be found at address `auth` (same as `https://auth`), and a cluster called `grafana` which can be found at `127.0.0.1:3000` (same as `localhost:3000`)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12364:1469,config,configmap,1469,https://hail.is,https://github.com/hail-is/hail/pull/12364,3,['config'],"['config', 'configmap']"
Modifiability,"p support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to HTTPResponse.read would raise an OverflowError on Python 3.9 and earlier.</li>; </ul>; <h2>1.26.10</h2>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a>.</strong></p>; <p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <p>:closed_lock_with_key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.11 (2022-07-25)</h1>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would; raise an <code>OverflowError</code> on Python 3.9 and earlier.</li>; </ul>; <h1>1.26.10 (2022-07-07)</h1>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP; instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/aa3def7d242525e6e854991247c4b68583d15135""><code>aa3def7</code></a> Release 1.26.11</li>; <li><a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12104:1452,config,configuring,1452,https://hail.is,https://github.com/hail-is/hail/pull/12104,2,['config'],"['configured', 'configuring']"
Modifiability,"p support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to HTTPResponse.read would raise an OverflowError on Python 3.9 and earlier.</li>; </ul>; <h2>1.26.10</h2>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a>.</strong></p>; <p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <p>:closed_lock_with_key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.12 (2022-08-22)</h1>; <ul>; <li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module.; Both will be removed in v2.x. See this <code>GitHub issue &lt;https://github.com/urllib3/urllib3/issues/2680&gt;</code>_; for justification and info on how to migrate.</li>; </ul>; <h1>1.26.11 (2022-07-25)</h1>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would; raise an <code>OverflowError</code> on Python 3.9 and earlier.</li>; </ul>; <h1>1.26.10 (2022-07-07)</h1>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12140:1777,config,configuring,1777,https://hail.is,https://github.com/hail-is/hail/pull/12140,2,['config'],"['configured', 'configuring']"
Modifiability,"p/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:4085,Plugin,Plugins,4085,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Plugin'],['Plugins']
Modifiability,"p>; </li>; <li>; <p>Do not close provided file handles with libtiff <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7199"">#7199</a>; [radarhere]</p>; </li>; <li>; <p>Convert to HSV if mode is HSV in getcolor() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7226"">#7226</a>; [radarhere]</p>; </li>; <li>; <p>Added alpha_only argument to getbbox() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7123"">#7123</a>; [radarhere. hugovk]</p>; </li>; <li>; <p>Prioritise speed in <em>repr_png</em> <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7242"">#7242</a>; [radarhere]</p>; </li>; <li>; <p>Do not use CFFI access by default on PyPy <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7236"">#7236</a>; [radarhere]</p>; </li>; <li>; <p>Limit size even if one dimension is zero in decompression bomb check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7235"">#7235</a>; [radarhere]</p>; </li>; <li>; <p>Use --config-settings instead of deprecated --global-option <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7171"">#7171</a>; [radarhere]</p>; </li>; <li>; <p>Better C integer definitions <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6645"">#6645</a>; [Yay295, hugovk]</p>; </li>; <li>; <p>Fixed finding dependencies on Cygwin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7175"">#7175</a>; [radarhere]</p>; </li>; <li>; <p>Changed grabclipboard() to use PNG instead of JPG compression on macOS <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7219"">#7219</a>; [abey79, radarhere]</p>; </li>; <li>; <p>Added in_place argument to ImageOps.exif_transpose() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7092"">#7092</a>; [radarhere]</p>; </li>; <li>; <p>Fixed calling putpalette() on L and LA images before load() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7187"">#7187</a>; [radarhere]</p>; </li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:11169,config,config-settings,11169,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['config'],['config-settings']
Modifiability,"p[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""hail.is/sha"":""1c6dbf20333a"" ""app"":""batch""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""1c6dbf20333a""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:4b4139c73fe9be3bee6c2895aa74059e157eb861d2bdac7d2304ba44b5421f88"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""labels"":map[""app"":""batch""] ""name"":""batch""] ""spec"":map[""ports"":[map[""port"":'P' ""protocol"":""TCP"" ""targetPort"":'\u1388']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; make: *** [deploy-batch] Error 1; Makefile:45: recipe for target 'deploy-batch' failed; ```; [deploy.log](https://github.com/hail-is/hail/files/2504429/deploy.log). Service accounts:; ```; error: the server doesn't have a resource type ""service-accounts""; # kubectl get serviceaccounts ; NAME SECRETS AGE; batch-svc 1 9h; default 1 113d; # kubectl get serviceaccounts ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:4392,config,configuration,4392,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,parameterize TableRead with a TableReader,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5139:0,parameteriz,parameterize,0,https://hail.is,https://github.com/hail-is/hail/pull/5139,1,['parameteriz'],['parameterize']
Modifiability,parameterize en/decoder,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2859:0,parameteriz,parameterize,0,https://hail.is,https://github.com/hail-is/hail/pull/2859,1,['parameteriz'],['parameterize']
Modifiability,parameterize some IRSuite tests to avoid c++ recompilation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5582:0,parameteriz,parameterize,0,https://hail.is,https://github.com/hail-is/hail/pull/5582,1,['parameteriz'],['parameterize']
Modifiability,parameterized (matrix)table file by rvd spec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2828:0,parameteriz,parameterized,0,https://hail.is,https://github.com/hail-is/hail/pull/2828,1,['parameteriz'],['parameterized']
Modifiability,"part of the work for getting TableMapPartitions working. you can use `Ref` or `In` nodes that have type `TStream` as emittable streams. the variable/argument must be bound to a `Iterator[RegionValue]`, which will be iterated over and emitted as part of the stream. this stream can then be composed like other streams. there's no fancy region management going on in this PR since i'm going to handle that by deep copying in TableMapPartitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7677:140,variab,variable,140,https://hail.is,https://github.com/hail-is/hail/pull/7677,1,['variab'],['variable']
Modifiability,pass spark configuration options to HailContext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1510:11,config,configuration,11,https://hail.is,https://github.com/hail-is/hail/issues/1510,1,['config'],['configuration']
Modifiability,"pendabot.com/pytest-dev/pytest-html/issues/230"">#230</a>) <a href=""https://github.com/hugovk""><code>@​hugovk</code></a></li>; </ul>; <h2>Minor Changes</h2>; <ul>; <li>Add support for py39 (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/345"">#345</a>) <a href=""https://github.com/ssbarnea""><code>@​ssbarnea</code></a></li>; <li>Enable py38 testing (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/326"">#326</a>) <a href=""https://github.com/ssbarnea""><code>@​ssbarnea</code></a></li>; <li>Strip ANSI escape sequences when ansi2html is missing (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/315"">#315</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Make the links column in the results table sortable (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/324"">#324</a>) <a href=""https://github.com/gnikonorov""><code>@​gnikonorov</code></a></li>; <li>Make the maximum asset filename length configurable. (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/313"">#313</a>) <a href=""https://github.com/D3X""><code>@​D3X</code></a></li>; <li>Fix broken development docs (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/316"">#316</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Update link to Tox (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/301"">#301</a>) <a href=""https://github.com/gnikonorov""><code>@​gnikonorov</code></a></li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Add ESLint to project (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/367"">#367</a>) <a href=""https://github.com/gnikonorov""><code>@​gnikonorov</code></a></li>; <li>Assure scm versioning is pypa compatible (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/364"">#364</a>) <a href=""https://github.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11524:2127,config,configurable,2127,https://hail.is,https://github.com/hail-is/hail/pull/11524,1,['config'],['configurable']
Modifiability,perationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:176); 	at is.hail.utils.richUtils.RichContextRDD.writePartitions(RichContextRDD.scala:105); 	at is.hail.utils.richUtils.RichRDD$.writePartitions$extension(RichRDD.scala:209); 	at is.hail.linalg.BlockMatrix.write(BlockMatrix.scala:871); 	at is.hail.expr.ir.BlockMatrixNativeWriter.apply(BlockMatrixWriter.scala:46); 	at is.hail.expr.ir.BlockMatrixNativeWriter.apply(BlockMatrixWriter.scala:39); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:858); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:57); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:4838,rewrite,rewrite,4838,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['rewrite'],['rewrite']
Modifiability,piled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); E 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:354); E 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(Lowering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:1631,Rewrite,RewriteBottomUp,1631,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['Rewrite'],['RewriteBottomUp']
Modifiability,"ple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd40949b1fc822d2b7e6517dc0ac993b62badaff8 /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9973,sandbox,sandbox,9973,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,plugins,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/353:0,plugin,plugins,0,https://hail.is,https://github.com/hail-is/hail/issues/353,1,['plugin'],['plugins']
Modifiability,plus some router config fixes. Mainly I'm PR'ing this so I can use the cached base image when testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6936:17,config,config,17,https://hail.is,https://github.com/hail-is/hail/pull/6936,1,['config'],['config']
Modifiability,port GRCh38 VEP properties config to new json format,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4245:27,config,config,27,https://hail.is,https://github.com/hail-is/hail/issues/4245,1,['config'],['config']
Modifiability,"ps://github-redirect.dependabot.com/cbeust/testng/pull/2814"">cbeust/testng#2814</a></li>; <li>Streamline TestResult due to expectedExceptions by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2815"">cbeust/testng#2815</a></li>; <li>Unexpected test runs count with retry analyzer by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2816"">cbeust/testng#2816</a></li>; <li>Make PackageUtils compliant with JPMS by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2817"">cbeust/testng#2817</a></li>; <li>Ability to retry a data provider during failures by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2820"">cbeust/testng#2820</a></li>; <li>Refactoring by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2821"">cbeust/testng#2821</a></li>; <li>Fixing bug with DataProvider retry by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2822"">cbeust/testng#2822</a></li>; <li>Add config key for callback discrepancy behavior by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2823"">cbeust/testng#2823</a></li>; <li>Upgrading versions by <a href=""https://github.com/krmahadevan""><code>@​krmahadevan</code></a> in <a href=""https://github-redirect.dependabot.com/cbeust/testng/pull/2824"">cbeust/testng#2824</a></li>; <li>Fix <a href=""https://github-redirect.dependabot.com/cbeust/testng/issues/2770"">#2770</a>: FileAlreadyExistsException on copy by <a href=""h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:3523,Refactor,Refactoring,3523,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['Refactor'],['Refactoring']
Modifiability,"ps://github.com/googleapis/java-storage/commit/3345ac9eec286ee3108c08bdbe263eba59085ad3""><code>3345ac9</code></a> test: add test to verify <code>lifecycle.rule.condition.age_days = 0</code> (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1846"">#1846</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/45dc983a4af8e7feb937263ce611bd34eda37e03""><code>45dc983</code></a> feat: update GrpcBlobReadChannel to allow seek/limit after read (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1834"">#1834</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/b8f43169a504080c55eadc3428d0d7966efdc3d4""><code>b8f4316</code></a> build(deps): update dependency org.apache.maven.plugins:maven-dependency-plug...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/e532a590fd351bb2020b571d21662fbee629038e""><code>e532a59</code></a> build(deps): update dependency org.apache.maven.plugins:maven-surefire-plugin...</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.17.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-version=1.106.0&new-version=2.17.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will rec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12598:15980,plugin,plugins,15980,https://hail.is,https://github.com/hail-is/hail/pull/12598,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"ps://github.com/googleapis/java-storage/commit/c8bf3c70cca81ed87a52939fe7da58889c8f55ce""><code>c8bf3c7</code></a> fix: update GrpcStorageImpl#update to support fine-grained update of BucketIn...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/3345ac9eec286ee3108c08bdbe263eba59085ad3""><code>3345ac9</code></a> test: add test to verify <code>lifecycle.rule.condition.age_days = 0</code> (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1846"">#1846</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/45dc983a4af8e7feb937263ce611bd34eda37e03""><code>45dc983</code></a> feat: update GrpcBlobReadChannel to allow seek/limit after read (<a href=""https://github-redirect.dependabot.com/googleapis/java-storage/issues/1834"">#1834</a>)</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/b8f43169a504080c55eadc3428d0d7966efdc3d4""><code>b8f4316</code></a> build(deps): update dependency org.apache.maven.plugins:maven-dependency-plug...</li>; <li><a href=""https://github.com/googleapis/java-storage/commit/e532a590fd351bb2020b571d21662fbee629038e""><code>e532a59</code></a> build(deps): update dependency org.apache.maven.plugins:maven-surefire-plugin...</li>; <li>Additional commits viewable in <a href=""https://github.com/googleapis/java-storage/compare/v1.106.0...v2.17.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.cloud:google-cloud-storage&package-manager=gradle&previous-version=1.106.0&new-version=2.17.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12598:15763,plugin,plugins,15763,https://hail.is,https://github.com/hail-is/hail/pull/12598,1,['plugin'],['plugins']
Modifiability,"ps://redirect.github.com/python-pillow/Pillow/issues/7497"">#7497</a>; [ZachNagengast, nulano, radarhere]</p>; </li>; <li>; <p>Attempt memory mapping when tile args is a string <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7565"">#7565</a>; [radarhere]</p>; </li>; <li>; <p>Fill identical pixels with transparency in subsequent frames when saving GIF <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7568"">#7568</a>; [radarhere]</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-pillow/Pillow/commit/6956d0b2853f5c7ec5f6ec4c60725c5a7ee73aeb""><code>6956d0b</code></a> 10.2.0 version bump</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/31c8dacdc727673e9099f1ac86019714cdccec67""><code>31c8dac</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7675"">#7675</a> from python-pillow/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/40a3f91af2c78870676a13629b5902bab4ab4cf0""><code>40a3f91</code></a> Merge pull request <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7674"">#7674</a> from nulano/url-example</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/cb41b0cc78eeefbd9ed2ce8c10f8d6d4c405a706""><code>cb41b0c</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/de62b25ed318f1604aa4ccd6f942a04c6b2c8b59""><code>de62b25</code></a> fix image url in &quot;Reading from URL&quot; example</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/7c526a6c6bdc7cb947f0aee1d1ee17c266ff6c61""><code>7c526a6</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/d93a5ad70bf94dbb63bdbfb19491a02976574d6d""><code>d93a5ad</code></a> Merge pull request <a href=""https://redirect.github.com/pyth",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:13358,config,config,13358,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['config'],['config']
Modifiability,"puteOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.130-bea04d9c79b5; Error summary: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:20032,plugin,plugin,20032,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,"puty; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. Fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3326,config,config-proxy,3326,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['config-proxy']
Modifiability,"py"", line 3269, in run_cell_async; has_raised = await self.run_ast_nodes(code_ast.body, cell_name,; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3448, in run_ast_nodes; if await self.run_code(code, result, async_=asy):; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3508, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-2-40c87378e50b>"", line 1, in <module>; aiohttp.ClientSession(); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x104dac8b0>; source_traceback: Object created at (most recent call last):; File ""/Users/dking/miniconda3/bin/ipython"", line 8, in <module>; sys.exit(start_ipython()); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/__init__.py"", line 128, in start_ipython; return launch_new_instance(argv=argv, **kwargs); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/traitlets/config/application.py"", line 1043, in launch_instance; app.start(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/ipapp.py"", line 318, in start; self.shell.mainloop(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 888, in mainloop; self.interact(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 881, in interact; self.run_cell(code, store_history=True); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3009, in run_cell; result = self._run_cell(; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3064, in _run_cell; result = runner(coro); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner; coro.send(None); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/cor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13421:4111,config,config,4111,https://hail.is,https://github.com/hail-is/hail/pull/13421,1,['config'],['config']
Modifiability,"py"", line 3269, in run_cell_async; has_raised = await self.run_ast_nodes(code_ast.body, cell_name,; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3448, in run_ast_nodes; if await self.run_code(code, result, async_=asy):; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3508, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-3-40c87378e50b>"", line 1, in <module>; aiohttp.ClientSession(); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x104daeec0>; source_traceback: Object created at (most recent call last):; File ""/Users/dking/miniconda3/bin/ipython"", line 8, in <module>; sys.exit(start_ipython()); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/__init__.py"", line 128, in start_ipython; return launch_new_instance(argv=argv, **kwargs); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/traitlets/config/application.py"", line 1043, in launch_instance; app.start(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/ipapp.py"", line 318, in start; self.shell.mainloop(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 888, in mainloop; self.interact(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 881, in interact; self.run_cell(code, store_history=True); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3009, in run_cell; result = self._run_cell(; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3064, in _run_cell; result = runner(coro); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner; coro.send(None); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/cor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13421:6130,config,config,6130,https://hail.is,https://github.com/hail-is/hail/pull/13421,1,['config'],['config']
Modifiability,"py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/local/lib/python3.6/http/client.py"", line 1354, in getresponse; response.begin(); File ""/usr/local/lib/python3.6/http/client.py"", line 307, in begin; version, status, reason = self._read_status(); File ""/usr/local/lib/python3.6/http/client.py"", line 268, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/local/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); File ""/usr/local/lib/python3.6/ssl.py"", line 1012, in recv_into; return self.read(nbytes, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 874, in read; return self._sslobj.read(len, buffer); File ""/usr/local/lib/python3.6/ssl.py"", line 631, in read; v = self._sslobj.read(len, buffer); socket.timeout: The read operation timed out. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send; timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 638, in urlopen; _stacktrace=sys.exc_info()[2]); File ""/usr/local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 368, in increment; raise six.reraise(type(error), error, _stacktrace); File ""/usr/local/lib/python3.6/site-packages/urllib3/packages/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8053:1293,adapt,adapters,1293,https://hail.is,https://github.com/hail-is/hail/issues/8053,3,['adapt'],['adapters']
Modifiability,"py4j-0.10.9.5-src.zip/py4j/java_gateway.py"", line 1321, in __call__; File ""/opt/conda/default/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 35, in deco; raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; hail.utils.java.FatalError: HailException: zip: length mismatch: 62164, 104. Java stack trace:; is.hail.utils.HailException: zip: length mismatch: 62164, 104; 	at __C8160Compiled.__m8201split_ToArray(Emit.scala); 	at __C8160Compiled.__m8169split_CollectDistributedArray(Emit.scala); 	at __C8160Compiled.__m8164split_Let(Emit.scala); 	at __C8160Compiled.apply(Emit.scala); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$4(CompileAndEvaluate.scala:61); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2(CompileAndEvaluate.scala:61); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$2$adapted(CompileAndEvaluate.scala:59); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:140); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:140); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:59); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:33); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486:3104,adapt,adapted,3104,https://hail.is,https://github.com/hail-is/hail/issues/13486,1,['adapt'],['adapted']
Modifiability,"questreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3235,config,config,3235,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['config']
Modifiability,"r simplifying the implementation of `ArrayFilter`. There is basically no nice way to implement filter otherwise without introducing some significant code duplication.; - ~~The stream ""parameter"", as well as the `Empty` return, are not very useful for the basic streams in this PR. However, they simplify the implementation of `ArrayFlatMap` (aka ""composing"" two parameterized streams).~~ NOTE (to Patrick): I decided to abandon the ""empty"" return idea in favor of just providing ""default states"" that always yield empty streams. **Implementation Notes**. - The implementation makes great use of Scala's type system. Most of the streams are implemented first in a very type aware manner, where it is easy to reason about the types of data flowing in and out, before being instantiated with EmitTriplets and Envs which don't hold very much type information. For instance, we have the following helper for `map`:; ```scala; Parameterized[P, A].map(f: A => B): Parameterized[P, B]; ```; The emitter instantiates P = `Any`, A = `EmitTriplet`, B = `EmitTriplet` :/. - Complex streams will have non trivial control flow and jumps. Therefore `init` and `step` both take `JoinPointBuilder`s and return `Code[Ctrl]`, to indicate that they may create join points and do jumps. - I've utilized a cute continuation passing style trick in multiple functions. Instead of `init` ""returning"" `Missing`/`Start(s0)` (which is impossible, since these are compile time data structures; they won't exist during JVM runtime), init takes a continuation `k: Init[S] => Code[Ctrl]`, which it must call with one of these values. To use `init`, you can simply provide a pattern matching lambda, which closely resembles the syntax you would normally use to pattern match on a returned value, e.g. (from `contMap`):; ```scala; self.init(mb, jb, param) {; case Missing => k(Missing); case Start(s) => Code(setup, k(Start(s))); }; ```; This technique actually cleans up the implementation significantly, especially moving forward to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7228:2662,Parameteriz,Parameterized,2662,https://hail.is,https://github.com/hail-is/hail/pull/7228,1,['Parameteriz'],['Parameterized']
Modifiability,"r-space values in ConditionSets and in; VariableScalars (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3042"">#3042</a>, <a href=""https://redirect.github.com/fonttools/fonttools/issues/3043"">#3043</a>).</li>; <li>[ttProgram] Handle string input to Program.fromAssembly() (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3038"">#3038</a>).</li>; <li>[otlLib] Added a config option to emit GPOS 7 lookups, currently disabled by default; because of a macOS bug (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3034"">#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3027"">#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning; None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3032"">#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3031"">#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built; with feaLib from FEA feature file. Also, added support for building multiple VFs; defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3024"">#3024</a>).</li>; <li>[mtiLib] Only add <code>Debg</code> table with lookup names when <code>FONTTOOLS_LOOKUP_DEBUGGING</code>; env variable is set (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3023"">#3023</a>).</li>; </ul>; <h2>4.39.0 (released 2023-03-06)</h2>; <ul>; <li>[mtiLib] Optionally add <code>Debg</code> debug info for MTI feature builds (<a href=""https://redirect.github.com/fonttools/fonttools/issues/3018"">#3018</a>).</li>; <li>[ttx] Support reading input file from standard input using special <code>-</code> character,; simi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:12869,variab,variable,12869,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['variab'],['variable']
Modifiability,r.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1141); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1157); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:91); 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:74); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:125); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:121); 	at is.hail.backend.spark.SparkBackend.lookupOrCompileCachedFunction(SparkBackend.scala:273); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:40); 	at is.hail.expr.ir.lowering.TableStageToRVD$.apply(RVDToTableStage.scala:112); 	at is.hail.backend.spark.SparkBackend.lowerDistributedSort(SparkBackend.scala:689); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:110); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:8862,Rewrite,RewriteBottomUp,8862,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"r::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:3851,plugin,plugin,3851,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,ractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Trav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:3672,Rewrite,RewriteBottomUp,3672,https://hail.is,https://github.com/hail-is/hail/issues/6458,3,"['Rewrite', 'rewrite']","['RewriteBottomUp', 'rewrite']"
Modifiability,"randomly assigned Patrick. I expected to get an error like:; ```; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: <<empty key>>; Index Expressions: int32; ```; But instead got:; ```; # ipython; import hail Python 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.utils.ra; In [2]: t = hl.utils.range_table(10) . In [3]: t.describe() ; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'idx': int32 ; ----------------------------------------; Key: ['idx']; ----------------------------------------. In [4]: t = t.key_by() . In [5]: t[t.idx] ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-5-a44cae835751> in <module>; ----> 1 t[t.idx]. /usr/local/lib/python3.7/site-packages/hail/table.py in __getitem__(self, item); 366 else:; 367 try:; --> 368 return self.index(*wrap_to_tuple(item)); 369 except TypeError as e:; 370 raise TypeError(f""Table.__getitem__: invalid index argument(s)\n"". /usr/local/lib/python3.7/site-packages/hail/table.py in index(self, all_matches, *exprs); 1530 """"""; 1531 try:; -> 1532 return self._index(*exprs, all_matches=all_matches); 1533 except TableIndexKeyError as err:; 1534 key_type, exprs = err.args. /usr/local/lib/python3.7/site-packages/hail/table.py in _index(self, all_matches, *exprs); 1554 ; 1555 is_interval = (len(exprs) == 1; -> 1556 and isinstance(self.key[0].dtype, hl.tinterval); 1557 and exprs[0].dtype == self.key[0].dtype.point_type); 1558 . </usr/local/lib/python3.7/site-packages/decorator.py:decorator-gen-570> in __getitem__(self, item). /usr/local/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 583 def wrapper(__original_func, *args, **kw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6663:384,enhance,enhanced,384,https://hail.is,https://github.com/hail-is/hail/issues/6663,1,['enhance'],['enhanced']
Modifiability,rator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsPartition(RichContextRDDRegionValue.scala:37); 	at is.hail.io.RichContextRDDLong$.$anonfun$writeRows$2(RichContextRDDRegionValue.scala:234); 	at is.hail.utils.richUtils.RichContextRDD$.writeParts(RichContextRDD.scala:42); 	at is.hail.utils.richUtils.RichContextRDD.$anonfun$writePartitions$1(RichContextRDD.scala:107); 	at is.hail.utils.richUtils.RichContextRDD.$anonfun$writePartitions$1$adapted(RichContextRDD.scala:105); 	at is.hail.sparkextras.ContextRDD.$anonfun$cmapPartitionsWithIndex$2(ContextRDD.scala:259); 	at is.hail.utils.richUtils.RichContextRDD.$anonfun$cleanupRegions$2(RichContextRDD.scala:60); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:2609,adapt,adapted,2609,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['adapt'],['adapted']
Modifiability,raversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:2388,Rewrite,RewriteBottomUp,2388,https://hail.is,https://github.com/hail-is/hail/issues/9128,2,['Rewrite'],['RewriteBottomUp']
Modifiability,raversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldC,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:3868,Rewrite,RewriteBottomUp,3868,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"rc1 but wasn't documented.</p>; </li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9355"">#9355</a>: Fixed error message prints function decorators when using assert in Python 3.8 and above.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivial/Internal Changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9521"">#9521</a>: Add test coverage to assertion rewrite path.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest/commit/3f12087fe0d86a319216653b08b66a96d400bee2""><code>3f12087</code></a> [pre-commit.ci] auto fixes from pre-commit.com hooks</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/bc3021cdfd76507aa3d9e278bd885da9bc1907b2""><code>bc3021c</code></a> Prepare release version 7.0.1</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/591d476f14e3e83d90fbea75d326a93c5e368708""><code>591d476</code></a> Merge pull request <a href=""https://github-re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:3512,config,configuration,3512,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['config'],['configuration']
Modifiability,"rdance[:2].map(lambda x: hl.sum(x[:2])))>0)). /home/hail/hail.zip/hail/table.py in aggregate(self, expr); 1107 analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}); 1108 ; -> 1109 result_json = base._jt.aggregateJSON(expr._ast.to_hql()); 1110 return expr.dtype._from_json(result_json); 1111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: VerifyError: Bad local variable type; Exception Details:; Location:; is/hail/codegen/generated/C423.apply(Lis/hail/annotations/Region;[Lis/hail/annotations/aggregators/RegionValueAggregator;JZJZ)V @2710: iload; Reason:; Type top (current frame, locals[130]) is not assignable to integer; Current Frame:; bci: @2710; flags: { }; locals: { 'is/hail/codegen/generated/C423', 'is/hail/annotations/Region', '[Lis/hail/annotations/aggregators/RegionValueAggregator;', long, long_2nd, integer, long, long_2nd, integer, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, integer, long, long_2nd, top, integer, integer, top, top, top, integer, long, long_2nd, integer, top, integer, integer, integer, long, long_2nd, integer, long, long_2nd, integer, integer, integer }; stack: { 'is/hail/codegen/generated/C423' }; Bytecode:; 0x0000000: 1508 3630 1530 9900 06a7 0008 1606 a700; 0x0000010: 0614 0020 3731 1530",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3729:2387,variab,variable,2387,https://hail.is,https://github.com/hail-is/hail/issues/3729,1,['variab'],['variable']
Modifiability,"re the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](https://github.com/kubernetes/kubernetes/pull/61231#pullrequestreview-104364784) (what; the hell?), ergo Confused Deputy. I also partly resolved the batch [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem)! Batch will only; use its TLS identity when making callbacks for CI jobs. This makes CI quite; powerful, but we control it. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:2890,config,config,2890,https://hail.is,https://github.com/hail-is/hail/pull/8513,3,['config'],"['config', 'configuration']"
Modifiability,"re-PEP8 methods (such as <code>ParserElement.parseString</code>) will start to raise <code>DeprecationWarnings</code>. 3.2.0 should get released some time later in 2023. I currently plan to completely drop the pre-PEP8 methods in pyparsing 4.0, though we won't see that release until at least late 2023 if not 2024. So there is plenty of time to convert existing parsers to the new function names before the old functions are completely removed. (Big help from Devin J. Pohly in structuring the code to enable this peaceful transition.)</p>; <p>Version 3.2.0 will also discontinue support for Python versions 3.6 and 3.7.</p>; <ul>; <li>; <p>API ENHANCEMENT: <code>Optional(expr)</code> may now be written as <code>expr | &quot;&quot;</code></p>; <p>This will make this code:</p>; <pre><code>&quot;{&quot; + Optional(Literal(&quot;A&quot;) | Literal(&quot;a&quot;)) + &quot;}&quot;; </code></pre>; <p>writable as:</p>; <pre><code>&quot;{&quot; + (Literal(&quot;A&quot;) | Literal(&quot;a&quot;) | &quot;&quot;) + &quot;}&quot;; </code></pre>; <p>Some related changes implemented as part of this work:</p>; <ul>; <li><code>Literal(&quot;&quot;)</code> now internally generates an <code>Empty()</code> (and no longer raises an exception)</li>; <li><code>Empty</code> is now a subclass of <code>Literal</code></li>; </ul>; <p>Suggested by Antony Lee (issue <a href=""https://redirect.github.com/pyparsing/pyparsing/issues/412"">#412</a>), PR (<a href=""https://redirect.github.com/pyparsing/pyparsing/issues/413"">#413</a>) by Devin J. Pohly.</p>; </li>; <li>; <p>Added new class property <code>identifier</code> to all Unicode set classes in <code>pyparsing.unicode</code>, using the class's values for <code>cls.identchars</code> and <code>cls.identbodychars</code>. Now Unicode-aware parsers that formerly wrote:</p>; <pre><code>ppu = pyparsing.unicode; ident = Word(ppu.Greek.identchars, ppu.Greek.identbodychars); </code></pre>; <p>can now write:</p>; <pre><code>ident = ppu.Greek.identifier; # or; # id",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13334:985,ENHANCE,ENHANCEMENT,985,https://hail.is,https://github.com/hail-is/hail/pull/13334,2,['ENHANCE'],['ENHANCEMENT']
Modifiability,read SPARK_HOME from environment variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1119:33,variab,variable,33,https://hail.is,https://github.com/hail-is/hail/pull/1119,1,['variab'],['variable']
Modifiability,"reate and delete API instead of copying the user's gsa from the production namespace. This relies on / tests that the delete user endpoint is properly deleting cloud identities when the users are deleted (previously broken in GCP but fixed in this PR.; - The developer role no longer implicitly deletes and recreates a corresponding namespace. I wanted adding developers to test namespaces not to have side-effects that leaked out of the namespace. A follow-up PR will incorporate the ability for a developer to request an on-demand dev namespace, which should be made a lot easier after these changes. I think this also means that we can remove some permissions from the auth K8s ServiceAccount since it no longer needs the ability to create and delete namespaces.; - A fixed-but-sufficient number of oauth2 callbacks are hard-coded into the oauth2 secret from GCP/azure and then allocated to a given namespace. This is fairly self-contained, all that needs to happen is to tell `auth` what callback to use and rewrite those callback urls in gateway to route back to the appropriate auth. This is done only for test namespaces, production still just uses `auth.hail.is/oauth2callback`. This gets around a long-standing limitation of Google oauth2 clients where there is no programmatic way to change the oauth2 callbacks. ### What has stayed the same; - The lifecycle of dev and test namespaces has not changed (future PR).; - The semantics of a dev deploy has not changed (other than a different oauth2 callback which the user will not notice). ### Testing; I've tested this branch in my own google project both by deploying main and updating to this branch and by deploying this branch from the beginning. The CI in my project ran both some dev deploys and a test batch because I PR'd this branch against my fork. Tested that I could access both my dev namespace and the test namespace in that deployment. I manually verified that dev cloud identities in the test namespace were deleted after the n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12751:1873,rewrite,rewrite,1873,https://hail.is,https://github.com/hail-is/hail/pull/12751,1,['rewrite'],['rewrite']
Modifiability,"recursive flag for glob notebook search by <a href=""https://github.com/paoloalba""><code>@​paoloalba</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1785"">jupyter/nbconvert#1785</a></li>; <li>Updates for sphinx 5.0 support by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1788"">jupyter/nbconvert#1788</a></li>; <li>Fixed unique div ids in lab template, fixed <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1759"">#1759</a> by <a href=""https://github.com/veghdev""><code>@​veghdev</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1761"">jupyter/nbconvert#1761</a></li>; <li>WebPDFExporter: Emulate media print by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1791"">jupyter/nbconvert#1791</a></li>; <li>Fix fonts overriden by user stylesheet by inheriting styles by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1793"">jupyter/nbconvert#1793</a></li>; <li>Fix lab template output alignment by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1795"">jupyter/nbconvert#1795</a></li>; <li>Add qtpdf and qtpng exporters by <a href=""https://github.com/davidbrochart""><code>@​davidbrochart</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1611"">jupyter/nbconvert#1611</a></li>; <li>Fix linters by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1825"">jupyter/nbconvert#1825</a></li>; <li>Remove downloaded CSS from repository by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a hr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12126:2220,inherit,inheriting,2220,https://hail.is,https://github.com/hail-is/hail/pull/12126,1,['inherit'],['inheriting']
Modifiability,"recursive flag for glob notebook search by <a href=""https://github.com/paoloalba""><code>@​paoloalba</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1785"">jupyter/nbconvert#1785</a></li>; <li>Updates for sphinx 5.0 support by <a href=""https://github.com/blink1073""><code>@​blink1073</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1788"">jupyter/nbconvert#1788</a></li>; <li>Fixed unique div ids in lab template, fixed <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/issues/1759"">#1759</a> by <a href=""https://github.com/veghdev""><code>@​veghdev</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1761"">jupyter/nbconvert#1761</a></li>; <li>WebPDFExporter: Emulate media print by <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1791"">jupyter/nbconvert#1791</a></li>; <li>Fix fonts overriden by user stylesheet by inheriting styles by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1793"">jupyter/nbconvert#1793</a></li>; <li>Fix lab template output alignment by <a href=""https://github.com/dakoop""><code>@​dakoop</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1795"">jupyter/nbconvert#1795</a></li>; <li>[pre-commit.ci] pre-commit autoupdate by <a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a> in <a href=""https://github-redirect.dependabot.com/jupyter/nbconvert/pull/1796"">jupyter/nbconvert#1796</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jupyter/nbconvert/commit/27a7fcbd9cb55d1c30818b1c8b5918bb178a243f""><code>27a7fcb</code></a> Release 7.0.0</li>; <li><a href=""https://github.com/jupyter/nbconve",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12126:8376,inherit,inheriting,8376,https://hail.is,https://github.com/hail-is/hail/pull/12126,1,['inherit'],['inheriting']
Modifiability,"recy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:6315,config,config-NAME,6315,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config-NAME']
Modifiability,"redirect.dependabot.com/psf/black/issues/3168"">#3168</a>)</li>; <li>When using <code>--skip-magic-trailing-comma</code> or <code>-C</code>, trailing commas are stripped from; subscript expressions with more than 1 element (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3209"">#3209</a>)</li>; <li>Implicitly concatenated strings inside a list, set, or tuple are now wrapped inside; parentheses (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3162"">#3162</a>)</li>; <li>Fix a string merging/split issue when a comment is present in the middle of implicitly; concatenated strings on its own line (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3227"">#3227</a>)</li>; </ul>; <h3><em>Blackd</em></h3>; <ul>; <li><code>blackd</code> now supports enabling the preview style via the <code>X-Preview</code> header (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3217"">#3217</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Black now uses the presence of debug f-strings to detect target version (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3215"">#3215</a>)</li>; <li>Fix misdetection of project root and verbose logging of sources in cases involving; <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Immediate <code>.gitignore</code> files in source directories given on the command line are now; also respected, previously only <code>.gitignore</code> files in the project root and; automatically discovered directories were respected (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3237"">#3237</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Recommend using BlackConnect in IntelliJ IDEs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3150"">#3150</a>)</li>; </ul>; <h3>Integrations</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:7863,Config,Configuration,7863,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['Config'],['Configuration']
Modifiability,"reemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:4233,config,configuration,4233,https://hail.is,https://github.com/hail-is/hail/pull/12095,4,"['Config', 'config']","['ConfigMap', 'configuration']"
Modifiability,"ref=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105687"">kubernetes/kubernetes#105687</a>, <a href=""https://github.com/alculquicondor""><code>@​alculquicondor</code></a>)</li>; <li>Kube-apiserver: Fixes handling of CRD schemas containing literal null values in enums. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104969"">kubernetes/kubernetes#104969</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-apiserver: The <code>rbac.authorization.k8s.io/v1alpha1</code> API version is removed; use the <code>rbac.authorization.k8s.io/v1</code> API, available since v1.8. The <code>scheduling.k8s.io/v1alpha1</code> API version is removed; use the <code>scheduling.k8s.io/v1</code> API, available since v1.14. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104248"">kubernetes/kubernetes#104248</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-scheduler: support for configuration file version <code>v1beta1</code> is removed. Update configuration files to v1beta2(xref: <a href=""https://github-redirect.dependabot.com/kubernetes/enhancements/issues/2901"">kubernetes/enhancements#2901</a>) or v1beta3 before upgrading to 1.23. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104782"">kubernetes/kubernetes#104782</a>, <a href=""https://github.com/kerthcet""><code>@​kerthcet</code></a>)</li>; <li>KubeSchedulerConfiguration provides a new field <code>MultiPoint</code> which will register a plugin for all valid extension points (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105611"">kubernetes/kubernetes#105611</a>, <a href=""https://github.com/damemi""><code>@​damemi</code></a>) [SIG Scheduling and Testing]</li>; <li>Kubelet should reject pods whose OS doesn't match the node's OS label. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105292"">kubernetes/kubernetes#105292",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:8383,config,configuration,8383,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['config'],['configuration']
Modifiability,"ref=""https://github.com/krzysztof-pawlik-gat""><code>@​krzysztof-pawlik-gat</code></a></li>; <li>Keep sort preference for previously sorted columns (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/220"">#220</a>) <a href=""https://github.com/wanam""><code>@​wanam</code></a></li>; <li>Fix assets file naming to work across both *nix and windows (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/223"">#223</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Remove unused and undocumented markers (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/224"">#224</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Append a line break after captured log sections (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/217"">#217</a>) <a href=""https://github.com/borntyping""><code>@​borntyping</code></a></li>; <li>Handle when report title is stored as an environment variable (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/203"">#203</a>) <a href=""https://github.com/BeyondEvil""><code>@​BeyondEvil</code></a></li>; <li>Removed extraneous space from anchor tag (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/192"">#192</a>) <a href=""https://github.com/chardbury""><code>@​chardbury</code></a></li>; <li>Stop filtering out falsy environment values (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/180"">#180</a>) <a href=""https://github.com/crazymerlyn""><code>@​crazymerlyn</code></a></li>; <li>Always define <strong>version</strong> even if get_distribution() fails (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-html/issues/178"">#178</a>) <a href=""https://github.com/nicoddemus""><code>@​nicoddemus</code></a></li>; <li>Disable sort on environment table when metadata is ordered (<a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11524:7228,variab,variable,7228,https://hail.is,https://github.com/hail-is/hail/pull/11524,1,['variab'],['variable']
Modifiability,refactor ArraySort IR to take comparison expression,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5084:0,refactor,refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/5084,1,['refactor'],['refactor']
Modifiability,refactor backend stuff in preparation for scheduler's backend,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6301:0,refactor,refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/6301,1,['refactor'],['refactor']
Modifiability,refactor collect as set bool,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3821:0,refactor,refactor,0,https://hail.is,https://github.com/hail-is/hail/pull/3821,1,['refactor'],['refactor']
Modifiability,refactored linreg as a map rather than a join,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/989:0,refactor,refactored,0,https://hail.is,https://github.com/hail-is/hail/pull/989,1,['refactor'],['refactored']
Modifiability,"remove unused let; push aggfilter into aggmap (if possible); fuse aggmaps; Added ir.Binds (list of variables bound by an IR); Added ir.Mentions (whether a variable is free in an IR); propagate NA where strict; fuse ArrayMaps. Also improved ir.Pretty (more headers, was missing some closing parens). This cleans up most of the small IR opportunities I've seen in @konradjk's scripts, although probably won't make a huge difference (except maybe pushing AggFilter into AggMap).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3429:99,variab,variables,99,https://hail.is,https://github.com/hail-is/hail/pull/3429,2,['variab'],"['variable', 'variables']"
Modifiability,remove vep configs human_ancestor and conservation file for 0.2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1728:11,config,configs,11,https://hail.is,https://github.com/hail-is/hail/issues/1728,1,['config'],['configs']
Modifiability,removed unused variable buffer from ReadRowsRDD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2258:15,variab,variable,15,https://hail.is,https://github.com/hail-is/hail/pull/2258,1,['variab'],['variable']
Modifiability,"reordering output (<a href=""https://redirect.github.com/pygments/pygments/issues/2407"">#2407</a>, <a href=""https://redirect.github.com/pygments/pygments/issues/2410"">#2410</a>, <a href=""https://redirect.github.com/pygments/pygments/issues/2412"">#2412</a>)</p>; </li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pygments/pygments/commit/04a75bd5a75bfe27f0b582dd83c85e62f9475581""><code>04a75bd</code></a> Prepare 2.15.1 release.</li>; <li><a href=""https://github.com/pygments/pygments/commit/fdf182a7af85b1deeeb637ca970d31935e7c9d52""><code>fdf182a</code></a> Improve Java properties lexer (<a href=""https://redirect.github.com/pygments/pygments/issues/2404"">#2404</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/c5a2b23adaaadc08a7586a5eda72e9f7d6171012""><code>c5a2b23</code></a> Update CHANGES</li>; <li><a href=""https://github.com/pygments/pygments/commit/c97762448b1e4eac8d74b8d88415f23c32aa0cdd""><code>c977624</code></a> Refactor PythonConsoleLexer as a DelegatingLexer (<a href=""https://redirect.github.com/pygments/pygments/issues/2412"">#2412</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/50dd4d80e25c4c4afab503d41b471a536ed2af13""><code>50dd4d8</code></a> Python console: do not require output that looks like a traceback to be valid...</li>; <li><a href=""https://github.com/pygments/pygments/commit/96a0cdf200ab8a36dc5f6f748f3b9d01c05cb91b""><code>96a0cdf</code></a> PythonTracebackLexer: minor tweak in docstring</li>; <li><a href=""https://github.com/pygments/pygments/commit/569eea6ee85ec4d679bb38a890c167b58ee727dd""><code>569eea6</code></a> Enable Sphinx nitpicky mode and fix warnings (<a href=""https://redirect.github.com/pygments/pygments/issues/2403"">#2403</a>)</li>; <li><a href=""https://github.com/pygments/pygments/commit/b018a65cb6ef51596c2cb8d6c97f0d79d9fa2ae7""><code>b018a65</code></a> Prepare for next release.</li>; <li>See full diff in <a href=""https://github.com/pygment",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12909:2515,Refactor,Refactor,2515,https://hail.is,https://github.com/hail-is/hail/pull/12909,1,['Refactor'],['Refactor']
Modifiability,"request and response bodies from; being written to the API audit log. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/94986"">kubernetes/kubernetes#94986</a>, <a href=""https://github.com/tkashem""><code>@​tkashem</code></a>) [SIG API Machinery, Auth, Cloud Provider and Testing]</li>; <li>A small regression in Service updates was fixed. The circumstances are so unlikely that probably nobody would ever hit it. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104601"">kubernetes/kubernetes#104601</a>, <a href=""https://github.com/thockin""><code>@​thockin</code></a>)</li>; <li>Added a feature gate <code>StatefulSetAutoDeletePVC</code>, which allows PVCs automatically created for StatefulSet pods to be automatically deleted. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/99728"">kubernetes/kubernetes#99728</a>, <a href=""https://github.com/mattcary""><code>@​mattcary</code></a>)</li>; <li>Client-go impersonation config can specify a UID to pass impersonated uid information through in requests. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104483"">kubernetes/kubernetes#104483</a>, <a href=""https://github.com/margocrawf""><code>@​margocrawf</code></a>)</li>; <li>Create HPA v2 from v2beta2 with some fields changed. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/102534"">kubernetes/kubernetes#102534</a>, <a href=""https://github.com/wangyysde""><code>@​wangyysde</code></a>) [SIG API Machinery, Apps, Auth, Autoscaling and Testing]</li>; <li>Ephemeral containers graduated to beta and are now available by default. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105405"">kubernetes/kubernetes#105405</a>, <a href=""https://github.com/verb""><code>@​verb</code></a>)</li>; <li>Fix kube-proxy regression on UDP services because the logic to detect stale connections was not considering if the endpoint was ready. (<a href=""https://g",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:2835,config,config,2835,https://hail.is,https://github.com/hail-is/hail/pull/11957,1,['config'],['config']
Modifiability,"resolves #14749. Leave the override in Backend as well to avoid duplication. Future enhancements may enable us to construct a ServiceBackend from argv alone, allowing this to be reverted. ## Security Assessment. Delete all except the correct answer:; - This change has no security impact. ### Impact Description; This change restores known good functionality.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14750:84,enhance,enhancements,84,https://hail.is,https://github.com/hail-is/hail/pull/14750,1,['enhance'],['enhancements']
Modifiability,"resting thing is, when I tried to convert the exactly same data in local computer using singularity instead of docker, it worked. Also, for the the other chromosomes with less variants but same samples, such as chr13, it worked well in Terra. Since we will convert multiple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9703,sandbox,sandbox,9703,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,"rets/ConfigMaps feature to Beta and enable the feature by default.; This allows to set <code>Immutable</code> field in Secrets or ConfigMap object to mark their contents as immutable. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89594"">kubernetes/kubernetes#89594</a>, <a href=""https://github.com/wojtek-t""><code>@​wojtek-t</code></a>) [SIG Apps and Testing]</li>; <li>Remove <code>BindTimeoutSeconds</code> from schedule configuration <code>KubeSchedulerConfiguration</code> (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91580"">kubernetes/kubernetes#91580</a>, <a href=""https://github.com/cofyc""><code>@​cofyc</code></a>) [SIG Scheduling and Testing]</li>; <li>Remove kubescheduler.config.k8s.io/v1alpha1 (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/89298"">kubernetes/kubernetes#89298</a>, <a href=""https://github.com/gavinfish""><code>@​gavinfish</code></a>) [SIG Scheduling]</li>; <li>Reserve plugins that fail to reserve will trigger the unreserve extension point (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92391"">kubernetes/kubernetes#92391</a>, <a href=""https://github.com/adtac""><code>@​adtac</code></a>) [SIG Scheduling and Testing]</li>; <li>Resolve regression in <code>metadata.managedFields</code> handling in update/patch requests submitted by older API clients (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/91748"">kubernetes/kubernetes#91748</a>, <a href=""https://github.com/apelisse""><code>@​apelisse</code></a>)</li>; <li>Scheduler: optionally check for available storage capacity before scheduling pods which have unbound volumes (alpha feature with the new <code>CSIStorageCapacity</code> feature gate, only works for CSI drivers and depends on support for the feature in a CSI driver deployment) (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/92387"">kubernetes/kubernetes#92387</a>, <a href=""https:/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11462:12140,plugin,plugins,12140,https://hail.is,https://github.com/hail-is/hail/pull/11462,1,['plugin'],['plugins']
Modifiability,rettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); at is.hail.backend.spark.Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:3520,adapt,adapted,3520,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['adapt'],['adapted']
Modifiability,rewrite MatrixExplodeRows using compiled IR functions; add tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4069:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4069,1,['rewrite'],['rewrite']
Modifiability,rewrite README to focus more on software dev,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4600:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4600,1,['rewrite'],['rewrite']
Modifiability,rewrite Region.scala to build on the RegionPool backed region.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4859:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4859,1,['rewrite'],['rewrite']
Modifiability,rewrite add_index as scans,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4149:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4149,1,['rewrite'],['rewrite']
Modifiability,rewrite annotate_rows in terms of select_rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3230:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/3230,1,['rewrite'],['rewrite']
Modifiability,rewrite cpp decoder stuff to take input stream directly,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4676:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4676,1,['rewrite'],['rewrite']
Modifiability,rewrite cxx iterator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4956:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4956,1,['rewrite'],['rewrite']
Modifiability,rewrite hl.agg.corr in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6729:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/6729,1,['rewrite'],['rewrite']
Modifiability,rewrite hl.agg.hist in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6730:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/6730,1,['rewrite'],['rewrite']
Modifiability,rewrite interval joins,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4487:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4487,1,['rewrite'],['rewrite']
Modifiability,rewrite sample in python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4133:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/4133,1,['rewrite'],['rewrite']
Modifiability,rewrite sample names programatically,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/945:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/issues/945,1,['rewrite'],['rewrite']
Modifiability,rewrite select/annotate/drop to go through `_select_cols` in Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3336:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/3336,1,['rewrite'],['rewrite']
Modifiability,rewrite staged aggregator interface,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6652:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/6652,1,['rewrite'],['rewrite']
Modifiability,rewrite trio_matrix in Python.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5132:0,rewrite,rewrite,0,https://hail.is,https://github.com/hail-is/hail/pull/5132,1,['rewrite'],['rewrite']
Modifiability,"rg/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.4.0</h2>; <p>Changelog: <a href=""https://www.sphinx-doc.org/en/master/changes.html"">https://www.sphinx-doc.org/en/master/changes.html</a></p>; <h2>v4.3.1</h2>; <p>No release notes provided.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sphinx-doc/sphinx/blob/5.x/CHANGES"">sphinx's changelog</a>.</em></p>; <blockquote>; <h1>Release 5.0.2 (released Jun 17, 2022)</h1>; <h2>Features added</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10523"">#10523</a>: HTML Theme: Expose the Docutils's version info tuple as a template; variable, <code>docutils_version_info</code>. Patch by Adam Turner.</li>; </ul>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10538"">#10538</a>: autodoc: Inherited class attribute having docstring is documented even; if :confval:<code>autodoc_inherit_docstring</code> is disabled</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10509"">#10509</a>: autosummary: autosummary fails with a shared library</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10497"">#10497</a>: py domain: Failed to resolve strings in Literal. Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10523"">#10523</a>: HTML Theme: Fix double brackets on citation references in Docutils 0.18+.; Patch by Adam Turner.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10534"">#10534</a>: Missing CSS for nav.contents in Docutils 0.18+. Patch by Adam Turner.</li>; </ul>; <h1>Release 5.0.1 (released Jun 03, 2022)</h1>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10498"">#10498</a>: gettext: TypeError is raised when sorting warni",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11925:1755,Inherit,Inherited,1755,https://hail.is,https://github.com/hail-is/hail/pull/11925,1,['Inherit'],['Inherited']
Modifiability,ring.LowerTableIR$.lower$2(LowerTableIR.scala:1050); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2192); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1050); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1361); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1050); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1654); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:823); 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14529:8146,rewrite,rewrite,8146,https://hail.is,https://github.com/hail-is/hail/issues/14529,1,['rewrite'],['rewrite']
Modifiability,"rk releases</li>; <li><a href=""https://github.com/apache/spark/commit/001d8b0cddcec46a44e7c6e31612dc2baada05d5""><code>001d8b0</code></a> [SPARK-37554][BUILD] Add PyArrow, pandas and plotly to release Docker image d...</li>; <li><a href=""https://github.com/apache/spark/commit/9dd4c07475c82f922c29d67a4db4bb42676c5c07""><code>9dd4c07</code></a> [SPARK-37730][PYTHON][FOLLOWUP] Split comments to comply pycodestyle check</li>; <li><a href=""https://github.com/apache/spark/commit/bc54a3f0c2e08893702c3929bfe7a9d543a08cdb""><code>bc54a3f</code></a> [SPARK-37730][PYTHON] Replace use of MPLPlot._add_legend_handle with MPLPlot....</li>; <li><a href=""https://github.com/apache/spark/commit/c5983c1691f20590abf80b17bdc029b584b89521""><code>c5983c1</code></a> [SPARK-38018][SQL][3.2] Fix ColumnVectorUtils.populate to handle CalendarInte...</li>; <li><a href=""https://github.com/apache/spark/commit/32aff86477ac001b0ee047db08591d89e90c6eb8""><code>32aff86</code></a> [SPARK-39447][SQL][3.2] Avoid AssertionError in AdaptiveSparkPlanExec.doExecu...</li>; <li><a href=""https://github.com/apache/spark/commit/be891ad99083564a7bf7f421e00b2cc4759a679f""><code>be891ad</code></a> [SPARK-39551][SQL][3.2] Add AQE invalid plan check</li>; <li><a href=""https://github.com/apache/spark/commit/1c0bd4c15a28d7c6a2dca846a5b8d0eb1d152aae""><code>1c0bd4c</code></a> [SPARK-39656][SQL][3.2] Fix wrong namespace in DescribeNamespaceExec</li>; <li><a href=""https://github.com/apache/spark/commit/3d084fe3217bea9af4c544f10ead8a2e5b97dad4""><code>3d084fe</code></a> [SPARK-39677][SQL][DOCS][3.2] Fix args formatting of the regexp and like func...</li>; <li>Additional commits viewable in <a href=""https://github.com/apache/spark/compare/v3.1.3...v3.2.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pyspark&package-manager=pip&previous-version=3.1.3&new-version=3.2.2)](https://docs.github.com/en/github/managing-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12452:1466,Adapt,AdaptiveSparkPlanExec,1466,https://hail.is,https://github.com/hail-is/hail/pull/12452,1,['Adapt'],['AdaptiveSparkPlanExec']
Modifiability,"rm aggregation aware of the nearest containing node which defines what is being aggregated over. This is done by making a new variable name---here called ""agg_capability""---which is added to the child context by any node which (re)defines the meaning of aggregation (`TableMapRows`, `AggFilter`, etc.), and which is implicitly referenced by any node which performs an aggregation (`ApplyAggOp`, `AggFilter`, etc.). This requires nodes to be able to bind variables which shadow variables already bound by parents, which it turns out wasn't handled correctly by the CSE algorithm. Fixing this required several changes:; * I moved free variable computation to a lazy value on `BaseIR`. This way, each time we see a subtree `x`, we can recompute the max depth of the binding sites of all of `x`'s free variables, since those binding sites may be different than last time we saw `x`. This also required splitting the free variables into value, agg, and scan sets, so they can be looked up in the correct context (previously context lookup was always done at the `Ref` node, at which point the variable was in the value context).; * Now, in the `CSEPrintPass`, we have to recompute the same binding depth calculation that was done in the analysis pass, so we know which binding site to look at (previously I just searched all binding sites in scope, but with shadowing handled correctly we don't have sufficient information to decide which binding site is valid). This requires maintaining contexts in the print pass, which is annoying because we are now traversing the tree of `Renderable` children, which is not exactly the same as the IR tree.; * To fix this, I duplicated all methods involving binding structure on `BaseIR`. To avoid having to write twice as many methods on concrete IR classes, I made the methods taking the index of the `Renderable` child (e.g. `renderable_bindings`) be the primary methods which are overridden. For all IR nodes, there is a map from IR child index to Renderable chil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7479:994,variab,variables,994,https://hail.is,https://github.com/hail-is/hail/pull/7479,2,['variab'],"['variable', 'variables']"
Modifiability,"rn pip will not pick it; up automatically. The reason is that is has <code>multidict &lt; 6</code> set in; the distribution package metadata (see :pr:<code>6950</code>). Please, use; <code>aiohttp ~= 3.8.3, != 3.8.1</code> instead, if you can.</p>; <h2>Bugfixes</h2>; <ul>; <li>Added support for registering :rfc:<code>OPTIONS &lt;9110#OPTIONS&gt;</code>; HTTP method handlers via :py:class:<code>~aiohttp.web.RouteTableDef</code>.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/4663"">#4663</a>)</li>; <li>Started supporting :rfc:<code>authority-form &lt;9112#authority-form&gt;</code> and; :rfc:<code>absolute-form &lt;9112#absolute-form&gt;</code> URLs on the server-side.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/6227"">#6227</a>)</li>; <li>Fixed Python 3.11 incompatibilities by using Cython 0.29.25.; (<a href=""https://github-redirect.dependabot.com/aio-libs/aiohttp/issues/6396"">#6396</a>)</li>; <li>Extended the <code>sock</code> argument typing declaration of the</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/blob/master/CHANGES.rst"">aiohttp's changelog</a>.</em></p>; <blockquote>; <h1>3.8.3 (2022-09-21)</h1>; <p>.. attention::</p>; <p>This is the last :doc:<code>aiohttp &lt;index&gt;</code> release tested under; Python 3.6. The 3.9 stream is dropping it from the CI and the; distribution package metadata.</p>; <h2>Bugfixes</h2>; <ul>; <li>; <p>Increased the upper boundary of the :doc:<code>multidict:index</code> dependency; to allow for the version 6 -- by :user:<code>hugovk</code>.</p>; <p>It used to be limited below version 7 in :doc:<code>aiohttp &lt;index&gt;</code> v3.8.1 but; was lowered in v3.8.2 via :pr:<code>6550</code> and never brought back, causing; problems with dependency pins when upgrading. :doc:<code>aiohttp &lt;index&gt;</code> v3.8.3; fixes that by",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12296:2359,Extend,Extended,2359,https://hail.is,https://github.com/hail-is/hail/pull/12296,1,['Extend'],['Extended']
Modifiability,"rom Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sure how much mileage we can get out of layer squashing. You can take a look at a service-base build [here](https://gist.github.com/danking/830af0688970c176ff25dbfeb4b222e7). Note the stdout comes first and then I `cat` the ""trace"" file which is a very weirdly formatted series of JSON objects that give more detailed information. You can clean it up a bit with `jq -c '(.Lo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:1394,layers,layers,1394,https://hail.is,https://github.com/hail-is/hail/pull/10502,1,['layers'],['layers']
Modifiability,rows_table / cols_table should inherit globals,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2810:31,inherit,inherit,31,https://hail.is,https://github.com/hail-is/hail/issues/2810,1,['inherit'],['inherit']
Modifiability,"rror from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=namespaces"", GroupVersionKind: ""/v1, Kind=Namespace""; Name: ""batch-pods"", Namespace: """"; Object: &{map[""apiVersion"":""v1"" ""kind"":""Namespace"" ""metadata"":map[""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""name"":""batch-pods"" ""namespace"":""""]]}; from server for: ""deployment.yaml"": namespaces ""batch-pods"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get namespaces in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=serviceaccounts"", GroupVersionKind: ""/v1, Kind=ServiceAccount""; Name: ""batch-svc"", Namespace: ""batch-pods""; Object: &{map[""kind"":""ServiceAccount"" ""metadata"":map[""name"":""batch-svc"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""apiVersion"":""v1""]}; from server for: ""deployment.yaml"": serviceaccounts ""batch-svc"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get serviceaccounts in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""rbac.authorization.k8s.io/v1, Resource=roles"", GroupVersionKind: ""rbac.authorization.k8s.io/v1, Kind=Role""; Name: ""batch-pods-admin"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""rbac.authorization.k8s.io/v1"" ""kind"":""Role"" ""metadata"":map[""name"":""batch-pods-admin"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""rules"":[map[""apiGroups"":[""""] ""resources"":[""pods""] ""verbs"":[""get"" ""list"" ""watch"" ""create"" ""update"" ""patch"" ""delete""]] map[""apiGroups"":[""""] ""resources"":[""pods/log""] ""verbs"":[""get""]]]]}; from server for: ""deployment.yaml"": roles.rbac.authorization.k8s.io ""batch-pods-admin"" is fo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:1013,config,configuration,1013,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,"rt it properly due to <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/2282"">reasons listed in this issue</a>. If you are a user of this module please leave a comment.</li>; <li>Changed <code>HTTPConnection.request_chunked()</code> to not erroneously emit multiple <code>Transfer-Encoding</code> headers in the case that one is already specified.</li>; <li>Fixed typo in deprecation message to recommend <code>Retry.DEFAULT_ALLOWED_METHODS</code>.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.8 (2022-01-07)</h2>; <ul>; <li>Added extra message to <code>urllib3.exceptions.ProxyError</code> when urllib3 detects that; a proxy is configured to use HTTPS but the proxy itself appears to only use HTTP.</li>; <li>Added a mention of the size of the connection pool when discarding a connection due to the pool being full.</li>; <li>Added explicit support for Python 3.11.</li>; <li>Deprecated the <code>Retry.MAX_BACKOFF</code> class property in favor of <code>Retry.DEFAULT_MAX_BACKOFF</code>; to better match the rest of the default parameter names. <code>Retry.MAX_BACKOFF</code> is removed in v2.0.</li>; <li>Changed location of the vendored <code>ssl.match_hostname</code> function from <code>urllib3.packages.ssl_match_hostname</code>; to <code>urllib3.util.ssl_match_hostname</code> to ensure Python 3.10+ compatibility after being repackaged; by downstream distributors.</li>; <li>Fixed absolute imports, all imports are now relative.</li>; </ul>; <h1>1.26.7 (2021-09-22)</h1>; <ul>; <li>Fixed a bug with HTTPS hostname verification involving IP addresses and lack; of SNI. (Issue <a href=""https://github-redirect.dependabot.com/url",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11532:3549,config,configured,3549,https://hail.is,https://github.com/hail-is/hail/pull/11532,1,['config'],['configured']
Modifiability,rtual.TLocus); 	at is.hail.expr.ir.ExtractIntervalFilters$.minimumValueByType(ExtractIntervalFilters.scala:42); 	at is.hail.expr.ir.ExtractIntervalFilters$.openInterval(ExtractIntervalFilters.scala:94); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.Tra,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:3639,Rewrite,RewriteBottomUp,3639,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"ructural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2090,variab,variables,2090,https://hail.is,https://github.com/hail-is/hail/pull/11328,2,['variab'],['variables']
Modifiability,runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:223); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:915); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:583); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:48); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:11); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:11); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:43); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:11864,rewrite,rewrite,11864,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['rewrite'],['rewrite']
Modifiability,runJob(SparkContext.scala:2126); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:989); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:166); 	at is.hail.rvd.RVD.writeRowsSplit(RVD.scala:936); 	at is.hail.expr.ir.MatrixValue.write(MatrixValue.scala:214); 	at is.hail.expr.ir.MatrixNativeWriter.apply(MatrixWriter.scala:39); 	at is.hail.expr.ir.WrappedMatrixWriter.apply(MatrixWriter.scala:24); 	at is.hail.expr.ir.Interpret$.run(Interpret.scala:586); 	at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:84691,rewrite,rewrite,84691,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['rewrite'],['rewrite']
Modifiability,"rver: Fixes handling of CRD schemas containing literal null values in enums. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104969"">kubernetes/kubernetes#104969</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-apiserver: The <code>rbac.authorization.k8s.io/v1alpha1</code> API version is removed; use the <code>rbac.authorization.k8s.io/v1</code> API, available since v1.8. The <code>scheduling.k8s.io/v1alpha1</code> API version is removed; use the <code>scheduling.k8s.io/v1</code> API, available since v1.14. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104248"">kubernetes/kubernetes#104248</a>, <a href=""https://github.com/liggitt""><code>@​liggitt</code></a>)</li>; <li>Kube-scheduler: support for configuration file version <code>v1beta1</code> is removed. Update configuration files to v1beta2(xref: <a href=""https://github-redirect.dependabot.com/kubernetes/enhancements/issues/2901"">kubernetes/enhancements#2901</a>) or v1beta3 before upgrading to 1.23. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104782"">kubernetes/kubernetes#104782</a>, <a href=""https://github.com/kerthcet""><code>@​kerthcet</code></a>)</li>; <li>KubeSchedulerConfiguration provides a new field <code>MultiPoint</code> which will register a plugin for all valid extension points (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105611"">kubernetes/kubernetes#105611</a>, <a href=""https://github.com/damemi""><code>@​damemi</code></a>) [SIG Scheduling and Testing]</li>; <li>Kubelet should reject pods whose OS doesn't match the node's OS label. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/105292"">kubernetes/kubernetes#105292</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>) [SIG Apps and Node]</li>; <li>Kubelet: turn the KubeletConfiguration v1beta1 <code>ResolverConfig</code> field from",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:8546,enhance,enhancements,8546,https://hail.is,https://github.com/hail-is/hail/pull/11957,2,['enhance'],['enhancements']
Modifiability,"ry>; <ul>; <li><a href=""https://github.com/sass/libsass-python/commit/b18db090672676d7c58fcd52e6ae0eb505993886""><code>b18db09</code></a> 0.22.0</li>; <li><a href=""https://github.com/sass/libsass-python/commit/22adb66fac69d058e8dccc0014563cd76e78349e""><code>22adb66</code></a> correct version number</li>; <li><a href=""https://github.com/sass/libsass-python/commit/b2436e282ae19ccb7be9318f3ddd0eb6cdb48be3""><code>b2436e2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/406"">#406</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/980b41f462ae07939515993781e72654b117bdce""><code>980b41f</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/cfffd417e56b7fd3aaf6034fa49083185714f6b7""><code>cfffd41</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/405"">#405</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/20b3cdade8a199e832521d0e44a2507bc75315e0""><code>20b3cda</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/940ef2e9f9dd4143d642a29156c94d0a3133a691""><code>940ef2e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/404"">#404</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/5f8470b48cb576f188ad7424327babb056ce152a""><code>5f8470b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/7c76abf3a14b1606cc6053f5d95d2e0fa231b97f""><code>7c76abf</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/403"">#403</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/40be5dc3e2c3260",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12482:1978,config,config,1978,https://hail.is,https://github.com/hail-is/hail/pull/12482,1,['config'],['config']
Modifiability,"s [msal-extensions](https://github.com/AzureAD/microsoft-authentication-extensions-for-python) from 0.3.1 to 1.0.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/releases"">msal-extensions's releases</a>.</em></p>; <blockquote>; <h2>MSAL Extensions for Python, 1.0.0</h2>; <p>This package is now considered stable and production-ready.</p>; <ul>; <li>New: Add a new platform-independent <code>build_encrypted_persistence()</code> API. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/87"">#87</a>, <a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Remove: Old TokenCache API which has been deprecated for 2 years. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/110"">#110</a>)</li>; <li>Enhancement: Make all platform-dependent parameters optional (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/103"">#103</a>)</li>; <li>Enhancement: Provide <code>PersistenceEncryptError</code> and <code>PersistenceDecryptError</code>, currently raised when encryption on Windows fails. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/108"">#108</a>)</li>; <li>Enhancement: The data file will be created with <code>600</code> permission when running in Unix-like systems. (<a href=""https://github-redirect.dependabot.com/AzureAD/microsoft-authentication-extensions-for-python/issues/107"">#107</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/AzureAD/microsoft-authentication-extensions-for-python/commit/a88fa673af3602fe7c8c922314599b0c245e7add""><code>a88fa67</code></a> Merge branch 'release-1.0.0'</li>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11992:1002,Enhance,Enhancement,1002,https://hail.is,https://github.com/hail-is/hail/pull/11992,1,['Enhance'],['Enhancement']
Modifiability,"s begin at Fri 2019-03-01 19:54:49 UTC, end at Fri 2019-03-01 20:11:51 UTC. --; Mar 01 19:59:03 dk-m systemd[1]: Started Jupyter Notebook.; Mar 01 19:59:04 dk-m python[5149]: [I 19:59:04.630 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.796 NotebookApp] All authentication is disabled. Anyone who can connect to this server will be able to run code.; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.802 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.803 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /usr/local/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /opt/conda/etc/jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [D 19:59:04.804 NotebookApp] Paths used for configuration of jupyter_notebook_config:; Mar 01 19:59:04 dk-m python[5149]: /root/.jupyter/jupyter_notebook_config.json; Mar 01 19:59:04 dk-m python[5149]: [W 19:59:04.904 NotebookApp] Error loading server extension jupyter_spark; Mar 01 19:59:04 dk-m python[5149]: Traceback (most recent call last):; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/notebook/notebookapp.py"", line 1575, in init_server_extensions; Mar 01 19:59:04 dk-m python[5149]: func(self); Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/__init__.py"", line 30, in load_jupyter_server_extension; Mar 01 19:59:04 dk-m python[5149]: from .handlers import SparkHandler; Mar 01 19:59:04 dk-m python[5149]: File ""/opt/conda/lib/python3.6/site-packages/jupyter_spark/handlers.py"", line 8, in <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505:1385,config,configuration,1385,https://hail.is,https://github.com/hail-is/hail/issues/5505,1,['config'],['configuration']
Modifiability,"s specified in this draft proposal:; <a href=""https://github.com/harfbuzz/boring-expansion-spec/blob/main/avar2.md"">https://github.com/harfbuzz/boring-expansion-spec/blob/main/avar2.md</a></li>; <li>[glifLib] Wrap underlying XML library exceptions with GlifLibError when parsing GLIFs, and also print the name and path of the glyph that fails to be parsed (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3029"">fonttools/fonttools#3029</a>).</li>; <li>[feaLib] Consult avar for normalizing user-space values in ConditionSets and in VariableScalars (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3042"">fonttools/fonttools#3042</a>, <a href=""https://redirect.github.com/fonttools/fonttools/pull/3043"">fonttools/fonttools#3043</a>).</li>; <li>[ttProgram] Handle string input to Program.fromAssembly() (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3038"">fonttools/fonttools#3038</a>).</li>; <li>[otlLib] Added a config option to emit GPOS 7 lookups, currently disabled by default because of a macOS bug (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3034"">fonttools/fonttools#3034</a>).</li>; <li>[COLRv1] Added method to automatically compute ClipBoxes (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3027"">fonttools/fonttools#3027</a>).</li>; <li>[ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning None. The regression was introduced in v4.27.0 (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3032"">fonttools/fonttools#3032</a>).</li>; <li>[sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (<a href=""https://redirect.github.com/fonttools/fonttools/pull/3031"">fonttools/fonttools#3031</a>).</li>; <li>[varLib] When building VF, do not overwrite a pre-existing <code>STAT</code> table that was built with feaLib from FEA feature file. Also, added support for building multiple VFs defined in Designspace v5 from <code>fonttools varLib</code> script (<a href=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12910:2344,config,config,2344,https://hail.is,https://github.com/hail-is/hail/pull/12910,1,['config'],['config']
Modifiability,"s work to do, when accepting objects as props, use a `<PureComponent>`. This will tell React to check the reference for diff, rather than deep value compare. Obviously much faster to do the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in effect implementing a shouldComponentUpdate that checks reference equality (prevProp !== currentProp). References: ; 1. https://reactjs.org/docs/react-api.html#reactpurecomponent. #### Memo stateless components; Stateless/functional components (i.e those than don't extend React.Component or React.PureComponent, i.e `(props) => <div>Hello {props.name}</div>`), can be memoized. As in a typical memoized function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:10437,extend,extend,10437,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['extend'],['extend']
Modifiability,"s://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:6784,config,configuration,6784,https://hail.is,https://github.com/hail-is/hail/pull/8561,2,['config'],['configuration']
Modifiability,"s</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/13f39349c6950a881c1fe4fcd5984af2e8b7c220""><code>13f3934</code></a> Remove unnecessary skip from test_logfinish_hook as we require pytest&gt;=6.2</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/c76d5622f17135e892965d742377870eb9b07933""><code>c76d562</code></a> Skip test_warning_captured_deprecated_in_pytest_6 in pytest&gt;=7.1</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/5f78c7155e66ab73bdc7631c4ac6bfe684b82500""><code>5f78c71</code></a> Fix CHANGELOG header</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/c8bbc03e49d5a53b5da808c7328e8f3ad6ed2d7e""><code>c8bbc03</code></a> Release 2.5.0</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/8dbf3677dc7cc26ada33cf8a27d7ac51a9be467b""><code>8dbf367</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/738"">#738</a> from pytest-dev/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/a25c14bef59ad728e39cabc64f71190aaad73b0a""><code>a25c14b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/110c114025202d11570737be823de158d1bb8d99""><code>110c114</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/734"">#734</a> from nicoddemus/revamp-readme</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/83bdbf4b95c914a889d1faa8fba8d506bcc2f8c7""><code>83bdbf4</code></a> Revamp README</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/630c1eb6f2c31dcb4c38c75bb62f868237cdde94""><code>630c1eb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-xdist/issues/733"">#733</a> from baekdohyeop/feature-loadgroup</li>; <li><a href=""https://github.com/pytest-dev/pytest-xdist/commit/62e50d00977b41e17",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11491:4526,config,config,4526,https://hail.is,https://github.com/hail-is/hail/pull/11491,2,['config'],['config']
Modifiability,"s`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager. Its; `response_coroutine` field is a coroutine that includes the retry and; raise-for-status logic. - The `HailResolver` overrides domain name resolution to first consult the Hail; `address` service. `address` is effectively a domain name server. It watches; kubernetes services and publishes the pod IPs. It supports two name styles:; `service` and `service.namespace`. The former uses the deploy config to; determine in which namespace to find the given service. Currently, the; client-side library only looks up IPs for `shuffler` and `address`. - `BlockingClientSession` and `BlockingContextManager` wrap the; aforementioned `httpx` classes. `BlockingClientResponse` wraps an; `aiohttp.ClientResponse`. ---. Examples of correct usage:. A blocking HTTPS request:. ```python3; with httpx.blocking_client_session() as session:; with session.post(url, json=config, headers=headers) as resp:; assert resp.status == 200; print(resp.text()); ```. An asynchronous HTTPS request to auth:; ```python3; async with httpx.client_session() as session:; async with session.get(; deploy_config.url('auth', '/api/v1alpha/userinfo'),; headers=headers) as resp:; assert resp.status == 200; print(await resp.json()); ```. A blocking HTTPS session with a large default timeout:. ```python3; httpx.blocking_client_session(; headers=service_auth_headers(deploy_config, 'query'),; timeout=aiohttp.ClientTimeout(total=600)); ```. cc: @catoverdrive @Dania-Abuhijleh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:2859,config,config,2859,https://hail.is,https://github.com/hail-is/hail/pull/9554,1,['config'],['config']
Modifiability,"sandbox/hail [master|𝚫8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/serializers.py in <module>; 69 xrange = range; 70 ; ---> 71 from pyspark import cloudpickle; 72 from pyspark.util import _exception_message; 73 . ~/sandbox/hail/venv/3.8/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:10437,sandbox,sandbox,10437,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['sandbox'],['sandbox']
Modifiability,scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:45); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:600); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$4(SparkBackend.scala:636); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3(SparkBackend.scala:631); 	at is.hail.backend.spark.SparkBackend.$anonfun$execute$3$adapted(SparkBackend.scala:630); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(E,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:12705,adapt,adapted,12705,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['adapt'],['adapted']
Modifiability,scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:463); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:499); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:8566,adapt,adapted,8566,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['adapt'],['adapted']
Modifiability,scala:20); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:416); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:452); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:646); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:646); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290:5772,adapt,adapted,5772,https://hail.is,https://github.com/hail-is/hail/issues/12290,1,['adapt'],['adapted']
Modifiability,scala:27); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:13); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:21); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:35); 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:168); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14529:9080,adapt,adapted,9080,https://hail.is,https://github.com/hail-is/hail/issues/14529,1,['adapt'],['adapted']
Modifiability,scala:27); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:9391,adapt,adapted,9391,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['adapt'],['adapted']
Modifiability,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:58); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:63); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:462); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:498); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486:4606,adapt,adapted,4606,https://hail.is,https://github.com/hail-is/hail/issues/13486,1,['adapt'],['adapted']
Modifiability,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.execute$1(EvalRelationalLets.scala:13); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.lower$1(EvalRelationalLets.scala:21); 	at is.hail.expr.ir.lowering.EvalRelationalLets$.apply(EvalRelationalLets.scala:35); 	at is.hail.expr.ir.lowering.EvalRelationalLetsPass.transform(LoweringPass.scala:170); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:15842,adapt,adapted,15842,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['adapt'],['adapted']
Modifiability,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:381); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:417); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:22268,adapt,adapted,22268,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:13); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:450); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:486); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.ba,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:9083,adapt,adapted,9083,https://hail.is,https://github.com/hail-is/hail/issues/12531,3,['adapt'],['adapted']
Modifiability,scala:45); at is.hail.types.virtual.Type.ordering(Type.scala:204); at is.hail.rvd.PartitionBoundOrdering$.$anonfun$apply$1(PartitionBoundOrdering.scala:16); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); at scala.collection.TraversableLike.map(TraversableLike.scala:286); at scala.collection.TraversableLike.map$(TraversableLike.scala:279); at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198); at is.hail.rvd.PartitionBoundOrdering$.apply(PartitionBoundOrdering.scala:16); at is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:62); at is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:33); at is.hail.rvd.RVDPartitioner.copy(RVDPartitioner.scala:225); at is.hail.expr.ir.lowering.TableStage.extendKeyPreservesPartitioning(LowerTableIR.scala:534); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2035); at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1051); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1655); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:3373,extend,extendKeyPreservesPartitioning,3373,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['extend'],['extendKeyPreservesPartitioning']
Modifiability,"se <code>#!/bin/sh</code> on windows for hook script.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2182"">#2182</a> issue by <a href=""https://github.com/hushigome-visco""><code>@​hushigome-visco</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2187"">#2187</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; </ul>; <h1>2.16.0 - 2021-11-30</h1>; <h3>Features</h3>; <ul>; <li>add warning for regexes containing <code>[\/]</code> or <code>[/\\]</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2053"">#2053</a> PR by <a href=""https://github.com/radek-sprta""><code>@​radek-sprta</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2043"">#2043</a> issue by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>move hook template back to <code>bash</code> resolving shebang-portability issues.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2065"">#2065</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>add support for <code>fail_fast</code> at the individual hook level.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2097"">#2097</a> PR by <a href=""https://github.com/colens3""><code>@​colens3</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/1143"">#1143</a> issue by <a href=""https://github.com/potiuk""><code>@​potiuk</code></a>.</li>; </ul>; </li>; <li>allow passthrough of <code>GIT_CONFIG_KEY_*</code>, <code>GIT_CONFIG_VALUE_*</code>, and; <code>GIT_CONFIG_COUNT</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2136"">#2136</a> PR by <a href=""https://github.com/emzeat""><code>@​emzeat</code></a>.</li>; </",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11460:11035,portab,portability,11035,https://hail.is,https://github.com/hail-is/hail/pull/11460,2,['portab'],['portability']
Modifiability,"se to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; incoming:; - admin-pod; - router; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. Site accepts incoming requests; from the principals named admin-pod and router. Site is not permitted to make; any outgoing requests. `create_certs.py` will create a new secret named; `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests. If site makes an HTTP request to a server and that server does not return a; certificate in `site-outgoing.pem`, it will immediately halt the connection. I; intend (though do not currently) site to also reject incoming requests that are; not accompanied by a certificate in `site-incoming.pem`. I describe the [trouble; with that later](#incoming-trust). There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod and image-fetcher. Deploy will run `create_certs` on every master deploy. Newly deployed services",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:7089,config,config-http,7089,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['config'],['config-http']
Modifiability,"se); r2_adj = r2_adj.sparsify_triangle(); r2_adj = r2_adj.checkpoint(f'{tmp}/adj', overwrite=args.overwrite). if __name__ == '__main__':; main(); ```. ### Version. 0.2.128. ### Relevant log output. ```shell; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.128-17247d8990c6; LOGGING: writing to /home/edmund/.local/src/hail/hail-20240508-1553-0.2.128-17247d8990c6.log; Traceback (most recent call last):; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/ed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:2123,adapt,adapter,2123,https://hail.is,https://github.com/hail-is/hail/issues/14537,1,['adapt'],['adapter']
Modifiability,"service] make user cache thread-safe; - [ ] (@tpoterba) c315fcb0b1 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) 912c21f709 [shuffler] log ShuffleCodecSpec anytime it is created; - [x] (@daniel-goldstein) c2495837e7 [scala-lsm] bugfix: least key may equal greatest key; - [x] (@daniel-goldstein) 5fb3db703e [services] discovered new transient error; - [x] (@daniel-goldstein) 9cd0999938 [shuffler] more assertions in ShuffleClient; - [x] (@daniel-goldstein) a71a3c9b8c [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [x] (@daniel-goldstein) 41b06aeaa8 [query-service] move hail.jar earlier in Dockerfile; - [x] (@daniel-goldstein) 8df4029698 [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 0354e1f557 [query-service] simplify socket handling; - [x] (@jigold) 6690a4decc [batch] teach JVMJob where to find the hail configuration files; - [x] (@daniel-goldstein) ae2e3d2996 [query-service] switch to services team approved logging; - [ ] (@tpoterba) b18f86e647 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 6d5d0b68af [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) 0d42df8b08 [query-service] run tests against query service; - [x] (@jigold) f9d361e686 [query-service] aiohttp.ClientSession must be created in async code; - [ ] (@cseed) c35f2e10e3 [query-service][hail][build.yaml] address miscellaneous comments from cotton; - [x] (@daniel-goldstein) 7f1b1363e9 [query-service] use two containers sharing an empty volume; - [x] (@daniel-goldstein) 2a8f23404a [query-service] in the service, log everything to stdout; - [x] (trivial) d8104a1dc4 [query-service] do not catch CancelledError; - [x] (trivial) efcb345185 [query-service] slightly more useful error message when socket dies; - [ ] (@tpoterba) f79c4023cf [shuffler] if we have an ExecuteContext, use it; - [x] (@daniel-goldstein,fyi: @tpoterba) 259f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10100:1378,config,configuration,1378,https://hail.is,https://github.com/hail-is/hail/pull/10100,1,['config'],['configuration']
Modifiability,setFunctions$.exportVCF$extension(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$m,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946:11517,Config,Configuration,11517,https://hail.is,https://github.com/hail-is/hail/issues/3946,1,['Config'],['Configuration']
Modifiability,"sf/black/issues/2945"">#2945</a>)</li>; <li>Avoid magic-trailing-comma in single-element subscripts (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2942"">#2942</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not format <code>__pypackages__</code> directories by default (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2836"">#2836</a>)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loading (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2896"">#2896</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>In verbose, mode, log when <em>Black</em> is using user-level config (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2861"">#2861</a>)</li>; </ul>; <h3>Packaging</h3>; <ul>; <li>Fix Black to work with Click 8.1.0 (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2966"">#2966</a>)</li>; <li>On Python 3.11 and newer, use the standard library's <code>tomllib</code> instead of <code>tomli</code>; (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2903"">#2903</a>)</li>; <li><code>black-primer</code>, the deprecated internal devtool, has been removed and copied to a; <a href=""https://github",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:1761,config,config,1761,https://hail.is,https://github.com/hail-is/hail/pull/11696,2,['config'],['config']
Modifiability,"shoudn't be terribly surprising that `rand_unif` has weird behavior, but here's a case that is definitely The Wrong Thing:. ```; Python 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 13:19:00); Type 'copyright', 'credits' or 'license' for more information; IPython 7.6.1 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl. In [2]: r = hl.rand_unif(0, 1). In [3]: hl.eval(r); Out[3]: 0.5387579341676381. In [4]: hl.eval(hl.tuple([r, r])); Out[4]: (0.5387579341676381, 0.5387579341676381); ```. okay, this makes sense becuase they have the same seed:; ```; In [5]: print(hl.tuple([r, r])._ir); (MakeTuple (0 1) (ApplySeeded rand_unif 806694938962853089 Float64 (Apply toFloat64 Float64 (I32 0)) (Apply toFloat64 Float64 (I32 1))) (ApplySeeded rand_unif 806694938962853089 Float64 (Apply toFloat64 Float64 (I32 0)) (Apply toFloat64 Float64 (I32 1)))); ```. how about this:; ```; In [6]: hl.eval(hl.range(2).map(lambda x: r)); Out[6]: [0.5387579341676381, 0.9394799645512691]; ```. odd. but maybe rand_unif inside an iteration has some semantics for advancing the RNG (like an aggregation). ```; In [7]: p = 1 - r. In [8]: hl.eval(hl.range(2).map(lambda x: p)); Out[8]: [0.46124206583236194, 0.06052003544873086]; ```; ok... ```; In [9]: hl.eval((p, hl.range(2).map(lambda x: p))); Out[9]: (0.46124206583236194, [0.46124206583236194, 0.46124206583236194]); ```; wtf? . if you look in the logs, its explained by the fact that only the final IR triggers CSE:; ```; (Let __cse_1; (ApplyBinaryPrimOp Subtract; (ApplyIR toFloat64 Float64; (I32 1)); (ApplySeeded rand_unif 806694938962853089 Float64; (ApplyIR toFloat64 Float64; (I32 0)); (ApplyIR toFloat64 Float64; (I32 1)))); (MakeTuple (0 1); (Ref __cse_1); (ArrayMap __uid_5; (ArrayRange; (I32 0); (I32 2); (I32 1)); (Ref __cse_1)))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7572:287,enhance,enhanced,287,https://hail.is,https://github.com/hail-is/hail/issues/7572,1,['enhance'],['enhanced']
Modifiability,"singleton subscriptions; e.g., <code>nptyping.Float[64]</code></li>; <li>Resolve forward references</li>; <li>Expand and better handle <code>TypeVar</code></li>; <li>Add intershpinx reference link for <code>...</code> to <code>Ellipsis</code> (as is just an alias)</li>; </ul>; <h2>1.15.3</h2>; <ul>; <li>Prevents reaching inner blocks that contains <code>if TYPE_CHECKING</code></li>; </ul>; <h2>1.15.2</h2>; <ul>; <li>Log a warning instead of crashing when a type guard import fails to resolve</li>; <li>When resolving type guard imports if the target module does not have source code (such is the case for C-extension modules) do nothing instead of crashing</li>; </ul>; <h2>1.15.1</h2>; <ul>; <li>Fix <code>fully_qualified</code> should be <code>typehints_fully_qualified</code></li>; </ul>; <h2>1.15.0</h2>; <ul>; <li>Resolve type guard imports before evaluating annotations for objects</li>; <li>Remove <code>set_type_checking_flag</code> flag as this is now done by default</li>; <li>Fix crash when the <code>inspect</code> module returns an invalid python syntax source</li>; <li>Made formatting function configurable using the option <code>typehints_formatter</code></li>; </ul>; <h2>1.14.1</h2>; <ul>; <li>Fixed <code>normalize_source_lines()</code> messing with the indentation of methods with decorators that have parameters starting; with <code>def</code>.</li>; <li>Handle <code>ValueError</code> or <code>TypeError</code> being raised when signature of an object cannot be determined</li>; <li>Fix <code>KeyError</code> being thrown when argument is not documented (e.g. <code>cls</code> argument for class methods, and <code>self</code> for; methods)</li>; </ul>; <h2>1.14.0</h2>; <ul>; <li>Added <code>typehints_defaults</code> config option allowing to automatically annotate parameter defaults.</li>; </ul>; <h2>1.13.1</h2>; <ul>; <li>Fixed <code>NewType</code> inserts a reference as first argument instead of a string</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11503:1835,config,configurable,1835,https://hail.is,https://github.com/hail-is/hail/pull/11503,2,['config'],['configurable']
Modifiability,"sion at 0x104ab3850>. In [3]: aiohttp.ClientSession(); <ipython-input-1-028690903e5f>:7: DeprecationWarning: The object should be created within an async function; oldinit(self, *args, **kwargs); Out[3]: <aiohttp.client.ClientSession at 0x104dac8b0>. In [4]: aiohttp.ClientSession(); <ipython-input-1-028690903e5f>:7: DeprecationWarning: The object should be created within an async function; oldinit(self, *args, **kwargs); Out[4]: <aiohttp.client.ClientSession at 0x104daeec0>. In [5]:. Do you really want to exit ([y]/n)? y; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x104ab3850>; source_traceback: Object created at (most recent call last):; File ""/Users/dking/miniconda3/bin/ipython"", line 8, in <module>; sys.exit(start_ipython()); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/__init__.py"", line 128, in start_ipython; return launch_new_instance(argv=argv, **kwargs); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/traitlets/config/application.py"", line 1043, in launch_instance; app.start(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/ipapp.py"", line 318, in start; self.shell.mainloop(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 888, in mainloop; self.interact(); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py"", line 881, in interact; self.run_cell(code, store_history=True); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3009, in run_cell; result = self._run_cell(; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3064, in _run_cell; result = runner(coro); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner; coro.send(None); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/IPython/cor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13421:2092,config,config,2092,https://hail.is,https://github.com/hail-is/hail/pull/13421,1,['config'],['config']
Modifiability,"so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tells `clang` to not error if an included file does not exist. This can be helpful if the project's `Makefile` knows how to generate these header files. In our example, `catch.hpp` can be generated by `hail/src/main/c/Makefile` by downloading it from GitHub.; ```; # rm -rf bar.h; # clang -MM foo.cpp ; foo.cpp:1:10: fatal error: 'bar.h' file not found; #include ""bar.h""; ^~~~~~~; 1 error generated.; # clang -MM foo.cpp -MG; foo.o: foo.cpp bar.h; ```. The `-MF file` argument tells `clang` to write to `file` instead of stdout. The `-MMD` argument is used if we *also want to compile the file*. The `-MM` argument defaults to adding `-E` meaning ""only run the preprocessor"".",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5331:1800,variab,variable,1800,https://hail.is,https://github.com/hail-is/hail/pull/5331,1,['variab'],['variable']
Modifiability,"sphinx/issues/9894"">#9894</a>: linkcheck: add option <code>linkcheck_exclude_documents</code> to disable link; checking in matched documents.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9793"">#9793</a>: sphinx-build: Allow to use the parallel build feature in macOS on macOS; and Python3.8+</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10055"">#10055</a>: sphinx-build: Create directories when <code>-w</code> option given</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9993"">#9993</a>: std domain: Allow to refer an inline target (ex. ``_<code>target name```) via :rst:role:</code>ref` role</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9981"">#9981</a>: std domain: Strip value part of the option directive from general index</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9391"">#9391</a>: texinfo: improve variable in <code>samp</code> role</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9578"">#9578</a>: texinfo: Add :confval:<code>texinfo_cross_references</code> to disable cross; references for readability with standalone readers</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9822"">#9822</a> (and <a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9062"">#9062</a>), add new Intersphinx role :rst:role:<code>external</code> for explict; lookup in the external projects, without resolving to the local project.</li>; </ul>; <h2>Bugs fixed</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9866"">#9866</a>: autodoc: doccomment for the imported class was ignored</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sphinx-doc/sphinx/commit/88f9647a223c77a29153683b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11522:4380,variab,variable,4380,https://hail.is,https://github.com/hail-is/hail/pull/11522,2,['variab'],['variable']
Modifiability,"splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) trunc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2573,variab,variables,2573,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['variab'],['variables']
Modifiability,sql migrations using the wrong config file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7706:31,config,config,31,https://hail.is,https://github.com/hail-is/hail/pull/7706,1,['config'],['config']
Modifiability,"ssembly' not specified. Setting to GRCh37; [Stage 22:======================================================>(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3099:2035,config,config,2035,https://hail.is,https://github.com/hail-is/hail/issues/3099,1,['config'],['config']
Modifiability,"st_write_data.py is not deterministic; - if I add/change one dataset, my commit explodes with changes to all the datasets (see above); - doctest_write_data.py has to be run by *me*, it's not run by CI. I also noticed that when you specify no `row_key` to `import_matrix_table` you get a row key called `row_id`, which is annoying. Anyway now when someone asks how to count the mutations in each gene by consequence type we can point them to the `counter` docs. ---. Adding a dataset caused a bunch of docs failure that lead me to change how we do doctesting. The changes are summarized below.; - ignore `python/.eggs`; - make `PARALLELISM` configurable in `Makefile`; - fix `make pytest` (it referenced a non-extant target); - add `make doctest` (this and `pytest` use setup.py to replicate the environment the user would have after installation, I prefer this approach because I need not manually install any dependencies, setup.py handles that, it also configures spark correctly without environment variables); - harmonize `doctest` and `pytest` parameters in `build.gradle` and `Makefile`; - clean up import order in `conftest.py` to match pylint's desired ordering; - use a `temple.TemporaryDirectory` for all doctest and test output, which is automatically cleaned up (if you want to interrogate it you can `ctrl-z` a running doctest); this allows us to not copy the entire python directory into a build directory before running pytest; - *important:* re-generate all input datasets on every run of the tests. Previously, there was a file `doctest_write_data.py` which you were supposed to run when you changed the datasets, but if Hail changes then the random datasets generated by `doctest_write_data.py` might change. This means when I came along to add a new dataset, I had to address all the test failures introduced since the last time `doctest_write_data.py`'s results were checked in. (the doctests still only take about 2 minutes); - I fixed several latent doc bugs caused by the afore",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6856:1171,config,configures,1171,https://hail.is,https://github.com/hail-is/hail/pull/6856,2,"['config', 'variab']","['configures', 'variables']"
Modifiability,stics: [2023-07-24 13:52:49.515]Container killed on request. Exit code is 137; [2023-07-24 13:52:49.517]Container exited with a non-zero exit code 137. ; [2023-07-24 13:52:49.518]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287:7152,adapt,adapted,7152,https://hail.is,https://github.com/hail-is/hail/issues/13287,1,['adapt'],['adapted']
Modifiability,store dataset configuration in a sensible way (not in global annotation),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/368:14,config,configuration,14,https://hail.is,https://github.com/hail-is/hail/issues/368,1,['config'],['configuration']
Modifiability,"sts/releases"">requests's releases</a>.</em></p>; <blockquote>; <h2>v2.27.1</h2>; <h2>2.27.1 (2022-01-05)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed parsing issue that resulted in the <code>auth</code> component being; dropped from proxy URLs. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a>)</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/psf/requests/blob/v2.27.1/HISTORY.md#2271-2022-01-05"">https://github.com/psf/requests/blob/v2.27.1/HISTORY.md#2271-2022-01-05</a></p>; <h2>v2.27.0</h2>; <h2>2.27.0 (2022-01-03)</h2>; <p><strong>Improvements</strong></p>; <ul>; <li>; <p>Officially added support for Python 3.10. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5928"">#5928</a>)</p>; </li>; <li>; <p>Added a <code>requests.exceptions.JSONDecodeError</code> to unify JSON exceptions between; Python 2 and 3. This gets raised in the <code>response.json()</code> method, and is; backwards compatible as it inherits from previously thrown exceptions.; Can be caught from <code>requests.exceptions.RequestException</code> as well. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5856"">#5856</a>)</p>; </li>; <li>; <p>Improved error text for misnamed <code>InvalidSchema</code> and <code>MissingSchema</code>; exceptions. This is a temporary fix until exceptions can be renamed; (Schema-&gt;Scheme). (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6017"">#6017</a>)</p>; </li>; <li>; <p>Improved proxy parsing for proxy URLs missing a scheme. This will address; recent changes to <code>urlparse</code> in Python 3.9+. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5917"">#5917</a>)</p>; </li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>; <p>Fixed defect in <code>extract_zipped_paths</code> which could result in an infinite loop; for some paths. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:1183,inherit,inherits,1183,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['inherit'],['inherits']
Modifiability,"such as chr13, it worked well in Terra. Since we will convert multiple plink files to hailmatrix table using Terra platform in future, I need to figure the problem out. Any advise would be appreciated. ### Version. 0.2.127. ### Relevant log output. ```shell; 2024/01/17 20:20:25 Starting container setup.; 2024/01/17 20:20:26 Done container setup.; 2024/01/17 20:20:27 Starting localization.; 2024/01/17 20:20:34 Localization script execution started...; 2024/01/17 20:20:34 Localizing input gs://fc-5a8938eb-1299-4afc-957f-afb53ef602b9/submissions/e8747e74-47d1-4f52-acfc-1ac7f81d79ba/VUMCBed2HailMatrix/683447d9-9342-4058-bcfc-ba21422d3121/call-Bed2HailMatrix/script -> /cromwell_root/script; 2024/01/17 20:20:36 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bed -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bed; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.fam -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.fam; 2024/01/17 20:59:18 Localizing input gs://hui-sandbox/ICA-AGD/plink1/chr12.bim -> /cromwell_root/hui-sandbox/ICA-AGD/plink1/chr12.bim; Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.fam...; / [0 files][ 0.0 B/910.3 KiB] / [1 files][910.3 KiB/910.3 KiB] Copying gs://hui-sandbox/ICA-AGD/plink1/chr12.bim...; / [1 files][910.3 KiB/369.7 MiB] - - [1 files][ 51.9 MiB/369.7 MiB] \ | | [1 files][107.6 MiB/369.7 MiB] / - - [1 files][162.3 MiB/369.7 MiB] \ \ [1 files][213.9 MiB/369.7 MiB] | / / [1 files][286.6 MiB/369.7 MiB] - \ \ [1 files][342.1 MiB/369.7 MiB] |; Operation completed over 2 objects/369.7 MiB.; | [2 files][369.7 MiB/369.7 MiB] 2024/01/17 20:59:27 Localization script execution complete.; 2024/01/17 20:59:38 Done localization.; 2024/01/17 20:59:39 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash hailgenetics/hail@sha256:3f22576793ce3161893aed2bd40949b1fc822d2b7e6517dc0ac993b62badaff8 /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.81879b1c; P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168:9918,sandbox,sandbox,9918,https://hail.is,https://github.com/hail-is/hail/issues/14168,1,['sandbox'],['sandbox']
Modifiability,support dbnsfp plugin in VEP,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/167:15,plugin,plugin,15,https://hail.is,https://github.com/hail-is/hail/issues/167,1,['plugin'],['plugin']
Modifiability,"t <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/343"">#343</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/7f01591fdbca66375a61d70e505a286550a1c1b1""><code>7f01591</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/814d42df9787494f01474116940782ab67da083f""><code>814d42d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/342"">#342</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/b08f9ca307ce64867070a3ca5ee5f1a6c5742069""><code>b08f9ca</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/89d6a1dda507abde79ff79b3fd95b9d013eaa02d""><code>89d6a1d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/340"">#340</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/93c70a9a9f350b24af796bb31d81182be4ac4b1f""><code>93c70a9</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/e836a7e7c3778ac34a8bd117c2ce701209097cd5""><code>e836a7e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/339"">#339</a> from sass/pre-commit-ci-update-config</li>; <li>Additional commits viewable in <a href=""https://github.com/sass/libsass-python/compare/0.19.2...0.21.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=libsass&package-manager=pip&previous-version=0.19.2&new-version=0.21.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11508:5095,config,config,5095,https://hail.is,https://github.com/hail-is/hail/pull/11508,1,['config'],['config']
Modifiability,"t <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/405"">#405</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/20b3cdade8a199e832521d0e44a2507bc75315e0""><code>20b3cda</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/940ef2e9f9dd4143d642a29156c94d0a3133a691""><code>940ef2e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/404"">#404</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/5f8470b48cb576f188ad7424327babb056ce152a""><code>5f8470b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/7c76abf3a14b1606cc6053f5d95d2e0fa231b97f""><code>7c76abf</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/403"">#403</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/40be5dc3e2c326006fc7f4f1ffcc3cada877c701""><code>40be5dc</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li>Additional commits viewable in <a href=""https://github.com/sass/libsass-python/compare/0.21.0...0.22.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=libsass&package-manager=pip&previous-version=0.21.0&new-version=0.22.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12482:2874,config,config,2874,https://hail.is,https://github.com/hail-is/hail/pull/12482,1,['config'],['config']
Modifiability,"t <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/406"">#406</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/980b41f462ae07939515993781e72654b117bdce""><code>980b41f</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/cfffd417e56b7fd3aaf6034fa49083185714f6b7""><code>cfffd41</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/405"">#405</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/20b3cdade8a199e832521d0e44a2507bc75315e0""><code>20b3cda</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/940ef2e9f9dd4143d642a29156c94d0a3133a691""><code>940ef2e</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/404"">#404</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/5f8470b48cb576f188ad7424327babb056ce152a""><code>5f8470b</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li><a href=""https://github.com/sass/libsass-python/commit/7c76abf3a14b1606cc6053f5d95d2e0fa231b97f""><code>7c76abf</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sass/libsass-python/issues/403"">#403</a> from sass/pre-commit-ci-update-config</li>; <li><a href=""https://github.com/sass/libsass-python/commit/40be5dc3e2c326006fc7f4f1ffcc3cada877c701""><code>40be5dc</code></a> [pre-commit.ci] pre-commit autoupdate</li>; <li>Additional commits viewable in <a href=""https://github.com/sass/libsass-python/compare/0.21.0...0.22.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=libsass&package-manager=pip&previous-version=0.21.0&new-version=0.22.0)](htt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12482:2426,config,config,2426,https://hail.is,https://github.com/hail-is/hail/pull/12482,1,['config'],['config']
Modifiability,"t array *must* be the JAR URL and the SHA-1 is confusing. Instead, there are now two keys in a JVM process specification:; 1. `jar_spec`, which may be either `{""type"": ""jar_url"", ""value"": ""gs://..../abc123....jar""}` or `{""type"":""git_revision"", ""value"": ""abc123...""}`.; 2. `argv`, an opaque list of strings which are passed, by the JVMEntryway, along with a few more args, to `is.hail.backend.service.Main`. The `Main` class dispatches to either `ServiceBackendSocketAPI2` or the `Worker` based on the first element of `argv`. Each class expects different contents in `argv` that suits its needs. Second, I completely eliminated the HAIL_SHA/revision from the Worker and Hail Query Java code. This was only ever used as unique name for the JAR. Instead, I just use the full JAR URL as a unique name for the JAR. If you need to defeat the cache, just create a new git commit before running `make -C query ipython`. If defeating the cache becomes a common problem, we can add a ""reload_jar"" parameter or similar to the job spec. Third, I renamed `push-jar` in `query/Makefile` to `upload-query-jar` to mirror the build.yaml step. Fourth, I embraced the use of `NAMEPSACE` in `query/Makefile` instead of relying on the minor hack that our laptop usernames match our namespace names. This does mean you need to always specify NAMESPACE when uploading a jar. Finally, a pleasant outcome of this change is the elimination of a bunch of conditional build.yaml logic in the service backend tests!. I think this will simplify the use of Hail Query by Australia et al. because I've isolated the use of hail-specific data to `query/Makefile`. If there's a way to access the relevant global-config variables from `query/Makefile`, I can also fix the `query/Makefile` to be deployment-independent. cc: @lgruen @illusional @tpoterba . [1] For our default namespace deployment, `gs://hail-query/jars/{GIT_REVISION}.jar`. In general, `{HAIL_QUERY_STORAGE_URI}{HAIL_QUERY_ACCEPTABLE_JAR_SUBFOLDER}/{GIT_REVISION}.jar`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645:3113,config,config,3113,https://hail.is,https://github.com/hail-is/hail/pull/11645,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"t clear from where this came.; - Removed use of the `subtitle` tag, which isn't actually an HTML tag?. Future work:. - Simplify our CSS. It's not possible to logically reason about our CSS. And it; interacts in bad ways with the latent RTD themes. I want a unified Hail visual; theme.; - Clean up the search-related JavaScript in nav-bottom.html and; search.html. These both seem too complicated to just make search work. ---. The thrust of this PR is to restructure Hail's website and documentation to; entirely rely on Jinja2 templates. Previously, we used a mix of Jinja2, XSLT,; and in-browser JavaScript DOM manipulation to piece together a web page. Now, all of Hail's non-service website derives from; `site/templates/base.html`. It is a Jinja2 template with four blocks: title,; meta_description, head, and content. It ensures that:; - Hail's CSS is loaded,; - the Hail icon is set,; - the fonts are loaded,; - the source code highlighter is loaded (prism.js, only used outside the docs); - the nav bar is present and configured.; The nav bar is somewhat complicated. There are two pieces. `nav-top.html`; contains the nav bar HTML elements. `nav-bottom.html` contains the JavaScript; code that hooks the search bar up to Algolia and sets the active page in the; navigation. I believe JavaScript which modifies the HTML DOM is traditionally; placed at the bottom of the `body` tag so that it is executed *after* the HTML; DOM is mostly rendered. That's why the navigation/search bar is split across two; files. I also load the `prism.js` source code highlighter at the end of the; body. All of the non-docs pages are defined by html files in `pages`. Each one of; these is a Jinja2 template which derives from the base template. `make render`; converts every template in `pages` into a real HTML file in `www`. Check out; 404.html for a simple example. Once I had the site in working order, I turned my eyes to the docs. I converted; `docs/_templates/layout.html`, the base template for our do",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9597:1951,config,configured,1951,https://hail.is,https://github.com/hail-is/hail/pull/9597,1,['config'],['configured']
Modifiability,t is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:33); at is.hail.rvd.RVDPartitioner.copy(RVDPartitioner.scala:225); at is.hail.expr.ir.lowering.TableStage.extendKeyPreservesPartitioning(LowerTableIR.scala:534); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:2035); at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:1051); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1655); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:728); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:1021); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:27); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:11); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:91); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); at scala.collection.mutable.ResizableArray.foreach(ResizableA,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:4190,rewrite,rewrite,4190,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['rewrite'],['rewrite']
Modifiability,t java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.exe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:4436,adapt,adapted,4436,https://hail.is,https://github.com/hail-is/hail/issues/12532,2,['adapt'],['adapted']
Modifiability,t org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844); 	at org.apache.spark.u,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:8442,adapt,adapted,8442,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['adapt'],['adapted']
Modifiability,t$.alreadyLowered(Interpret.scala:56); 	at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:53); 	at is.hail.expr.ir.InterpretNonCompilable$.$anonfun$apply$1(InterpretNonCompilable.scala:25); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.InterpretNonCompilable$.rewriteChildren$1(InterpretNonCompilable.scala:25); 	at is.hail.expr.ir.InterpretNonCompilable$.rewrite$1(InterpretNonCompilable.scala:54); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:67); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:12); 	at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:62); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:10984,rewrite,rewrite,10984,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['rewrite'],['rewrite']
Modifiability,"t, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 317 raise Py4JJavaError( 318 ""An error occurred while calling {0}{1}{2}.\n"". --> 319 format(target_id, ""."", name), value) 320 else: 321 raise Py4JError( Py4JJavaError: An error occurred while calling o68.apply. : org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.<init>(SparkContext.scala:76) is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:84) is.hail.HailContext$.apply(HailContext.scala:164) sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) java.lang.reflect.Method.invoke(Method.java:498) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:280) py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) py4j.commands.CallCommand.execute(CallCommand.java:79) py4j.GatewayConnection.run(GatewayConnection.java:214) java.lang.Thread.run(Thread.java:745) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2278) at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2274) at scala.Option.foreach(Option.scala:257) at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:5066,config,configureAndCreateSparkContext,5066,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['config'],['configureAndCreateSparkContext']
Modifiability,"t. All other batch users cannot use batch callbacks to; trick batch into issuing HTTP(S) requests to random services in our system; (because those services will reject a request from an untrusted principal). Note that the internal-gateway still only accepts HTTP for incoming; requests. Outgoing requests to the router are HTTPS. ---. New concepts:; - a type of secret `ssl-config-{principal}`; - a global configuration file: `tls/config.yaml`. The new secret must have:; - `{principal}-key.pem`: the principal's private key (identity).; - `{principal}-cert.pem`: the principal's certificate (proof of identity).; - `{principal}-trust.pem`: a list of ""trusted"" certificates.; and additionally must have one of:; - `ssl-config.json`: a Hail configuration file in json format.; - `ssl-config-http.conf` and `ssl-config-proxy.conf`: NGINX configuration files; that configure server-side TLS in the `http` block and client-side; (i.e. proxy-side) TLS in a location block.; - `ssl-config.curlrc`: a curl configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:3515,config,configuration,3515,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"t.com/lepture/mistune/pull/295"">lepture/mistune#295</a></p>; <h2>Version 2.0.1</h2>; <p>Fix XSS for image link syntax.</p>; <h2>Version 2.0.0</h2>; <p>First release of Mistune v2.</p>; <h2>Version 2.0.0 RC1</h2>; <p>In this release, we have a <strong>Security Fix</strong> for harmful links.</p>; <h2>Version 2.0.0 Alpha 1</h2>; <p>This is the first release of v2. An alpha version for users to have a preview of the new mistune.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/mistune/blob/master/docs/changes.rst"">mistune's changelog</a>.</em></p>; <blockquote>; <h2>Changelog</h2>; <p>Here is the full history of mistune v2.</p>; <p>Version 2.0.4</p>; <pre><code>; Released on Jul 15, 2022; <ul>; <li>Fix <code>url</code> plugin in <code>&amp;lt;a&amp;gt;</code> tag</li>; <li>Fix <code>*</code> formatting</li>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/3f422f1e84edae0f39756c45be453ecde534b755""><code>3f422f1</code></a> Version bump 2.0.3</li>; <li><a href=""https://github.com/lepture/mistune/commit/a6d43215132fe4f3d93f8d7e90ba83b16a0838b2""><code>a6d4321</code></a> Fix asteris emphasis regex CVE-2022-34749</li>; <li><a href=""https://github.com/lepture/m",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12064:1350,plugin,plugin,1350,https://hail.is,https://github.com/hail-is/hail/pull/12064,1,['plugin'],['plugin']
Modifiability,"t.com/lepture/mistune/pull/295"">lepture/mistune#295</a></p>; <h2>Version 2.0.1</h2>; <p>Fix XSS for image link syntax.</p>; <h2>Version 2.0.0</h2>; <p>First release of Mistune v2.</p>; <h2>Version 2.0.0 RC1</h2>; <p>In this release, we have a <strong>Security Fix</strong> for harmful links.</p>; <h2>Version 2.0.0 Alpha 1</h2>; <p>This is the first release of v2. An alpha version for users to have a preview of the new mistune.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/lepture/mistune/blob/master/docs/changes.rst"">mistune's changelog</a>.</em></p>; <blockquote>; <h2>Changelog</h2>; <p>Here is the full history of mistune v2.</p>; <p>Version 2.0.4</p>; <pre><code>; Released on Jul 15, 2022; <ul>; <li>Fix <code>url</code> plugin in <code>&amp;lt;a&amp;gt;</code> tag</li>; <li>Fix <code>*</code> formatting</li>; </ul>; <p>Version 2.0.3; </code></pre></p>; <p>Released on Jun 27, 2022</p>; <ul>; <li>Fix <code>table</code> plugin</li>; <li>Security fix for CVE-2022-34749</li>; </ul>; <p>Version 2.0.2</p>; <pre><code>; Released on Jan 14, 2022; <p>Fix <code>escape_url</code></p>; <p>Version 2.0.1; </code></pre></p>; <p>Released on Dec 30, 2021</p>; <p>XSS fix for image link syntax.</p>; <p>Version 2.0.0</p>; <pre><code>; Released on Dec 5, 2021; <p>This is the first non-alpha release of mistune v2.</p>; <p>Version 2.0.0rc1; </code></pre></p>; <p>Released on Feb 16, 2021</p>; <p>Version 2.0.0a6</p>; <pre><code>; &lt;/tr&gt;&lt;/table&gt; ; </code></pre>; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/lepture/mistune/commit/b92a5febd4da3d7097a3d2b8d7cac6f5d57ea20c""><code>b92a5fe</code></a> Version bump 2.0.4</li>; <li><a href=""https://github.com/lepture/mistune/commit/98a1c0afc51d4be719cb17401a35e62f46206915""><code>98a1c0a</code></a> Fix url plugin render, <a href=""https://github-redirect.dependabot.com/lepture/mistune/is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12066:1350,plugin,plugin,1350,https://hail.is,https://github.com/hail-is/hail/pull/12066,2,['plugin'],['plugin']
Modifiability,"t.html; 4. Structure, aforementioned; 5. Path to relatively performant desktop and mobile applications, via [Electron](https://getstream.io/blog/takeaways-on-building-a-react-based-app-with-electron/). [Visual Studio Code](https://github.com/Microsoft/vscode) and [Slack](https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview-2759690d9c7b) are good examples. Facebook Messenger written in React Native, which we have an even more straightforward path to.; 6. Low cognitive cost (relative to Angular, others. React is just a view layer, and has a tiny API. I've develop a large application in AngularJS, and have spent a bit of time with Angular2+. There is no comparison: Angular takes months to know well, React days at worst. Also, by not buying into a full framework, we achieve modularity: If we end up finding React too slow, even with [planned 2019 improvements](https://reactjs.org/blog/2018/11/27/react-16-roadmap.html), there are plenty of others view layers we can migrate to, without gutting our entire app. Next makes this somewhat trickier, but since it's mostly a light wrapper around Webpack, there is essentially no Next-specific code (just a bit in pages/_app.js and pages/_document.js). Migrating from Next won't be an issue, we would just lose some of the tooling benefits. 8. Typescript: static typing, syntax more familiar to OO-language devs (interfaces, types, classes). 9. Really amazing debug, tooling. React debug tools natively included in Chrome for instance. First class support for JSX (React-wrapped HTML) in popular IDE's, most obviously Visual Studio Code. ## Tech TL;DR; Mainly React. React should take about a day to learn well enough to make contributions. Guide: https://reactjs.org/docs/introducing-jsx.html. ```jsx; # Renders Hello World; # Biggest annoyance (may go away in 2019) is that ""class"" is not a valid tag (reserved by React); export default function SomePage() {; const name = 'Alex'. # Renders ""Hello Alex""; return (; <",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:4916,layers,layers,4916,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['layers'],['layers']
Modifiability,tIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:4380,Rewrite,RewriteBottomUp,4380,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"t_errors; return await retry_transient_errors_with_debug_string('', 0, f, *args, **kwargs); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hailtop/utils/utils.py"", line 819, in retry_transient_errors_with_debug_string; return await f(*args, **kwargs); File ""/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/hail/backend/service_backend.py"", line 517, in _read_output; raise reconstructed_error.maybe_user_error(ir); hail.utils.java.FatalError: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht. Java stack trace:; is.hail.utils.HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; at __C5681Compiled.__m5684begin_group_0(Emit.scala); at __C5681Compiled.__m5683begin_group_0(Emit.scala); at __C5681Compiled.apply(Emit.scala); at is.hail.backend.service.ServiceBackend.$anonfun$execute$1(ServiceBackend.scala:305); at is.hail.backend.service.ServiceBackend.$anonfun$execute$1$adapted(ServiceBackend.scala:305); at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:140); at is.hail.utils.package$.using(package.scala:657); at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:140); at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:305); at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:333); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$15(ServiceBackend.scala:718); at is.hail.backend.service.ServiceBackendSocketAPI2.withIRFunctionsReadFromInput(ServiceBackend.scala:812); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$14(ServiceBackend.scala:716); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$5(ServiceBackend.scala:673); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); at is.hail.utils.package$.using(package.scala:657); at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:4232,adapt,adapted,4232,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['adapt'],['adapted']
Modifiability,"te `0000 1111` is converted to the `Int` 15 and the byte; `1000 1111` is converted to the `Int` -113. The contract of; [`InputStream.read`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html#read--); is to return the unsigned integeral value of the next `Byte` or `-1` if we've reached the end of; the stream. `DataInputStream` treats any negative value as EOS which lead to perplexing EOSes; when reading data from GCS. 5. Retain the `gs://` protocol when reading MTs and Ts. `uriPath` strips *all* protocols. Before the; Query Service, these code paths were only used by the LocalBackend. In the LocalBackend, the only; URIs generated are `file://`. However, UNIX/JVM file system operations do not support URIs, they; want bare paths. 6. Implement missing cases for EShuffle and PShuffle. 7. BLAS and LAPACK need to be thread-local. 8. The LSM-tree used by the Shuffler needs to permit multiple values for the same key. There was; some subtlety here around fine-grained locking. Unfortunately, the LSM doesn't expose the right; operations (putIfAbsent) to make this easy, so I had to use a concurrent hash map of locks. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key. - Create a test query-gsa-key in test and dev namespaces. - Add terraform rules for the query service account. It already existed, but it was missing from the; Terraform file. You can verify the permissions grant by inspecting `gsutil iam get; gs://hail-query`. - The `query` user was missing from bootstrap-create-accounts. - Remove the auto-scaling and remove the k8s grace period. Neither of these really works right. We; will live without the auto-scaling for now. I have to fix the grace period thing before we let; users run real pipelines.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390:3723,config,config,3723,https://hail.is,https://github.com/hail-is/hail/pull/10390,1,['config'],['config']
Modifiability,"te(); r.getNewRegion()`. As a demonstration of how this should work, this PR does convert `EmitStream.{toArray, write}` to use `eltRegion` correctly, which are used in the consumer nodes `ToArray`, `ArraySort`, `ToSet`, `ToDict`, and `GroupByKey`. Going forward, the plan is to convert the rest of the consumer and producer nodes, before converting the transformer nodes (which both consume and produce streams), as correctness can only be tested on pipelines in which all nodes have been converted. ### Semantics. A stream producer is passed two regions from its consumer: `outerRegion` and `eltRegion`. The node being emitted does not own either region, so may not free/clear them. The only thing a producer may do with either region is to put data in them, by writing directly to them or by giving/sharing ownership of a producer-owned region to them. Consumers' contract:; * The lifetime of `outerRegion` must contain the lifetime of the producer stream, i.e. `outerRegion` must be valid before the producer's `setup0` is called, and must still be valid when the producer's `close0` is called. Thus a producer may use `outerRegion` to store state that persists between elements, or it may use a region it owns itself.; * When the producer's `pull` is called, `eltRegion` must be valid (it does not need to be valid for setup/close). Producers' contract:; * When the consumer's `push` is called, the pushed value must be owned by either `outerRegion` or `eltRegion` (really `eltRegion`; using `outerRegion` is correct but unnecessarily extends the lifetime of the value). This can be accomplished by writing directly to the consumer's region, or by giving/sharing ownership of a producer-owned region to it. Thus the consumer may assume the pushed value is valid until it frees/clears `eltRegion`. If the consumer needs the value to live longer, it must deep-copy the value to another region. These allow us to reason about (and, in principle, to prove) the correctness of each node independently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106:2517,extend,extends,2517,https://hail.is,https://github.com/hail-is/hail/pull/9106,1,['extend'],['extends']
Modifiability,"ted dependencies (we have a lot!). See [here](https://mill-build.com/mill/Intro_to_Mill.html) and [here](https://mill-build.com/mill/Builtin_Commands.html) for more details. ## IntelliJ setup. To import from scratch:; * delete any `.idea` directories in the hail root, or `hail/` subdirectory (you could try skipping this, but you're on your own); * in the `hail/` subdirectory, you can also delete `.classpath`, `.gradle`, `.project`, `.settings/`, `build/` (we still use this for a few things, but most of it is dead gradle output, and it's safe to delete it all and start clean), `build.gradle`, `gradle/`, `gradlew`, `gradlew.bat`, `pgradle`, `settings.gradle`; * run `mill mill.bsp.BSP/install` to generate the `.bsp` config directory (bsp is the Build Server Protocol); * In IntelliJ, go to File->Open, and choose the hail root directory; * When the project is open, go to File->Project Structure; * in the Project pane, set an sdk (8 or 11), and set the language level to 8; * in the Modules pane, delete the existing root module, click the plus sign -> Import Module, choose the `hail/` subdirectory, and choose ""Import module from external model"" and `BSP`; * you should see a progress bar at the bottom as it imports the project; * when it's done, quit and reopen IntelliJ. There should now be a bsp icon (two bars with two arrows between them) on the right, where the gradle elephant used to be. Just like before, sometimes you'll need to click the ""reload"" icon in there if things get wonky.; * if it says ""scalafmt configuration detected"", go ahead and enable the formatter. ## Metals setup. * delete any `.metals` directories; * open the hail repo in VSCode (even if you won't use VSCode, this seems to be the best way to get metals set up initially); * it should ask you to import a Mill build; * when that finishes, at the bottom it should say it's connected to a Bloop build server. In general, I think using Mill as the BSP directly will work best, but I don't have much experience t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147:3777,config,config,3777,https://hail.is,https://github.com/hail-is/hail/pull/14147,1,['config'],['config']
Modifiability,terator.scala:1431); 	at is.hail.asm4s.ModuleBuilder.classesBytes(ClassBuilder.scala:152); 	at is.hail.expr.ir.EmitClassBuilder.resultWithIndex(EmitClassBuilder.scala:706); 	at is.hail.expr.ir.WrappedEmitClassBuilder.resultWithIndex(EmitClassBuilder.scala:174); 	at is.hail.expr.ir.WrappedEmitClassBuilder.resultWithIndex$(EmitClassBuilder.scala:174); 	at is.hail.expr.ir.EmitFunctionBuilder.resultWithIndex(EmitClassBuilder.scala:1078); 	at is.hail.expr.ir.Emit.$anonfun$emitI$238(Emit.scala:2400); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:336); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:2341); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:630); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:748); 	at is.hail.expr.ir.TableTextFinalizer.writeMetadata(TableWriter.scala:552); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:748); 	at is.hail.expr.ir.Emit.emitVoid$1(Emit.scala:627); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$5$adapted(Emit.scala:644); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4(Emit.scala:644); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$4$adapted(Emit.scala:643); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:1011); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3(Emit.scala:643); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$3$adapted(Emit.scala:641); 	at scala.collection.Iterator.foreach(Iterator.scala:943); 	at scala.collection.Iterator.foreach$(Iterator.scala:943); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:641); 	at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12531:6243,adapt,adapted,6243,https://hail.is,https://github.com/hail-is/hail/issues/12531,1,['adapt'],['adapted']
Modifiability,"tes below</strong>)</p>; <h2>Deprecations</h2>; <ul>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9488"">#9488</a>: If custom subclasses of nodes like <code>pytest.Item</code>{.interpreted-text role=&quot;class&quot;} override the; <code>__init__</code> method, they should take <code>**kwargs</code>. See; <code>uncooperative-constructors-deprecated</code>{.interpreted-text role=&quot;ref&quot;} for details.</p>; <p>Note that a deprection warning is only emitted when there is a conflict in the; arguments pytest expected to pass. This deprecation was already part of pytest; 7.0.0rc1 but wasn't documented.</p>; </li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9355"">#9355</a>: Fixed error message prints function decorators when using assert in Python 3.8 and above.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9396"">#9396</a>: Ensure <code>pytest.Config.inifile</code>{.interpreted-text role=&quot;attr&quot;} is available during the <code>pytest_cmdline_main &lt;_pytest.hookspec.pytest_cmdline_main&gt;</code>{.interpreted-text role=&quot;func&quot;} hook (regression during <code>7.0.0rc1</code>).</li>; </ul>; <h2>Improved Documentation</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9404"">#9404</a>: Added extra documentation on alternatives to common misuses of [pytest.warns(None)]{.title-ref} ahead of its deprecation.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9505"">#9505</a>: Clarify where the configuration files are located. To avoid confusions documentation mentions; that configuration file is located in the root of the repository.</li>; </ul>; <h2>Trivial/Internal Changes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9521"">#9521</a>: Add test coverage to assertion rewrite path.</li>; </ul>; <!-- raw HTML o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:2872,Config,Config,2872,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['Config'],['Config']
Modifiability,"tes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""1c6dbf20333a""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:4b4139c73fe9be3bee6c2895aa74059e157eb861d2bdac7d2304ba44b5421f88"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""] ""labels"":map[""app"":""batch""] ""name"":""batch""] ""spec"":map[""ports"":[map[""port"":'P' ""protocol"":""TCP"" ""targetPort"":'\u1388']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; make: *** [deploy-batch] Error 1; Makefile:45: recipe for target 'deploy-batch' failed; ```; [deploy.log](https://github.com/hail-is/hail/files/2504429/deploy.log). Service accounts:; ```; error: the server doesn't have a resource type ""service-accounts""; # kubectl get serviceaccounts ; NAME SECRETS AGE; batch-svc 1 9h; default 1 113d; # kubectl get serviceaccounts -n batch-pods; NAME SECRETS AGE; default 1 7d; deploy-svc 1 1d; test-svc 1 1d; # kubectl get serviceaccounts -n test ; NAME SECRETS AGE; default 1 1d; ```. Apparently this is caused by a lack of permissions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609:4669,config,configuration,4669,https://hail.is,https://github.com/hail-is/hail/issues/4609,1,['config'],['configuration']
Modifiability,"tes/kubernetes/pull/106163"">kubernetes/kubernetes#106163</a>, <a href=""https://github.com/aojea""><code>@​aojea</code></a>) [SIG API Machinery, Apps, Architecture, Auth, Autoscaling, CLI, Cloud Provider, Contributor Experience, Instrumentation, Network, Node, Release, Scalability, Scheduling, Storage, Testing and Windows]</li>; <li>If a conflict occurs when creating an object with <code>generateName</code>, the server now returns an &quot;AlreadyExists&quot; error with a retry option. (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104699"">kubernetes/kubernetes#104699</a>, <a href=""https://github.com/vincepri""><code>@​vincepri</code></a>)</li>; <li>Implement support for recovering from volume expansion failures (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106154"">kubernetes/kubernetes#106154</a>, <a href=""https://github.com/gnufied""><code>@​gnufied</code></a>) [SIG API Machinery, Apps and Storage]</li>; <li>In kubelet, log verbosity and flush frequency can also be configured via the configuration file and not just via command line flags. In other commands (kube-apiserver, kube-controller-manager), the flags are listed in the &quot;Logs flags&quot; group and not under &quot;Global&quot; or &quot;Misc&quot;. The type for <code>-vmodule</code> was made a bit more descriptive (<code>pattern=N,...</code> instead of <code>moduleSpec</code>). (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/106090"">kubernetes/kubernetes#106090</a>, <a href=""https://github.com/pohly""><code>@​pohly</code></a>) [SIG API Machinery, Architecture, CLI, Cluster Lifecycle, Instrumentation, Node and Scheduling]</li>; <li>Introduce <code>OS</code> field in the PodSpec (<a href=""https://github-redirect.dependabot.com/kubernetes/kubernetes/pull/104693"">kubernetes/kubernetes#104693</a>, <a href=""https://github.com/ravisantoshgudimetla""><code>@​ravisantoshgudimetla</code></a>)</li>; <li>Introduce <code>v1beta3</code> API",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11957:4912,config,configured,4912,https://hail.is,https://github.com/hail-is/hail/pull/11957,2,['config'],"['configuration', 'configured']"
Modifiability,"the `@benchmark` decorator).; By convention, benchmarks are python tests whose names are prefixed by `benchmark_` and are located in files with the same prefix.; Nothing enforces this, however, so you could name your benchmarks `test_*` and put them in files named `test_*.py`.; Benchmarks may import and use any test code or utilities defined in `test/`.; The results of each benchmark are outputted as json lines (`.jsonl`) to the file specified by the `--output` pytest arg or stdout. The folder structure should be familiar, resembling our `test/` directory.; I believe this is flexible enough to add `hailtop` benchmarks should we so wish:; ```; pytest.ini - hoisted from `test/` to include benchmark marks; benchmark/; - conftest.py for custom pytest command line args ; - hail/; - confest.py for custom plugin that runs hail benchmarks; - benchmark_*.py hail query benchmark code; - tools/; - shared utilites, including the `@benchmark`; ```; Supporting pytest fixtures required writing a custom plugin to run benchmarks, as using off-the-shelf; solutions like `pytest-benchmark` would forbid method level fixtures like `tmp_path` etc.; The plugin is designed to run ""macro-benchmarks"" (ie long-running tests) and fully supports pytest parameterisation.; For each benchmark, the plugin initialises hail and then repeats (for a number of iterations defined by the pytest mark); acquiring fixtures, timing invocation and tearing-down fixtures, finally stopping hail. It is therefore unsuitable for; microbenchmarks, for which we currenly have none in python. If we add them we'd need to tweak this so support them.; Perhaps an inner loop or something. The process of submitting benchmarks to batch is greatly simplified as the old `Makefile` infrastructure for ; building wheels and docker images etc has been replaced with the script `benchmark_in_batch.py`.; Benchmark images are now based off the `hail-dev` image built in CI (or via the `hail-dev-image` make target). ; Furthermore, you can ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565:1400,plugin,plugin,1400,https://hail.is,https://github.com/hail-is/hail/pull/14565,1,['plugin'],['plugin']
Modifiability,the former `ascending` and `onKey` parameters are just rewrites of the comparison expression.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5084:55,rewrite,rewrites,55,https://hail.is,https://github.com/hail-is/hail/pull/5084,1,['rewrite'],['rewrites']
Modifiability,"the gcloud command write to `stderr` which gets interpreted as errors in the logs and causes [a lot of noise](https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aseverity%3DERROR%0Aresource.labels.namespace_name%3D%22default%22%0Aresource.labels.container_name%3D%22image-fetcher%22;timeRange=PT3H?project=hail-vdc&folder=true&organizationId=548622027621&query=%0A). . Added `gcr.io` because without it `configure-docker` would configure all the registries it can find, which is unnecessary. Also removed `-x` from the site script since each line of that goes to error logs and none of it is helpful. If errors occur they will show up in some other form.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9991:440,config,configure-docker,440,https://hail.is,https://github.com/hail-is/hail/pull/9991,2,['config'],"['configure', 'configure-docker']"
Modifiability,"the number of used bits is a (statically known) constant. We use this to ensure the number of used bits is known statically.; 	; Types:; - missingness; - treat as a type constructor `optional<T>`, i.e. base types don't encode missingness. Emits a single bit in the encoding. Can invert this bit to control whether missing values come first or last in the ordering. If missing, nothing is emitted after.; - sort-order; - treat reversing the default ordering as a type constructor `reverse<T>`; - simply inverts the encoding bitwise; - primitive types; - same as in datafusion, encoding has same size as original type; - signed integers - flip the sign bit; - floating point numbers - if sign bit is set, invert all bits, otherwise only flip the sign bit; - arrays; - before each element and after last element, emit continuation bit (0 if no more elements); - pad before each element. This prevents a variable number of missing bits packing into a byte; - strings and byte-arrays; - simply use null-terminated strings (being careful to do this in a unicode-safe way); - structs; - simply concatenate element encodings. safe because codes are prefix-free; - key structs; - support variable length ""interval endpoints""; - e.g. for a key type `struct<t1, t2>`, the interval `[{a}, {a, b})` contains all keys with first field `a` and second field less than `b`. We break it into two ""interval endpoints"", `({a}, -1)` and `({a, b}, -1)`, which consist of a struct value which is a prefix of the key struct type, and a ""sign"". In this case, both endpoints ""lean left"".; - needed for working with partitioners at runtime; - like an array with fixed but heterogenous types and a max length; - before each element and after last element, emit two continuation bits; - `00` - end of key, leans left (less than all longer keys with this prefix); - `01` - continue, or after last key field of actual key value (not interval endpoint); - unambiguous because key value can't terminate early, and can't continue past",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14396:2036,variab,variable,2036,https://hail.is,https://github.com/hail-is/hail/issues/14396,1,['variab'],['variable']
Modifiability,the parallel header reading doesn't change the config settings.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6282:47,config,config,47,https://hail.is,https://github.com/hail-is/hail/issues/6282,1,['config'],['config']
Modifiability,the uri got dropped in refactoring on PR that just went in #5686. Reverted the change and added a test to cluster sanity check.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5709:23,refactor,refactoring,23,https://hail.is,https://github.com/hail-is/hail/pull/5709,1,['refactor'],['refactoring']
Modifiability,"there's also a small amount of refactoring that was necessary to send TableIRSuite.testRangeCount through the lowered, jvm-emitted path to test the CollectDistributedArray implementation. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6087:31,refactor,refactoring,31,https://hail.is,https://github.com/hail-is/hail/pull/6087,1,['refactor'],['refactoring']
Modifiability,these are the last of the changes from #6480; really this should be several distinct PRs and if I have a chance to refactor the tests I'll split it apart. notes to follow in the morning.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6518:115,refactor,refactor,115,https://hail.is,https://github.com/hail-is/hail/pull/6518,1,['refactor'],['refactor']
Modifiability,"this code](https://github.com/broadinstitute/gnomad_methods/blob/0d2e71f93d5cfceebfad2ab538e47d8457d57d6d/gnomad/sample_qc/sex.py#L18-L46):. ```python; male = karyotype_expr == xy_karyotype_str; female = karyotype_expr == xx_karyotype_str; x_nonpar = locus_expr.in_x_nonpar(); y_par = locus_expr.in_y_par(); y_nonpar = locus_expr.in_y_nonpar(); return (; hl.case(missing_false=True); .when(female & (y_par | y_nonpar), hl.null(hl.tcall)); .when(male & (x_nonpar | y_nonpar) & gt_expr.is_het(), hl.null(hl.tcall)); .when(male & (x_nonpar | y_nonpar), hl.call(gt_expr[0], phased=False)); .default(gt_expr); ); ```; A single partition is taking a very long time to compute. Manual sampling of stack traces via `jstack` or the Spark UI reveals we spend a lot of time in computing the inPar predicates:; ```; app//is.hail.utils.Interval.contains(Interval.scala:67); app//is.hail.variant.ReferenceGenome.$anonfun$inPar$1(ReferenceGenome.scala:298); app//is.hail.variant.ReferenceGenome.$anonfun$inPar$1$adapted(ReferenceGenome.scala:298); app//is.hail.variant.ReferenceGenome$Lambda$924/0x00000008009b2840.apply(Unknown Source); app//scala.collection.IndexedSeqOptimized.prefixLengthImpl(IndexedSeqOptimized.scala:41); app//scala.collection.IndexedSeqOptimized.exists(IndexedSeqOptimized.scala:49); app//scala.collection.IndexedSeqOptimized.exists$(IndexedSeqOptimized.scala:49); app//scala.collection.mutable.ArrayOps$ofRef.exists(ArrayOps.scala:198); app//is.hail.variant.ReferenceGenome.inPar(ReferenceGenome.scala:298); app//is.hail.variant.ReferenceGenome.inYPar(ReferenceGenome.scala:302)__C9622collect_distributed_array_table_native_writer.__m10668inYPar(Unknown Source)__C9622collect_distributed_array_table_native_writer.__m10656split_Let(Unknown Source)__C9622collect_distributed_array_table_native_writer.__m10638split_ToArray_region3_65(Unknown Source)__C9622collect_distributed_array_table_native_writer.__m10638split_ToArray(Unknown Source)__C9622collect_distributed_array_table_native_writer.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13862:1027,adapt,adapted,1027,https://hail.is,https://github.com/hail-is/hail/issues/13862,1,['adapt'],['adapted']
Modifiability,this is necessary for rewrite rules to preserve the values generated by seeded functions correctly. I believe this is the last piece for #4017. (fixes #4017),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4165:22,rewrite,rewrite,22,https://hail.is,https://github.com/hail-is/hail/pull/4165,1,['rewrite'],['rewrite']
Modifiability,"this is the very first part of an effort to make build.yaml steps that should ultimately work in any cloud deployment cloud-agnostic from the perspective of build.yaml. In this case, it removes the gcp project environment variable from the build.yaml step and instead has bootstrap_create_accounts.py read it from a `GCPConfig`. This `GCPConfig` is constructed from fields of the global config, but the actual application code doesn't care about which secret it's coming from.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10957:222,variab,variable,222,https://hail.is,https://github.com/hail-is/hail/pull/10957,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"this release. People with a &quot;+&quot; by; their names contributed a patch for the first time.</p>; <ul>; <li>Bas van Beek</li>; <li>Charles Harris</li>; <li>Matthew Barber</li>; <li>Matti Picus</li>; <li>Ralf Gommers</li>; <li>Ross Barnowski</li>; <li>Sebastian Berg</li>; <li>Sicheng Zeng +</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 13 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22368"">#22368</a>: BUG: Add <code>__array_api_version__</code> to <code>numpy.array_api</code> namespace</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22370"">#22370</a>: MAINT: update sde toolkit to 9.0, fix download link</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22382"">#22382</a>: BLD: use macos-11 image on azure, macos-1015 is deprecated</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22383"">#22383</a>: MAINT: random: remove <code>get_info</code> from &quot;extending with Cython&quot;...</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22384"">#22384</a>: BUG: Fix complex vector dot with more than NPY_CBLAS_CHUNK elements</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22387"">#22387</a>: REV: Loosen <code>lookfor</code>'s import try/except again</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22388"">#22388</a>: TYP,ENH: Mark <code>numpy.typing</code> protocols as runtime checkable</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22389"">#22389</a>: TYP,MAINT: Change more overloads to play nice with pyright</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22390"">#22390</a>: TST,TYP: Bump mypy to 0.981</li>; <li><a href=""https://github-redirect.dependabot.com/numpy/numpy/pull/22391"">#22391</a>: DOC: Update delimiter param description.</li>; <li><a href=""https://github-redirect.dep",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12441:1897,extend,extending,1897,https://hail.is,https://github.com/hail-is/hail/pull/12441,1,['extend'],['extending']
Modifiability,"this the exception (attached the log):. ``` ; File ""/tmp/7f8f775e-2ec3-40ee-ad5b-3e4df5649682/annotate_and_generate_scores_cloud.py"", line 24, in <module>; .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/dataset.py"", line 3095, in vep; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/context.py"", line 81, in run_command; File ""/home/teamcity/TeamCityAgent1/work/591c293e3f6bfb1d/python/hail/java.py"", line 5, in jarray; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 228, in __setitem__; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_collections.py"", line 211, in __set_item; File ""/usr/lib/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 323, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:; java.lang.IllegalArgumentException: array element type mismatch; at java.lang.reflect.Array.set(Native Method); at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:141); at py4j.commands.ArrayCommand.execute(ArrayCommand.java:94); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745) ; ```. [hail.txt](https://github.com/hail-is/hail/files/725407/hail.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1284:161,config,config,161,https://hail.is,https://github.com/hail-is/hail/issues/1284,1,['config'],['config']
Modifiability,"thlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ ipython ; Python 3.8.6 (default, Jan 27 2021, 15:42:20) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.21.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-e24d842d2b9a> in <module>; ----> 1 import hail. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/__init__.py in <module>; 32 # F401 '.expr.*' imported but unused; 33 # E402 module level import not at top of file; ---> 34 from .table import Table, GroupedTable, asc, desc # noqa: E402; 35 from .matrixtable import MatrixTable, GroupedMatrixTable # noqa: E402; 36 from .expr import * # noqa: F401,F403,E402. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/hail/table.py in <module>; 2 import itertools; 3 import pandas; ----> 4 import pyspark; 5 from typing import Optional, Dict, Callable; 6 . ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/__init__.py in <module>; 49 ; 50 from pyspark.conf import SparkConf; ---> 51 from pyspark.context import SparkContext; 52 from pyspark.rdd import RDD, RDDBarrier; 53 from pyspark.files import SparkFiles. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/context.py in <module>; 29 from py4j.protocol import Py4JError; 30 ; ---> 31 from pyspark import accumulators; 32 from pyspark.accumulators import Accumulator; 33 from pyspark.broadcast import Broadcast, BroadcastPickleRegistry. ~/sandbox/hail/venv/3.8/lib/python3.8/site-packages/pyspark/accumulators.py in <module>; 95 import socketserver as SocketServer; 96 import threading; ---> 97 from pyspark.serializers import read_int, PickleSerializer; 98 ; 99 . ~/sandbox/hail/venv/3.8/lib/python3.8/s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:10246,sandbox,sandbox,10246,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['sandbox'],['sandbox']
Modifiability,"thub.com/axios/axios/commit/82c94555917834770bd1389fc0b4cd9ba35ec3fe""><code>82c9455</code></a> Create SECURITY.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3981"">#3981</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/5b457116e31db0e88fede6c428e969e87f290929""><code>5b45711</code></a> Security fix for ReDoS (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3980"">#3980</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/5bc9ea24dda14e74def0b8ae9cdb3fa1a0c77773""><code>5bc9ea2</code></a> Update ECOSYSTEM.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3817"">#3817</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e72813a385c32e4c3eeaeb4fcc4437dd124bbbcf""><code>e72813a</code></a> Fixing README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3818"">#3818</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e10a0270e988a641ba0f01509c4c3ba657afe5a5""><code>e10a027</code></a> Fix README typo under Request Config (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3825"">#3825</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/e091491127893a476b0223ab72f788c3b30fc082""><code>e091491</code></a> Update README.md (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3936"">#3936</a>)</li>; <li><a href=""https://github.com/axios/axios/commit/b42fbad57b093bb7214991161c5355bd46b864d0""><code>b42fbad</code></a> Removed un-needed bracket</li>; <li><a href=""https://github.com/axios/axios/commit/520c8dccdef92cccbe51ea7cd96ad464c6401914""><code>520c8dc</code></a> Updating CI status badge (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3953"">#3953</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/axios/axios/compare/v0.21.1...v0.21.2"">compare view</a></li>; </ul>; </details>; <details>; <summary>Maintainer changes</summary>; <p>This version was pushed to npm by <a href=""https://www.n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11080:12667,Config,Config,12667,https://hail.is,https://github.com/hail-is/hail/pull/11080,2,['Config'],['Config']
Modifiability,"till; accessible with <code>import importlib;metadata.metadata('pylint')</code>.</p>; </li>; <li>; <p>COPYING has been renamed to LICENSE for standardization.</p>; </li>; <li>; <p>Fix false-positive <code>used-before-assignment</code> in function returns.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/4301"">#4301</a></p>; </li>; <li>; <p>Updated <code>astroid</code> to 2.5.3</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/blob/main/ChangeLog"">pylint's changelog</a>.</em></p>; <blockquote>; <h1>What's New in Pylint 2.12.2?</h1>; <p>Release date: 2021-11-25</p>; <ul>; <li>; <p>Fixed a false positive for <code>unused-import</code> where everything; was not analyzed properly inside typing guards.</p>; </li>; <li>; <p>Fixed a false-positive regression for <code>used-before-assignment</code> for; typed variables in the body of class methods that reference the same class</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5342"">#5342</a></p>; </li>; <li>; <p>Specified that the <code>ignore-paths</code> option considers &quot;&quot; to represent a; windows directory delimiter instead of a regular expression escape; character.</p>; </li>; <li>; <p>Fixed a crash with the <code>ignore-paths</code> option when invoking the option; via the command line.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5437"">#5437</a></p>; </li>; <li>; <p>Fixed handling of Sphinx-style parameter docstrings with asterisks. These; should be escaped with by prepending a &quot;&quot;.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5406"">#5406</a></p>; </li>; <li>; <p>Add <code>endLine</code> and <code>endColumn</code> keys to output of <code>JSONReporter</code>.</p>; <p>Closes <a href=""https://github-redirect.dependab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11461:3649,variab,variables,3649,https://hail.is,https://github.com/hail-is/hail/pull/11461,2,['variab'],['variables']
Modifiability,"ting decisions dynamically circumvents this limitation. However, this prevents usage of NGINX [upstream](http://nginx.org/en/docs/http/ngx_http_upstream_module.html) blocks that provide connection pooling, at least in the community edition, and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:2239,config,configure,2239,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,['config'],"['configuration', 'configure']"
Modifiability,tionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:631); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:693); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:459); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:458); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:458); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:458); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:456); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:6713,adapt,adapted,6713,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['adapt'],['adapted']
Modifiability,tionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.withExecuteContext$1(ServiceBackend.scala:634); E 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:697); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:462); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:461); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:461); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:6427,adapt,adapted,6427,https://hail.is,https://github.com/hail-is/hail/issues/13074,2,['adapt'],['adapted']
Modifiability,tionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:356); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:6,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:8040,Rewrite,RewriteBottomUp,8040,https://hail.is,https://github.com/hail-is/hail/issues/12983,4,['Rewrite'],['RewriteBottomUp']
Modifiability,tionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:356); 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles.scala:20); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.transform(LoweringPass.scala:157); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerAndExecuteShufflesPass.apply(LoweringPass.scala:151); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); 	at scala.collection.mutable.ResizableArray.foreach(Res,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:8017,Rewrite,RewriteBottomUp,8017,https://hail.is,https://github.com/hail-is/hail/issues/12983,4,['Rewrite'],['RewriteBottomUp']
Modifiability,"tl command line argument parsing code. While the interface remains largely the same, a few changes were made to how options are handled. We first introduce a bit of terminology. In a shell command invocation like `$ cmd a -o c`, `a`, `-o` and `c` are called parameters. `a` and `c` which do not start with dashes, are called arguments. `-o`, which starts with a dash, is an option. This PR makes the following changes:. - For dataproc commands taking extra gcloud parameters, all parameters after a double-dash (--) are passed to gcloud.; - The actual rule is slightly more complicated, but I think the above rule is the right take away. In detail, extra parameters are passed to gcloud. Unknown options (starting with a dash) before `--` are reported as an error. So arguments (not options) before `--` and all parameters after are passed to gcloud. ; - Short options don't need a `=` when specifying a value. It is now `-p2`, not `-p=2`.; - While I was making breaking changes, I changed `dataproc submit` `--gcloud_configuration` to `--gcloud-configuration`. I am happy to undo this one.; - Group arguments must go before the next command. Write `hailctl dataproc --beta start ...` not `hailctl dataproc start --beta ...`, which is an error since `start` has no option `--beta`. This PR rewrites argument parsing to use click instead of argparse: https://click.palletsprojects.com/en/7.x/. Things you need to know about click:; - A group is a group of commands or subgroups, like `hailctl dataproc`, `hailctl batch`, etc. Groups are defined like this:; ; ```; @hailctl.group(; help=""Manage the Hail Batch service.""); def batch():; pass; ```; - A command in a group is defined like this:. ```; @batch.command(; help=""Get a particular batch's info.""); @click.argument('batch_id', type=int); @click.option('--output-format', '-o',; type=click.Choice(['yaml', 'json']),; default='yaml', show_default=True,; help=""Specify output format"",); def get(batch_id, output_format):; ...; ```. The command decor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842:1072,config,configuration,1072,https://hail.is,https://github.com/hail-is/hail/pull/9842,1,['config'],['configuration']
Modifiability,"to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this latency is the request to blob storage. While GCS and ABS are relatively fast and highly available, Batch in Azure Terra requires first obtaining SAS tokens from the Terra control plane, which can introduce much higher and more variable latency. There have also been occurrences in the past of corrupted or deleted specs, which introduce unexpected failure modes that should error the job but instead disrupt the scheduling loop. Many of these problems would be mitigated by moving the read from object storage outside of the `/api/v1alpha/batches/jobs/create` endpoint. The endpoint should push this read into the asynchronous task that ultimately runs the job and therefore return its acknowledgement to the driver faster. If the worker encounters errors later on while reading the spec, those should result in `error`ing the job instead of raising a 500 in the scheduling request. ### Version. 0.2.129. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:2229,variab,variable,2229,https://hail.is,https://github.com/hail-is/hail/issues/14456,1,['variab'],['variable']
Modifiability,to make further refactoring easier.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10857:16,refactor,refactoring,16,https://hail.is,https://github.com/hail-is/hail/pull/10857,1,['refactor'],['refactoring']
Modifiability,"to v0.9.26 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2196"">#2196</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f8bb658e9ff3cba5e745acae13ec4094a1a48d5"">4f8bb65</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.26.0...v2.26.1"">2.26.1</a> (2023-08-14)</h2>; <h3>Bug Fixes</h3>; <ul>; <li>Make use of ImmutableMap.Builder#buildOrThrow graceful (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2159"">#2159</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e9746f856e9204c1c0ec62f19e6f71ff8a0b9750"">e9746f8</a>)</li>; <li>Update gRPC writeAndClose to only set finish_write on the last message (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2163"">#2163</a>) (<a href=""https://github.com/googleapis/java-storage/commit/95df758d6753005226556177e68a3e9c630c789b"">95df758</a>)</li>; </ul>; <h3>Dependencies</h3>; <ul>; <li>Update dependency org.graalvm.buildtools:native-maven-plugin to v0.9.24 (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2158"">#2158</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4f5682a4f6d6d5372a2d382ae3e47dace490ca0d"">4f5682a</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/java-storage/compare/v2.25.0...v2.26.0"">2.26.0</a> (2023-08-03)</h2>; <h3>Features</h3>; <ul>; <li>Implement BufferToDiskThenUpload BlobWriteSessionConfig (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2139"">#2139</a>) (<a href=""https://github.com/googleapis/java-storage/commit/4dad2d5c3a81eda7190ad4f95316471e7fa30f66"">4dad2d5</a>)</li>; <li>Introduce new BlobWriteSession (<a href=""https://redirect.github.com/googleapis/java-storage/issues/2123"">#2123</a>) (<a href=""https://github.com/googleapis/java-storage/commit/e0191b518e50a49fae0691894b50f0c5f33fc6af"">e0191b5</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</su",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13624:11567,plugin,plugin,11567,https://hail.is,https://github.com/hail-is/hail/pull/13624,1,['plugin'],['plugin']
Modifiability,"tps://redirect.github.com/python-pillow/Pillow/issues/7242"">#7242</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Limit size even if one dimension is zero in decompression bomb check <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7235"">#7235</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Restored 32-bit support <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7234"">#7234</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed deleted file from codecov.yml and increased coverage threshold <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7232"">#7232</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed support for 32-bit <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7228"">#7228</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use --config-settings instead of deprecated --global-option <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7171"">#7171</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Better C integer definitions <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6645"">#6645</a> [<a href=""https://github.com/Yay295""><code>@​Yay295</code></a>]</li>; <li>Fixed finding dependencies on Cygwin <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7175"">#7175</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improved checks in font_render <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7218"">#7218</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Change <code>grabclipboard()</code> to use PNG compression on macOS <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7219"">#7219</a> [<a href=""https://github.com/abey79""><code>@​abey79</code></a>]</li>; <l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:2681,config,config-settings,2681,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['config'],['config-settings']
Modifiability,"tps://redirect.github.com/python-pillow/Pillow/issues/7883"">#7883</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improve speed of loading QOI images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7925"">#7925</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added RGB to I;16N conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7920"">#7920</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Add --report argument to <strong>main</strong>.py to omit supported formats <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7818"">#7818</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Added RGB to I;16, I;16L and I;16B conversion <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7918"">#7918</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix editable installation with custom build backend and configuration options <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7658"">#7658</a> [<a href=""https://github.com/nulano""><code>@​nulano</code></a>]</li>; <li>Fix putdata() for I;16N on big-endian <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7209"">#7209</a> [<a href=""https://github.com/Yay295""><code>@​Yay295</code></a>]</li>; <li>Determine MPO size from markers, not EXIF data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7884"">#7884</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Improved conversion from RGB to RGBa, LA and La <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7888"">#7888</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Support FITS images with GZIP_1 compression <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7894"">#7894</a> [<a href=""https://github.com/radarhere""><code>@​radarhere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:1990,config,configuration,1990,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['config'],['configuration']
Modifiability,"tps://www.github.com/googleapis/python-logging/commit/e3a1eba74dd8b67bcc73a78f784189ef2a9927c2"">e3a1eba</a>)</li>; <li>use structured logging on GCF with python 3.7 (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/434"">#434</a>) (<a href=""https://www.github.com/googleapis/python-logging/commit/5055919f70c82b38de6d1fa7f1df6006865a857b"">5055919</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/python-logging/blob/main/CHANGELOG.md"">google-cloud-logging's changelog</a>.</em></p>; <blockquote>; <h2><a href=""https://github.com/googleapis/python-logging/compare/v2.7.0...v3.0.0"">3.0.0</a> (2022-01-27)</h2>; <h3>⚠ BREAKING CHANGES</h3>; <ul>; <li>make logging API more friendly to use (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/422"">#422</a>)</li>; <li>api consistency between HTTP and Gapic layers (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/375"">#375</a>)</li>; <li>support string-encoded json (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/339"">#339</a>)</li>; <li>Infer default resource in logger (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/315"">#315</a>)</li>; <li>support json logs (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/316"">#316</a>)</li>; <li>deprecate AppEngineHandler and ContainerEngineHandler (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/310"">#310</a>)</li>; </ul>; <h3>Features</h3>; <ul>; <li>add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/python-logging/issues/472"">#472</a>) (<a href=""https://github.com/googleapis/python-logging/commit/81ca8c616acb988be1fbecfc2a0b1a5b39280149"">81ca8c6</a>)</li>; <li>add json_fields ext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11574:6073,layers,layers,6073,https://hail.is,https://github.com/hail-is/hail/pull/11574,1,['layers'],['layers']
Modifiability,"tqdm.notebook and pkg_resources take .55s and .37s to import, respectively. humanize imports pkg_resources which is why that is moved to. Moving these out of the top level improve cold-start time from 1.5s to .5s, and never even need to be imported for invocations such as `hailctl config list`, whose runtime is entirely import time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11210:282,config,config,282,https://hail.is,https://github.com/hail-is/hail/pull/11210,1,['config'],['config']
Modifiability,"ts/issues/5924"">#5924</a>)</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/requests/blob/main/HISTORY.md"">requests's changelog</a>.</em></p>; <blockquote>; <h2>2.27.1 (2022-01-05)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed parsing issue that resulted in the <code>auth</code> component being; dropped from proxy URLs. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6028"">#6028</a>)</li>; </ul>; <h2>2.27.0 (2022-01-03)</h2>; <p><strong>Improvements</strong></p>; <ul>; <li>; <p>Officially added support for Python 3.10. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5928"">#5928</a>)</p>; </li>; <li>; <p>Added a <code>requests.exceptions.JSONDecodeError</code> to unify JSON exceptions between; Python 2 and 3. This gets raised in the <code>response.json()</code> method, and is; backwards compatible as it inherits from previously thrown exceptions.; Can be caught from <code>requests.exceptions.RequestException</code> as well. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5856"">#5856</a>)</p>; </li>; <li>; <p>Improved error text for misnamed <code>InvalidSchema</code> and <code>MissingSchema</code>; exceptions. This is a temporary fix until exceptions can be renamed; (Schema-&gt;Scheme). (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/6017"">#6017</a>)</p>; </li>; <li>; <p>Improved proxy parsing for proxy URLs missing a scheme. This will address; recent changes to <code>urlparse</code> in Python 3.9+. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5917"">#5917</a>)</p>; </li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>; <p>Fixed defect in <code>extract_zipped_paths</code> which could result in an infinite loop; for some paths. (<a href=""https://github-redirect.dependabot.com/psf/requests/issues/5851"">#5851</a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11528:4294,inherit,inherits,4294,https://hail.is,https://github.com/hail-is/hail/pull/11528,2,['inherit'],['inherits']
Modifiability,"turn_value(; File ""/opt/homebrew/lib/python3.10/site-packages/py4j/protocol.py"", line 326, in get_return_value; raise Py4JJavaError(; py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x4d740d85) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x4d740d85; 	at org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213); 	at org.apache.spark.storage.StorageUtils$.<clinit>(StorageUtils.scala); 	at org.apache.spark.storage.BlockManagerMasterEndpoint.<init>(BlockManagerMasterEndpoint.scala:109); 	at org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:371); 	at org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:311); 	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:359); 	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:189); 	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:458); 	at is.hail.backend.spark.SparkBackend$.configureAndCreateSparkContext(SparkBackend.scala:148); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:230); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104); 	at java.base/java.lang.reflect.Method.invoke(Method.java:578); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:1589); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12630:2831,config,configureAndCreateSparkContext,2831,https://hail.is,https://github.com/hail-is/hail/issues/12630,1,['config'],['configureAndCreateSparkContext']
Modifiability,"type; (namely, <code>module</code>, <code>keyword</code>, <code>operator</code>, <code>object</code>, <code>exception</code>,; <code>statement</code>, and <code>builtin</code>) in the :rst:dir:<code>index</code> directive, and; set the removal version to Sphinx 9. Patch by Adam Turner.</li>; </ul>; <h2>Features added</h2>; <ul>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11415"">#11415</a>: Add a checksum to JavaScript and CSS asset URIs included within; generated HTML, using the CRC32 algorithm.</li>; <li>:meth:<code>~sphinx.application.Sphinx.require_sphinx</code> now allows the version; requirement to be specified as <code>(major, minor)</code>.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11011"">#11011</a>: Allow configuring a line-length limit for object signatures, via; :confval:<code>maximum_signature_line_length</code> and the domain-specific variants.; If the length of the signature (in characters) is greater than the configured; limit, each parameter in the signature will be split to its own logical line.; This behaviour may also be controlled by options on object description; directives, for example :rst:dir:<code>py:function:single-line-parameter-list</code>.; Patch by Thomas Louf, Adam Turner, and Jean-François B.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/10983"">#10983</a>: Support for multiline copyright statements in the footer block.; Patch by Stefanie Molin</li>; <li><code>sphinx.util.display.status_iterator</code> now clears the current line; with ANSI control codes, rather than overprinting with space characters.</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11431"">#11431</a>: linkcheck: Treat SSL failures as broken links.; Patch by Bénédikt Tran</li>; <li><a href=""https://redirect.github.com/sphinx-doc/sphinx/issues/11157"">#11157</a>: Keep the <code>translated</code> attribute on translated nodes.</li>; <li><a href=""https://redirect.github.com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13295:2567,config,configured,2567,https://hail.is,https://github.com/hail-is/hail/pull/13295,1,['config'],['configured']
Modifiability,"ub-redirect.dependabot.com/psf/black/issues/3168"">#3168</a>)</li>; <li>When using <code>--skip-magic-trailing-comma</code> or <code>-C</code>, trailing commas are stripped from subscript expressions with more than 1 element (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3209"">#3209</a>)</li>; <li>Implicitly concatenated strings inside a list, set, or tuple are now wrapped inside parentheses (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3162"">#3162</a>)</li>; <li>Fix a string merging/split issue when a comment is present in the middle of implicitly concatenated strings on its own line (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3227"">#3227</a>)</li>; </ul>; <h3><em>Blackd</em></h3>; <ul>; <li><code>blackd</code> now supports enabling the preview style via the <code>X-Preview</code> header (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3217"">#3217</a>)</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Black now uses the presence of debug f-strings to detect target version (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3215"">#3215</a>)</li>; <li>Fix misdetection of project root and verbose logging of sources in cases involving <code>--stdin-filename</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3216"">#3216</a>)</li>; <li>Immediate <code>.gitignore</code> files in source directories given on the command line are now also respected, previously only <code>.gitignore</code> files in the project root and automatically discovered directories were respected (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3237"">#3237</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Recommend using BlackConnect in IntelliJ IDEs (<a href=""https://github-redirect.dependabot.com/psf/black/issues/3150"">#3150</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Vim plugin: prefix messages with <code>Black: </code> so it's clear they come from Black (<a href=""ht",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12174:2841,Config,Configuration,2841,https://hail.is,https://github.com/hail-is/hail/pull/12174,1,['Config'],['Configuration']
Modifiability,"ucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:1731,Plugin,Plugins,1731,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,uleBuilder.classesBytes(ClassBuilder.scala:152); 	at is.hail.expr.ir.EmitClassBuilder.resultWithIndex(EmitClassBuilder.scala:660); 	at is.hail.expr.ir.WrappedEmitClassBuilder.resultWithIndex(EmitClassBuilder.scala:155); 	at is.hail.expr.ir.WrappedEmitClassBuilder.resultWithIndex$(EmitClassBuilder.scala:155); 	at is.hail.expr.ir.EmitFunctionBuilder.resultWithIndex(EmitClassBuilder.scala:1052); 	at is.hail.expr.ir.Emit.$anonfun$emitI$225(Emit.scala:2315); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:320); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:2256); 	at is.hail.expr.ir.Emit.emitI$3(Emit.scala:2458); 	at is.hail.expr.ir.Emit.$anonfun$emit$22(Emit.scala:2546); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:415); 	at is.hail.expr.ir.Emit.emit(Emit.scala:2545); 	at is.hail.expr.ir.Emit.emit$1(Emit.scala:591); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:624); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1(Emit.scala:549); 	at is.hail.expr.ir.Emit.$anonfun$emitVoidInSeparateMethod$1$adapted(Emit.scala:547); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWithBuilder(EmitClassBuilder.scala:985); 	at is.hail.expr.ir.Emit.emitVoidInSeparateMethod(Emit.scala:547); 	at is.hail.expr.ir.Emit.emitInSeparateMethod(Emit.scala:571); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:760); 	at is.hail.expr.ir.Emit.emitI$1(Emit.scala:600); 	at is.hail.expr.ir.Emit.$anonfun$emitVoid$26(Emit.scala:715); 	at is.hail.expr.ir.RelationalWriter.writeMetadata(TableWriter.scala:341); 	at is.hail.expr.ir.Emit.emitVoid(Emit.scala:715); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3(Emit.scala:70); 	at is.hail.expr.ir.Emit$.$anonfun$apply$3$adapted(Emit.scala:68); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:18); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedVoid(EmitCodeBuilder.scala:28); 	at is.hail.expr.ir.EmitMethodBuilder.voidWith,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533:19744,adapt,adapted,19744,https://hail.is,https://github.com/hail-is/hail/issues/12533,1,['adapt'],['adapted']
Modifiability,"un(ServerImpl.java:817); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15864,Plugin,Plugins,15864,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,"unts distinctlyKeyed firstKey; lastKey)]; }; !37 = ToArray(!s21); !38 = WriteMetadata(!37) [""{\""name\"":\""TableSpecWriter\"",\""path\"":\""/tmp/foo.ht\"",\""typ\"":{\""rowType\"":\""Struct{locus:Locus(GRCh38),alleles:Array[String],data:Array[Struct{}]}\"",\""key\"":[\""locus\"",\""alleles\""],\""globalType\"":\""Struct{new_globals:Array[Struct{}]}\""},\""rowRelPath\"":\""rows\"",\""globalRelPath\"":\""globals\"",\""refRelPath\"":\""references\"",\""log\"":true}""]; !39 = Begin(!34, !36, !38); WriteMetadata(!39) [""{\""name\"":\""RelationalWriter\"",\""path\"":\""/tmp/foo.ht\"",\""overwrite\"":true,\""maybeRefs\"":{\""references\"":[\""GRCh38\""]}}""]. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:23); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:23); 	at is.hail.utils.package$.fatal(package.scala:89); 	at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:17); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:29); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:19); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:205); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:308); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$sco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245:15954,adapt,adapted,15954,https://hail.is,https://github.com/hail-is/hail/issues/14245,1,['adapt'],['adapted']
Modifiability,update deploy script to generate some cloudtools configs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4248:49,config,configs,49,https://hail.is,https://github.com/hail-is/hail/pull/4248,1,['config'],['configs']
Modifiability,update query on batch tmp config flag,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13258:26,config,config,26,https://hail.is,https://github.com/hail-is/hail/pull/13258,1,['config'],['config']
Modifiability,"ure storage. Basic summary of the changes:; 	- Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; 	- Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new `azure-mgmt-storage` package requirement.; 	- Updated `AzureAsyncFS` to use `(account, credential)` tuple as internal `BlobServiceClient` cache key; 	- Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token ; 	- Update `RouterFS.ls` function and associated `listfiles` function to allow for trailing query strings during path traversal ; 	- Change to existing behavior: `LocalAsyncFSURL.__str__`no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; 	- Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions; - Updated InputResource to not include the SAS token as part of the destination file name . `test_fs.py` has been updated to respect the new model, where it is no longer safe to extend URLs by just appending new segments with + ""/"" because there may be a query string. But actually running those tests for the SAS case will require some new test variables to allow the test code to generate SAS tokens (`build.yaml/test_hail_python_fs`): ; ```; export HAIL_TEST_AZURE_ACCOUNT=hailtest; export HAIL_TEST_AZURE_CONTAINER=hail-test-4nxei; # Required for SAS testing on Azure; export HAIL_TEST_AZURE_RESGRP=hailms02; export HAIL_TEST_AZURE_SUBID=12ab51c6-da79-4a99-8dec-3d2decc97343; ```; So the SAS case is disabled for now (`test_fs.py`):; ```; @pytest.fixture(params=['file', 'gs', 's3', 'hail-az', 'router/file', 'router/gs', 'router/s3', 'router/hail-az']) # 'sas/hail-az'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12877:1415,extend,extend,1415,https://hail.is,https://github.com/hail-is/hail/pull/12877,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"url configuration file.; A [PEM file](https://en.wikipedia.org/wiki/Privacy-Enhanced_Mail) is a; base64 encoding standard for certificates and keys. Our certificates are; standard TLS [X.509 certificates](https://en.wikipedia.org/wiki/X.509). Our; private keys are RSA 4096 keys. The configuration file lists every principal in the system and the ""ssl-mode"" of; the system. The ""ssl-mode"" is inspired by MySQL's ssl-mode's and is one of (in; order from most to least secure): VERIFY_CA, REQUIRED, DISABLED. |mode | incoming connections must use TLS | clients verify hostnames on server cert | servers only accept trusted clients |; |---|---|---|---|; |VERIFY_CA|yes|yes|yes|; |REQUIRED|yes|no|no|; |DISABLED|no|no|no|. `create_certs.py` converts this global configuration file into a secret for each; principal. For NGINX principals, we generate the nginx conf in; `create_certs.py`. Unfortunately, I have no simple way to change the ports and; `ssl` status on nginx servers. For DISABLED, we send empty configuration; files. For REQUIRED, we load server certs and client certs, but we do not verify; (proxied) servers. I load the client certificates anyway so that I can smoke; test them before I require servers verify them. For VERIFY_CA, we load server; certs, load client certs, verify clients, and verify (proxied) servers. For Hail principals, we only generate a json configuration; file containing the ssl mode and some named paths. The new `hailtop/ssl.py`; module defines the mapping from configuration to Python's; [`SSLContext`](https://docs.python.org/3.6/library/ssl.html#ssl.SSLContext). There is also one ""curl"" principal: the admin-pod. REQUIRED and DISABLED are; mostly the same because required passes the `insecure` flag. As curl is; client-only, there is no notion of ""incoming connection"". ---. FAQ. Does this recreate certs on each deploy?. Yes. How do services speak to each other while a deploy is happening? Newly deployed; services will only trust newly deployed clients?. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:4515,config,configuration,4515,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['config'],['configuration']
Modifiability,"use <code>#!/bin/sh</code> on windows for hook script.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2182"">#2182</a> issue by <a href=""https://github.com/hushigome-visco""><code>@​hushigome-visco</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2187"">#2187</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; </ul>; <h2>pre-commit v2.16.0</h2>; <h3>Features</h3>; <ul>; <li>add warning for regexes containing <code>[\/]</code> or <code>[/\\]</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2053"">#2053</a> PR by <a href=""https://github.com/radek-sprta""><code>@​radek-sprta</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2043"">#2043</a> issue by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>move hook template back to <code>bash</code> resolving shebang-portability issues.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2065"">#2065</a> PR by <a href=""https://github.com/asottile""><code>@​asottile</code></a>.</li>; </ul>; </li>; <li>add support for <code>fail_fast</code> at the individual hook level.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2097"">#2097</a> PR by <a href=""https://github.com/colens3""><code>@​colens3</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/1143"">#1143</a> issue by <a href=""https://github.com/potiuk""><code>@​potiuk</code></a>.</li>; </ul>; </li>; <li>allow passthrough of <code>GIT_CONFIG_KEY_*</code>, <code>GIT_CONFIG_VALUE_*</code>, and <code>GIT_CONFIG_COUNT</code>.; <ul>; <li><a href=""https://github-redirect.dependabot.com/pre-commit/pre-commit/issues/2136"">#2136</a> PR by <a href=""https://github.com/emzeat""><code>@​emzeat</code></a>.</li>; </u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11460:4553,portab,portability,4553,https://hail.is,https://github.com/hail-is/hail/pull/11460,2,['portab'],['portability']
Modifiability,"ustomPlugin and DescribeConnector responses to return errors from asynchronous resource creation.</li>; </ul>; <h1>1.21.9</h1>; <ul>; <li>api-change:<code>finspace-data</code>: [<code>botocore</code>] Add new APIs for managing Users and Permission Groups.</li>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Add repositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/boto3/commit/67b84e02c185294c54a8e49510d4cb962e89cee2""><code>67b84e0</code></a> Merge branch 'release-1.21.13'</li>; <li><a href=""https://github.com/boto/boto3/commit/99acd545b20fe30ffa2f589a674c5a7ad74c266b""><code>99acd54</code></a> Bumping version to 1.21.13</li>; <li><a href=""https://github.com/boto/boto3/commit/83a8f662655bada44d442df7f33cb20d71ead257""><code>83a8f66</code></a> Add changelog entries from botocore",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11504:5007,config,configured,5007,https://hail.is,https://github.com/hail-is/hail/pull/11504,1,['config'],['configured']
Modifiability,"ustomPlugin and DescribeConnector responses to return errors from asynchronous resource creation.</li>; </ul>; <h1>1.21.9</h1>; <ul>; <li>api-change:<code>finspace-data</code>: [<code>botocore</code>] Add new APIs for managing Users and Permission Groups.</li>; <li>api-change:<code>amplify</code>: [<code>botocore</code>] Add repositoryCloneMethod field for hosting an Amplify app. This field shows what authorization method is used to clone the repo: SSH, TOKEN, or SIGV4.</li>; <li>api-change:<code>fsx</code>: [<code>botocore</code>] This release adds support for the following FSx for OpenZFS features: snapshot lifecycle transition messages, force flag for deleting file systems with child resources, LZ4 data compression, custom record sizes, and unsetting volume quotas and reservations.</li>; <li>api-change:<code>fis</code>: [<code>botocore</code>] This release adds logging support for AWS Fault Injection Simulator experiments. Experiment templates can now be configured to send experiment activity logs to Amazon CloudWatch Logs or to an S3 bucket.</li>; <li>api-change:<code>route53-recovery-cluster</code>: [<code>botocore</code>] This release adds a new API option to enable overriding safety rules to allow routing control state updates.</li>; <li>api-change:<code>amplifyuibuilder</code>: [<code>botocore</code>] We are adding the ability to configure workflows and actions for components.</li>; <li>api-change:<code>athena</code>: [<code>botocore</code>] This release adds support for updating an existing named query.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds support for new AMI property 'lastLaunchedTime'</li>; <li>api-change:<code>servicecatalog-appregistry</code>: [<code>botocore</code>] AppRegistry is deprecating Application and Attribute-Group Name update feature. In this release, we are marking the name attributes for Update APIs as deprecated to give a heads up to our customers.</li>; </ul>; <h1>1.21.8</h1>; <ul>; <li>api-chan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11486:3560,config,configured,3560,https://hail.is,https://github.com/hail-is/hail/pull/11486,1,['config'],['configured']
Modifiability,"ut. ```shell; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.128-17247d8990c6; LOGGING: writing to /home/edmund/.local/src/hail/hail-20240508-1553-0.2.128-17247d8990c6.log; Traceback (most recent call last):; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.local/share/pyenv/versions/3.9.18/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>; cli.main(); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main; run(); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file; runpy.run_path(target, run_name=""__main__""); File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path; return _run_module_code(code, init_globals, run_name,; File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code; _run_code(code, mod_globals, init_globals,; File ""/home/edmund/.vscode/extensions/ms-python.debugpy-2024.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/test.py"", line 34, in <module>; main(); File ""/home/edmund/.local/src/hail/test.py"", line 28, in main; r2_adj = r2_adj.checkpoint(f'{tmp}/adj', overwrite=args.overwrite); Fi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14537:2317,adapt,adapter,2317,https://hail.is,https://github.com/hail-is/hail/issues/14537,1,['adapt'],['adapter']
Modifiability,util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed. /usr/local/lib/python3.8/dist-packages/hail/backend/service_backend.py:477: FatalError; ------------------------------ Captured log call ----,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:22448,adapt,adapted,22448,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['adapt'],['adapted']
Modifiability,util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E ; E Hail version: 0.2.115-f6017673dbb6; E Error summary: RuntimeException: Stream is already closed.; ```. ### Version. 0.2.115-f6017673dbb6. ### Relevant log output. ```shell; ________________________________ test_spectra_4 _________________,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:8830,adapt,adapted,8830,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['adapt'],['adapted']
Modifiability,"v/pytest/commit/6ca733e8f19fa5c4271bf3e5bb295c8b62757e4a""><code>6ca733e</code></a> Enable testing with Python 3.11 (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9511"">#9511</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/ac37b1b1139eaa71b3bcb16b630abfc0223241ef""><code>ac37b1b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9671"">#9671</a> from nicoddemus/backport-9668</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/c891e402ac28f20dd3d018dc25f1ea1a273997be""><code>c891e40</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9672"">#9672</a> from nicoddemus/backport-9669</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/e2753a2b8b55de73adcc992036d0dc52facdbab9""><code>e2753a2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9669"">#9669</a> from hugovk/ci-only-update-plugin-list-for-upstream</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/b5a154c1d961dbc19a3c00d798de2f27aaa5ace5""><code>b5a154c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9668"">#9668</a> from hugovk/test-me-latest-3.10</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/0fae45bb6e4ecf177afdfa3bf03738813ec7b913""><code>0fae45b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9660"">#9660</a> from pytest-dev/backport-9646-to-7.0.x</li>; <li><a href=""https://github.com/pytest-dev/pytest/commit/37d434f5fcb5f80188b3d5b8f22d418dc191b955""><code>37d434f</code></a> [7.0.x] Delay warning about collector/item diamond inheritance</li>; <li>Additional commits viewable in <a href=""https://github.com/pytest-dev/pytest/compare/6.2.5...7.0.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:5643,plugin,plugin-list-for-upstream,5643,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['plugin'],['plugin-list-for-upstream']
Modifiability,"va.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:16173,Plugin,Plugins,16173,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['Plugin'],['Plugins']
Modifiability,va.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19443,adapt,adapted,19443,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['adapt'],['adapted']
Modifiability,"valid importing of <code>importlib.readers</code> in Python 3.9.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9610"">#9610</a>: Restore [UnitTestFunction.obj]{.title-ref} to return unbound rather than bound method.; Fixes a crash during a failed teardown in unittest TestCases with non-default [__init__]{.title-ref}.; Regressed in pytest 7.0.0.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9636"">#9636</a>: The <code>pythonpath</code> plugin was renamed to <code>python_path</code>. This avoids a conflict with the <code>pytest-pythonpath</code> plugin.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9642"">#9642</a>: Fix running tests by id with <code>::</code> in the parametrize portion.</li>; <li><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9643"">#9643</a>: Delay issuing a <code>~pytest.PytestWarning</code>{.interpreted-text role=&quot;class&quot;} about diamond inheritance involving <code>~pytest.Item</code>{.interpreted-text role=&quot;class&quot;} and; <code>~pytest.Collector</code>{.interpreted-text role=&quot;class&quot;} so it can be filtered using <code>standard warning filters &lt;warnings&gt;</code>{.interpreted-text role=&quot;ref&quot;}.</li>; </ul>; <h2>7.0.0</h2>; <h1>pytest 7.0.0 (2022-02-03)</h1>; <p>(<strong>Please see the full set of changes for this release also in the 7.0.0rc1 notes below</strong>)</p>; <h2>Deprecations</h2>; <ul>; <li>; <p><a href=""https://github-redirect.dependabot.com/pytest-dev/pytest/issues/9488"">#9488</a>: If custom subclasses of nodes like <code>pytest.Item</code>{.interpreted-text role=&quot;class&quot;} override the; <code>__init__</code> method, they should take <code>**kwargs</code>. See; <code>uncooperative-constructors-deprecated</code>{.interpreted-text role=&quot;ref&quot;} for details.</p>; <p>Note that a deprection warning is only emitted when there is a conflict in the; arguments p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11516:1431,inherit,inheritance,1431,https://hail.is,https://github.com/hail-is/hail/pull/11516,3,['inherit'],['inheritance']
Modifiability,"vals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutable.Set.empty, mutable.Set.empty); extract.analyze(cond, ref.name, Some(rw), trueSet); Some((extract.rewrite(cond, rw), trueSet)); }; }; }; ```; `trueSet` is the set of intervals which contains all rows where `cond` is true. This set is passed back into `analyze` in a second pass, which asks it to rewrite `cond` to something equivalent, under the assumption that all keys are contained in `trueSet`. The abstraction of runtime values tracks two types of information:; * Is this value a reference to / copy of one of the key fields of this row? We need to know this to be able to recognize comparisons with key values, which we want to extract to interval filters.; * For boolean values (including, ultimately, the filter predicate itself), we track three sets of intervals of the key type: overapproximations of when the bool is true, false, and missing. Overapproximation here means, for example, if the boolean evaluates to true in some row with key `k`, then `k` must be contained in the ""true"" set of intervals. But it's completely fine if the set of intervals contains keys of rows where the bool is not true. In particular, a boolean about which we know nothing (e.g. it's just some non-key boolean field in the dataset) is represented by an abstract boolean value where all three sets are the set of all ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:2533,rewrite,rewrite,2533,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['rewrite'],['rewrite']
Modifiability,"vc9f</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/740380c59ca2a7c2dceca19e5dba99f6b7060e62""><code>740380c</code></a> Bump cryptography from 41.0.3 to 41.0.4 (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3131"">#3131</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d9f85a749488188c286cd50606d159874db94d5f""><code>d9f85a7</code></a> Release 2.0.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/d41f4122966f7f4f5f92001ad518e5d9dafcc886""><code>d41f412</code></a> Undeprecate pyOpenSSL module (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3127"">#3127</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/b6c04cb3e62ef5a0e4947d037c12fb3ca79e024a""><code>b6c04cb</code></a> Fix a link to &quot;absolute URI&quot; definition (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3128"">#3128</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/af7c78fa30f5a4e265911371d0c59b6baeddca0f""><code>af7c78f</code></a> refactor: change double conditional to one (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3118"">#3118</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/34c13c8e68df6f89890ba08b9fc4fbf87ed21669""><code>34c13c8</code></a> Refer to current internet standards in docs on proxies (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3124"">#3124</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/a3e94f218cd8297db73302eadae235f0c832a809""><code>a3e94f2</code></a> Fix a name of an attribute in docs (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3125"">#3125</a>)</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/da69d4f4f95bc7ef9307fc8e0499c2121f1e4791""><code>da69d4f</code></a> Fix docs build (<a href=""https://redirect.github.com/urllib3/urllib3/issues/3123"">#3123</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/urllib3/urllib3/compare/1.26.16...2.0.6"">compare view</a></li>; </ul>; </de",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13768:13140,refactor,refactor,13140,https://hail.is,https://github.com/hail-is/hail/pull/13768,2,['refactor'],['refactor']
Modifiability,"ved the code). - `.strip()` the GitHub token in case there are newlines. - print the SHA being deployed in the log statement. - add `hail-ci-build.sh` to CI, which just invokes `make test-in-cluster`(which in turn runs `test-in-cluster.sh`. - `test-in-cluster.sh` copies the secrets for testing to the expected locations and exposes the pod in which it is running with an internal service, recent changes to `site` [redirect sub URLs of ci.test.is to services named using this scheme](https://github.com/hail-is/hail/blob/master/site/hail.nginx.conf#L38-L41). GitHub uses these URLs to send updates to the CI under test about the watched repositories. - `test-locally.sh` now installs `../batch` into the currently running `pip` before testing (NB: if you edit batch and run the tests without committing the changes you've made to batch, this will pass tests but fail when pushed to a PR!). - `test-locally.sh` activates the `hail-ci` conda environment itself because it was not being propagated from the `Makefile`. I don't know why, but this is a simple fix. - `test-locally.sh` starts the ci after the repository is created. CI will print error messages if a watched repository doesn't exist. - `test/test-ci.py` now uses access tokens for all interaction with GitHub, previously it relied on the latent privileges that I and Cotton had in our environments. - `test/test-ci.py` uses a temporary, but not automatically deleted, directory when the environment variable `IN_CLUSTER` is set to `true` (to which it is set by `test-in-cluster.sh`). I noticed that, when running in a batch job pod, if an error occurred, `pytest` failed to print any error information and instead failed because the current working directory no longer existed. I found very little information on Google about this. It seems safe to not clean up temporary directories created in the batch job pod because pods are ephemeral. cc: @cseed. Assigning to @tpoterba since he has the most context on this stuff other than Cotton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4474:1929,variab,variable,1929,https://hail.is,https://github.com/hail-is/hail/pull/4474,1,['variab'],['variable']
Modifiability,"ver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15675,Plugin,Plugins,15675,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,['Plugin'],['Plugins']
Modifiability,"ver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:15441,plugin,plugin,15441,https://hail.is,https://github.com/hail-is/hail/issues/14513,2,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,"we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.8.x branch, and on adding new features on the master branch.</p>; <p>This release requires Python <code>3.8+</code> and <code>NumPy 1.17.3</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>A sparse array API has been added for early testing and feedback; this; work is ongoing, and users should expect minor API refinements over; the next few releases.</li>; <li>The sparse SVD library PROPACK is now vendored with SciPy, and an interface; is exposed via <code>scipy.sparse.svds</code> with <code>solver='PROPACK'</code>. It is currently; default-off due to potential issues on Windows that we aim to; resolve in the next release, but can be optionally enabled at runtime for; friendly testing with an environment variable setting of <code>USE_PROPACK=1</code>.</li>; <li>A new <code>scipy.stats.sampling</code> submodule that leverages the <code>UNU.RAN</code> C; library to sample from arbitrary univariate non-uniform continuous and; discrete distributions</li>; <li>All namespaces that were private but happened to miss underscores in; their names have been deprecated.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.fft</code> improvements</h1>; <p>Added an <code>orthogonalize=None</code> parameter to the real transforms in <code>scipy.fft</code>; which controls whether the modified definition of DCT/DST is used without; changing the overall scaling.</p>; <p><code>scipy.fft</code> backend registration is now smoother, operating with a single</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/b5d8",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11538:1793,variab,variable,1793,https://hail.is,https://github.com/hail-is/hail/pull/11538,1,['variab'],['variable']
Modifiability,"with `eq`. That way `freshName` becomes just `new Name()`, with stronger guarantees that the new name doesn't occur anywhere in the current IR, without needing to maintain global state as we do now.; * get rid of `NormalizeNames`, instead enforcing the global uniqueness of names as a basic invariant of the IR (typecheck could also check this invariant); * keep a string in the `Name`, but no longer require it to be unique. Instead it's just a suggestion for how to show the name in printouts, adding a uniqueifying suffix as needed. With `NormalizeNames` gone, this would let us preserve meaningful variable names further in the lowering pipeline.; * possibly keep other state in the `Name`, for example to allow a more efficient implementation of environments, similar to the `mark` state on `BaseIR`. This is obviously a large change, but there are only a few conceptual pieces (appologies for not managing to separate these out):; * attempt to minimize the number of locations in which the `Name` constructor is called, to make future refactorings easier; * add `freshName()`, which just wraps `genUID()`, returning a `Name`; * convert IR construction to use the convenience methods in `ir.package`, which take scala lambdas to represent blocks with bound variables, instead of manually creating new variable names; * replace uses of the magic constant variable names (`row`, `va`, `sa`, `g`, `global`) with constants (`TableIR.{rowName, globalName}`, `MatrixIR.{rowName, colName, entryName, globalName}`); * the above changes modified the names we use for bound variables in many places. That shouldn't matter, but it cought a couple bugs where it did.; * `NormalizeNames` optionally allows the IR to contain free variables. But it didn't do anything to ensure the newly generated variable names are distinct from any contained free variables. Thus it was possible to rename a bound variable to mistakenly capture a contained free variable. I've fixed that.; * `SimplifySuite` compared simplifi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14547:1372,refactor,refactorings,1372,https://hail.is,https://github.com/hail-is/hail/pull/14547,1,['refactor'],['refactorings']
Modifiability,"without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which internal namespaces (dev and PR) are currently active. This requires relatively small changes to CI to track active namespaces but overall is a pretty small change. Note that this does not introduce a dependency on CI to support production traffic, only development traffic.; 3. Deployments that run more than 1 replica (but really can be all of them) are run behind Headless Services, which expose the underlying pod IPs so Envoy can handle load-balancing instead of kube-proxy. This allows Envoy to make smart load-balancing decisions and correctly enforce rate-limiting when using connection pools. The namespace tracking in CI in Point 2 is possible before we make any changes to our networking, so that comes first in #12093. Point 3 is taken care of in #12094, and the rest of Point 2 and Point 1, everything to do with Envoy, is in this PR. ### Additional QoL improvements; - Envoy by default exposes Prometheus metrics that we can use to easily monitor things like rate-limiting, request failures and durations; - Since all Envoy configuration is in the configmap, we don't need to build any images. I suppose we could have done this with NGINX, so this isn't something to fault NGINX for. Just another small win buried in these changes. cc @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:5664,config,configuration,5664,https://hail.is,https://github.com/hail-is/hail/pull/12095,2,['config'],"['configmap', 'configuration']"
Modifiability,"xed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>v13.5.1</h2>; <p>Very minor update to URL highlighting</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/Textualize/rich/blob/master/CHANGELOG.md"">rich's changelog</a>.</em></p>; <blockquote>; <h2>[13.7.1] - 2023-02-28</h2>; <h3>Fixed</h3>; <ul>; <li>Updated the widths of some characters <a href=""https://redirect.github.com/Textualize/rich/pull/3289"">Textualize/rich#3289</a></li>; </ul>; <h2>[13.7.0] - 2023-11-15</h2>; <h3>Added</h3>; <ul>; <li>Adds missing parameters to Panel.fit <a href=""https://redirect.github.com/Textualize/rich/issues/3142"">Textualize/rich#3142</a></li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Some text goes missing during wrapping when it contains double width characters <a href=""https://redirect.github.com/Textualize/rich/issues/3176"">Textualize/rich#3176</a></li>; <li>Ensure font is correctly inherited in exported HTML <a href=""https://redirect.github.com/Textualize/rich/issues/3104"">Textualize/rich#3104</a></li>; <li>Fixed typing for <code>FloatPrompt</code>.</li>; </ul>; <h2>[13.6.0] - 2023-09-30</h2>; <h3>Added</h3>; <ul>; <li>Added Python 3.12 to classifiers.</li>; </ul>; <h2>[13.5.3] - 2023-09-17</h2>; <h3>Fixed</h3>; <ul>; <li>Markdown table rendering issue with inline styles and links <a href=""https://redirect.github.com/Textualize/rich/issues/3115"">Textualize/rich#3115</a></li>; <li>Fix Markdown code blocks on a light background <a href=""https://redirect.github.com/Textualize/rich/issues/3123"">Textualize/rich#3123</a></li>; </ul>; <h2>[13.5.2] - 2023-08-01</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed Text.expand_tabs assertion error</li>; </ul>; <h2>[13.5.1] - 2023-07-31</h2>; <h3>Fixed</h3>; <ul>; <li>Fix tilde character (<code>~</code>) not included in link regex when printing to console <a href=""https://redirect.github.com/Textualize/rich/issues/3057",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14376:2847,inherit,inherited,2847,https://hail.is,https://github.com/hail-is/hail/pull/14376,2,['inherit'],['inherited']
Modifiability,xpr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1489); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:121); 	at is.hail.expr.ir.Compile$.$anonfun$apply$4(Compile.scala:90); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction(Backend.scala:279); 	at is.hail.backend.BackendWithCodeCache.lookupOrCompileCachedFunction$(Backend.scala:274); 	at is.hail.backend.spark.SparkBackend.lookupOrCompileCachedFunction(SparkBackend.scala:298); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:50); 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$5(CompileAndEvaluate.scala:66); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:66); 	at is.hail.expr.ir.CompileAndEvaluate$.evalToIR(CompileAndEvaluate.scala:28); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:30); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:59); 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:64); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:83); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:32); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:30); 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:29); 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:78); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:21); 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:19); 	at scala.collection.mutable.ResizableArray.foreac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:14908,rewrite,rewrite,14908,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['rewrite'],['rewrite']
Modifiability,xpr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:205); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$extractAndRewrite$6.apply(ExtractIntervalFilters.scala:201); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:201); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractAndRewrite(ExtractIntervalFilters.scala:151); 	at is.hail.expr.ir.ExtractIntervalFilters$.extractPartitionFilters(ExtractIntervalFilters.scala:249); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:266); 	at is.hail.expr.ir.ExtractIntervalFilters$$anonfun$apply$2.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:15); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$$anonfun$1.apply(RewriteBottomUp.scala:6); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:3858,Rewrite,RewriteBottomUp,3858,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['Rewrite'],['RewriteBottomUp']
Modifiability,"y as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.12 (2022-08-22)</h1>; <ul>; <li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module.; Both will be removed in v2.x. See this <code>GitHub issue &lt;https://github.com/urllib3/urllib3/issues/2680&gt;</code>_; for justification and info on how to migrate.</li>; </ul>; <h1>1.26.11 (2022-07-25)</h1>; <ul>; <li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would; raise an <code>OverflowError</code> on Python 3.9 and earlier.</li>; </ul>; <h1>1.26.10 (2022-07-07)</h1>; <ul>; <li>Removed support for Python 3.5</li>; <li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP; instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/a5b29ac1025f9bb30f2c9b756f3b171389c2c039""><code>a5b29ac</code></a> Add outputs.hashes to build action</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/a0b22f820639e6212994ba147f76b60d88185792""><code>a0b22f8</code></a> Release 1.26.12</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/13f11172648e880bb4a385fc4425420cd2534516""><code>13f1117</code></a> [1.26] Add SLSA generic generator to publish workflow</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/f95b9640604e7dd70d50d81f68fd14a36c082841""><code>f95b964</code></a> Add deprecation warnings for pyOpenSSL and the [secure] extra</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/aa3def7d242525e6e854991247c4b68583d15135""><code>aa3def7</code></a> Release 1.26.11</li>; <li",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12140:2781,config,configuring,2781,https://hail.is,https://github.com/hail-is/hail/pull/12140,2,['config'],"['configured', 'configuring']"
Modifiability,"y list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:2455,refactor,refactoring,2455,https://hail.is,https://github.com/hail-is/hail/pull/3727,1,['refactor'],['refactoring']
Modifiability,"y/commit/38f1e30e8137ccc1aad6a4f113eb4360c6206539""><code>38f1e30</code></a> Update version to 0.942</li>; <li><a href=""https://github.com/python/mypy/commit/1c836685da13f11287ae6d6931c04337f881ec40""><code>1c83668</code></a> Let overload item have a wider return type than implementation (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12435"">#12435</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/67088e558dc24a2c6c231db542a367923dfdc049""><code>67088e5</code></a> Pin the version of bugbear (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12436"">#12436</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/367b29d4aac16fc7493abffe2df0d8f477c23923""><code>367b29d</code></a> Make order of processing the builtins SCC predictable (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12431"">#12431</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/f81b228e66d8a95cc39247f189e7be7e894f7f92""><code>f81b228</code></a> Fix inheritance false positives with dataclasses/attrs (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12411"">#12411</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/7e09c2a100209072429e290d2f7b9b8007b8629c""><code>7e09c2a</code></a> Support overriding dunder attributes in Enum subclass (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12138"">#12138</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/837543efb616b14e2f800db6962d216621dee4d7""><code>837543e</code></a> Fix crash in match statement if class name is undefined (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12417"">#12417</a>)</li>; <li><a href=""https://github.com/python/mypy/commit/6606dbe98d09170d3ad810bc791a16d99ceb2281""><code>6606dbe</code></a> Allow non-final <strong>match_args</strong> and overriding (<a href=""https://github-redirect.dependabot.com/python/mypy/issues/12415"">#12415</a>)</li>; <li><a href=""https://github.com/python/mypy/commit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11667:1159,inherit,inheritance,1159,https://hail.is,https://github.com/hail-is/hail/pull/11667,2,['inherit'],['inheritance']
Modifiability,"yload); 216 path = action_routes[action]; 217 port = self._backend_server_port; → 218 resp = self._requests_session.post(f’http://localhost:{port}{path}', data=data); 219 if resp.status_code >= 400:; 220 error_json = orjson.loads(resp.content). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs); 626 def post(self, url, data=None, json=None, **kwargs):; 627 r""""“Sends a POST request. Returns :class:Response object.; 628; 629 :param url: URL for the new :class:Request object.; (…); 634 :rtype: requests.Response; 635 “””; → 637 return self.request(“POST”, url, data=data, json=json, **kwargs). File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 584 send_kwargs = {; 585 “timeout”: timeout,; 586 “allow_redirects”: allow_redirects,; 587 }; 588 send_kwargs.update(settings); → 589 resp = self.send(prep, **send_kwargs); 591 return resp. File ~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs); 700 start = preferred_clock(); 702 # Send the request; → 703 r = adapter.send(request, **kwargs); 705 # Total elapsed time of the request (approximately); 706 elapsed = preferred_clock() - start. File ~/Library/Python/3.9/lib/python/site-packages/requests/adapters.py:501, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies); 486 resp = conn.urlopen(; 487 method=request.method,; 488 url=url,; (…); 497 chunked=chunked,; 498 ); 500 except (ProtocolError, OSError) as err:; → 501 raise ConnectionError(err, request=request); 503 except MaxRetryError as e:; 504 if isinstance(e.reason, ConnectTimeoutError):; 505 # TODO: Remove this in 3.0.0: see #2811. ConnectionError: (‘Connection aborted.’, RemoteDisconnected(‘Remote end closed connection without response’)); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14557:2579,adapt,adapter,2579,https://hail.is,https://github.com/hail-is/hail/issues/14557,2,['adapt'],"['adapter', 'adapters']"
Modifiability,"ype$lzycompute(TextTableReader.scala:347); at is.hail.expr.ir.TextTableReader.fullType(TextTableReader.scala:347); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at is.hail.expr.ir.IRParser$$anonfun$table_ir_1$2.apply(Parser.scala:1231); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:1231); at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:1205); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$2.apply(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse(Parser.scala:1664); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1675); at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1674); at is.hail.expr.ir.IRParser.parse_table_ir(Parser.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-2684f0214a05; Error summary: NoSuchMethodError: org.apache.hadoop.conf.Configuration.getPropsWithPrefix(Ljava/lang/String;)Ljava/util/Map;; ```. This is caused by a backwards incompatible change introduced in Google's Hadoop connector in version 2.1.0 https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/323. As of 2.1.0 Google's Hadoop connector relies on Hadoop 2.8.3. Unfortunately, there are no Spark releases with Hadoop 2.8.3 yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:7655,Config,Configuration,7655,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['Config'],['Configuration']
Modifiability,"ypehints_fully_qualified</code></li>; </ul>; <h2>1.15.0</h2>; <ul>; <li>Resolve type guard imports before evaluating annotations for objects</li>; <li>Remove <code>set_type_checking_flag</code> flag as this is now done by default</li>; <li>Fix crash when the <code>inspect</code> module returns an invalid python syntax source</li>; <li>Made formatting function configurable using the option <code>typehints_formatter</code></li>; </ul>; <h2>1.14.1</h2>; <ul>; <li>Fixed <code>normalize_source_lines()</code> messing with the indentation of methods with decorators that have parameters starting; with <code>def</code>.</li>; <li>Handle <code>ValueError</code> or <code>TypeError</code> being raised when signature of an object cannot be determined</li>; <li>Fix <code>KeyError</code> being thrown when argument is not documented (e.g. <code>cls</code> argument for class methods, and <code>self</code> for; methods)</li>; </ul>; <h2>1.14.0</h2>; <ul>; <li>Added <code>typehints_defaults</code> config option allowing to automatically annotate parameter defaults.</li>; </ul>; <h2>1.13.1</h2>; <ul>; <li>Fixed <code>NewType</code> inserts a reference as first argument instead of a string</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/1ef84886873b80ff62ed1ea76e111dd9e96dbf18""><code>1ef8488</code></a> Release 1.17.0</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/aa345ca0475dbe8f4da7f4c7c56832c8d8e6884a""><code>aa345ca</code></a> Add <code>typehints_use_rtype</code> option (<a href=""https://github-redirect.dependabot.com/tox-dev/sphinx-autodoc-typehints/issues/218"">#218</a>)</li>; <li><a href=""https://github.com/tox-dev/sphinx-autodoc-typehints/commit/a022d1a430db886decf2221b712bc3bd881f5e86""><code>a022d1a</code></a> inspect.getsource can raise TypeError (<a href=""https://github-redirect.dependabot.c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11503:2467,config,config,2467,https://hail.is,https://github.com/hail-is/hail/pull/11503,2,['config'],['config']
Modifiability,"yter/jupyter_client/compare/v7.3.2...37ca37d865db260e7da6fa85339be450d6fd3c3c"">Full Changelog</a>)</p>; <h3>Bugs fixed</h3>; <ul>; <li>Add local-provisioner entry point to pyproject.toml Fixes <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/800"">#800</a> <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/801"">#801</a> (<a href=""https://github.com/utkonos""><code>@​utkonos</code></a>)</li>; </ul>; <h3>Contributors to this release</h3>; <p>(<a href=""https://github.com/jupyter/jupyter_client/graphs/contributors?from=2022-06-06&amp;to=2022-06-07&amp;type=c"">GitHub contributors page for this release</a>)</p>; <p><a href=""https://github.com/search?q=repo%3Ajupyter%2Fjupyter_client+involves%3Autkonos+updated%3A2022-06-06..2022-06-07&amp;type=Issues""><code>@​utkonos</code></a></p>; <h2>7.3.2</h2>; <p>(<a href=""https://github.com/jupyter/jupyter_client/compare/v7.3.1...c81771416d9e09e0e92be799f3e8549d0db57e43"">Full Changelog</a>)</p>; <h3>Enhancements made</h3>; <ul>; <li>Correct <code>Any</code> type annotations. <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/791"">#791</a> (<a href=""https://github.com/joouha""><code>@​joouha</code></a>)</li>; </ul>; <h3>Maintenance and upkeep improvements</h3>; <ul>; <li>[pre-commit.ci] pre-commit autoupdate <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/792"">#792</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; <li>Use hatch backend <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/789"">#789</a> (<a href=""https://github.com/blink1073""><code>@​blink1073</code></a>)</li>; <li>[pre-commit.ci] pre-commit autoupdate <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/pull/788"">#788</a> (<a href=""https://github.com/pre-commit-ci""><code>@​pre-commit-ci</code></a>)</li>; <li>Use flit build backend <a href=""https://github-redirect.dependabot.com/jupyter/jupy",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12110:6283,Enhance,Enhancements,6283,https://hail.is,https://github.com/hail-is/hail/pull/12110,1,['Enhance'],['Enhancements']
Modifiability,"ython-jsonschema/jsonschema/compare/v4.10.2...v4.10.3"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.2...v4.10.3</a></p>; <h2>v4.10.2</h2>; <ul>; <li>Fix a second place where subclasses may have added attrs attributes (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.1...v4.10.2</a></p>; <h2>v4.10.1</h2>; <ul>; <li>Fix Validator.evolve (and APIs like <code>iter_errors</code> which call it) for cases; where the validator class has been subclassed. Doing so wasn't intended to be; public API, but given it didn't warn or raise an error it's of course; understandable. The next release however will make it warn (and a future one; will make it error). If you need help migrating usage of inheriting from a; validator class feel free to open a discussion and I'll try to give some; guidance (<a href=""https://github-redirect.dependabot.com/python-jsonschema/jsonschema/issues/982"">#982</a>).</li>; </ul>; <p><strong>Full Changelog</strong>: <a href=""https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1"">https://github.com/python-jsonschema/jsonschema/compare/v4.10.0...v4.10.1</a></p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python-jsonschema/jsonschema/blob/main/CHANGELOG.rst"">jsonschema's changelog</a>.</em></p>; <blockquote>; <h1>v4.15.0</h1>; <ul>; <li>A specific API Reference page is now present in the documentation.</li>; <li><code>$ref</code> on earlier drafts (specifically draft 7 and 6) has been &quot;fixed&quot; to; follow the specified behavior when present alongside a sibling <code>$id</code>.; Specifically the ID is now properly ignored, and references are resolved; a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12163:3456,inherit,inheriting,3456,https://hail.is,https://github.com/hail-is/hail/pull/12163,1,['inherit'],['inheriting']
Modifiability,"ze check was missing (Adam Kaczmarek); Fixed: GITHUB-2709: Testnames not working together with suites in suite (Martin Aldrin); Fixed: GITHUB-2704: IHookable and IConfigurable callback discrepancy (Krishnan Mahadevan); Fixed: GITHUB-2637: Upgrade to JDK11 as the minimum JDK requirements (Krishnan Mahadevan); Fixed: GITHUB-2734: Keep the initial order of listeners (Andrei Solntsev); Fixed: GITHUB-2359: Testng <a href=""https://github.com/BeforeGroups""><code>@​BeforeGroups</code></a> is running in parallel with testcases in the group (Anton Velma); Fixed: Possible StringIndexOutOfBoundsException in XmlReporter (Anton Velma); Fixed: GITHUB-2754: <a href=""https://github.com/AfterGroups""><code>@​AfterGroups</code></a> is executed for each &quot;finished&quot; group when it has multiple groups defined (Anton Velma)</p>; <p>7.5; Fixed: GITHUB-2701: Bump gradle version to 7.3.3 to support java17 build (ZhangJian He); Fixed: GITHUB-2646: Streamline Logging Across TestNG (Krishnan Mahadevan); Fixed: GITHUB-2658: Inheritance + dependsOnMethods (Krishnan Mahadevan)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/cbeust/testng/commit/b94395dea479308ea3fe825269730b960f44d805""><code>b94395d</code></a> Bump version to 7.7.1 for release</li>; <li><a href=""https://github.com/cbeust/testng/commit/89dc5845fcb46c26af187e50ea907a7382d06e72""><code>89dc584</code></a> Streamline overloaded assertion methods for Groovy</li>; <li><a href=""https://github.com/cbeust/testng/commit/5ac0021d14f7eb00804fe235aaefc5c2fbce57d1""><code>5ac0021</code></a> Adding release notes</li>; <li><a href=""https://github.com/cbeust/testng/commit/c0e1e772f1fc0ab2142f3a4114a2b8cfe60fa7e1""><code>c0e1e77</code></a> Adjust version reference in deprecation msgs.</li>; <li><a href=""https://github.com/cbeust/testng/commit/011527d9bf0f91a40539f5e5467cc106888810d9""><code>011527d</code></a> Bump version to 7.7.0 for r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12665:14593,Inherit,Inheritance,14593,https://hail.is,https://github.com/hail-is/hail/pull/12665,1,['Inherit'],['Inheritance']
Modifiability,"~Stacked on #10905~. This PR refactors; * BinarySearch; * BLAS/LAPACK wrapper methods; * CodeBuilder.assign (the SSettable overload); * SSettable.store - Most uses now call with an SValue, but the SCode overload is still used in SCode subclasses.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10906:29,refactor,refactors,29,https://hail.is,https://github.com/hail-is/hail/pull/10906,1,['refactor'],['refactors']
Modifiability,"~Stacked on #10906~. This PR refactors `MethodBuilder.invoke` and `EmitCodeBuilder.invoke(S)Code` to take/return values. * `invoke` now takes a `CodeBuilderLike`. It is used in places where there is only access to a `CodeBuilder` (not an `EmitCodeBuilder`), so I had to use the generic interface, and had to move a couple methods on `EmitCodeBuilder` to `CodeBuilderLike`. I have never understood this Emit/non-Emit split; would be a great simplification if we could collapse it.; * This change pushed some (S)Code->(S)Value refactorings inside some aggregator implementations, which generate and invoke internal methods.; * I had to keep a version of `MethodBuilder.invoke` that doesn't take a CodeBuilder, for use in `ThisLazyFieldRef.get`. Will have to think more about how this should work when Code is (mostly) gone. Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit `load(cb: CodeBuilder): Value[T]`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10907:29,refactor,refactors,29,https://hail.is,https://github.com/hail-is/hail/pull/10907,2,['refactor'],"['refactorings', 'refactors']"
Modifiability,"~Stacked on #14404~. This set of changes started out as refactoring and trying to better understand `extract.scala`. But the bigger change ended up being getting rid of AggLet in favor of allowing bindings in `Let` to be agg/scan bindings. So each binding in a `Let` now has a binding scope, encoded using the pre-existing `Scope` enum. I also renamed `Let` to `Block`. While I think that's a better name for what `Let` has become, and what I hope it will further become in the future, I initially tried to keep the name `Let`. I changed it so that I could keep `Let.apply` the way it is, specialized to bindings all in the eval scope. We can change all the uses of `Let.apply` seperately, I didn't want to pollute this already large pr with that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14402:56,refactor,refactoring,56,https://hail.is,https://github.com/hail-is/hail/pull/14402,1,['refactor'],['refactoring']
Modifiability,"~Stacked on #8917~. Adds `zipPartitions`, `extendKeyPreservesPartitioning`, `orderedJoin`, and `alignAndZipPartitions` methods to `TableStage`, trying to mirror the `RVD` implementation, especially concerning the partitioning logic. Adds lowering case for `TableJoin`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8921:43,extend,extendKeyPreservesPartitioning,43,https://hail.is,https://github.com/hail-is/hail/pull/8921,1,['extend'],['extendKeyPreservesPartitioning']
Modifiability,"~~Stacked on #8140~~. This PR adds support for the new streams in the emitter for ArrayFilter, ArrayZip, ArrayFlatMap, If, and Let. It also changes the semantics of `Stream` slightly: Before, a producer had to close itself before calling EOS on its consumer. Now, the consumer is responsible for closing the producer after the producer calls EOS. This makes flatMap slightly more complicated, but it allows a significant simplification to zip. To implement the different modes of `ArrayZip`, I added `extendNA` and `take`. `extendNA` turns a stream into an infinite stream, where values after the end are missing, to allow the consumer to decide what to do when the stream ends. `take` is then needed to end infinite streams. Both use `COption` to encode whether the child stream has ended / whether to end the child stream. I still intend to convert uses of `COption` to use `CodeConditional` after lowering is unblocked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8156:501,extend,extendNA,501,https://hail.is,https://github.com/hail-is/hail/pull/8156,2,['extend'],['extendNA']
Modifiability,"~~Stacked on #9320~~. Use the framework introduced in #9320 to make the IR parser stack safe. This touches a lot of lines, but it is a completely mechanical refactoring. I did some preliminary benchmarking by timing the parse of the IR in `test_ld_prune`. (I chose that test fairly arbitrarily, and we can probably find better test cases, or generate synthetic large IRs.) Running a loop parsing the same IR repeatedly, with 10 burn-in rounds, and 60 timed rounds, I got the following results:; * On main; ```; quartiles = [4.55816, 5.209579, 5.647135]; avg = 5.332496950000001, std = 1.13761574944604; ```; * Using `StackSafe`, without the optimization in `repUntil`; ```; quantiles = [4.610798, 5.09793, 7.159075]; avg = 5.7519015000000016, std = 1.612397075594852; ```; * Using `StackSafe, with the optimization which reuses the `cont` closure, instead of allocating a new one for each token.; ```; quantiles = [4.466849, 4.826873, 5.719238]; avg = 5.2787357833333335, std = 1.272006325411254; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9332:157,refactor,refactoring,157,https://hail.is,https://github.com/hail-is/hail/pull/9332,1,['refactor'],['refactoring']
Modifiability,"~~Stride for a dimension should be calculated based off the length * stride of the previous innermost dimension with length > 1. I was remembering to do that for the stride but just taking the length of the adjacent dimension.~~. ~~Length-1 dimensions have stride 0 so broadcasting happens implicitly without the need for checks while indexing into the ndarray.~~. Broadcasting was previously handled implicitly by having 0 stride for dimensions of length 1. This doesn't actually work in all scenarios and conflates the concept of a multidimensional index with the underlying representation. Since we already compute the shapes of all intermediate NDArrays before doing any iteration, we can identify broadcasting ""statically"". Instead, loop variables associated with a braodcast are replaced with 0 when indexing into the backing array. This method is more robust and I've already seen works with slicing, which will follow this PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6154:743,variab,variables,743,https://hail.is,https://github.com/hail-is/hail/pull/6154,1,['variab'],['variables']
Performance,	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:458); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:456); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:456); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:456); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor90.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:7539,concurren,concurrent,7539,https://hail.is,https://github.com/hail-is/hail/issues/12976,2,['concurren'],['concurrent']
Performance,	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:459); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:134); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); E 	at is.hail.backend.service.Main$.main(Main.scala:15); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); E ; E java.net.SocketTimeoutException: connect timed out; E 	at java.net.PlainSocketImpl.socketConnect(Native Method); E 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); E 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); E 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); E 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); E 	at java.net.Socket.connect(Socket.java:607); E 	at is.hail.relocated.org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13074:7253,concurren,concurrent,7253,https://hail.is,https://github.com/hail-is/hail/issues/13074,1,['concurren'],['concurrent']
Performance,	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.r,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3158,concurren,concurrent,3158,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6079,Optimiz,Optimize,6079,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.34-914bd8a10ca2; Error summary: HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:118134,concurren,concurrent,118134,https://hail.is,https://github.com/hail-is/hail/issues/8469,2,['concurren'],['concurrent']
Performance," 	at is.hail.backend.Backend.execute(Backend.scala:77); 	at is.hail.backend.Backend.executeJSON(Backend.scala:96); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 20 times, most recent failure: Lost task 40.19 in stage 7.0 (TID 3171, seqr-loading-cluster-sw-z91p.c.seqr-project.internal, executor 14): is.hail.utils.HailException: cannot set missing field for required type +PCStruct{info:PCStruct{ALLELEID:PInt32}}; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:210); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:974); 	at is.hail.rvd.RVD$$anonfun$24$$anonfun$apply$17.apply(RVD.scala:967); 	at is.hail.utils.FlipbookIterator$$anon$5.<init>(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:174); 	at is.hail.utils.FlipbookIterator.map(FlipbookIterator.scala:145); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:967); 	at is.hail.rvd.RVD$$anonfun$24.apply(RVD.scala:963); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(KeyedRVD.scala:147); 	at is.hail.rvd.KeyedRVD$$anonfun$orderedLeftJoinDistinct$1.apply(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:51226,load,loading-cluster-sw-,51226,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loading-cluster-sw-']
Performance, 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:172); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:171); 	at is.hail.utils.package$.using(package.scala:657); 	at is.hail.backend.service.Worker$.main(Worker.scala:169); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.lang.NullPointerException: null; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionPutTask.call(JsonResumableSessionPutTask.java:201); 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:81); 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937:5390,concurren,concurrent,5390,https://hail.is,https://github.com/hail-is/hail/issues/13937,1,['concurren'],['concurrent']
Performance, 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:241); 	at is.hail.utils.package$.using(package.scala:669); 	at is.hail.io.index.IndexWriterUtils.writeMetadata(IndexWriter.scala:253); 	at __C511collect_distributed_array_table_native_writer.apply_region3_328(Unknown Source); 	at __C511collect_distributed_array_table_native_writer.apply(Unknown Source); 	at __C511collect_distributed_array_table_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:87); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:86); 	at is.hail.backend.service.Worker$.$anonfun$main$9(Worker.scala:198); 	at is.hail.services.package$.retryTransientErrors(package.scala:187); 	at is.hail.backend.service.Worker$.$anonfun$main$8(Worker.scala:197); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.service.Worker$.main(Worker.scala:195); 	at is.hail.backend.service.Main$.main(Main.scala:9); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14430:2243,concurren,concurrent,2243,https://hail.is,https://github.com/hail-is/hail/pull/14430,6,['concurren'],['concurrent']
Performance, 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5572,concurren,concurrent,5572,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance, 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5710,Load,LoadPlink,5710,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901:6780,concurren,concurrent,6780,https://hail.is,https://github.com/hail-is/hail/issues/3901,2,['concurren'],['concurrent']
Performance, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:8406,concurren,concurrent,8406,https://hail.is,https://github.com/hail-is/hail/issues/3465,5,['concurren'],['concurrent']
Performance, 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4114:3337,concurren,concurrent,3337,https://hail.is,https://github.com/hail-is/hail/issues/4114,1,['concurren'],['concurrent']
Performance, 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:5388,concurren,concurrent,5388,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['concurren'],['concurrent']
Performance," ### Make your app do ONLY server-side routing; Meaning every time you click on a link in your page, you hit the server, just like the first visited page. . Simply use `<a>` directly. ### Caching and sidecar requests; Broadly, there are three strategies: browser caching, server caching, and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:14902,perform,performance-cost-of-server-side-rendered-react-node-js,14902,https://hail.is,https://github.com/hail-is/hail/pull/5162,2,['perform'],"['performance', 'performance-cost-of-server-side-rendered-react-node-js']"
Performance," ### What went wrong (all error messages here, including the full java stack trace): HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ:Array[Float64],AS_MQRankSum:Array[Float64],AS_QD:Float64,AS_ReadPosRankSum:Array[Float64],AS_SOR:Array[Float64],BaseQRankSum:Float64,DP:Int32,DS:Boolean,ExcessHet:Float64,FS:Float64,HRun:Int32,HaplotypeScore:Float64,InbreedingCoeff:Float64,LikelihoodRankSum:Float64,MLEAC:Array[Int32],MLEAF:Array[Float64],MQ:Float64,MQ0:Int32,MQRankSum:Float64,OND:Float64,QD:Float64,RPA:Array[Int32],RU:String,ReadPosRankSum:Float64,ReverseComplementedAlleles:Boolean,SOR:Float64,STR:Boolean,SwappedAlleles:Boolean},a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{AB:Float64,AD:Array[+Int32],DP:Int32,GQ:Int32,GT:Call,MQ0:Int32,PL:Array[+Int32],SB:Array[+Int32]}}; after: Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{ABHet:Float64,ABHom:Float64,AC:Array[Int32],AF:Array[Float64],AN:Int32,AS_BaseQRankSum:Array[Float64],AS_FS:Array[Float64],AS_InbreedingCoeff:Array[Float64],AS_InsertSizeRankSum:Array[Float64],AS_MQ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4827:479,optimiz,optimization,479,https://hail.is,https://github.com/hail-is/hail/issues/4827,1,['optimiz'],['optimization']
Performance," (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthli",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6698,cache,cached,6698,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:3221,Load,LoadMatrix,3221,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance," (most recent call last):; File ""<ipython-input-7-ccfd397235b8>"", line 6, in raises; raise ValueError(message); ValueError: unretrieved case; ```. 5. And here is an example of ""retrieving"" the exception (by calling `Task.exception`). Notice we do not get the ""Task exception was never retrieved"" message. ```; In [8]: async def foo():; ...: async def raises(message):; ...: try:; ...: await asyncio.sleep(100); ...: finally:; ...: raise ValueError(message); ...:; ...: t = asyncio.create_task(raises('retrieved case')); ...: await asyncio.sleep(0) # let the other task run for a moment; ...: t.cancel(); ...: await asyncio.wait([t]); ...: print((t, t.done(), t.cancelled(), t.exception())); ...:; ...: asyncio.run(foo()); ...:; (<Task finished name='Task-599' coro=<foo.<locals>.raises() done, defined at <ipython-input-8-5fa396151822>:2> exception=ValueError('retrieved case')>, True, False, ValueError('retrieved case')); ```. ---. One more thing of interest, I confirmed that the ExitStack throws the first exception it encountered *after* performing all callbacks. ```ipython; In [6]: with ExitStack() as exit:; ...: def foo(i):; ...: print(str(i)); ...: raise ValueError(i); ...: exit.callback(foo, 1); ...: exit.callback(foo, 2); 2; 1; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[6], line 1; ----> 1 with ExitStack() as exit:; 2 def foo(i):; 3 print(str(i)). File ~/miniconda3/lib/python3.10/contextlib.py:576, in ExitStack.__exit__(self, *exc_details); 572 try:; 573 # bare ""raise exc_details[1]"" replaces our carefully; 574 # set-up context; 575 fixed_ctx = exc_details[1].__context__; --> 576 raise exc_details[1]; 577 except BaseException:; 578 exc_details[1].__context__ = fixed_ctx. File ~/miniconda3/lib/python3.10/contextlib.py:561, in ExitStack.__exit__(self, *exc_details); 559 assert is_sync; 560 try:; --> 561 if cb(*exc_details):; 562 suppressed_exc = True; 563 pending_raise = False. File ~/min",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13876:4837,perform,performing,4837,https://hail.is,https://github.com/hail-is/hail/pull/13876,1,['perform'],['performing']
Performance," **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 890 exprs, named_exprs, self._row_indices,; 891 protect_keys=True); --> 892 return self._select('Table.select', value_struct=hl.struct(**row)); 893 ; 894 @typecheck_method(exprs=oneof(str, Expression)). ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct, do_process_joins); 433 ; 434 if do_process_joins:; --> 435 base, cleanup = self._process_joins(row); 436 analyze(caller, row, self._row_indices); 437 else:. ~/hail/python/hail/table.py in _process_joins(self, *exprs); 1484 def broadcast_f(left, data, jt):; 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1486 return process_joins(self, exprs, broadcast_f); 1487 ; 1488 def cache(self):. ~/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 364 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); 365 data_json = t._to_json(data); --> 366 left = broadcast_f(left, data_json, t._jtype); 367 ; 368 def cleanup(table):. ~/hail/python/hail/table.py in broadcast_f(left, data, jt); 1483 def _process_joins(self, *exprs):; 1484 def broadcast_f(left, data, jt):; -> 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); 1486 return process_joins(self, exprs, broadcast_f); 1487 . ~/apache/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. ~/hail/python/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3785:3791,cache,cache,3791,https://hail.is,https://github.com/hail-is/hail/issues/3785,1,['cache'],['cache']
Performance," -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2312,Optimiz,Optimize,2312,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance," -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 roo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2451,Optimiz,Optimize,2451,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance, --gvcf-gq-bands 17 --gvcf-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --gvcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 --gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47 --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --indel-size-to-eliminate-in-ref-model 10 --use-alleles-trigger false --disable-optimizations false --just-determine-active-regions false --dont-genotype false --max-mnp-distance 0 --dont-trim-active-regions false --max-disc-ar-extension 25 --max-gga-ar-extension 300 --padding-around-indels 150 --padding-around-snps 20 --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 --recover-dangling-heads false --do-not-recover-dangling-branches false --min-dangling-branch-length 4 --consensus false --max-num-haplotypes-in-population 128 --error-correct-kmers false --min-pruning 2 --debug-graph-transformations false --kmer-length-for-read-error-correction 25 --min-observations-for-kmer-to-be-solid 20 --likelihood-calculation-engine PairHMM --base-quality-score-threshold 18 --pair-hmm-gap-continuation-penalty 10 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --native-pair-hmm-threads 4 --native-pair-hmm-use-double-precision fal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:2226,optimiz,optimizations,2226,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['optimiz'],['optimizations']
Performance," 140.241861 ms; 2018-10-09 15:04:36 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 1015 bytes result sent to driver; 2018-10-09 15:04:36 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 395 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:36 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:36 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.412 s; 2018-10-09 15:04:36 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.679005 s; 2018-10-09 15:04:36 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: table8508c46074; 2018-10-09 15:04:36 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: CACHE TABLE `table8508c46074`; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: `table8508c46074`; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 15.850234 ms; 2018-10-09 15:04:37 CodeGenerator: INFO: Code generated in 9.347112 ms; 2018-10-09 15:04:37 SparkContext: INFO: Starting job: sql at NativeMethodAccessorImpl.java:0; 2018-10-09 15:04:37 DAGScheduler: INFO: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 15:04:37 DAGScheduler: INFO: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions; 2018-10-09 15:04:37 DAGScheduler: INFO: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 15:04:37 DAGScheduler: INFO: Parents of final stage: List(ShuffleMapStage 1); 2018-10-09 15:04:37 DAGScheduler: INFO: Missing parents: List(ShuffleMapStage 1); 2018-10-09 15:04:37 DAGScheduler: INFO: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents; 2018-10-09 15:04:37 MemoryStore: INFO: Block",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:18283,CACHE,CACHE,18283,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['CACHE'],['CACHE']
Performance," 1; CHECKCAST [Ljava/lang/Object;; ALOAD 2; CHECKCAST scala/collection/mutable/ArrayBuffer; INVOKESPECIAL is/hail/codegen/generated/C0.apply ([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; ARETURN; MAXSTACK = 3; MAXLOCALS = 3; }; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-297>"", line 2, in filter_genotypes; File ""/tmp/spark-0721abd3-c72d-4439-a655-c09fddad864c/userFiles-7c41df44-c5b2-44b1-924e-8f73a9aa8148/hail.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Java stack trace:; java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:10710,load,loadOrDefineClass,10710,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadOrDefineClass']
Performance," 233s, 233s; linreg with 10 PCs, 8 cores: 23s, 24s, 24s; pca, 8 cores: 179s. hail command:; ~/hail/build/install/hail/bin/hail read -i ~/data/profile75.vds linreg -c ~/data/profile75.cov -f ~/data/profile.fam -o ~/data/profile75.linreg. read: 1570.888546; linreg: 58496.508588. plink vcf command to create bed/bim/fam:; ./plink --vcf ~/data/profile75.vcf.bgz; - rename plink.bed/bim/fam to profile75.bed/bim/fam; - create covar with FID column by doubling first column of cov file (use cut and paste in bash). plink linreg command:; time ./plink --bfile profile75 --double-id --pheno ~/data/profile.pheno --allow-no-sex --covar ~/data/profile75.covar --linear --out ~/data/profile75.plink. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/profile75.plink.log.; Options in effect:; --allow-no-sex; --bfile profile75; --covar /Users/Jon/data/profile75.covar; --double-id; --linear; --out /Users/Jon/data/profile75.plink; --pheno /Users/Jon/data/profile.pheno. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 74885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/profile75.plink.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you may want; to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.914041.; 74885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/profile75.plink.assoc.linear ... done. real 0m38.837s; user 0m38.609s; sys 0m0.187s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/61:1286,load,loaded,1286,https://hail.is,https://github.com/hail-is/hail/pull/61,3,['load'],['loaded']
Performance," 2619 ; 2620 Examples; (...); 2628 Number of rows, number of cols.; 2629 """"""; 2630 count_ir = ir.MatrixCount(self._mir); -> 2631 return Env.backend().execute(count_ir). File ~/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py:180, in Backend.execute(self, ir, timed); 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; --> 180 raise e.maybe_user_error(ir) from None; 181 if ir.typ == tvoid:; 182 value = None. File ~/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py:178, in Backend.execute(self, ir, timed); 176 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 177 try:; --> 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; 180 raise e.maybe_user_error(ir) from None. File ~/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py:214, in Py4JBackend._rpc(self, action, payload); 212 if resp.status_code >= 400:; 213 error_json = orjson.loads(resp.content); --> 214 raise fatal_error_from_java_error_triplet(error_json['short'], error_json['expanded'], error_json['error_id']); 215 return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: FileNotFoundException: File not found: gs://danking/chr*.vcf. Java stack trace:; java.io.FileNotFoundException: File not found: gs://danking/chr*.vcf; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:984); 	at is.hail.io.fs.HadoopFS.fileListEntry(HadoopFS.scala:175); 	at is.hail.io.fs.HadoopFS.fileListEntry(HadoopFS.scala:87); 	at is.hail.io.fs.FS.fileListEntry(FS.scala:417); 	at is.hail.io.fs.FS.fileListEntry$(FS.scala:417); 	at is.hail.io.fs.HadoopFS.fileListEntry(HadoopFS.scala:87); 	at is.hail.expr.ir.analyses.SemanticHash$.getFileHash(SemanticHash.scala:373); 	at is.hail.expr.ir.analyses.SemanticHash$.$anonfun$encode$18(SemanticHash.scala:198); 	at scala.collection.immutable.List.foreach(List.scala:431); 	at is.hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13915:3968,load,loads,3968,https://hail.is,https://github.com/hail-is/hail/issues/13915,1,['load'],['loads']
Performance," </li>; <li>; <p>Determine MPO size from markers, not EXIF data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7884"">#7884</a>; [radarhere]</p>; </li>; <li>; <p>Improved conversion from RGB to RGBa, LA and La <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7888"">#7888</a>; [radarhere]</p>; </li>; <li>; <p>Support FITS images with GZIP_1 compression <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7894"">#7894</a>; [radarhere]</p>; </li>; <li>; <p>Use I;16 mode for 9-bit JPEG 2000 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7900"">#7900</a>; [scaramallion, radarhere]</p>; </li>; <li>; <p>Raise ValueError if kmeans is negative <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7891"">#7891</a>; [radarhere]</p>; </li>; <li>; <p>Remove TIFF tag OSUBFILETYPE when saving using libtiff <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7893"">#7893</a>; [radarhere]</p>; </li>; <li>; <p>Raise ValueError for negative values when loading P1-P3 PPM images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7882"">#7882</a>; [radarhere]</p>; </li>; <li>; <p>Added reading of JPEG2000 palettes <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7870"">#7870</a>; [radarhere]</p>; </li>; <li>; <p>Added alpha_quality argument when saving WebP images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7872"">#7872</a>; [radarhere]</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/python-pillow/Pillow/commit/5c89d88eee199ba53f64581ea39b6a1bc52feb1a""><code>5c89d88</code></a> 10.3.0 version bump</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/63cbfcfdea2d163ec93bae8d283fcfe4b73b5dc7""><code>63cbfcf</code></a> Update CHANGES.rst [ci skip]</li>; <li><a href=""https://github.com/python-pillow/Pillow/commit/2776126aa9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:12637,load,loading,12637,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance," <ul>; <li>Setting or accessing <code>json_encoder</code> or <code>json_decoder</code> raises a; deprecation warning. :issue:<code>4732</code></li>; </ul>; <h2>Version 2.2.0</h2>; <p>Released 2022-08-01</p>; <ul>; <li>; <p>Remove previously deprecated code. :pr:<code>4667</code></p>; <ul>; <li>Old names for some <code>send_file</code> parameters have been removed.; <code>download_name</code> replaces <code>attachment_filename</code>, <code>max_age</code>; replaces <code>cache_timeout</code>, and <code>etag</code> replaces <code>add_etags</code>.; Additionally, <code>path</code> replaces <code>filename</code> in; <code>send_from_directory</code>.</li>; <li>The <code>RequestContext.g</code> property returning <code>AppContext.g</code> is; removed.</li>; </ul>; </li>; <li>; <p>Update Werkzeug dependency to &gt;= 2.2.</p>; </li>; <li>; <p>The app and request contexts are managed using Python context vars; directly rather than Werkzeug's <code>LocalStack</code>. This should result; in better performance and memory use. :pr:<code>4682</code></p>; <ul>; <li>Extension maintainers, be aware that <code>_app_ctx_stack.top</code>; and <code>_request_ctx_stack.top</code> are deprecated. Store data on; <code>g</code> instead using a unique prefix, like; <code>g._extension_name_attr</code>.</li>; </ul>; </li>; <li>; <p>The <code>FLASK_ENV</code> environment variable and <code>app.env</code> attribute are; deprecated, removing the distinction between development and debug; mode. Debug mode should be controlled directly using the <code>--debug</code>; option or <code>app.run(debug=True)</code>. :issue:<code>4714</code></p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pallets/flask/commit/a1c478bc93d3dc018a6e7a1ba3cf5409553c9df3""><code>a1c478b</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/pallets/flask/issues/4755"">#4755</a> from",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12206:6606,perform,performance,6606,https://hail.is,https://github.com/hail-is/hail/pull/12206,1,['perform'],['performance']
Performance," BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1070); at io.netty.uti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2302,concurren,concurrent,2302,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance," ClientSecretCredential.get_token succeeded; 2023-09-06T21:45:25 INFO batch_client.aioclient aioclient.py:809:_submit created batch 191; submit job bunches ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 1/1 0:00:00 0:00:00; 2023-09-06T21:47:17 WARNING hailtop.utils utils.py:842:retry_transient_errors_with_debug_string A transient error occured. We will automatically retry. Do not be alarmed. We have thus far seen 2 transient errors (next delay: 3.794s). The most recent error was <class 'asyncio.exceptions.TimeoutError'> . . +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (140387515627072) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++; FAILED; _________________ test_always_run_job_private_instance_cancel __________________. client = <hailtop.batch_client.client.BatchClient object at 0x7fae899806a0>. def test_always_run_job_private_instance_cancel(client: BatchClient):; b = create_batch(client); resources = {'machine_type': smallest_machine_type()}; j = b.create_job(DOCKER_ROOT_IMAGE, ['true'], resources=resources, always_run=True); b.submit(); b.cancel(); > status = j.wait(). io/test/test_batch.py:1487: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; usr/local/lib/python3.9/dist-packages/hailtop/batch_client/client.py:84: in wait; return async_to_blocking(self._async_job.wait()); usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py:156: in async_to_blocking; return loop.run_until_complete(task); usr/lib/python3.9/a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13582:1635,concurren,concurrent,1635,https://hail.is,https://github.com/hail-is/hail/issues/13582,1,['concurren'],['concurrent']
Performance," ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ConvertToSafeValue' (2): 12.828ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'InitializeCompiledFunction' (2): 14.536ms; 2019-11-06 18",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2746,Optimiz,Optimize,2746,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance," I learned early that the issue was somehow related to method splitting, but it was far more devious than I expected, and the code in LIR method splitting is completely correct. The particular case I was debugging had expected behavior when trying to write the missing bits of four fields A,B,C,D, where A and B were missing and C and D were present. These should have written the byte. 1<<0 | 1<<1 | 0<<2 | 0<<3; ==> b00000011; ==> 3. But instead wrote the byte `b00000001 or 1`, incorrectly leading readers to try to read field B when it was missing (and not written). This is due to the load-bearing and incorrect type of an I2B instruction generated [here](https://github.com/hail-is/hail/blob/8bd9b7b2224b77372a72f02f2b13806267892a35/hail/src/main/scala/is/hail/types/encoded/EBaseStruct.scala#L107). I2B is an instruction that truncates an integer to a byte, and it is used in various places in code generation but primarily encoding missing bits in arrays and structs. . I2B loads a byte to the stack, not a boolean. TypeInfos are mostly non-structural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:1062,load,loads,1062,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['loads']
Performance, JVMEntryway: INFO: 8: gs://cpg-bioheart-hail/batch-tmp/tmp/hail/sRjJqvkZ3l9nmKuUErfNZv/jHpWQ6lemx/out; 2024-11-05 02:43:37.202 JVMEntryway: INFO: Yielding control to the QoB Job.; 2024-11-05 02:43:37.206 ServiceBackendAPI$: INFO: BatchClient allocated.; 2024-11-05 02:43:37.207 ServiceBackendAPI$: INFO: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurren,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2683,concurren,concurrent,2683,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance," MatrixTable, GroupedMatrixTable # noqa: E402. /opt/conda/lib/python3.7/site-packages/hail/table.py in <module>; 4 import numpy as np; ----> 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence. ModuleNotFoundError: No module named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""; --> 222 return pc.load(handles.handle, encoding=None); 223 except UnicodeDecodeError:; 224 # e.g. can occur for files written in py27; see GH#28645 and GH#31988. /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in load(fh, encoding, is_verbose); 272 up.is_verbose = is_verbose; 273 ; --> 274 return up.load(); 275 except (ValueError, TypeError):; 276 raise. /opt/conda/lib/python3.7/pickle.py in load(self); 1086 raise EOFError; 1087 assert isinstance(key, bytes_types); -> 1088 dispatch[key[0]](self); 1089 except _Stop as stopinst:; 1090 return stopinst.value. /opt/conda/lib/python3.7/pickle.py in load_stack_global(self); 1383 if type(name) is not str or type(module) is not str:; 1384 raise UnpicklingError(""STACK_GLOBAL requires str""); -> 1385 self.append(self.find_class(module, name)); 1386 dispatch[STACK_GLOBAL[0]] = load_stack_global; 1387 . /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in find_class(self, module, name); 204 key = (module, name); 205 module, name = _class_locations_map.get(key, key); --> 206 return super().find_class(module, name); 207 ; 208 . /opt/conda/lib/python3.7/pickle.py in find_class(self, module, name); 1424 elif module in _compat_pickle.IMPORT_MAPPING:; 1425 modu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:2928,load,load,2928,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance," Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4810 bytes); 2018-10-09 14:46:41 Executor: INFO: Running task 0.0 in stage 0.0 (TID 0); 2018-10-09 14:46:41 Executor: INFO: Fetching spark://10.32.119.167:61636/jars/sparklyr-2.2-2.11.jar with timestamp 1539121597559; 2018-10-09 14:46:41 TransportClientFactory: INFO: Successfully created connection to /10.32.119.167:61636 after 11 ms (0 ms spent in bootstraps); 2018-10-09 14:46:41 Utils: INFO: Fetching spark://10.32.119.167:61636/jars/sparklyr-2.2-2.11.jar to /private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-02128b51-f37e-4798-84bb-d3e3819e51be/userFiles-7053b92c-9117-46b6-8c52-752fee2701e9/fetchFileTemp5171194947676284646.tmp; 2018-10-09 14:46:41 Executor: INFO: Adding file:/private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-02128b51-f37e-4798-84bb-d3e3819e51be/userFiles-7053b92c-9117-46b6-8c52-752fee2701e9/sparklyr-2.2-2.11.jar to class loader; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 142.013327 ms; 2018-10-09 14:46:41 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 972 bytes result sent to driver; 2018-10-09 14:46:41 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 406 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:41 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:41 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.420 s; 2018-10-09 14:46:41 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.669318 s; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: table7e606a8b83f4; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: CACHE TABLE `tabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:34788,load,loader,34788,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['load'],['loader']
Performance," When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## Solution. This PR addresses the two goals outlined above and does so through using Envoy, a load-balancer/proxy that is well-suited to this sort of highly-dynamic cluster configuration. Envoy does not have the constraint that all upstream services must be available at start-time, and has a very convenient API for updating the cluster configuration without the need for restarting the process or dropping traffic. This makes regularly updating the cluster configuration whenever new test namespaces are created relatively straightforward and non-disruptive to traffic in other namespaces. The high-level approach is as follows:. 1. Envoy-based gateways and internal-gateways will load their routing configuration from a Kubernetes ConfigMap, which they watch for changes and reconcile their configuration when the ConfigMap changes. The ConfigMap can be populated with a manual deploy and is populated from the beginning with production routes (i.e. batch.hail.is gets routed to batch.default); 2. When running CI, CI will regularly update the ConfigMap with additional routes based on which inte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:3625,load,load-balancer,3625,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load-balancer']
Performance," Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2355__l2315split_large_block Z; 11332 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:5629,load,loaded,5629,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['loaded']
Performance," Zain (1) +</li>; </ul>; <p>A total of 14 people contributed to this release.; People with a &quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/scipy/scipy/commit/656076ca6b490f587e9bd9c4cd10cb259a687c5b""><code>656076c</code></a> MAINT: wheel push 1.9.2 [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/ad0d0f907010fbc8b66cdbe8ce0af2683881a309""><code>ad0d0f9</code></a> REL: set 1.9.2 released [wheel build]</li>; <li><a href=""https://github.com/scipy/scipy/commit/d9ad9801323653a2015b4d3e80d6d3ea93b6c021""><code>d9ad980</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17150"">#17150</a> from tylerjereddy/treddy_scipy_192_more_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/6b098c25223e224ff44101f86bbc86efecffe1d9""><code>6b098c2</code></a> TST: optimize.milp: remove problematic timeout/iteration test</li>; <li><a href=""https://github.com/scipy/scipy/commit/24dce9760b87934f1be046ec817c758b0f3952dc""><code>24dce97</code></a> DOC: stats.pearsonr: typo in coeffic<em>i</em>ent (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17153"">#17153</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/a6ba7cad3b54c35d2ccb55c595691689004742c1""><code>a6ba7ca</code></a> MAINT: misc 1.9.2 updates</li>; <li><a href=""https://github.com/scipy/scipy/commit/ed9760e60a28b8f13e5644494033e2dab9aafbcd""><code>ed9760e</code></a> MAINT: stats.pearson3: fix ppf for negative skew (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17055"">#17055</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/6fb67007dd7105755057f3379fb7ef423eae524e""><code>6fb6700</code></a> FIX: optimize.milp: return feasible solution if available on timeout/node lim...</li>; <li><a href=""https",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12352:1931,optimiz,optimize,1931,https://hail.is,https://github.com/hail-is/hail/pull/12352,1,['optimiz'],['optimize']
Performance," `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(Heart",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:1854,load,loadClass,1854,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['load'],['loadClass']
Performance," ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:1059,Load,LoadVCF,1059,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance," a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corresponding change to the; `global-config` secret. We'll should notify Leo et al. about the Terraform changes. ---. Small changes and fixes:. - Rename `key.json` to `/worker-key.json` for clarity, this is the worker's own GCP service account; key.; - Create a test query-gsa-key in test and dev namespaces.; - Add terraform rules for the query service account. It already existed, but it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1533,load,load,1533,https://hail.is,https://github.com/hail-is/hail/pull/10279,1,['load'],['load']
Performance," able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; -----------. ```; In [1]: %%time; ...: import hail as hl; ...: mt = hl.balding_nichols_model(n_populations=2,; ...: n_variants=10000,; ...: n_samples=10000,; ...: n_partitions=100); ...: mt = mt.select_entries(gt = hl.float(mt.GT.n_alt_alleles())); ...: da = hl.experimental.dnd.array(mt, 'gt'); ...: da.write('/tmp/in.da', overwrite=True); In [3]: %%time; ...: bm = hl.linalg.BlockMatrix.from_entry_expr(mt.gt); In [5]: %%time; ...: (bm @ bm.T).write('/tmp/foo.bm', overwrite=True); In [7]: %%time; ...: import hail as hl; ...: da = hl.experimental.dnd.read('/tmp/in.da'); ...: (da @ da.T).write('/tmp/out.da', overwrite=True); ```. Block matrix performed the matrix multiply in 19.3s. DNDArray performed the; matrix multiply in 37.6s. Block Matrix:; ![Screen Shot 2020-05-26 at 1 37 51 PM](https://user-images.githubusercontent.com/106194/82932367-54630200-9f56-11ea-86f4-94726c36d727.png). DNDArray:; ![Screen Shot 2020-05-26 at 1 37 08 PM](https://user-images.githubusercontent.com/106194/82932387-5927b600-9f56-11ea-820e-97d1eb443d8f.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864:2644,perform,performed,2644,https://hail.is,https://github.com/hail-is/hail/pull/8864,2,['perform'],['performed']
Performance," already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start\n data=kwargs\n File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query\n json.loads(what.decode('utf8')))\naiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown')\n""; }; },; ""start_time"": 1580760856630,; ""end_time"": 1580760867038; },; ""spec"": {; ""command"": [; ""/bin/bash"",; ""-c"",; ""set -e; mkdir -p /io/pipeline/pipeline-3dea50d54013/__TASK__18/; /bin/true""; ],; ""image"": ""ubuntu:18.04"",; ""job_id"": 19,; ""mount_docker_socket"": false,; ""resources"": {; ""cpu"": ""0.001"",; ""memory"": ""375M""; },; ""secrets"": [; {; ""namespace"": ""dking"",; ""name"": ""dking-gsa-key"",; ""mount_path"": ""/gsa-key"",; ""mount_in_copy"": true; }; ],; ""env"": []; },; ""attributes"": {; ""name"": ""18""; }; }; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8029:2208,load,loads,2208,https://hail.is,https://github.com/hail-is/hail/issues/8029,1,['load'],['loads']
Performance, an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6(ServiceBackend.scala:462); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:461); 	at is.hail.utils.package$.usi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4654,concurren,concurrent,4654,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance," and as a result the gateways will create and terminate a TCP connection per http request. This likely causes minor delays on the front-end through gateway, but this hampers performance greatly in job scheduling. The batch driver is forced to establish a new TCP connection and do an SSL handshake with the internal-gateway multiple times per job, which is expensive and slow. We currently have to dedicate a 2-core NGINX sidecar for the batch-driver just to terminate TLS with internal-gateway and free up cycles in the batch-driver python process. By using proper persistent connections, we can reduce the TLS overhead to single-digit percents of a core. This leads to the first goal of this transition: configure our load balancers to know the full cluster configuration at any point in time so they can properly maintain connection pools with upstream services. However, this is not the only problem. Each ""upstream"" Service in Kubernetes may consist of multiple underlying pods but Kubernetes Services as we use them don't provide proper load-balancing when mixed with persistent connections. When we declare a Service for say, batch in default, Kubernetes adds a DNS record for `batch.default` that resolves to a single IP pointing at kube-proxy. When a new TCP connection is established with kube-proxy for that IP, it rolls the dice (using `iptables`) and assigns that connection to a particular pod to which it will forward all subsequent packets. From the load-balancer's perspective, there is only one IP address, and only one place to open connections. The load-balancer doesn't have the information to actually load-balance once we have a functioning connection pool. This can lead to really unbalanced scenarios when preemptible pods come and go. This leads to our second goal: instead of routing all requests through kube-proxy, use Kubernetes Headless Services to expose all pod IPs underlying a Service so that our proxies can properly load-balance across persistent connections. ## S",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095:2576,load,load-balancing,2576,https://hail.is,https://github.com/hail-is/hail/pull/12095,1,['load'],['load-balancing']
Performance," and service-worker caching. In this project we will likely use all three. Server caching is an excellent strategy for pages that serve only public data. In this strategy we pre-generate the static html, serve that, and invalidate the cache once in a while. An example of this can be found in https://github.com/hail-is/hail/pull/5162/commits/e131a931c58a204104d45d0010341423b1ab9500; * Care needs to be taken with the server-side option, not to leak authentication state, since this will, at least by default, be shared across all users. . # Styleguide; 1. Typescript everywhere. # Performance; 1. [React SSR vs Nunjucks](https://malloc.fi/performance-cost-of-server-side-rendered-react-node-js) ; * [React SSR performance (well, React DOM in general) is a focus for 2019](https://github.com/facebook/react/issues/13525); ![v2-chart-1](https://user-images.githubusercontent.com/5543229/51345305-9af24380-1a68-11e9-8f5c-024ca96e42c1.png); 2. React vs VanillaJS. Depends on what you measure, it's either 50% slower or many times faster.; * https://github.com/krausest/js-framework-benchmark; * React authors claim this is an unrealistic environment, and that their scheduler is tuned to provide smooth/non-hitching UI interactions, at some cost to the speed with which 100,000 elements can be appended to a page. ; * Some consider this to be more reliable: https://localvoid.github.io/uibench/; * Here React performs many times better than vanilla JS for some operations.; * I should probably figure out exactly why. In practice, React in 2019 will likely be the best performing UI solution available, with of course the exception of some very well optimized JS. This is because of React Fiber's time slice mode, which will effectively allow UI operations, like user input, [to preempt other operations](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html). ### How React works; React Fiber, the new reconciling/scheduling algorithm: https://github.com/acdlite/react-fiber-architecture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:15438,tune,tuned,15438,https://hail.is,https://github.com/hail-is/hail/pull/5162,4,"['optimiz', 'perform', 'tune']","['optimized', 'performing', 'performs', 'tuned']"
Performance," applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://192.168.248.80:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.100-2ea2615a797a; LOGGING: writing to /; --------------------------------------------------------------------------; mt.filter_rows(mt.locus.position==2867101).count_rows(); ```; ### Expected ; Return a count of rows with that condition. ### Error ; ```; FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:208); at is.hail.expr.ir.LoweredTableReader$.makeCoercer(TableIR.scala:135); at is.hail.expr.ir.GenericTableValue.getLTVCoercer(GenericTableValue.scala:137); at is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:162); at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1798); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:717); at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:697); at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:903); at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:467); at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:472); at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:73); at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.evaluate$1(LowerOrInterpretNonCompilable.scala:27); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:67); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53); at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpre",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280:2405,Load,LoadVCF,2405,https://hail.is,https://github.com/hail-is/hail/issues/12280,1,['Load'],['LoadVCF']
Performance," at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13270,Load,LoadVCF,13270,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance," at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <details>; <summary>The last TableIR logged</summary>. ```; 2023-10-13 02:14:44.213 : INFO: after optimize: darrayLowerer, after LowerAndExecuteShuffles: IR size 232: . !ht = TableRead [Table{global:Struct{},key:[locus,alleles],row:Struct{locus:Locus(GRCh38),alleles:Array[String],filters:Set[String],a_index:Int32,was_split:Boolean,variant_qc:Struct{gq_stats:Struct{mean:Float64,stdev:Float64,min:Float64,max:Float64},call_rate:Float64,n_called:Int64,n_not_called:Int64,n_filtered:Int64,n_het:Int64,n_non_ref:Int64,het_freq_hwe:Float64,p_value_hwe:Float64,p_value_excess_het:Float64},info:Struct{AC:Array[Int32],AF:Array[Float64],AN:Int32,homozygote_count:Array[Int32]},`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[Struct{GT:Call,GQ:Int32,RGQ:Int32,FT:String,AD:Array[Int32]}]}}, False, (TableNativeZippedReader gs://prod-drc-broad/aou-wgs-delta-small_callsets_gq0/v7.1/acaf_threshold_v7.1/splitMT/delta_basis_without_ext_aian_prod_gq0_3regions.acaf_threshold.split.mt/rows gs://prod-drc-broad/aou-wgs-delta-small_callsets_gq0/v7.1/acaf_threshold_v7.1/splitMT/delta_basis_without_ext_aian_prod_gq0_3regions.acaf_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:7722,optimiz,optimize,7722,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['optimiz'],['optimize']
Performance, at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:807); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:796); at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$create$extension(RichHadoopConfiguration.scala:24); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:296); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:45); at is.hail.io.reference.FASTAReader$$anonfun$setup$1.apply(FASTAReader.scala:44); at is.hail.utils.package$.using(package.scala:587); at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:293); at is.hail.io.reference.FASTAReader$.setup(FASTAReader.scala:44); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at is.hail.io.reference.FASTAReader$$anonfun$getLocalFastaFileName$1.apply(FASTAReader.scala:30); at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:901); at is.hail.io.reference.FASTAReader$.getLocalFastaFileName(FASTAReader.scala:30); at is.hail.io.reference.SerializableReferenceSequenceFile.value$lzycompute(FASTAReader.scala:18); at is.hail.io.reference.SerializableReferenceSequenceFile.value(FASTAReader.scala:17); at is.hail.io.reference.FASTAReader.<init>(FASTAReader.scala:77); at is.hail.variant.ReferenceGenome.addSequenceFromReader(ReferenceGenome.scala:354); at is.hail.codegen.generated.C2.method2(Unknown Source); at is.hail.codegen.generated.C2.apply(Unknown Source); at is.hail.codegen.generated.C2.apply(Unknown Source); at is.hail.expr.ir.TableFilter$$anonfun$execute$3.apply(TableIR.scala:286); at is.hail.expr.ir.TableFilter$$anonfun$execute$3.apply(TableIR.scala:286); at is.hail.expr.ir.TableValue$$anonfun$6.apply(TableValue.scala:54); at is.hail.expr.ir.TableValue$$anonfun$6.apply(TableValue.scala:54); at is.hail.rvd.RVD$$anonfun$filterWithContext$1$$anonfun$apply$9.apply(RVD.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5371:1604,concurren,concurrent,1604,https://hail.is,https://github.com/hail-is/hail/issues/5371,1,['concurren'],['concurrent']
Performance," backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for the duration of the execution of an alternative version of Hail. ---. I've already updated the cluster with the new bucket and the corres",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:1103,load,loadClass,1103,https://hail.is,https://github.com/hail-is/hail/pull/10279,2,['load'],"['loadClass', 'loading']"
Performance," bb bf`, is converted by Java into the UTF-16 BOM, `fe ff`. This is apparently [a well known Java bug](https://stackoverflow.com/questions/1835430/byte-order-mark-screws-up-file-reading-in-java)? This looks pretty annoying to fix in Scala/Java because we'd have to muck around with Spark's `hadoopFile` infrastructure to figure out where it is actually reading from a file. ```; # hexdump /tmp/bar; 0000000 ef bb bf 73 61 6d 70 6c 65 5f 69 64 0a 66 6f 6f; 0000010 0a ; 0000011; # ipython; import hail asPython 3.7.3 (default, Mar 27 2019, 09:23:15) ; Type 'copyright', 'credits' or 'license' for more information; IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl ; hl.import_; In [2]: t = hl.import_table('/tmp/bar') ; ...: t.describe() ; ...: t = t.key_by('sample_id') ; Initializing Spark and Hail with default parameters...; using hail jar at /usr/local/lib/python3.7/site-packages/hail/hail-all-spark.jar; 19/06/13 14:08:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.1; SparkUI available at http://wm06b-953.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.14-5cb00c115421; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20190613-1408-0.2.14-5cb00c115421.log; 2019-06-13 14:08:15 Hail: INFO: Reading table with no type imputation; Loading column '?sample_id' as type 'str' (type not specified). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; '﻿sample_id': str ; ----------------------------------------; Key: []; ----------------------------------------; --------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6342:1186,load,load,1186,https://hail.is,https://github.com/hail-is/hail/issues/6342,1,['load'],['load']
Performance," because this PR addresses questions of state and will take care of this. ```python; self.exit_code = pod.status.container_statuses[0].state.terminated.exit_code; ```. We should probably do something like . ```python; self.exit_code = max(status.state.terminated.exit_code for status in pod.status.container_statuses); ```; although I also see that in update_job_with_pod we effectively restrict to a single container. I'm not sure why this limit exists, but if needed, should probably occur during creation. In the upcoming PR, which moves state to MySQL 5.7+, and a different server model (async), I think it would be neat to represent meta-state (across all containers, and potentially the job subgraph whose first node is the inspected job) as:. ```go; const (; 	Cancelled = -3; 	Initialized = -2; 	Created = -1; ); ```. with values >=0 being the maximum of the linux error codes, 0-255, of the subgraph. Simple queries. Alternative is to use NULL when not completed, but when used in a client would require a null check, or potentially have surprising side effects (i.e where the default value is 0). We could also use a separate, text-based status field, but I will store a queryable JSON field containing the full status as well. In a similar vein, we have some state race conditions. For instance:. ```python; self.pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(generate_name='job-{}-'.format(self.id),; labels={; 'app': 'batch-job',; 'hail.is/batch-instance': instance_id,; 'uuid': uuid.uuid4().hex; }),; spec=pod_spec). self._pod_name = None; self.exit_code = None. self._state = 'Created'; log.info('created job {}'.format(self.id)). self._create_pod(); ```. Here, every time pod creation fails, _state will be misaligned, and will have potential side effects (say in get_log). One solution could be to validate and rewind state in _create_pod. In any case, I will do my best to address state questions in the upcoming PR, and will close this when / if we approve it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5118:1358,race condition,race conditions,1358,https://hail.is,https://github.com/hail-is/hail/issues/5118,1,['race condition'],['race conditions']
Performance," by; their names contributed a patch for the first time.</p>; <ul>; <li>Bas van Beek</li>; <li>Charles Harris</li>; <li>Khem Raj +</li>; <li>Mark Harfouche</li>; <li>Matti Picus</li>; <li>Panagiotis Zestanakis +</li>; <li>Peter Hawkins</li>; <li>Pradipta Ghosh</li>; <li>Ross Barnowski</li>; <li>Sayed Adel</li>; <li>Sebastian Berg</li>; <li>Syam Gadde +</li>; <li>dmbelov +</li>; <li>pkubaj +</li>; </ul>; <h2>Pull requests merged</h2>; <p>A total of 17 pull requests were merged for this release.</p>; <ul>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22965"">#22965</a>: MAINT: Update python 3.11-dev to 3.11.</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22966"">#22966</a>: DOC: Remove dangling deprecation warning</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22967"">#22967</a>: ENH: Detect CPU features on FreeBSD/powerpc64*</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22968"">#22968</a>: BUG: np.loadtxt cannot load text file with quoted fields separated...</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22969"">#22969</a>: TST: Add fixture to avoid issue with randomizing test order.</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22970"">#22970</a>: BUG: Fix fill violating read-only flag. (<a href=""https://redirect.github.com/numpy/numpy/issues/22959"">#22959</a>)</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22971"">#22971</a>: MAINT: Add additional information to missing scalar AttributeError</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22972"">#22972</a>: MAINT: Move export for scipy arm64 helper into main module</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22976"">#22976</a>: BUG, SIMD: Fix spurious invalid exception for sin/cos on arm64/clang</li>; <li><a href=""https://redirect.github.com/numpy/numpy/pull/22989"">#22989</a>: BUG: Ensure correct loop order in sin, cos, and arctan2</li>; <li><a href=""https://redirect.github",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12898:1537,load,loadtxt,1537,https://hail.is,https://github.com/hail-is/hail/pull/12898,2,['load'],"['load', 'loadtxt']"
Performance," cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4762,cache,cached,4762,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5578,cache,cached,5578,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # <a name=""parray""></a> PDict. An abstract class for immutable unordered collections of key-value pairs. All keys must have one PType, and all values must have one (possibly differe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:4973,load,loadElement,4973,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadElement']
Performance," deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Itera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:3175,Load,LoadMatrix,3175,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance," determine which concrete file system to use. For example, a; router fs can `open` both `gs://danking/abc` and `s3a://danking/abc`. Each Hail Query Python Backend is associated with one file system class. This PR associates the; ServiceBackend with `RouterFS`, enabling `hl.current_backend().fs.open`, `hl.hadoop_open`, etc. to; read from S3, GCS, ABS, and the local file system. We should deprecate `hail.utils.hadoop_utils`; because it is not Hadoop-specific. We should instead advertise the class-based `hail.fs` or create a; new function-based interface (e.g. `hl.fs.open(...)`. # Test Clean-up. The Hail Query local and spark tests should now work in Azure. I moved all the `hail.fs` and; `hailtop.aiotools.fs` tests into two build.yaml steps: `test_hail_python_fs` and; `test_hail_scala_fs`. These tests are exhaustive: they test every file system: S3, ABS, and GCS. The only file system tests that remain in the Hail Query tests are the tests of; `hail.utils.hadoop_utils`. The hadoop tests are not exhaustive: they only test the *current* file; system. In Azure, they test ABS. In Google, they test GCS. I have not decided yet if we should enable the hail python tests in Azure. It seems mostly wasteful. # Local Cache. I added a local cache directory. It defaults to `$XDG_CONFIG_HOME/hail/cache` or; `~/.config/hail/cache` if `XDG_CONFIG_HOME` is not set. I store Python reference genome metadata; here. # Batch Attributes. The ServiceBackend `batch_attributes` attribute specifies the attributes for any batch created by; the ServiceBackend. I modified the tests so that the test function name is use the ""name"" of the; batch. When a Hail Query driver job executes `parallelizeAndComputeWithIndex`, it uses its name with; a unique suffix as the name of the batch of worker jobs. # Changes to Hail Scala. I think the only changes outside of `is.hail.backend.service` and `is.hail.services` are:. 1. More retries in GoogleStorageFS. 2. Make MatrixSpecHelper and TableSpecHelper serializable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194:6236,Cache,Cache,6236,https://hail.is,https://github.com/hail-is/hail/pull/11194,4,"['Cache', 'cache']","['Cache', 'cache']"
Performance," did:. ```py; import hail as hl; mt = hl.balding_nichols_model(3, 100, 100); mt.aggregate_entries(hl.agg.mean(mt.GT.n_alt_alleles())); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ERROR: dlopen(""/tmp/libhail6105307987842221044.so""): /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); java.lang.UnsatisfiedLinkError: /tmp/libhail6105307987842221044.so: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/libhail6105307987842221044.so); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:25); 	at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); 	at is.hail.annotations.Region.<init>(Region.scala:34); 	at is.hail.annotations.Region$.apply(Region.scala:16); 	at is.hail.annotations.Region$.scoped(Region.scala:20); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1771); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapCols.execute(MatrixIR.scala:1558); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixMapRows.execute(MatrixIR.scala:1352); 	at is.hail.expr.ir.MatrixMapGlobals.execute(MatrixIR.scala:1832); 	at is.hail.expr.ir.MatrixKeyRowsBy.execute(MatrixIR.scala:1317); 	at is.hail.expr.ir.MatrixM",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733:1403,load,load,1403,https://hail.is,https://github.com/hail-is/hail/issues/4733,1,['load'],['load']
Performance," fix is to use `array_elements_required=False`. ```; hl.import_vcf(..., array_elements_required=False); ```. ---. ### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/checkpoint.20with.20missing.20fields. ```; is.hail.utils.HailException: gs://jn-vcf-cleanup-central1/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626/McCarroll-Macosko-UM1-BICAN-Express-WGS-2023-0626.vcf.gz:offset 1344376382: error while parsing line; chr1	10403	.	ACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	A,ACCCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	.	LowQual	AC=1,1;AF=0.250,0.250;AN=4;AS_QUALapprox=0|23|45;AS_VQSLOD=.,.;AS_YNG=.,.;QUALapprox=45	GT:AD:GQ:RGQ	./.	0/1:23,7,0:20:23	./.	./.	./.	0/2:6,0,4:35:45	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./.	./. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:21); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:21); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1934); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1922); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C2005collect_distributed_array_matrix_native_writer.apply_region1_27(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C2005collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$6(BackendUtils.scala:52); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$5(BackendUtils.scala:51); 	at is.hail.backend.spark.SparkBackendComputeRDD.compute(SparkBackend.scala:751); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:329",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13346:1052,Load,LoadVCF,1052,https://hail.is,https://github.com/hail-is/hail/issues/13346,1,['Load'],['LoadVCF']
Performance," for; commonly used functions, improvements to F2PY, and better documentation.</p>; <p>The Python versions supported in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12809:2329,load,loads,2329,https://hail.is,https://github.com/hail-is/hail/pull/12809,2,['load'],['loads']
Performance," gcr.io/hail-vdc/query:tfkm2kev7zcf'). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 372, in run; await self.ensure_image_is_pulled(); File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 363, in ensure_image_is_pulled; docker.images.pull, self.image); File ""/usr/local/lib/python3.7/site-packages/batch/worker/worker.py"", line 111, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 442, in wait_for; return fut.result(); File ""/usr/local/lib/python3.7/asyncio/futures.py"", line 181, in result; raise self._exception; File ""/usr/local/lib/python3.7/asyncio/tasks.py"", line 249, in __step; result = coro.send(None); File ""/usr/local/lib/python3.7/site-packages/aiodocker/images.py"", line 104, in _handle_list; async with cm as response:; File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.7/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, ""unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""); ```. [1] The Docker specifications are confusing. After reading; [v1](https://github.com/moby/moby/blob/master/image/spec/v1.md) and; [v1.2](https://github.com/moby/moby/blob/master/image/spec/v1.2.md), it seems that the ""repository""; is everything before the last colon and the ""image name suffix"" is everything after the last; colon. For example, in `server:8080/abc/def:123`, the repository is `server:8080/abc/def` and the; ""image name suffix"" is `123`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9902:4010,load,loads,4010,https://hail.is,https://github.com/hail-is/hail/pull/9902,2,"['load', 'perform']","['loads', 'perform']"
Performance," get_return_value(; 1322 answer, self.gateway_client, self.target_id, self.name); 1324 for temp_arg in temp_args:; 1325 temp_arg._detach(). File ~/miniconda3/lib/python3.10/site-packages/hail/backend/py4j_backend.py:35, in handle_java_exception.<locals>.deco(*args, **kwargs); 33 tpl = Env.jutils().handleForPython(e.java_exception); 34 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 35 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 36 except pyspark.sql.utils.CapturedException as e:; 37 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 38 'Hail version: %s\n'; 39 'Error summary: %s' % (e.desc, e.stackTrace, hail.__version__, e.desc)) from None. FatalError: ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'). Java stack trace:; java.lang.ClassCastException: class is.hail.types.physical.stypes.concrete.SIndexablePointer cannot be cast to class is.hail.types.physical.stypes.concrete.SJavaArrayString (is.hail.types.physical.stypes.concrete.SIndexablePointer and is.hail.types.physical.stypes.concrete.SJavaArrayString are in unnamed module of loader 'app'); 	at is.hail.expr.ir.functions.RegistryFunctions.unwrapReturn(Functions.scala:364); 	at is.hail.expr.ir.Emit.$anonfun$emitI$85(Emit.scala:1173); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:352); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:1153); 	at is.hail.expr.ir.streams.EmitStream$.is$hail$expr$ir$streams$EmitStream$$emit$1(EmitStream.scala:148); 	at is.hail.expr.ir.streams.EmitStream$.produce(EmitStream.scala:321); 	at is.hail.expr.ir.Emit.emitStream$2(Emit.scala:821); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:1177); 	at is.hail.expr.ir.Emit.$anonfun$emitSplitMethod$1(Emit.scala:607); 	at is.hail.expr.ir.Emit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:4120,load,loader,4120,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['load'],['loader']
Performance," hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2018,cache,cached,2018,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," href=""https://redirect.github.com/python-pillow/Pillow/issues/7823"">#7823</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Allow writing IFDRational to UNDEFINED tag <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7840"">#7840</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix logged tag name when loading Exif data <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7842"">#7842</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use maximum frame size in IHDR chunk when saving APNG images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7821"">#7821</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Prevent opening P TGA images without a palette <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7797"">#7797</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use palette when loading ICO images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7798"">#7798</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use consistent arguments for load_read and load_seek <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7713"">#7713</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Turn off nullability warnings for macOS SDK <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7827"">#7827</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fix shift-sign issue in Convert.c <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7838"">#7838</a> [<a href=""https://github.com/r-barnes""><code>@​r-barnes</code></a>]</li>; <li>winbuild: Refactor dependency versions into constants <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7843"">#7843</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14439:6848,load,loading,6848,https://hail.is,https://github.com/hail-is/hail/pull/14439,3,['load'],['loading']
Performance," idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5392,cache,cached,5392,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," in <module>; 4 import numpy as np; ----> 5 import pyspark; 6 from typing import Optional, Dict, Callable, Sequence. ModuleNotFoundError: No module named 'pyspark'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_233/4275665471.py in <module>; ----> 1 combined_pandas = pd.read_pickle(gwas_pandas_file). /opt/conda/lib/python3.7/site-packages/pandas/io/pickle.py in read_pickle(filepath_or_buffer, compression, storage_options); 220 # ""No module named 'pandas.core.sparse.series'""; 221 # ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""; --> 222 return pc.load(handles.handle, encoding=None); 223 except UnicodeDecodeError:; 224 # e.g. can occur for files written in py27; see GH#28645 and GH#31988. /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in load(fh, encoding, is_verbose); 272 up.is_verbose = is_verbose; 273 ; --> 274 return up.load(); 275 except (ValueError, TypeError):; 276 raise. /opt/conda/lib/python3.7/pickle.py in load(self); 1086 raise EOFError; 1087 assert isinstance(key, bytes_types); -> 1088 dispatch[key[0]](self); 1089 except _Stop as stopinst:; 1090 return stopinst.value. /opt/conda/lib/python3.7/pickle.py in load_stack_global(self); 1383 if type(name) is not str or type(module) is not str:; 1384 raise UnpicklingError(""STACK_GLOBAL requires str""); -> 1385 self.append(self.find_class(module, name)); 1386 dispatch[STACK_GLOBAL[0]] = load_stack_global; 1387 . /opt/conda/lib/python3.7/site-packages/pandas/compat/pickle_compat.py in find_class(self, module, name); 204 key = (module, name); 205 module, name = _class_locations_map.get(key, key); --> 206 return super().find_class(module, name); 207 ; 208 . /opt/conda/lib/python3.7/pickle.py in find_class(self, module, name); 1424 elif module in _compat_pickle.IMPORT_MAPPING:; 1425 module = _compat_pickle.IMPORT_MAPPING[module]; -> 1426 __import__(module, level=0); 1427 if self.proto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004:3016,load,load,3016,https://hail.is,https://github.com/hail-is/hail/issues/14004,1,['load'],['load']
Performance," in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages. ---. Things for you to verify:; - does every service load *its own unqiue* ssl-config secret?; - does every service use an SSL context for serving?; - does everyone who makes http requests to internal services use an SSL client; session?; - do all TLS-secured nginx instances include their http.conf in the `http`; section and the `proxy.conf` file in any proxying sections?; - Do the SSLContext's from `ssl.py` and the nginx configurations generated by; `create_certs.py` actually ensure security?. Post-merge actions:; - deploy gateway; - deploy internal-gateway; - deploy router-resolver. Anticipated outages:. - Before a service is redeployed it will be inaccessible from the outside; because the router will try to speak to it on HTTPS. Services that speak to; one another (ci<->batch, everyone<->auth) will lose connections while the; deploy is happening. Deploy should move smoothly because CI will completely; transmit the deploy batch to batch before batch goes dark.; - dev namespaces will be broken until the owner redeploys the router, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561:12716,load,load,12716,https://hail.is,https://github.com/hail-is/hail/pull/8561,1,['load'],['load']
Performance," in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.immutable.Set; at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$6.apply(ExportVCF.scala:185); at scala.Option.map(Option.scala:145); at org.broadinstitute.hail.driver.ExportVCF$.org$broadinstitute$hail$driver$ExportVCF$$appendRow$1(ExportVCF.scala:185); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:278); at org.broadinstitute.hail.driver.ExportVCF$$anonfun$run$1$$anonfun$apply$11.apply(ExportVCF.scala:276); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1109); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/785:2084,concurren,concurrent,2084,https://hail.is,https://github.com/hail-is/hail/issues/785,2,['concurren'],['concurrent']
Performance, java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). java.lang.ClassCastException: class is.hail.types.virtual.TStruct cannot be cast to class is.hail.types.virtual.TIterable (is.hail.types.virtual.TStruct and is.hail.types.virtual.TIterable are in unnamed module of loader 'app'); 	at is.hail.expr.ir.InferType$.apply(InferType.scala:115); 	at is.hail.expr.ir.IR.typ(IR.scala:36); 	at is.hail.expr.ir.IR.typ$(IR.scala:33); 	at is.hail.expr.ir.ToStream.typ(IR.scala:300); 	at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr_1$81(Parser.scala:1111); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2157); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2153); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2157); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.usin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13699:6069,load,loader,6069,https://hail.is,https://github.com/hail-is/hail/issues/13699,1,['load'],['loader']
Performance," kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1455,cache,cached,1455,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is region splitting, which is used to split large blocks of LIR. In order to insert a split, any variables on the stack are stored in local variables before the split and loaded from those locals after the split. The second is method splitting, which is used to split large single methods. A single-exit group of blocks can be split into a separate method, and we have some machinery for replacing control flow instructions (which I will not go into here, for they are not relevant now), as well as handling local variables that are used across a method split. These shared Local variables are replaced by fields on a ""spills"" class which is allocated any time a split method is called. Spilled local `store`s are rewritten as field `store`s, and `load`s are rewritten as field `load`s. # What was the problem here?. A region split was inserted *directly between* the `I2B` instruction and the call to `OutputBuffer.write`. This meant that the result of `I2B` was stored in a local variable and read in the subsequent block. **The incorrect TypeInfo of Boolean was used for that local variable**, but this seems not to pose a problem -- both Boolean and Byte use a single slot, and so the code still works even with the wrong variable type. However, the method splitter then **generated a method split at the same point where the region was split**. This means that the local variable resulting from I2B is spilled to a class field on the spills class. Our incorrectly-Boolean local becomes an incorrectly-Boolean **field**, and this is where things go wrong -- it seems as though Boolean class fields (appropriately) truncate on store and load a single bit. Our value of `3` was stored as a class Boolean, and came out `1`. The fact th",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:2741,load,load,2741,https://hail.is,https://github.com/hail-is/hail/pull/11328,2,['load'],['load']
Performance," org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:5865,Load,LoadVCF,5865,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance," region, cloud); 107 raise ValueError(f'Region {repr(region)} not available for dataset'; 108 f' {repr(name)} on cloud platform {repr(cloud)}.\n'; 109 f'Available regions: {regions}.'); 111 path = [dataset['url'][cloud][region]; 112 for dataset in datasets[name]['versions']; 113 if all([dataset['version'] == version,; 114 dataset['reference_genome'] == reference_genome])]; --> 115 assert len(path) == 1; 116 path = path[0]; 117 if path.startswith('s3://'):. AssertionError: ; ```. I'm a new Hail user and don't have the full context here, but it seems like there are at least three problems:. 1. An assert failed in production code, which indicates either the presence of a bug or an incorrect use of assert (e.g. using assert to check for value errors).; 2. The assert has no corresponding error message, so the user learns that something has gone wrong but can't easily tell what.; 3. The assert is bare. Bare asserts can get optimized out of code in ways that are difficult to foresee in advance, and are generally deprecated in favor of the `if error_condition: raise AssertionError(...)` pattern (see: https://discuss.python.org/t/stop-ignoring-asserts-when-running-in-optimized-mode/13132). **The Big Picture**. The bare assert pattern is used over 3k times in Hail. To be fair, many of these usages occur in test directories, where they're fine. But they also occur in application code, and often in the dangerous form `assert(expr1, expr2)` which will never fail (because a tuple with two falsy elements is truthy in python). These asserts are never actually getting checked. . Fixing all of them would be a heavy lift. One compromise solution might be to add a bare assert rule to the linter (e.g. https://pypi.org/project/flake8-assert-msg/). This would prevent the introduction of further bare asserts to the codebase, and encourage authors to clean up existing bare asserts on files they touch. The `assert` keyword is an unfortunate language wart that makes it very easy for developer",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952:1439,optimiz,optimized,1439,https://hail.is,https://github.com/hail-is/hail/issues/12952,1,['optimiz'],['optimized']
Performance," rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6363,cache,cached,6363,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," run_until_complete; self.run_forever(); /usr/lib/python3.7/asyncio/base_events.py:541: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; if self._debug and timeout != 0:; t0 = self.time(); event_list = self._selector.select(timeout); dt = self.time() - t0; if dt >= 1.0:; level = logging.INFO; else:; level = logging.DEBUG; nevent = len(event_list); if timeout is None:; logger.log(level, 'poll took %.3f ms: %s events',; dt * 1e3, nevent); elif nevent:; logger.log(level,; 'poll %.3f ms took %.3f ms: %s events',; timeout * 1e3, dt * 1e3, nevent); elif dt >= 1.0:; logger.log(level,; 'poll %.3f ms took %.3f ms: timeout',; timeout * 1e3, dt * 1e3); else:; event_list = self._selector.select(timeout); se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705:2244,queue,queue,2244,https://hail.is,https://github.com/hail-is/hail/pull/10705,1,['queue'],['queue']
Performance," run_until_complete; self.run_forever(); /usr/lib/python3.9/asyncio/base_events.py:601: in run_forever; self._run_once(); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <_UnixSelectorEventLoop running=False closed=False debug=False>. def _run_once(self):; """"""Run one full iteration of the event loop.; ; This calls all currently ready callbacks, polls for I/O,; schedules the resulting callbacks, and finally schedules; 'call_later' callbacks.; """"""; ; sched_count = len(self._scheduled); if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and; self._timer_cancelled_count / sched_count >; _MIN_CANCELLED_TIMER_HANDLES_FRACTION):; # Remove delayed calls that were cancelled if their number; # is too high; new_scheduled = []; for handle in self._scheduled:; if handle._cancelled:; handle._scheduled = False; else:; new_scheduled.append(handle); ; heapq.heapify(new_scheduled); self._scheduled = new_scheduled; self._timer_cancelled_count = 0; else:; # Remove delayed calls that were cancelled from head of queue.; while self._scheduled and self._scheduled[0]._cancelled:; self._timer_cancelled_count -= 1; handle = heapq.heappop(self._scheduled); handle._scheduled = False; ; timeout = None; if self._ready or self._stopping:; timeout = 0; elif self._scheduled:; # Compute the desired timeout.; when = self._scheduled[0]._when; timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT); ; event_list = self._selector.select(timeout); self._process_events(event_list); ; # Handle 'later' callbacks that are ready.; end_time = self.time() + self._clock_resolution; while self._scheduled:; handle = self._scheduled[0]; if handle._when >= end_time:; break; handle = heapq.heappop(self._scheduled); handle._scheduled = False; self._ready.append(handle); ; # This is the only place where callbacks are actually *called*.; # All other places just add them to ready.; # Note: We run all currently scheduled callbacks, but not any; # callbacks scheduled by callback",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13997:3155,queue,queue,3155,https://hail.is,https://github.com/hail-is/hail/issues/13997,1,['queue'],['queue']
Performance, scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Hail version: 0.2.46-6e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:17369,concurren,concurrent,17369,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['concurren'],['concurrent']
Performance, scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:448). Driver stack,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:8071,concurren,concurrent,8071,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['concurren'],['concurrent']
Performance," stage 1.0 (TID 11, localhost, executor driver): com.esotericsoftware.kryo.KryoException: sun.reflect.generics.reflectiveObjects.NotImplementedException; Serialization trace:; m (is.hail.annotations.aggregators.KeyedRegionValueAggregator); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:366); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:315); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:386); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: sun.reflect.generics.reflectiveObjects.NotImplementedException; 	at is.hail.annotations.UnKryoSerializable$class.write(UnsafeRow.scala:15); 	at is.hail.annotations.UnsafeRow.write(UnsafeRow.scala:141); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:505); 	at com.esotericsoftware.kryo.serializers.DefaultSerializers$KryoSerializableSerializer.write(DefaultSerializers.java:503); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serialize",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215:1391,concurren,concurrent,1391,https://hail.is,https://github.com/hail-is/hail/issues/4215,1,['concurren'],['concurrent']
Performance, storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJsonParser.java:2584) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishAndReturnString(UTF8StreamJsonParser.java:2560) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8St,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:3076,concurren,concurrent,3076,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance," supported in this release are 3.8-3.10, Python 3.7; has been dropped. Note that 32 bit wheels are only provided for Python; 3.8 and 3.9 on Windows, all other wheels are 64 bits on account of; Ubuntu, Fedora, and other Linux distributions dropping 32 bit support.; All 64 bit wheels are also linked with 64 bit integer OpenBLAS, which should fix; the occasional problems encountered by folks using truly huge arrays.</p>; <h2>Expired deprecations</h2>; <h3>Deprecated numeric style dtype strings have been removed</h3>; <p>Using the strings <code>&quot;Bytes0&quot;</code>, <code>&quot;Datetime64&quot;</code>, <code>&quot;Str0&quot;</code>, <code>&quot;Uint32&quot;</code>,; and <code>&quot;Uint64&quot;</code> as a dtype will now raise a <code>TypeError</code>.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19539"">gh-19539</a>)</p>; <h3>Expired deprecations for <code>loads</code>, <code>ndfromtxt</code>, and <code>mafromtxt</code> in npyio</h3>; <p><code>numpy.loads</code> was deprecated in v1.15, with the recommendation that; users use <code>pickle.loads</code> instead. <code>ndfromtxt</code> and <code>mafromtxt</code> were both; deprecated in v1.17 - users should use <code>numpy.genfromtxt</code> instead with; the appropriate value for the <code>usemask</code> parameter.</p>; <p>(<a href=""https://redirect.github.com/numpy/numpy/pull/19615"">gh-19615</a>)</p>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/numpy/numpy/commit/4adc87dff15a247e417d50f10cc4def8e1c17a03""><code>4adc87d</code></a> Merge pull request <a href=""https://redirect.github.com/numpy/numpy/issues/20685"">#20685</a> from charris/prepare-for-1.22.0-release</li>; <li><a href=""https://github.com/numpy/numpy/commit/fd66547557f57c430d41be2fc0764f74a62e8ccf""><code>fd66547</code></a> REL: Prepare for the NumPy 1.22.0 release.</li>; <li><a href=""https://github.com/numpy/numpy/commit/125304b035e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12809:2424,load,loads,2424,https://hail.is,https://github.com/hail-is/hail/pull/12809,2,['load'],['loads']
Performance," the latter. You can do even better than `PureComponent`. Use a regular `Component`, and specific a `shouldComponentUpdate() { }` method in that component. Within that method, write whatever checks needed, so that when a prop, or state changes, you return `true`, otherwise `false`. When true, the component will re-render. However, this allows you to react in a more fine-grained way, i.e instead of checking reference, check for the update of a specific property, or don't react to that object changing at all. Behind the scenes, PureComponent is in effect implementing a shouldComponentUpdate that checks reference equality (prevProp !== currentProp). References: ; 1. https://reactjs.org/docs/react-api.html#reactpurecomponent. #### Memo stateless components; Stateless/functional components (i.e those than don't extend React.Component or React.PureComponent, i.e `(props) => <div>Hello {props.name}</div>`), can be memoized. As in a typical memoized function, given one set of input (props), the result is cached, and the cached result is returned for n + 1 calls. References; 1. https://reactjs.org/docs/react-api.html#reactmemo; 2. https://scotch.io/tutorials/react-166-reactmemo-for-functional-components-rendering-control. ### Typescript; 2. https://reactjs.org/docs/react-api.html#reactmemo. #### And React Component prop definitions; https://levelup.gitconnected.com/ultimate-react-component-patterns-with-typescript-2-8-82990c516935. ### NextJS; https://nextjs.org/docs/; Next has 4 deviations from normal react:; 1) _app.js: Can be omitted. Wraps all other components. Is useful for global functions, because it is not reloaded when you change pages. Good place to place a header component, a footer, global data stores, or handle page transitions.; it has this shape:; ```js; <Container>; <Header />; <Component {...pageProps} />; <Footer />; </Container>; ```. 2) _document.js: Optional. Rendered only on the server, exactly one time. Wraps _app. Good place to define external resourc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:10631,cache,cached,10631,https://hail.is,https://github.com/hail-is/hail/pull/5162,2,['cache'],['cached']
Performance," the values are stored in the array and the names are stored in a separate dictionary mapping names to their indices; 2. a `dict<str, T>`, the name-value pairs are stored as dictionary key-value pairs; 3. a `struct{name1: T, name2: T, ... nameN: T}`, the name-value pairs are stored as field name, field string pairs. The third option is the most space efficient: the type stores the field names and there is no bookkeeping overhead per-set-of-named-values. The first two options repeat the field names for each occurrence (in particular, consider a Table field or MatrixTable entry-indexed field). The first two options needlessly encode the length (which is statically known). The third option is the most access-time-efficient: the offset of any named-value is known at hail compile time. The first two options require a logarithmic search of hail's dictionary tree representation. The third option is more user-friendly for accessing: `x.name`. The first is the least user-friendly: `x[indices[""name""]]`. The first and third options are the most cache-friendly for homogenous operations. The first uses `ArrayExpression.map`, so code size is `O(CODE)`. The third option's code size is `O(CODE * #VALUES)` because structs have no `map`-like primitive. The third option is also not user-friendly for homogenous operations (the user must repeat the code for each name-value pair). The third is the most self-documenting option. The number of fields and their names are visible in `ds.describe()`. The first is the next best because the dictionary is likely a global field that can be viewed with `x.indices.show()`. ---. ## Phase 1; Implement a new virtual type `tstaticdict<T, name1, name2, ..., nameN>` who's physical type is `PStruct` with N fields. These changes span Scala and Python. Implement `map` and `__getitem__` on `StaticDictExpression`s. `map` is implemented by code duplication. ## Phase 2; Implement a new physical type that implements homogenous operations without code duplication.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6881:1171,cache,cache-friendly,1171,https://hail.is,https://github.com/hail-is/hail/issues/6881,1,['cache'],['cache-friendly']
Performance," then data must be sorted by the key and the ordering of rows with equivalent keys is undefined. In this case, it is safe to `order_by(*t.key)`. # User Expectations. There is a subtle difference in how we treat zero-length keyed (ZLK) objects and non-zero-length keyed objects. We try to preserve the data ordering of ZLK objects. In particular, users would be pretty surprised if `import_table(..., key=[])` shuffled the rows. Additionally, users expect (and we document) that `mt.key_cols_by().cols()` does not shuffle the rows. In contrast, we treat a non-zero-length keyed object as if any ordering beyond that defined by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does it remove an `order_by(x, ...)` that precedes an `order_by()`. It does, however, remove a `key_by(x, ...)` that precedes an `order_by()` or a `key_by()`. ```; In [47]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().order_by().show() ; ...: ; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 1 |; | 2 |; | 0 |; +---------+. In [48]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx) ; ...: t = t.order_by().show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+. In [49]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.order_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+. In [50]: t = hl.utils.range_table",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6929:4939,Optimiz,Optimizer,4939,https://hail.is,https://github.com/hail-is/hail/issues/6929,1,['Optimiz'],['Optimizer']
Performance," to establish the interface and behavior. I expect several follow-on PRs:. - Revise the original copy interface proposal and add to dev-docs.; - ~~Parallelizes the transfers concurrently with async and across multiple threads.~~; - ~~After parallizing, copy will involve a lot of paralellism. Throwing an exception on the first error will be very non-deterministic. Instead, copy will return a report that collects all the errors that were encountered in the course of copying, and summarizes how many files/bytes were copied.~~; - Use multi-process parallelism; - Avoid overwriting the destination if it exists and has a matching checksum (or size).; - ~~Introduce multi-part transfers~~; - add a post-pass for Google Storage to detect file-and-directory errors.; - Adds support for S3.; - Add `hailctl cp ...` (PR); - Use copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the source, so stat'ing the sources and destinations are all overlapping. This avoids dependencies where I have to e.g. stat the i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:1077,perform,perform,1077,https://hail.is,https://github.com/hail-is/hail/pull/9822,1,['perform'],['perform']
Performance," to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822:4484,concurren,concurrent,4484,https://hail.is,https://github.com/hail-is/hail/issues/1822,1,['concurren'],['concurrent']
Performance," top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val extract = new ExtractIntervalFilters(ctx, ref.typ.asInstanceOf[TStruct].typeAfterSelectNames(key)); val trueSet = extract.analyze(cond, ref.name); if (trueSet == extract.KeySetLattice.top); None; else {; val rw = extract.Rewrites(mutable.Set.empty, mutable.Set.empty); extract.analyze(cond, ref.name, Some(rw), trueSet); Some((extract.rewrite(cond, rw), trueSet)); }; }; }; ```; `trueSet` is the set of intervals which contains all rows where `cond` is true. This set is passed back into `analyze` in a second pass, which asks it to rewrite `cond` to something equivalent, under the assumption that all keys are contained in `trueSet`. The abstraction of runtime values tracks two types of information:; * Is this value a reference to",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:1770,optimiz,optimized,1770,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['optimiz'],['optimized']
Performance," uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:1701,cache,cached,1701,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance," which prevents reading and allocation of LID and RSID (also improved python-type-checking for `row_fields` and `entry_fields`)~ Moved to #3779 and #3778. - ~fixed table-table joins to _not_ always coerce (thus computing partition keys of) the right-hand table~ Moved to #3723 . - ~added a check that prevents globals and sample annotations copying when they're not used in the body of a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727:1604,load,load,1604,https://hail.is,https://github.com/hail-is/hail/pull/3727,1,['load'],['load']
Performance," with equivalent keys is undefined. In this case, it is safe to `order_by(*t.key)`. # User Expectations. There is a subtle difference in how we treat zero-length keyed (ZLK) objects and non-zero-length keyed objects. We try to preserve the data ordering of ZLK objects. In particular, users would be pretty surprised if `import_table(..., key=[])` shuffled the rows. Additionally, users expect (and we document) that `mt.key_cols_by().cols()` does not shuffle the rows. In contrast, we treat a non-zero-length keyed object as if any ordering beyond that defined by the key is irrelevant. Is this surprising to a user? Suppose they had a text file ordered by `family id, sample id`, they might reasonably expect that `import_table(..., key=['family id'])` does not reorder the rows within a family. Hail doesn't guarantee this even though we do make pains to not reorder the same data imported by `import_table(..., key=[])`. # Ordering and the Optimizer. Currently, the Hail optimizer does not remove a `choose_cols` that precedes a `order_by()`. Nor does it remove an `order_by(x, ...)` that precedes an `order_by()`. It does, however, remove a `key_by(x, ...)` that precedes an `order_by()` or a `key_by()`. ```; In [47]: import hail as hl ; ...: mt = hl.utils.range_matrix_table(3,3) ; ...: mt = mt.key_cols_by().choose_cols([1,2,0]) ; ...: mt.cols().order_by().show() ; ...: ; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 1 |; | 2 |; | 0 |; +---------+. In [48]: t = hl.utils.range_table(3) ; ...: t = t.order_by(-t.idx) ; ...: t = t.order_by().show() ; +-------+; | idx |; +-------+; | int32 |; +-------+; | 2 |; | 1 |; | 0 |; +-------+. In [49]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.order_by().show() ; +-------+-------+; | idx | x |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | -1 |; | 2 | -2 |; +-------+-------+. In [50]: t = hl.utils.range_table(3) ; ...: t = t.key_by(x=-t.idx) ; ...: t = t.key_by().show(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6929:4970,optimiz,optimizer,4970,https://hail.is,https://github.com/hail-is/hail/issues/6929,1,['optimiz'],['optimizer']
Performance," ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Worker$.main(Worker.scala:164) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 's",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25086,concurren,concurrent,25086,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/hailtop/utils/utils.py"", line 33, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/batch/google_storage.py"", line 53, in _read_gs_file; content = f.download_as_string(); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 697, in download_as_string; self.download_to_file(string_buffer, client=client, start=start, end=end); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 638, in download_to_file; _raise_from_invalid_response(exc); File ""/usr/local/lib/python3.6/dist-packages/google/cloud/storage/blob.py"", line 2034, in _raise_from_invalid_response; raise exceptions.from_http_status(response.status_code, message, response=response); google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/download/storage/v1/b/hail-batch2-nru9x/o/cd50b95a89914efb897965a5e982a29d%2F1%2F1%2Fsetup%2Fjob.log?alt=media: ('Request failed with status code', 403, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); ```. - The first error is caused by the driver object is only available on the batch2-driver instance. Now the front end sends a request to the driver to get the logs if the job is running. I purposefully left the driver and front end calling the same function in case the job terminates before the driver handles the request from the front end. - Unified the Instance_ID between the front_end and the driver. I thought this was causing the second bug where gcs was unauthorized, but Tim was running in the default namespace, so they had the same instance id. - I checked and we're loading the same gsa-key in both the front end and driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7412:6420,load,loading,6420,https://hail.is,https://github.com/hail-is/hail/pull/7412,1,['load'],['loading']
Performance,"# --- DRAFT ---. # People. @hail-is/tensors . # Scope of Document; Python API, types, IR, optimizer, compiler. # Notation. e[[x/v]] means substitute occurrences of the variable v with the expression x in; the expression e. f[[ e ]] is just application of the function `f` but with the advantage that it doesn't look like any python or Scala syntax (so it's obviously referring to the meta-language rather than the languages we're building here). I'm going to consistently use ""distributed"" to talk about BlockMatrix-y things and ""small"" to refer to things that live in the ""value"" IR. # DistributedTensorIR. Some thoughts on TensorIR (fruits of discussion among the group):. TensorIR ::= TensorLiteral(); | TensorContract(TensorIR, TensorIR, Int, Int, body: IR); | TensorMap2(TensorIR, TensorIR, body: IR); | TensorMap(TensorIR, body: IR); | TensorSelectGlobals(TensorIR, body: IR). In the body of TensorContract and TensorMap2, four refs are free: `l`, `r`, `i`, and `j`. In the body of TensorMap, three refs are free: `e`, `i`, `j`. In the body of TensorContract, all four refs are aggregables. In the TensorMap and TensorMap2, they are scalar values. No aggregations are allowed in the body of TensorSelectGlobals. It is just `SparkContext.broadcast`. ## From Python. C[[ u @ v ]] := TensorContract(; C[[ u ]],; C[[ v ]],; 1,; 0,; hl.agg.sum(l * r)). C[[ u + v ]] := TensorMap2(; C[[ u ]],; C[[ v ]],; l + r). C[[ u + 1 ]] := TensorMap(; C[[ u ]],; e + I32(1)). C[[ u + hl.ndarray(...) ]] := TensorMap(; TensorSelectGlobals(; C[[ u ]],; uuid1,; C[[ hl.ndarray(...) ]]); e + NDArrayIndex(GetField(""globals"", uuid1), i, j)). ## Transformations. This representation admits elegant transformations:. TensorMap2(TensorMap(u, x), v, body); <=>; TensorMap2(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). TensorMap(TensorMap(u, x), y); <=>; TensorMap(u, Let(uuid1, x, y[Ref(uuid1)/e])). TensorContraction(TensorMap(u, x), v, body); <=>; TensorContraction(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). the ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5195:90,optimiz,optimizer,90,https://hail.is,https://github.com/hail-is/hail/issues/5195,1,['optimiz'],['optimizer']
Performance,"# Current situation. In several places, we call a method `emitNDArrayStandardStrides`, which has the effect of calling `emitDeforestedNDArray`, since that is known to always emit things in column major order. However, the downside of this is that we were doing unnecessary copies of the data, even when it was already in column major order, by constructing an `NDArrayEmitter` that just emitted an NDArray and then looked up values from it:. ```; case _ =>; val ndt = emit(x); val ndAddress = mb.genFieldThisRef[Long](); val setup = (ndAddress := ndt.value[Long]); val xP = x.pType.asInstanceOf[PNDArray]. val shapeAddress = new Value[Long] {; def get: Code[Long] = xP.shape.load(ndAddress); }; val shapeTuple = new CodePTuple(xP.shape.pType, shapeAddress). val shapeArray = (0 until xP.shape.pType.nFields).map(i => shapeTuple.apply[Long](i)). new NDArrayEmitter[C](nDims, shapeArray,; xP.shape.pType, xP.elementType, setup, ndt.setup, ndt.m) {; override def outputElement(elemMB: EmitMethodBuilder[C], idxVars: IndexedSeq[Value[Long]]): Code[_] =; xP.loadElementToIRIntermediate(idxVars, ndAddress, elemMB); }; ```. # New Situation. We now have `emitNDArrayColumnMajorStrides`, which calls `emit` on an ndarray, checks if the emitted thing is column major, and only does a copy if it needs to. This uses new `LinalgCodeUtils` methods `checkColumnMajor` and `createColumnMajorCode`. Everything else in `LinalgCodeUtils` was unused / old style and I removed them.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9428:675,load,load,675,https://hail.is,https://github.com/hail-is/hail/pull/9428,2,['load'],"['load', 'loadElementToIRIntermediate']"
Performance,"# The disappearing bit; ; This is one of the sneakiest bugs I've ever worked on. I learned early that the issue was somehow related to method splitting, but it was far more devious than I expected, and the code in LIR method splitting is completely correct. The particular case I was debugging had expected behavior when trying to write the missing bits of four fields A,B,C,D, where A and B were missing and C and D were present. These should have written the byte. 1<<0 | 1<<1 | 0<<2 | 0<<3; ==> b00000011; ==> 3. But instead wrote the byte `b00000001 or 1`, incorrectly leading readers to try to read field B when it was missing (and not written). This is due to the load-bearing and incorrect type of an I2B instruction generated [here](https://github.com/hail-is/hail/blob/8bd9b7b2224b77372a72f02f2b13806267892a35/hail/src/main/scala/is/hail/types/encoded/EBaseStruct.scala#L107). I2B is an instruction that truncates an integer to a byte, and it is used in various places in code generation but primarily encoding missing bits in arrays and structs. . I2B loads a byte to the stack, not a boolean. TypeInfos are mostly non-structural since they rarely influence the bytecode generated. Here is an exception, and that's where the method splitter comes in. Method splitting exists in the Hail compiler because not only does the JVM have limits on how large methods can be, but also the JIT compiler handles small methods much more effectively than large methods (and so splitting a large method into two small ones can make an order of magnitude or more in performance difference). We have three forms of method splitting in the Hail Query compiler. The first is a heuristic and greedy IR-level method splitter that generates new methods every X IR nodes, simply based on node count. However, the size of code generated by each IR can vary widely (`I32` vs `LowerBoundOnOrderedCollection` for instance), and so we have two other kinds of splitting that operate on the LIR level. The first is regio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:670,load,load-bearing,670,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['load'],['load-bearing']
Performance,"## Basic philosophy; https://developers.google.com/web/fundamentals/performance/prpl-pattern/. Hybrid server-side client-side rendered application, with eager pre-loading of resources. First page visited is server rendered. All client-side code asynchronously fetched, using service workers if available. HTML is ""hydrated""/bound by React, and from then on has the responsiveness of a client-side application. This is what we have in the current pull request. Initial view / first page ready in ~10ms, DOMContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:68,perform,performance,68,https://hail.is,https://github.com/hail-is/hail/pull/4931,3,"['latency', 'load', 'perform']","['latency', 'loading', 'performance']"
Performance,"### Description. In this pull request, I add a function to perform a Cochran-Mantel-Haenszel statistical test for association. This pull request closes #13481. ### Testing. I add unit tests. Since I have not used R before (the [associated GitHub issue](https://github.com/hail-is/hail/issues/13481) suggests using R to create test cases), I created the unit tests from examples that I found on the internet. I linked these sources in the code for the unit tests. I built the documentation locally and inspected it to confirm that it matches my expectations. I am having trouble testing the docstring examples locally. When I run `make -C hail doctest-query`, the tests error due to a checksum exception. ### Discussion. ~I have not added an example to the documentation that uses a matrix table yet. (This is an acceptance criteria in #13481.) I wanted to get some advice about the best way to do this. I think ideally, the example would have a binary phenotype, an allele to test for association, and some stratifying variable. I tried to search through the existing code to find suitable example matrix tables in the docstrings, but I didn't find anything promising. I would appreciate help here.~. Update: thanks to @patrick-schultz's recommendation, I have added an example using a matrix table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255:59,perform,perform,59,https://hail.is,https://github.com/hail-is/hail/pull/14255,1,['perform'],['perform']
Performance,"### Description. Today our APIs are ""documented"" only through the list of endpoint handlers in implementation code ([example](https://github.com/hail-is/hail/blob/main/batch/batch/front_end/front_end.py#L239)). We can and should:; - Create OpenAPI documentation for our APIs (maybe per-service, maybe once in the gateway?); - Host swagger page/pages for exploring and testing out APIs . ### Security Impact. High. ### Security Impact Description. ""None"" for the creation of documentation, since we do not believe that documenting our APIs is inherently risky. ""High"" for hosting a new functional component on our web endpoints. Mitigating factor: swagger pages are loaded as static html with no need (or ability) to interact with other functional components, except through the same public APIs as are already accessible. ### Appsec Signoff. - [ ] Reviewed and approved",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14725:665,load,loaded,665,https://hail.is,https://github.com/hail-is/hail/issues/14725,1,['load'],['loaded']
Performance,"### Hail version:; `08224c6ab`; ### What you did:; Tried to load a plink dataset that was split by chromosome.; ```; files = [(; f'gs://fc-9a7c5487-04c9-4182-b3ec-13de7f6b409b/genotype/ukb_cal_chr{i}_v2.bed',; f'gs://fc-9a7c5487-04c9-4182-b3ec-13de7f6b409b/genotype/ukb_snp_chr{i}_v2.bim'; ) for i in range(1,23)]; mts = [hl.import_plink(bed=f[0],bim=f[1],fam=""gs://phenotype_31063/ukb31063.fam"") for f in files]; mt = mts[0].union_rows(*mts[1:]); ```; ### What went wrong (all error messages here, including the full java stack trace):; It loaded each plink file serially rather than in parallel, thus wasting many cores of my cluster (and my time).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3975:60,load,load,60,https://hail.is,https://github.com/hail-is/hail/issues/3975,2,['load'],"['load', 'loaded']"
Performance,"### Hail version:; `784ab2796878`; ### What you did:; ```; In [1]: import hail as hl; ...: ; ...: t1kg = hl.balding_nichols_model(3, 100, 100); ...: print(t1kg.describe()); ...: t1kg = t1kg_sm.repartition(500) ; ...: t1kg = t1kg._filter_partitions([1]); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean}},col_key:[sample_idx],col:Struct{sample_idx:Int32,pop:Int32},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],ancestral_af:Float64,af:Array[Float64],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{GT:Call}}; after: Matrix{global:Struct{bn:Struct{n_populations:Int32,n_samples:Int32,n_variants:Int32,n_partitions:Int32,pop_dist:Array[Int32],fst:Array[Float64],mixture:Boolean}},col_key:[sample_idx],col:Struct{sample_idx:Int32,pop:Int32},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],ancestral_af:Float64,af:Array[Float64],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{GT:Call}}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527:441,optimiz,optimization,441,https://hail.is,https://github.com/hail-is/hail/issues/4527,1,['optimiz'],['optimization']
Performance,"### Hail version:; ```; 0.2.7-14ce9228174e; ```; ### What you did:; ```; mt = hl.import_bgen('... a bgen file with ~500k samples ...', entry_fields=['GT']); mt = mt.select_rows().select_cols().select_entries('GT'); mt.count(); ```; becomes; ```; 2019-01-08 18:19:48 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (loc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5100:278,optimiz,optimize,278,https://hail.is,https://github.com/hail-is/hail/issues/5100,1,['optimiz'],['optimize']
Performance,"### What happened?. - When a job creates a log file in excess of about half a GB, loading the job page can cause the batch front-end pod to crash as it loads the log file into memory and interpolates it directly into the job page. It should instead provide an endpoint to stream the job log and include that as an iframe or something in the job page. - When a job creates a log file in excess of 2GiB the batch worker can get into a bad state as it fails to upload the log. Again, it does not stream the log file, instead loading the whole log into memory as `bytes` and tries to upload that, but asyncio ssl has a limit of max-int sized non-streaming payloads. ### Version. 0.2.112. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852:82,load,loading,82,https://hail.is,https://github.com/hail-is/hail/issues/12852,3,['load'],"['loading', 'loads']"
Performance,"### What happened?. A simple `hl.init()` fails, that used to work. Maybe an error with Spark, not an expert. ### Version. 0.2.108. ### Relevant log output. ```shell; ~ » python3; Python 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; >>> hl.init(); 2023-01-27 17:15:28.940 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1758>"", line 2, in init; File ""/opt/homebrew/lib/python3.10/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/homebrew/lib/python3.10/site-packages/hail/context.py"", line 345, in init; return init_spark(; File ""<decorator-gen-1760>"", line 2, in init_spark; File ""/opt/homebrew/lib/python3.10/site-packages/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/homebrew/lib/python3.10/site-packages/hail/context.py"", line 424, in init_spark; backend = SparkBackend(; File ""/opt/homebrew/lib/python3.10/site-packages/hail/backend/spark_backend.py"", line 188, in __init__; self._jbackend = hail_package.backend.spark.SparkBackend.apply(; File ""/opt/homebrew/lib/python3.10/site-packages/py4j/java_gateway.py"", line 1304, in __call__; return_value = get_return_value(; File ""/opt/homebrew/lib/python3.10/site-packages/py4j/protocol.py"", line 326, in get_return_value; raise Py4JJavaError(; py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x4d740d85) cannot access class sun.n",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12630:442,load,load,442,https://hail.is,https://github.com/hail-is/hail/issues/12630,1,['load'],['load']
Performance,"### What happened?. Batch does not guarantee that there is always at most 1 running attempt for a job at any given time. While rare, this double scheduling can sometimes happen so there is a background task that checks the database for ""orphaned"" attempts -- attempts that are running but are not noted as the current attempt for the relevant job -- and stops them to reduce wasted spend. This query that polls the database for attempts to remove does a needless scan of the instances table. I'll describe below the process by which I discovered the inefficiency:. 1. GCP Cloud SQL has a nice feature Query Insights, in which reports latencies and rows scanned by popular queries. For singular queries it can show a graph of the query and indicate bottlenecks. The below query is currently scanning over a million rows of the instances table is taking on average 642.37 ms:. https://github.com/hail-is/hail/blob/091e6612752010880a130cf4010897e87ea2a864/batch/batch/driver/canceller.py#L373-L382. as shown here from Query Insights:; ![Screenshot 2024-04-11 at 10 31 17 AM](https://github.com/hail-is/hail/assets/24440116/d807b383-7825-4ff5-ad04-6869f0402dd0). 2. The thick edge on the instances scan indicates that the where condition for instances is not using an index. We can verify this by explaining the query against the DB:; ```; > kssh admin-pod; > mysql; mysql> use batch;; mysql> EXPLAIN SELECT attempts.*; -> FROM attempts; -> INNER JOIN jobs ON attempts.batch_id = jobs.batch_id AND attempts.job_id = jobs.job_id; -> LEFT JOIN instances ON attempts.instance_name = instances.name; -> WHERE attempts.start_time IS NOT NULL; -> AND attempts.end_time IS NULL; -> AND ((jobs.state != 'Running' AND jobs.state != 'Creating') OR jobs.attempt_id != attempts.attempt_id); -> AND instances.`state` = 'active'; -> ORDER BY attempts.start_time ASC; -> LIMIT 300\G;. *************************** 1. row ***************************; id: 1; select_type: SIMPLE; table: instances; partitions: NULL; type: A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14460:748,bottleneck,bottlenecks,748,https://hail.is,https://github.com/hail-is/hail/issues/14460,1,['bottleneck'],['bottlenecks']
Performance,"### What happened?. Batch now has the ability to offer a metadata server endpoint in the network namespaces of a `DockerJob`. We should add this functionality to `JVMJob`s so that QoB jobs can use the google default credential flow instead of relying on a GSA key file in the job container. While the implementation here could be trivial, we should make sure to load test it properly as QoB jobs can be very short (100s of ms). The simplest route would be to create/close a metadata server in the `JVMContainer` at the start/end of every job. If this incurs a penalty, since these `JVMContainer`s are long-lived we can run a long-lived metadata server and swap out the underlying credentials when user jobs start/stop. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14487:362,load,load,362,https://hail.is,https://github.com/hail-is/hail/issues/14487,1,['load'],['load']
Performance,"### What happened?. Below is a high level overview of how the batch driver communicates scheduled jobs to worker nodes. Scheduling loop on the driver:; 1. Select N ready jobs from the database to schedule on available workers; 2. Compute placement of a subset of the jobs in available slots in the worker pool; 3. Concurrently call `/api/v1alpha/batches/jobs/create` on available workers for each placed job. If/when the request completes successfully, the job is marked as scheduled.; 4. Once all requests complete, goto 1. On the worker, what happens inside `/api/v1alpha/batches/jobs/create`:; 1. Read metadata describing the job to schedule from the request body; 2. Using that information, load the full job spec from blob storage; 3. Spawn a task to run the job asynchronously; 4. Respond to the driver with a 200. The key point relevant to this issue is that the driver currently must wait for all the requests to workers in an iteration to complete before it starts the next iteration of the scheduler. This leaves the scheduler vulnerable to problematic workers or workers that happen to be preempted during the scheduling process. So, the driver sets a [2 second timeout](https://github.com/hail-is/hail/blob/b27737f67bf9e69f1abed2fec07fc7c921790ef8/batch/batch/driver/job.py#L585) on the call to `/api/v1alpha/batches/jobs/create`. Additionally, this general design means that in the event of a request timeout or transient error, Batch cannot guarantee that there is always at most one concurrent running attempt for a given job. This ends up being a fine (and intentional) concession in practice because the idempotent design of preemptible jobs tends to cover this scenario, but it is regardless wasted compute and cost to users. Nevertheless, we strive to minimize cases where we might halt the scheduling loop or double-schedule work, and one way to do that in the current design is to minimize the variance in latency of `/api/v1alpha/batches/jobs/create`. The largest source of this ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14456:314,Concurren,Concurrently,314,https://hail.is,https://github.com/hail-is/hail/issues/14456,2,"['Concurren', 'load']","['Concurrently', 'load']"
Performance,"### What happened?. Ben submitted a pipeline where the first 85% of jobs run in us-central1 while the last 15% run in us-east1. The autoscaler only looks at the head of the job queue and then sorts the result set to figure out the regions to spin up instances in. The scheduler looks at the entire job queue and then sorts the result set to figure out the regions to spin up instances in. The sort order placed us-east1 before us-central1. Concretely, the autoscaler is spinning up instances in us-central1 only while the scheduler is trying to schedule jobs in us-east1. See also: https://github.com/hail-is/hail/pull/13268. ### Version. 0.2.118. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13269:177,queue,queue,177,https://hail.is,https://github.com/hail-is/hail/issues/13269,2,['queue'],['queue']
Performance,"### What happened?. Consider these two programs. The first is faster, taking advantage of the fact that the count of the number of first alternate allele observations depends only on `variant_data`. Could Hail do this automatically? Could we modify variant_qc to facilitate this optimization?; ```; vds = hl.vds.read_vds(""some_very_big.vds""); mt = vds.variant_data; mt = hl.split_multi_hts(mt); mt = hl.variant_qc(mt); mt = mt.annotate_rows(AC100 = mt.variant_qc.AC[1] > 99); mt = mt.filter_rows(mt.AC100); vds.variant_data = mt; mt = hl.vds.to_dense_mt(vds); mt.write(""filtered.mt"", overwrite=True); ```; ```; vds = hl.vds.read_vds(""some_very_big.vds""); mt = hl.vds.to_dense_mt(vds); mt = hl.split_multi_hts(mt); mt = hl.variant_qc(mt); mt = mt.annotate_rows(AC100 = mt.variant_qc.AC[1] > 99); mt = mt.filter_rows(mt.AC100); mt.write(""filtered.mt"", overwrite=True); ```. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13695:279,optimiz,optimization,279,https://hail.is,https://github.com/hail-is/hail/issues/13695,1,['optimiz'],['optimization']
Performance,"### What happened?. Creating a billing project is fairly annoying right now. I have to perform 2 + N_USERS form submission:. 1. Create the billing project on the billing project page (I have to scroll to the bottom of the page, which is now large).; 2. Go to the billing limit page and set the limit (I have to search for the BP name).; 3. For each user, go to the billing project page, search for the BP name, enter the first user, press enter. I want a form like:. ```; Billing project name: ______; Billing project limit: ______; Billing project users:; _____; _____; _____; ```. The users should just be a multi-line textbox. It should disable autocorrect etc. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13859:87,perform,perform,87,https://hail.is,https://github.com/hail-is/hail/issues/13859,1,['perform'],['perform']
Performance,"### What happened?. Currently, almost all of our tests are integration tests which require:; 1. Compiling Scala code.; 2. Building a JAR (takes ~30 seconds on my MBP); 3. Running pytest (can take as long as 20 seconds). All of this is a lot slower than iterating with a live running Scala process. We should have tests of various parts of the compiler operating at the IR level. For example, MatrixIR to TableIR lowering should have plenty of in Scala IR-level tests. Likewise for TableIR to CDAIR. The optimizer/simplifier should also have tests at each level which assert certain kinds of code is sufficiently cleaned up by the optimizer. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13638:503,optimiz,optimizer,503,https://hail.is,https://github.com/hail-is/hail/issues/13638,2,['optimiz'],['optimizer']
Performance,"### What happened?. Currently, in order to change the rate limit in `internal-gateway`, one has to manually edit `envoy.py` and redeploy CI. This is non-standard, time intensive and can be accidentally reverted if CI merges a new commit to `main`. CI already regularly updates the envoy configuration `internal-gateway` uses to account for services in new namespaces, so making the rate limit configurable should be a simple CRUD task that would greatly ease operation of batch under high load. One gotcha to keep in mind is that while we run CI as a control plane for our ""dynamic cluster topology"", it should still be possible to manually deploy `internal-gateway` in a standalone Batch cluster (see `internal-gateway/Makefile`), so `envoy.py` should still be runnable as a standalone script. ### Version. 0.2.128. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14399:489,load,load,489,https://hail.is,https://github.com/hail-is/hail/issues/14399,1,['load'],['load']
Performance,"### What happened?. Currently, the `ServiceBackend`'s implementation of collect distributed array submits a job group full of worker jobs (1 per partition) and waits for the job group to complete before reading the results of the worker jobs. For small analyses this is fine, but when a query has tens of thousands of partitions it can take time to schedule and complete all of the worker jobs and reading back those results on the driver can become a bottleneck. Below is one possible solution to this problem:. #### Expose log for job completions in a job group. The Query Driver should attempt to read worker job results while the stage is running, but to do this it needs the Batch API to provide an append-only log of completed jobs in a job group that the Query Driver can consume instead of issuing O(jobs) job status requests during each stage. It may be that this is already possible with the current database schema, but can at worst be achieved by creating an indexed column on jobs that contain the spot they completed in in the job group. . Completion of this feature would require:; - Carefully evaluating the Batch data model to determine if there are any database changes necessary to construct an append-only log of job completions in a job group from the state of the database; - If changes are needed, design and implement a batch front end API endpoint to query the log; - (Separately) Add support for streaming the log in the Scala BatchClient and use it to read partition results before the job group completes. ### Version. 0.2.132. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14607:452,bottleneck,bottleneck,452,https://hail.is,https://github.com/hail-is/hail/issues/14607,1,['bottleneck'],['bottleneck']
Performance,"### What happened?. Daniel M is seeing this bug running the script [here](https://github.com/broadinstitute/tgg_methods/blob/master/vrs/vrs_annotation_batch.py#L275) with the flag --annotate-original. The Zulip thread [here](https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Batch.20backend.20.22multiple.20clients.20uploading.22.3F/near/353337596) has more context as well. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2150, in run; await self.jvm.execute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:781,concurren,concurrent,781,https://hail.is,https://github.com/hail-is/hail/issues/12950,3,['concurren'],['concurrent']
Performance,### What happened?. Details here: https://discuss.hail.is/t/subset-matrix-table-to-a-medium-sized-list-of-variants/3362/5. ```; Java stack trace:; java.lang.ClassCastException: class org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to class is.hail.variant.Locus (org.apache.spark.sql.catalyst.expressions.GenericRow is in unnamed module of loader 'app'; is.hail.variant.Locus is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @62435e70); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:124); at is.hail.expr.JSONAnnotationImpex$.$anonfun$exportAnnotation$5(AnnotationImpex.scala:129); at is.hail.expr.JSONAnnotationImpex$.$anonfun$exportAnnotation$5$adapted(AnnotationImpex.scala:128); at scala.collection.generic.GenTraversableFactory.tabulate(GenTraversableFactory.scala:150); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:128); at is.hail.types.virtual.Type.toJSON(Type.scala:184); at is.hail.expr.JSONAnnotationImpex$.$anonfun$exportAnnotation$4(AnnotationImpex.scala:125); at is.hail.utils.Interval.toJSON(Interval.scala:103); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:125); at is.hail.expr.JSONAnnotationImpex$.$anonfun$exportAnnotation$1(AnnotationImpex.scala:113); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at scala.collection.TraversableLike.map(TraversableLike.scala:238); at scala.collection.TraversableLike.map$(TraversableLike.scala:231); at scala.collection.AbstractTraversable.map(Traversable.scala:108); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:113); at is.hail.expr.ir.Pretty.header(Pretty.scala:405); at is.hail.expr.ir.Pretty.pretty$1(Pretty,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:360,load,loader,360,https://hail.is,https://github.com/hail-is/hail/issues/13046,2,['load'],['loader']
Performance,"### What happened?. Figure out why the k8s cache fails. Is this due to asyncio task cancellation? Is it a known rare transient error?. If this is a rare transient error, we should retry this a limited number of times. Example: https://batch.hail.is/batches/8071211/jobs/186. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13909:43,cache,cache,43,https://hail.is,https://github.com/hail-is/hail/issues/13909,1,['cache'],['cache']
Performance,"### What happened?. Hail Batch never forgets a batch. All batches, jobs, and attempts are forever persisted in the Batch database. This is rarely a performance problem, as the indexes ensure that old rows are rarely ever looked at, but the fact that the database storage is monotonically increasing is something that we have to reckon with, and it makes migrations very time intensive. There are certainly many improvements that can be made to waste less space in the database (like #14623), but ultimately we will need to make a decision about how long we should persist batches. We should quantify the utility of historic batches, what might be a good cutoff or alternative process for expiring batches, and whether we should provide some sort of export that users can use to own information about their batches. I imagine the most relevant information would be cost and logs. ### Version. 0.2.132. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14626:148,perform,performance,148,https://hail.is,https://github.com/hail-is/hail/issues/14626,1,['perform'],['performance']
Performance,"### What happened?. Hail's google/azure credential classes do not require the caller to specify scopes when requesting access tokens, and thus default to a [very wide set of scopes](https://github.com/hail-is/hail/blob/91f5a0bfc30927014b60b11a353a4d95db009427/hail/python/hailtop/aiocloud/aiogoogle/credentials.py#L140), making those access tokens excessively powerful. An access token does not need to have the `https://www.googleapis.com/auth/appengine.admin` scope to read a blob from GCS. This poses an unnecessary risk if such a token were leaked. These classes should instead require that scopes be specified when requesting an access token, and call sights should specify the minimum set of scopes necessary to perform their function. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13530:718,perform,perform,718,https://hail.is,https://github.com/hail-is/hail/issues/13530,1,['perform'],['perform']
Performance,"### What happened?. Hana Snow is the engineer for SEQR. Previously, SEQR used elastic search as its datastore. Unfortunately, elastic search was very expensive because, to get reasonable performance, SEQR indexed nearly every field. The ES index was huge and the VM resources necessary to run an ES instance on that index were expensive (like 1000s USD per month). I've been supporting Hana as much as I can, but she needs someone who can be more dedicated and responsive than me. She uses a k8s cluster. She has a SEQR frontend deployment. She also has a Hail deployment (statefulset maybe?). The Hail pod has an SSD mounted read-only. That SSD has all the SEQR data in Hail Table form. There are many tables with annotations (variant metadata, like ""probability this variant is damaging"" or ""likely causes this to happen to the protein""). There are also ""per-family"" tables which contain all the sequences within a single family. Many queries are directly against a particular family. Those tables are small and quick to read. There's also one giant table containing all the sequences from all the families. That table is large and expensive to read. A lot of our engineering work has been around making sure queries against that table are fast. Tim, at one point, had enough of her system locally that he could experiment with running queries on his laptop against his SSD. He hacked on the queries themselves and on Hail itself until the bandwidth was fast enough that the queries should complete fast enough on the full dataset. Fast enough varies but generally a couple tens of seconds is OK. The work here is to pair with Hana to diagnose performance issues and make changes until the queries are acceptably fast. The first thing I would do is update her to the latest Hail (with the array decoder improvement as well as the memory overhead stuff on which Daniel is working). Then, with Hana's help, test the timing of some queries. If the queries are still too slow, your options are:; 1. Chec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882:187,perform,performance,187,https://hail.is,https://github.com/hail-is/hail/issues/13882,1,['perform'],['performance']
Performance,"### What happened?. Hello,. It looks like Hail has a hard coded check to only run on Java 8 and 11, despite Spark supporting Java 17 for a couple years now, including on spark 3.3.x, which is the currently used release for `pip install hail`: https://spark.apache.org/releases/spark-release-3-3-0.html#build. **Would it be possible to add Java 17 support**, or possibly even remove the Java version check in general so that it can track what underlying Spark does without additional updates? . There are a bunch of benefits of moving to Java 17, including:; 1. https://kstefanj.github.io/2021/11/24/gc-progress-8-17.html - Significant garbage collector improvements that will likely improve throughput and reduce costs; 2. https://vmnotescom.wordpress.com/2021/09/14/java-17-whats-new-removed-and-preview-in-jdk-17/ - Better Apple Silicon support. I know that darwin-aarch64 has been backported to 8 and 11, but 17 is faster on that platform.; 3. https://spark.apache.org/releases/spark-release-3-5-0.html#removals-behavior-changes-and-deprecations - The next release of Spark will require Java 17 as a minimum version, and making the change now is easier than making more changes all at once in the future.; . > The following features will be removed in the next Spark major release; > ; > Support for Java 8 and Java 11, and the minimal supported Java version will be Java 17; > Support for Scala 2.12, and the minimal supported Scala version will be 2.13. Also, requiring specifically Java 8 or 11 has led to some friction for students and researchers who are first evaluating hail. In the past few weeks, I've talked to a lot of students and researchers who wanted to evaluate hail, followed the documentation to install Azul Java 8 but already had an existing Java install and did not update their PATH or JAVA_HOME. Most of their existing Java versions were 17, as 17 is the current default on most Linux distros and a common one to have been installed via Brew in the past few years on Mac. Alt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433:691,throughput,throughput,691,https://hail.is,https://github.com/hail-is/hail/issues/14433,1,['throughput'],['throughput']
Performance,"### What happened?. Hi,; I am on a macOS Ventura and I have successfully installed hail (v 0.2.109) on a conda env. Everything seems to run properly, except that I don't get any plots. Bokeh was installed in the env, v1.4.0., pysark =3.13 and scala=2.11.8 are some relevant packages that may contribute to this issue. When starting Hail, this is the output I get:. 2023-02-20 11:07:38.798 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.1.3; SparkUI available at http://amaru-2.local:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.109-b71b065e4bb6; LOGGING: writing to /Users/alanmejiamaza/hail-20230220-1107-0.2.109-b71b065e4bb6.log. It seems to be that the issue comes from the spark version? which is the correct spark version for a conda env on a mac? I have followed the tutorials and seemed to work fine except for the plots. I don't have any output when invoking commands for plots. Can anyone tell me the specific versions needed to run all Hail properties?. Thanks. ### Version. 0.2.109. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717:426,load,load,426,https://hail.is,https://github.com/hail-is/hail/issues/12717,1,['load'],['load']
Performance,"### What happened?. In older versions of hail (tested with 0.2.115), when starting a dataproc cluster with VEP, e.g.; ```{bash}; hailctl dataproc start hail-test --region australia-southeast1 --project my-project --vep GRCh38 --packages gnomad --num-workers 2; ```; the dataproc cluster command would be provided the following environment variable through the `--metadata` flag: `VEP_REPLICATE=aus-sydney`. This variable is used within the script `gs://hail-common/hailctl/dataproc/0.2.115/vep-GRCh38.sh` to determine which bucket to pull the VEP cache data from. In more recent versions (tested with 0.2.130), this `VEP_REPLICATE` variable has been changed to `VEP_REPLICATE=australia-southeast1`, however the Australian bucket containing the VEP cache data is still `aus-sydney`, meaning that the VEP data is not copied into the dataproc cluster, and when trying to run VEP I get the error `No cache found for homo_sapiens, version 95`. ### Version. 0.2.130. ### Relevant log output. ```shell; FatalError: HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:547,cache,cache,547,https://hail.is,https://github.com/hail-is/hail/issues/14513,3,['cache'],['cache']
Performance,"### What happened?. JVMJobs exist to provide a warm JVM to Hail jobs; however, in practice, there are two issues:; 1. A JVM warmed for one JAR (i.e. version) of Hail has limited benefit for a different JAR. Only shared classes like those in `java.util` could have been JITed.; 2. As the number of non-JVM running jobs grows, the likelihood that a JVMJob lands on a worker with a warm JVM decreases. Suppose instead that, as a part of the deploy process, we executed a series of Hail pipelines using the LocalBackend and export the JIT cache. We then store *both* the JAR and the JIT cache in GCS. A user job loads both the JAR and the JIT cache and starts a fresh JVM that loads from that JIT cache. Every JVMJob now, by definition, lands on a hot JVM. References; - ""Compile Stashing"" https://docs.azul.com/prime/Compile-Stashing; - ""Tuning JIT Compilations"" https://docs.azul.com/prime/analyzing-tuning-warmup#tuning-jit-compilations; - ""ReadyNow Warm-Up Optimizer"" https://docs.azul.com/prime/analyzing-tuning-warmup#use-readynow-warm-up-optimizer. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13675:535,cache,cache,535,https://hail.is,https://github.com/hail-is/hail/issues/13675,8,"['Optimiz', 'cache', 'load', 'optimiz']","['Optimizer', 'cache', 'loads', 'optimizer']"
Performance,"### What happened?. Julia Sealock reported this https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/vep.20issue/near/352790173. We also saw it in test_dataproc. Cal also reported it.; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 4.0 failed 20 times, most recent failure: Lost task 56.19 in stage 4.0 (TID 48622) (jsealock-schema-sw-43bq.c.daly-neale-sczmeta.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 125; VEP Error output:; docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.; See 'docker run --help'. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.methods.VEP$.waitFor(VEP.scala:73); 	at is.hail.methods.VEP.$anonfun$execute$5(VEP.scala:231); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.utils.richUtils.RichContextRDD$$anon$1.hasNext(RichContextRDD.scala:69); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collectio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:561,cache,cache,561,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['cache'],['cache']
Performance,### What happened?. Local java tests failing. Caused by https://github.com/hail-is/hail/pull/13551. ```; java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.lib.input.TextInputFormat; at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581); at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178); at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522); ```. ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706:220,load,loader,220,https://hail.is,https://github.com/hail-is/hail/issues/13706,5,['load'],"['loadClass', 'loader']"
Performance,"### What happened?. Struct decoding currently uses `Region.loadBit` which:; 1. Calculates the address of the byte has this bit (e.g. the 65th bit is in the second byte).; 2. Loads the byte out of memory.; 3. Masks the bit out of the byte.; 4. Compares to zero. We don't have concrete data, but we suspect that the JVM can't avoid loading the byte out of memory 8 times. If we can instead load it once per 8 missing fields, there may be a speed up for structs that are frequently decoded (e.g. an entry struct). ### See also. - https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107 . ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13811:59,load,loadBit,59,https://hail.is,https://github.com/hail-is/hail/issues/13811,4,"['Load', 'load']","['Loads', 'load', 'loadBit', 'loading']"
Performance,"### What happened?. Suppose you're working with the [Wheat genome](https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Other.20genome/near/397467764). The following is seemingly correct code but it doesn't work:; ```python3; import hail as hl. rgwheat = hl.ReferenceGenome('Wheat', ...). hl.init(default_reference=rgwheat); ```; The first problem is that the `@typecheck` on `hl.init`, `hl.init_spark`, etc. only allows a built-in reference genome. . Even if we relax that requirement, we encounter a deeper problem: creating the reference genome initializes Hail. In particular, [we call `Env.backend()`](https://github.com/hail-is/hail/blob/main/hail/python/hail/genetics/reference_genome.py#L117-L118) (which calls `Env.hc()`, which forces initialization) so that we can call `add_reference`. What does initialization mean? Historically, it meant connection to or starting a JVM/Spark process. In QoB/ServiceBackend, initialization just loads configurations, it doesn't really do anything irreversible. Regardless of what it does, we only allow initialization *once*. OK, so, there's two possible routes to fix this problem:; 1. Rewrite `ReferenceGenome.__init__` such that it does not initialize Hail. You have to decide how reference genomes are ultimately communicated to the backend. Do you hang a list of all created reference genomes off of the `ReferenceGenome` class? Do you require explicit registering a la `hl.register_reference`? The latter seems a bit silly. The former seems OK, but you could also ...; 2. Allow modification of the default reference after initialization. The default reference genome is just a field on the HailContext: `_default_ref` which is accessed through `hl.default_reference()`. Just modify `hl.default_reference` to *return* the reference with no arguments and *set* the reference with one argument. Now this works:. ```python3; import hail as hl; rgwheat = hl.ReferenceGenome('Wheat', ...); hl.default_reference(rgwheat); mt = hl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13856:968,load,loads,968,https://hail.is,https://github.com/hail-is/hail/issues/13856,1,['load'],['loads']
Performance,"### What happened?. TBD. ### Version. 0.2.172. ### Relevant log output. ```shell; 24/02/05 11:52:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 3.3.2; SparkUI available at http://192.168.1.140:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d82c34a83360; LOGGING: writing to /private/tmp/varo/avro/hail-20240205-1152-0.2.127-d82c34a83360.log; 2024-02-05 11:53:03.679 Hail: INFO: import_gvs: Importing and writing site filters to temporary storage; Traceback (most recent call last):; File ""/Users/dking/projects/gatk/scripts/variantstore/wdl/extract/hail_gvs_import.py"", line 180, in <module>; create_vds(arguments, vds_path, references_path, temp_path, use_classic_vqsr,; File ""/Users/dking/projects/gatk/scripts/variantstore/wdl/extract/hail_gvs_import.py"", line 35, in create_vds; import_gvs.import_gvs(; File ""<decorator-gen-1896>"", line 2, in import_gvs; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/gatk/scripts/variantstore/wdl/extract/import_gvs.py"", line 211, in import_gvs; site.write(site_path, overwrite=True); File ""<decorator-gen-1224>"", line 2, in write; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/table.py"", line 2002, in write; Env.backend().execute(; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/Users/dking/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py"", l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14249:133,load,load,133,https://hail.is,https://github.com/hail-is/hail/issues/14249,1,['load'],['load']
Performance,"### What happened?. The basic problem is:; 1. The type for `ArrayMaximalIndependentSet` is `TArray(...)` where `...` is whatever the node type is. ; 2. We choose a PType based on the Type.; 3. We choose an SType based on the PType.; 4. `unwrapReturn` makes an incorrect assumption about which SType corresponds to a `TArray(String)`. In particular,; ```; Code.invokeScalaObject1[UnsafeIndexedSeq, IndexedSeq[Any]](Graph.getClass, ""maximalIndependentSet"", jEdges); ```; returns a Java array of whatever `svalueToJavaValue` returns. In that case, that's a `String[]` which we call `SJavaArrayString`. However, the SType chosen for `TArray(TString)` is `SIndexablePointer(SBinary)`. **I think the real fix here is to just pass region pointers into MaximalIndependentSet.** Just get an `elementIterator` from `PCanonicalArray` and use `loadElement`, etc. to populate the `Graph`. ```; In [1]: import hail as hl; ...: ht = hl.Table.parallelize([hl.Struct(i='A', j='B', kin=0.25), hl.Struct(i='A', j='C', kin=0.25), hl.Struct(i='D', j='E', kin=0.5)]); ...: hl.maximal_independent_set(ht.i, ht.j, False).collect(); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); Cell In[1], line 3; 1 import hail as hl; 2 ht = hl.Table.parallelize([hl.Struct(i='A', j='B', kin=0.25), hl.Struct(i='A', j='C', kin=0.25), hl.Struct(i='D', j='E', kin=0.5)]); ----> 3 hl.maximal_independent_set(ht.i, ht.j, False).collect(). File <decorator-gen-1148>:2, in collect(self, _localize, _timed). File ~/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py:584, in _make_dec.<locals>.wrapper(__original_func, *args, **kwargs); 581 @decorator; 582 def wrapper(__original_func, *args, **kwargs):; 583 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 584 return __original_func(*args_, **kwargs_). File ~/miniconda3/lib/python3.10/site-packages/hail/table.py:2162, in Table.collect(self, _localize, _timed);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13633:832,load,loadElement,832,https://hail.is,https://github.com/hail-is/hail/issues/13633,1,['load'],['loadElement']
Performance,### What happened?. The hailgenetics/hail and hailgenetics/hailtop images are commonly used by our users (the latter is used for the remove_tmpdir job). We should eagerly cache a few recent versions on the batch worker VMs to accelerate this common workload. ### Version. 0.2.124. ### Relevant log output. _No response_,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13913:171,cache,cache,171,https://hail.is,https://github.com/hail-is/hail/issues/13913,1,['cache'],['cache']
Performance,"### What happened?. The spark and local backends use `py4j` to execute methods on java backends. `py4j` uses a TCP socket and a text-based protocol to communicate between python and the jvm and handles marshaling of data between the two processes. Unfortunately it has poor memory performance with large byte arrays, as the text protocol requires base64 encoding byte arrays and it uses Java `String`s which, being UTF-16, more than double the size of the original data in memory. Hail should not use `py4j` for these operations and just open its own connection to the java backend. This gives us the control to not use more memory than is necessary to just ship bytes back and forth. This also provides an opportunity to deduplicate some code as the `ServiceBackend` already communicates writes its inputs over a socket instead of using `py4j` (there is no live JVM to communicate to in the `ServiceBackend` case, so it must serialize the requested operation to be run at a later time on a different machine). ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13756:281,perform,performance,281,https://hail.is,https://github.com/hail-is/hail/issues/13756,1,['perform'],['performance']
Performance,"### What happened?. This is a known issue in our underlying serialization library: https://github.com/uqfoundation/dill/issues/609. I've discovered that if you have dill 0.3.7 (latest) and try to use our image (which has dill 0.3.5.1) you get this error:. Traceback (most recent call last):; File ""<string>"", line 27, in <module>; File ""/usr/local/lib/python3.10/site-packages/dill/_dill.py"", line 373, in load; return Unpickler(file, ignore=ignore, **kwds).load(); File ""/usr/local/lib/python3.10/site-packages/dill/_dill.py"", line 646, in load; obj = StockUnpickler.load(self); File ""/usr/local/lib/python3.10/site-packages/dill/_dill.py"", line 805, in _create_code; return CodeType(args[0], 0, 0, *args[1:]); TypeError: code expected at most 16 arguments, got 19; Switching local dill to the remote version eliminates the error. Fix is to ensure the *de*-serializer is >0.3.5.1. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13535:406,load,load,406,https://hail.is,https://github.com/hail-is/hail/issues/13535,4,['load'],['load']
Performance,"### What happened?. This job is just issuing network requests, very little CPU. ---. Longer term musing:. They're also example of a relatively fast running job for which we'd probably want a ""fast queue"" where you get evicted after some short period of time, like 2 minutes. We could have these kinds of jobs fail over to ""slow queues"" if they run over time. This kind of queue is also valuable to use-cases like SEQR and other low latency UIs. Also, certain QoB jobs are known to be very fast (e.g. matrix type). ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13816:197,queue,queue,197,https://hail.is,https://github.com/hail-is/hail/issues/13816,4,"['latency', 'queue']","['latency', 'queue', 'queues']"
Performance,"### What happened?. Try writing to a bucket to which your service account has read-only access:; ```; hl.utils.range_table(5,n_partitions=5).write('gs://neale-bge/foo.ht'); ```. https://batch.hail.is/batches/8042383. The client gets an error like this:; ```; Java stack trace:; is.hail.relocated.com.google.cloud.storage.StorageException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/1-day/o/parallelizeAndComputeWithIndex%2FO3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=%2Fresult.0?alt=media; No such object: 1-day/parallelizeAndComputeWithIndex/O3mcL5QoyfBBMz0eSi7uIjGQkV3FuD5vCON_8i4r0ss=/result.0; 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(StorageException.java:165); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:298); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:729); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$20(StorageImpl.java:610); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:65); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1515); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:610); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:599); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:280); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:278); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:25); 	at is.hail.backend.service.ServiceBac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:903,load,load,903,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['load'],['load']
Performance,"### What happened?. We have reported to their GitHub, but we don't have a simple enough repro for them to make progress. https://github.com/Azure/azure-sdk-for-java/issues/35125. Personal correspondence with some MSFT researchers suggested there could be an issue with threading:; > It sort of reminds me of an issue we saw with Cromwell where their old akka pool code caused a bunch of unexpected network behavior that broke their API in certain cases. I've asked if BlobServiceClient is thread-safe or not. We share an object of that class, but none of the things it produces (e.g. blobs). We know that the java.io libraries can improperly drop an HTTP response if it is followed by a TCP RST. In particular, we've seen this happen when a server is load shedding and sends an HTTP ""429 Too Many Requests"" rapidly followed by a TCP RST. This might explain the ""Connection reset"" errors that we sometimes see. We have fewer intuitions about the ""Stream is already closed"". That specific error was reported to Azure in the aforementioned GitHub issue. We treat both stream is closed and connection reset as ""limited retry"" errors. We might retry too quickly. Our initial delay is `100ms * x` where `x` is drawn uniformly from `[0, 1]`. Perhaps we should try an initial delay of at least 1s? . For example, [Azure gives as an example retrying after 2s, 4s, 10s, and 30s](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-performance-checklist#timeout-and-server-busy-errors). Google's [code examples](https://cloud.google.com/storage/docs/retry-strategy#client-libraries_1) suggest an initial delay of 1s with a multiplier of 2. AWS seems to use 500ms as the [default base backoff for ""throttled"" exceptions](https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-core/src/main/java/com/amazonaws/retry/PredefinedBackoffStrategies.java#L39). ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13351:751,load,load,751,https://hail.is,https://github.com/hail-is/hail/issues/13351,3,"['load', 'perform', 'throttle']","['load', 'performance-checklist', 'throttled']"
Performance,"### What happened?. When I try select some rows (10) of a large matrixtable and convert it into a pandas dataframe the execution fails with `ClassTooLargeException`. The problem arises after I invoke `make_table()` and try to take some rows. I expected hail to be able to handle data with dimensions 10x3202, which is not too large. Data was downloaded from the 1000 Genomes ftp site: [link](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20201028_3202_phased/). ```; # load data; vcf_path = "".../1000G_b38_20201028_3202_phased/CCDG_14151_B01_GRM_WGS_2020-08-05_chr*.filtered.shapeit2-duohmm-phased.vcf.gz""; mt = hl.import_vcf(vcf_path, force_bgz=True). # select a few random variants; n_selected_variants = 10; selected_variants = np.random.choice(mt.rsid.collect(), n_selected_variants); selected_variants = hl.array(list(selected_variants)). (; mt.filter_rows(selected_variants.contains(mt.rsid)); .select_rows('rsid'); .select_entries('GT'); ).count(); ```; ```; [Stage 18:=====================================================>(255 + 1) / 256]; (10, 3202); ```. Trying to convert to pandas dataframe `.make_table().to_pandas()`, or even just taking 1 row `.make_table().take(1)` results in the following error:; ```; (; mt.filter_rows(selected_variants.contains(mt.rsid)); .select_rows('rsid'); .select_entries('GT'); ).make_table().take(1); ```; ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); Cell In[10], [line 5](vscode-notebook-cell:?execution_count=10&line=5); [1](vscode-notebook-cell:?execution_count=10&line=1) (; [2](vscode-notebook-cell:?execution_count=10&line=2) mt.filter_rows(selected_variants.contains(mt.rsid)); [3](vscode-notebook-cell:?execution_count=10&line=3) .select_rows('rsid'); [4](vscode-notebook-cell:?execution_count=10&line=4) .select_entries('GT'); ----> [5](vscode-notebook-cell:?execution_count=10&line=5) ).make_table().take(1). File ...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:516,load,load,516,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['load'],['load']
Performance,"### What happened?. When a job creates a log file in excess of 2GiB the batch worker can get into a bad state as it fails to upload the log. It does not stream the log file from disk, instead loading the whole log into memory as `bytes` and tries to upload that, but asyncio ssl has a limit of max-int sized non-streaming payloads. The batch worker should stream logs from disk when uploading them. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13329:192,load,loading,192,https://hail.is,https://github.com/hail-is/hail/issues/13329,1,['load'],['loading']
Performance,"### What happened?. When a job creates a log file in excess of about half a GB, loading the job page can cause the batch front-end pod to crash as it loads the log file into memory and interpolates it directly into the job page. The front-end should instead:. - Fully stream job logs in the log endpoint; - Show a truncated view of the log in the job page, with a pointer to download the full log if it's truncated. ### Version. 0.2.120. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13328:80,load,loading,80,https://hail.is,https://github.com/hail-is/hail/issues/13328,2,['load'],"['loading', 'loads']"
Performance,"### What happened?. When the QoB client on a user's laptop sends a request to create a QoB job, it sends a `jar_spec` parameter as part of the job spec that is either:; - `git_revision`: the git SHA that the hail was built with. The Batch front end takes this and resolves a URL for the published JAR that was created when that commit was merged to `main`.; - `jar_url`: A blob storage URL that points directly to the JAR to use. The Batch front end ensures that this URL is trusted. The `jar_url` setting is mainly for development and debugging purposes, allowing a dev or user to set a URL to a development JAR instead of using a merged commit. In normal configuration fashion, it is possible to set `jar_url` in `hailctl config`. This is an enormous footgun, as users may forget to unset this configuration and continue using the dev jar *even after they install a different hail wheel*. We must do two things:; 1. Remove the ability to set the jar_url through `hailctl` so as to avoid this footgun. Batch should also fully remove support for `jar_url`s so that any users who might be inadvertently using it are loudly alerted (though I suspect there are few if any such users now).; 2. Remove entirely the ability to specify a JAR other than that which was built along with the installed wheel. The proposed plan is to always send `git_revision` for QoB jobs. In order to enable development JARs, Batch should be augmented to search first for production JARs matching a certain revision, and then if that fails search a specified `dev/` subdirectory for the requested revision. These development JARs should not be cached on workers so as to enable debugging development without constant committing. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14539:1619,cache,cached,1619,https://hail.is,https://github.com/hail-is/hail/issues/14539,1,['cache'],['cached']
Performance,"### What happened?. `hl.maximal_independent_set` should return the same independent set regardless of the ordering of the input table. gnomAD team reports that the returned set can differ depending on whether or not the input table had been written or came directly from PC-Relate. I have yet to create a simple reproducible example. Permuting the entries in this array does not change the output. I always get 'a' and 'b'. I suspect this is because what really matters is the order in which we traverse the entries of the multi map which depends on the hash of the nodes. I think a durable fix might be to eliminate the MultiMap, insert all the nodes into the binary heap, then increment priority for each edge detected. This will perform more reflows of the heap, but eliminates the non-determinism of MultiMap iteration order. ```; import hail as hl; ht = hl.Table.parallelize([; hl.Struct(i=hl.Struct(s=x[0]), j=hl.Struct(s=x[1])); for x in [('c', 'a'), ('a', 'b'), ('b', 'c'), ]; ]); hl.maximal_independent_set(ht.i, ht.j, False).collect(); ```. ### Version. 0.2.122. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13635:732,perform,perform,732,https://hail.is,https://github.com/hail-is/hail/issues/13635,1,['perform'],['perform']
Performance,"### What happened?. `query_billing_projects_with_cost` runs a `select_and_fetchall` query against the database to load information about certain billing projects. It currently locks the affected rows in share mode, but I don't believe there's any reason to do this and it can lead to deadlock errors in `monitor_billing_limits`. It is also worth noting that in `monitor_billing_limits`, we might not want to reuse this method at all, as it only needs to load rows from the database that have exceeded their billing limit. In practice currently this doesn't much matter as the number of billing projects is fairly small, but it is still not ideal. ### Version. 0.2.128. ### Relevant log output. ```shell; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 900, in retry_long_running; return await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 944, in loop; await f(*args, **kwargs); File ""/usr/local/lib/python3.9/dist-packages/batch/driver/main.py"", line 1288, in monitor_billing_limits; records = await query_billing_projects_with_cost(db); File ""/usr/local/lib/python3.9/dist-packages/batch/utils.py"", line 165, in query_billing_projects_with_cost; async for record in db.select_and_fetchall(sql, tuple(args)):; File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 339, in select_and_fetchall; async for row in tx.execute_and_fetchall(sql, args, query_name):; File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 254, in execute_and_fetchall; await cursor.execute(sql, args); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/loc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14423:114,load,load,114,https://hail.is,https://github.com/hail-is/hail/issues/14423,2,['load'],['load']
Performance,"### What happened?. gnomAD went with a custom CUDA-implementation of KING instead of PC-Relate because `hl.pc_relate` was very slow on their gnomAD v4 1M sample dataset. I'm fairly certain the bottleneck is writing out a 1M by 1M dense matrix of 64-bit floating point numbers (aka the relatedness matrix). This matrix is too large. Our users only care about the small subset of entires indicating close relatedness between the samples [1]. Instead of writing a dense BlockMatrix, we should write a Hail Table with the columns `sample1`, `sample2`, and `kinship`. I have some (very old) skeleton code for this [here](https://github.com/hail-is/hail/compare/main...danking:hail:sparse-pc-relate). If I recall correctly, this only calculates the kinship coefficient, not the full set of coefficients. I'm not sure it even works currently, but it demonstrates how we can generate a BlockMatrixIR directly rather than trying to construct it using the Python-level API. ---. #### Footnotes. [1] Consider Figure 2 from [the PC-Relate paper (Conomos, et al. 2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/). Beyond 3rd degree relatedness (i.e. avuncular pair), SNP-based relatedness (as opposed to haplotype-based) isn't reliable (see Figures 6 and 7). 3rd degree pair have have kinship 0.0625, in expectation, so only keeping entries >=0.03 is very reasonable. gnomAD is even more aggressive [considering only 2nd degree or higher pairs](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/), which presumably corresponds to keeping only entries >=0.0625. ---. #### Further reading. - https://en.wikipedia.org/wiki/Coefficient_of_relationship. ### Version. 0.2.124",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13798:193,bottleneck,bottleneck,193,https://hail.is,https://github.com/hail-is/hail/issues/13798,2,"['bottleneck', 'load']","['bottleneck', 'loadings-and-random-forest-classifier-on-your-dataset']"
Performance,### What happened?. https://ci.azure.hail.is/batches/3775837/jobs/45. ```; E java.util.concurrent.ExecutionException: java.lang.RuntimeException: Stream is already closed.; E 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); E 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); E 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:150); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:44); E 	at __C256669Compiled.__m256730split_CollectDistributedArray(Emit.scala); E 	at __C256669Compiled.__m256689split_Let(Emit.scala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distributedSort(LowerDistributedSort.scala:163); E 	at is.hail.backend.service.ServiceBackend.lowerDistributedSort(ServiceBackend.scala:354); E 	at is.hail.backend.Backend.lowerDistributedSort(Backend.scala:100); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.$anonfun$apply$1(LowerAndExecuteShuffles.scala:23); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$4(RewriteBottomUp.scala:26); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:36); E 	at is.hail.expr.ir.lowering.LowerAndExecuteShuffles$.apply(LowerAndExecuteShuffles,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:87,concurren,concurrent,87,https://hail.is,https://github.com/hail-is/hail/issues/12976,3,['concurren'],['concurrent']
Performance,"### What happened?. https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/concatenate.20data.20from.20multiple.20files.20into.20table. - [ ] skip_n_rows parameter to import_table which skips the first n rows.; - [ ] add a filename field. This is all in service of loading a particular kind of single cell data format. Matrix MTX format https://broadinstitute.github.io/wot/file_formats/#:~:text=The%20MTX%20format%20is%20a,row%20and%20column%20indices%2C%20respectively. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14072:290,load,loading,290,https://hail.is,https://github.com/hail-is/hail/issues/14072,1,['load'],['loading']
Performance,"### What happened?. https://hail.zulipchat.com/#narrow/stream/128581-Cloud-support/topic/Inefficient.20computing.20in.20AoU.20workbench. At least the first one seems to be a genuine missed optimization in Hail; ```; import hail as hl; import os; bucket = os.getenv(""WORKSPACE_BUCKET""); vds_srwgs_path = os.getenv(""WGS_VDS_PATH""); vds = hl.vds.read_vds(vds_srwgs_path); vds = hl.vds.split_multi(vds, filter_changed_loci=False); vmt = vds.variant_data; vht = vmt.rows(); vht = vht.select('filters'); vht.write(f'{bucket}/aou_vat_with_filter_wlu.ht', overwrite=True); ```; The `vmt.rows()` should have avoided all entry-level work. This should really just explode the alleles array and write that to a file. That should be relatively quick. We should be able to reproduce this on any VDS we have, and see that the IR we actually execute still references the entry data. . ### Version. 0.2.107-2387bb00ceee. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13312:189,optimiz,optimization,189,https://hail.is,https://github.com/hail-is/hail/issues/13312,1,['optimiz'],['optimization']
Performance,"### What happened?. https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/QoB.20Error.3A.20GoogleJsonResponseException.3A.20404.20Not.20Found/near/398355473. > I was running hl.pca on the wheel you created for me -> 0.2.124-fcaafc533ec1. and there seems to be a transient error going on https://batch.hail.is/batches/8069235?q=state+%3D+failed, not sure whether this is the same as the previous ones. I just cancelled the job before error summary appears. and here is the code I am running:. ```python3; vat_ht = hl.read_table(get_aou_util_path(name=""vat"")); vat_ht = vat_ht.collect_by_key(); meta_ht = hl.read_table(get_sample_meta_path(annotation=True)); meta_ht = meta_ht.filter(~meta_ht.related); pops = args.pops.split("","") if (args.pops is not None) else POPS; for pop in pops:; mt = get_filtered_mt(analysis_type='variant', filter_variants=True, filter_samples=False,; adj_filter=True, pop=pop); variants_to_keep = vat_ht.filter(; (vat_ht.locus.in_autosome()) &; (hl.is_snp(vat_ht.alleles[0], vat_ht.alleles[1])) &; (vat_ht['values'][f'gvs_{pop}_af'][0] >= 0.0001) &; ((vat_ht.values[f""gvs_{pop}_an""][0] >= (N_SAMPLES[pop] * 2 * MIN_CALL_RATE[pop]))); ); print('Filtering Variants...'); mt = mt.filter_rows(hl.is_defined(variants_to_keep[mt.row_key])) # filter to high quality variants; print('Filtering Samples...'); mt = mt.filter_cols(hl.is_defined(meta_ht[mt.col_key])) # filter to unrelated samples -> later to project; print('Running PCA...'); eigenvalues, scores, loadings = hl.pca(; hl.int(hl.is_defined(mt.GT)),; compute_loadings=True,; k=50,; ); print('Writing tables...'); eigenvalues.write(; get_pca_ht_path(pop=pop, name='evals'),; overwrite=args.overwrite,; ); scores.write(; get_pca_ht_path(pop=pop, name='scores'),; overwrite=args.overwrite,; ); loadings.write(; get_pca_ht_path(pop=pop, name='loadings'),; overwrite=args.overwrite,; ); ```. ### Version. 0.2.126. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979:1498,load,loadings,1498,https://hail.is,https://github.com/hail-is/hail/issues/13979,3,['load'],['loadings']
Performance,#10957 introduced loading GCP-specific configuration from the global-config instead of environment variables. The auth-driver uses this but doesn't have a global-config mounted.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10962:18,load,loading,18,https://hail.is,https://github.com/hail-is/hail/pull/10962,1,['load'],['loading']
Performance,#13331 moved the initialization of the `WatchedBranch`s out of the top level in `ci.py` and into the end of `on_startup`. Notice that this new location is both after `app['task_manager'].ensure_future(update_loop(app))` is run *and* after an `await`-point. It's quite likely that the first time `update_loop` is run is *before* there are any watched branches. This loop then sleeps for five minutes before it runs again. I believe this is why sometimes on startup it can take multiple minutes before CI loads PR information.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13581:503,load,loads,503,https://hail.is,https://github.com/hail-is/hail/pull/13581,1,['load'],['loads']
Performance,"#14056 added a new optional field to the deploy config, `base_path` which is intended to phase out `default_namespace`. But for backwards compatibility reasons we cannot yet remove `default_namespace`. This should all work fine without breaking any workflows like switching back and forth between namespaces so long as `base_path` is not explicitly set in a developer's deploy config. But `hailctl dev config set <property> <value>` does not just set a single property, it loads the deploy config, sets the property, and then writes the whole deploy config back. If the deploy config does anything with default values, which it now does with `base_path`, this round trip does not work. Another simpler example is that currently in main, the following will make two changes to a deploy config not one:. ```; # deploy config of {'location': 'external', 'domain': 'hail.is', 'default_namespace': 'default'}; HAIL_DOMAIN=foo hailctl dev config set location gce. # deploy config will now read {'location': 'gce', 'domain': 'foo', 'default_namespace': 'default'}; ```. This PR should change `hailctl dev config set` so that the only changes that are made to the deploy config are the single property/value change described in the command.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14169:473,load,loads,473,https://hail.is,https://github.com/hail-is/hail/pull/14169,1,['load'],['loads']
Performance,"#9634 Introduced a large performance regression in the `linear_regression_rows_nd` benchmark (making it about 4x slower). This PR fixes that by doing two things:. 1. Move all the global into one single `annotate_globals` expression, so that CSE can work properly. This required fixing a bug in some ndarray expressions that were not correctly tracking their source tables. To make sure I was only referencing the global versions of this computed things, rather accidentally recomputing, I wrapped the global setup in a function to scope the variables. This improvement was minor, didn't hit the real root of the problem. 2. Much more significantly, and not 100% clear why: `process_y_group` is now a function that returns a python dictionary, instead of a hail struct. I can guess that the allocation required by making a struct was wasteful, but it seems crazy that it was ""make the benchmark 4x slower"" amounts of wasteful. . While this is not user facing yet, would be good to get this in before an eventual 0.2.60 release if we want to avoid benchmarks regressing between versions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9666:25,perform,performance,25,https://hail.is,https://github.com/hail-is/hail/pull/9666,1,['perform'],['performance']
Performance,$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.ir.BaseIR.mapChildren(BaseIR.scala:17); at is.hail.expr.ir.Simplify$.is$hail$expr$ir$Simplify$$visitNode(Simplify.scala:30); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$$anonfun$is$hail$expr$ir$Simplify$$simplifyValue$3.apply(Simplify.scala:35); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:19); at is.hail.expr.ir.Simplify$.apply(Simplify.scala:11); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:17); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:17); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:25); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:21); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:16); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:27); at is.hail.backend.Backend.is$hail$backend$Backend$$_execute(Backend.scala:90); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:78); ...; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8338:11964,Optimiz,Optimize,11964,https://hail.is,https://github.com/hail-is/hail/issues/8338,17,['Optimiz'],['Optimize']
Performance,$.$anonfun$main$5(ServiceBackend.scala:460); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). 2023-05-04 01:04:37.742 GoogleStorageFS$: INFO: close: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/dRRY6iUfFz/out; 2023-05-04 01:04:38.077 GoogleStorageFS$: INFO: closed: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/dRRY6iUfFz/out; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:41518,concurren,concurrent,41518,https://hail.is,https://github.com/hail-is/hail/issues/12983,6,['concurren'],['concurrent']
Performance,"$.fatal(package.scala:28); 	at is.hail.expr.ir.TextTableReader$.splitLine(TextTableReader.scala:107); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:379); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7$$anonfun$apply$8.apply(TextTableReader.scala:378); 	at is.hail.utils.WithContext.map(Context.scala:33); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:378); 	at is.hail.expr.ir.TextTableReader$$anonfun$28$$anonfun$apply$7.apply(TextTableReader.scala:408); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:385); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$14.apply(ContextRDD.scala:559); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:589); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:587); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Expected output:; ```; +-----+-----------+; | idx | value |; +-----+-----------+; | str | str |; +-----+-----------+; | ""1"" | ""\""foo\"""" |; | ""2"" | ""\""bar\"""" |; | ""3"" | ""\""baz\"""" |; +-----+-----------+; ```. Hail version: 0.2.12-9409c0635781. [test.txt](https://github.com/hail-is/hail/files/3049195/test.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5796:2176,concurren,concurrent,2176,https://hail.is,https://github.com/hail-is/hail/issues/5796,2,['concurren'],['concurrent']
Performance,$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12967,Load,LoadMatrix,12967,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5645,Load,LoadMatrix,5645,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:12459,concurren,concurrent,12459,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['concurren'],['concurrent']
Performance,$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGSc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5126,concurren,concurrent,5126,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['concurren'],['concurrent']
Performance,$anonfun$parse_value_ir$2(SparkBackend.scala:691); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$1(SparkBackend.scala:345); 	at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$1(SparkBackend.scala:690); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.spark.SparkBackend.parse_value_ir(SparkBackend.scala:689); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.124-b115f6a6ec23; Error summary: ClassCastException: class is.hail.types.virtual.TStruct cannot be cast to class is.hail.types.virtual.TIterable (is.hail.types.virtual.TStruct and is.hail.types.virtual.TIterable are in unnamed module of loader 'app'); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13699:8750,load,loader,8750,https://hail.is,https://github.com/hail-is/hail/issues/13699,1,['load'],['loader']
Performance,$apply$23.apply(ContextRDD.scala:308); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5d0f74cef4f2; Error summary: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790:13798,concurren,concurrent,13798,https://hail.is,https://github.com/hail-is/hail/issues/3790,2,['concurren'],['concurrent']
Performance,$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19156,Load,LoadVCF,19156,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.jdler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.invokeChalerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at org.apacxt.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelt io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelHandlerCtractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at io.nettyectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at io.netty at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent.Default; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1493); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGScheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:23771,concurren,concurrent,23771,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at org.apache.hadoop.fs.Path.initialize(Path.java:205); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.net.URI.checkPath(URI.java:1823); 	at java.net.URI.<init>(URI.java:745); 	at org.apache.hadoop.fs.Path.initialize(Path.java:202); 	at org.apache.hadoop.fs.Path.<init>(Path.java:171); 	at org.apache.hadoop.fs.Path.<init>(Path.java:93); 	at org.apache.hadoop.fs.Globber.glob(Globber.java:241); 	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globInternal(GoogleHadoopFileSystemBase.java:1370); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$concurrentGlobInternal$4(GoogleHadoopFileSystemBase.java:1279); 	at java.util.concurrent.FutureTask.run(FutureTa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:5425,concurren,concurrent,5425,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,"(ExecutionTimer.scala:59); at is.hail.backend.service.ServiceBackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <details>; <summary>The last TableIR logged</summary>. ```; 2023-10-13 02:14:44.213",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6701,concurren,concurrent,6701,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,"(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-6e815ac; Error summary: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; Unhandled exception in thread started by <bound method Thread.__bootstrap of <Thread(Thread-1, stopped daemon 140486823679744)>>; Traceback (most recent call last):; File ""/usr/lib/python2.7/threading.py"", line 783, in __bootstrap; self.__bootstrap_inner(); File ""/usr/lib/python2.7/threading.py"", line 823, in __bootstrap_inner; (self.name, _format_exc()))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:14765,concurren,concurrent,14765,https://hail.is,https://github.com/hail-is/hail/issues/2407,2,['concurren'],['concurrent']
Performance,(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$3(LocalBackend.scala:150); E 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:1038); E 	at is.hail.CancellingExecutorService.$anonfun$newTaskFor$2(package.scala:1090); E 	at is.hail.CancellingExecutorService$CancellingTask.run(package.scala:1067); E 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); E 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); E 	at is.hail.relocated.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:327); E 	at is.hail.CancellingExecutorService.execute(package.scala:1111); E 	at java.base/java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:184); E 	at is.hail.utils.package$.$anonfun$runAll$1(package.scala:1038); E 	at scala.collection.Iterator.foreach(Iterator.scala:943); E 	at scala.collection.Iterator.foreach$(Iterator.scala:943); E 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); E 	at scala.collection.IterableLike.foreach(IterableLike.scala:74); E 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73); E 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56); E 	at is.hail.utils.package$.runAll(package.scala:1038); E 	at is.hail.utils.package$.$anonfun$runAllKeepFirstError$3(package.scala:1054); E 	at is.hail.backend.local.LocalBackend.parallelizeAndComputeWithIndex(LocalBackend.scala:146); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:85); E 	at __C22901Compiled.__m23019split_CollectDistributedArray_region3_27(Emit.scala); E 	at __C22901Compiled.__m23019split_CollectDistributedArray(Emit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:4474,concurren,concurrent,4474,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['concurren'],['concurrent']
Performance,); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:19095,Load,LoadVCF,19095,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:2498,concurren,concurrent,2498,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['concurren'],['concurrent']
Performance,); at is.hail.expr.JSONAnnotationImpex$.exportAnnotation(AnnotationImpex.scala:113); at is.hail.expr.ir.Pretty.header(Pretty.scala:405); at is.hail.expr.ir.Pretty.pretty$1(Pretty.scala:463); at is.hail.expr.ir.Pretty.$anonfun$sexprStyle$4(Pretty.scala:453); at scala.collection.Iterator$$anon$10.next(Iterator.scala:459); at scala.collection.Iterator$ConcatIterator.next(Iterator.scala:230); at is.hail.utils.richUtils.RichIterator$$anon$3.next(RichIterator.scala:67); at is.hail.utils.prettyPrint.Doc$.advance$1(PrettyPrintWriter.scala:68); at is.hail.utils.prettyPrint.Doc$.render(PrettyPrintWriter.scala:139); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:163); at is.hail.utils.prettyPrint.Doc.render(PrettyPrintWriter.scala:167); at is.hail.expr.ir.Pretty.sexprStyle(Pretty.scala:466); at is.hail.expr.ir.Pretty.apply(Pretty.scala:429); at is.hail.expr.ir.Pretty$.apply(Pretty.scala:22); at is.hail.expr.ir.Optimize$.apply(Optimize.scala:45); at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:30); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:15); at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:13); at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); at is.hail.expr.ir.lowering.Lowering,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046:2817,Optimiz,OptimizePass,2817,https://hail.is,https://github.com/hail-is/hail/issues/13046,1,['Optimiz'],['OptimizePass']
Performance,); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.hadoop.hdfs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:17285,concurren,concurrent,17285,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['concurren'],['concurrent']
Performance,); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketException: Too many open files; at sun.nio.ch.Net.socket0(Native Method); at sun.nio.ch.Net.socket(Net.java:411); at sun.nio.ch.Net.socket(Net.java:404); at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:105); at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60); at java.nio.channels.SocketChannel.open(SocketChannel.java:145); at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62); at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1531); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1309); at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1262); at org.apache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293:7987,concurren,concurrent,7987,https://hail.is,https://github.com/hail-is/hail/issues/9293,1,['concurren'],['concurrent']
Performance,")</li>; </ul>; <h3>Configuration</h3>; <ul>; <li>Do not format <code>__pypackages__</code> directories by default (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2836"">#2836</a>)</li>; <li>Add support for specifying stable version with <code>--required-version</code> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2832"">#2832</a>).</li>; <li>Avoid crashing when the user has no homedir (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2814"">#2814</a>)</li>; <li>Avoid crashing when md5 is not available (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2905"">#2905</a>)</li>; <li>Fix handling of directory junctions on Windows (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2904"">#2904</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>Update pylint config documentation (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2931"">#2931</a>)</li>; </ul>; <h3>Integrations</h3>; <ul>; <li>Move test to disable plugin in Vim/Neovim, which speeds up loading (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2896"">#2896</a>)</li>; </ul>; <h3>Output</h3>; <ul>; <li>In verbose, mode, log when <em>Black</em> is using user-level config (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2861"">#2861</a>)</li>; </ul>; <h3>Packaging</h3>; <ul>; <li>Fix Black to work with Click 8.1.0 (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2966"">#2966</a>)</li>; <li>On Python 3.11 and newer, use the standard library's <code>tomllib</code> instead of <code>tomli</code>; (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2903"">#2903</a>)</li>; <li><code>black-primer</code>, the deprecated internal devtool, has been removed and copied to a; <a href=""https://github.com/cooperlees/black-primer"">separate repository</a> (<a href=""https://github-redirect.dependabot.com/psf/black/issues/2924"">#2924</a>)</li>; </ul>; <h3>Parser</h3>; <ul>; <li>Blac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696:1970,load,loading,1970,https://hail.is,https://github.com/hail-is/hail/pull/11696,2,['load'],['loading']
Performance,"* Add a lightweight DSL for writing IR in Scala, which made the lowerings much easier to write, and read. It is implemented in `IRBuilder`, and can be used by importing `IRBuilder._` into scope. It's not complete, and I want to make it eagerly typecheck eventually, but we can build on it.; * Make `execute` protected on `MatrixIR` and `TableIR`, making `Interpret` the official place to execute IR.; * Add a compiler pass lowering some `MatrixIR` to `TableIR`. The `Interpret` gateway to `execute` always lowers, so we can safely remove the execute methods of IR nodes which are rewritten by the lowering.; * Fix `LoadBgen` to not create entries arrays when `dropCols` is true.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4707:615,Load,LoadBgen,615,https://hail.is,https://github.com/hail-is/hail/pull/4707,1,['Load'],['LoadBgen']
Performance,"* add --metric argument to choose between best, median, ...; * add --min-time option to set minimum time for benchmark inclusion.; This lets us write benchmarks for things we expect to be fast, but where there's enough variance we don't really think the exact number is very meaningful in the current benchmarking system. They will still catch performance regressions, though!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7105:344,perform,performance,344,https://hail.is,https://github.com/hail-is/hail/pull/7105,1,['perform'],['performance']
Performance,* all the caches!. * two caches and a fix. * space between methods. * cache comments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2370:10,cache,caches,10,https://hail.is,https://github.com/hail-is/hail/pull/2370,3,['cache'],"['cache', 'caches']"
Performance,* don't use inefficient unapply; * Cache the returnType of AggSignatures so they persist through copies. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_big_aggregate_compilation 85.6% 4.113 3.519; ----------------------; Geometric mean: 85.6%; Simple mean: 85.6%; Median: 85.6%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7355:35,Cache,Cache,35,https://hail.is,https://github.com/hail-is/hail/pull/7355,1,['Cache'],['Cache']
Performance,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2039:901,optimiz,optimizations,901,https://hail.is,https://github.com/hail-is/hail/pull/2039,1,['optimiz'],['optimizations']
Performance,* works?. * address PR comments. * optimize imports. * use existing interfaces. * fix imports. * fix syntax error. * remove bad import. * add LDMatrix import. * fix tests for ldmatrix. * fix tests. * clean paths before writing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2124:35,optimiz,optimize,35,https://hail.is,https://github.com/hail-is/hail/pull/2124,1,['optimiz'],['optimize']
Performance,"*********** 2. row ***************************; id: 1; select_type: SIMPLE; table: attempts; partitions: NULL; type: ref; possible_keys: PRIMARY,attempts_instance_name; key: attempts_instance_name; key_len: 303; ref: batch.instances.name; rows: 91; filtered: 9.00; Extra: Using where; *************************** 3. row ***************************; id: 1; select_type: SIMPLE; table: jobs; partitions: NULL; type: eq_ref; possible_keys: PRIMARY,jobs_batch_id_state_always_run_cancelled,jobs_batch_id_state_always_run_inst_coll_cancelled,jobs_batch_id_update_id,jobs_batch_id_always_run_n_regions_regions_bits_rep_job_id,jobs_batch_id_ic_state_ar_n_regions_bits_rep_job_id,jobs_batch_id_job_group_id,jobs_batch_id_ic_state_ar_n_regions_bits_rep_job_group_id; key: PRIMARY; key_len: 12; ref: batch.attempts.batch_id,batch.attempts.job_id; rows: 1; filtered: 98.10; Extra: Using where; 3 rows in set, 1 warning (0.00 sec); ```. This is not great:; ```; rows: 1150201; filtered: 10.00; Extra: Using where; Using temporary; Using filesort; ```; what we want to see is a low number of rows, a high percent filtered, and something like `Extra: Using where; Using index;`. 3. We can then verify that this finding aligns with our current understanding of the database schema where there is no index on `instances.state`: ; https://github.com/hail-is/hail/blob/1f3a0503926b65f479dce6d5eb105236632f8d07/batch/sql/estimated-current.sql#L113-L138. ### Remaining questions; 1. Why do we need to join against the instances table to find orphaned attempts? Can we not?; 2. If we do need to join against the instances table, should we create an index on `instances.state`? How does `instances.removed` relate to this use case since it seems relevant and already has an index?. ### Deliverable; This query should perform indexed lookups of involved tables. The PR should compare the present and proposed EXPLAINs; and provide some manual timing comparisons. ### Version. 0.2.130. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14460:3962,perform,perform,3962,https://hail.is,https://github.com/hail-is/hail/issues/14460,1,['perform'],['perform']
Performance,"**Work in progress**; _worried the merging and rebasing was done poorly due to the message about merge conflicts at the bottom of this PR - would be useful if someone could let me know or walk me through resolving it_. **Additions:**. - adds method for Blanczos SVD, not yet following the exact interface of the current PCA call; - adds test for Blanczos SVD method; - adds multiple jupyter notebooks where this algorithm was implemented; - adds first version of benchmarking script; - update to requirements.txt regarding gcsfs version should probably be moved to separate PR. **Needs:**; - larger benchmarking; - better test; - hail method for Blanczos PCA to use exact interface and return eigenvalues, scores, and optional loadings as if it were the hail PCA method instead of the current non-centered SVD; - fix the norm(A - QQtA) computation - maybe make it blocked; - possibly block the error bound computation; - possibly replace the numpy library calls to SVD and QR decomposition with distributed hail versions, or at least the SVD call at a minimum since it is easier",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9222:727,load,loadings,727,https://hail.is,https://github.com/hail-is/hail/pull/9222,1,['load'],['loadings']
Performance,"**self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (139802248206080) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ------------------------------ live log teardown -------------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:2436,concurren,concurrent,2436,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,"**self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-1_0 (139802091452160) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~ Stack of ThreadPoolExecutor-0_0 (139802205742848) ~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). ~~~~~~~~~~~~~~~~~~~~~ Stack of asyncio_0 (139802248206080) ~~~~~~~~~~~~~~~~~~~~~; File ""/usr/lib/python3.9/threading.py"", line 937, in _bootstrap; self._bootstrap_inner(); File ""/usr/lib/python3.9/threading.py"", line 980, in _bootstrap_inner; self.run(); File ""/usr/lib/python3.9/threading.py"", line 917, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 81, in _worker; work_item = work_queue.get(block=True). +++++++++++++++++++++++++++++++++++ Timeout ++++++++++++++++++++++++++++++++++++. ------------------------------ live log teardown -------------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:1963,concurren,concurrent,1963,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,"+ reference_data = reference_data.drop('END'); + else: # if END is missing, add it, _add_end is a no-op if END is already present; + reference_data = VariantDataset._add_end(reference_data); +; vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; ```. There was nothing in the IR that stood out when I examined it, but I will admit that I'm not the best at digging into it. ### Version. https://github.com/chrisvittal/hail/tree/vds/repro-example. ### Relevant log output. ```shell; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadByte(Memory.java:130); E 	at is.hail.annotations.Region$.loadByte(Region.scala:28); E 	at is.hail.annotations.Region$.loadBit(Region.scala:86); E 	at __C23148collect_distributed_array_matrix_native_writer.__m23333split_ToArray(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region478_486(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region16_503(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region14_529(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAnd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:2643,load,loadByte,2643,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['load'],['loadByte']
Performance,"++++ Timeout ++++++++++++++++++++++++++++++++++++. ------------------------------ live log teardown -------------------------------; INFO hailtop.utils:utils.py:450 discarding exception; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 378, in rm_dir; await self.rmdir(path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 352, in rmdir; return await blocking_to_async(self._thread_pool, os.rmdir, path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 162, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 201, in result; raise self._exception; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 163, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); OSError: [Errno 39] Directory not empty: '/tmp/JnQ2m'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 409, in rmtree; await rm_dir(pool, contents_tasks_by_dir.get(path, []), path); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 387, in rm_dir; excs = [exc; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 389, in <listcomp>; for exc in [t.exception()]; File ""/usr/lib/python3.9/asyncio/futures.py"", line 214, in exception; raise exc; asyncio.exceptions.CancelledError. [2023-08-02 05:33:14] test/hail/utils/test_hl_hadoop_and_hail_fs.py::test_hadoop_methods_3[local] ERROR; [2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13361:3547,concurren,concurrent,3547,https://hail.is,https://github.com/hail-is/hail/issues/13361,1,['concurren'],['concurrent']
Performance,", returns srcAddress, skipping construction. # <a name=""parray""></a> PArray. An abstract class for immutable ordered collections where all elements are of a single type. Does not contain the value constructor (e.g allocate). ## Core Methods. ```scala; def allocate(region: Region, length: Int): Long = ...; def allocate(region: Code[Region], length: Code[Int]): Code[Long] = ...; ```. - Allocate the memory needed for an array of `length` length. Cannot exceed 2^31 entries. ```scala; def initialize(aoff: Long, length: Int, setMissing: Boolean = false) = ...; def stagedInitialize(aoff: Code[Long], length: Code[Int], setMissing: Boolean = false): Code[Unit] = ...; ```. - Initialize an allocated array by setting its elements to present or missing. ```scala; def isElementMissing(arrayAddress: Long, elementIndex: Int): Boolean= ...; def isElementMissing(arrayAddress: Long, elementIndex: Code[Int]): Code[Boolean] = ...; ```. - Does the element at the given index exist. ```scala; def loadLength(arrayAddress: Long): Int = ...; def loadLength(arrayAddress: Code[Long]): Code[Int] = ...; ```. - Gets the array length, will not exceed 2^31. ```scala; def loadElement(arrayAddress: Long, elementIndex: Int): Long = ...; def loadElement(arrayAddress: Code[Long], elementIndex: Code[Int]): Code[Long] = ...; ```. - Gets the address of the element at the given index.; - For pointer types loads the address at the offset into arrayAddress, otherwise returns that address. ## <a name=""parray""></a> PCanonicalArray. A growable array that is accessed by a pointer. ### Structure. Starting at `arrayAddress`:. [`4-byte length`, `n/8 byte missigness data`, `n * elementByteSize byte element data`]. # <a name=""parray""></a> PSet. An abstract class for immutable ordered collections where all elements are unique. ## Core Methods. ```scala; def arrayFundamentalType: PArray; ```. - The underlying array representation. ## <a name=""parray""></a> PCanonicalSet. A PCanonicalArray-backed implementation of PSet. # ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7988:4805,load,loadLength,4805,https://hail.is,https://github.com/hail-is/hail/issues/7988,1,['load'],['loadLength']
Performance,- Added convenience method to export each block of a BlockMatrix into its own file using `export_rectangles`; - Added static method to load rectangle files into a NumPy NDArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5516:135,load,load,135,https://hail.is,https://github.com/hail-is/hail/pull/5516,1,['load'],['load']
Performance,"- All subprocess calls are async. The UI is much more responsive now.; - Make refresh (rather than heal) non-reentrant. There was a race condition where we could update the Github state of a PR we were actively deploying. This seemed error prone.; - We need to lock the repos until the build is fully deployed. I now protect pr.heal.; - Set `batch_changed = True` whenever heal needs to be rerun becomes some of its inputs (build_state, source or target sha, collection of PRs) changed. Next up: deploy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5921:132,race condition,race condition,132,https://hail.is,https://github.com/hail-is/hail/pull/5921,1,['race condition'],['race condition']
Performance,"- Create a cache that stores an instance's token which can be looked up by the instance's name; - Use this cache in the active_instances_only decorator to avoid making DB request on every invocation; - Add monitoring of caches' hits, misses, evictions, and load latencies",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346:11,cache,cache,11,https://hail.is,https://github.com/hail-is/hail/pull/11346,4,"['cache', 'load']","['cache', 'caches', 'load']"
Performance,"- ForwardRelationalLets : 4.892ms, total 154.106ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 70.932ms, total 225.284ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 2.673ms, total 228.575ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 6.413ms, total 235.357ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.736ms, total 240.359ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 38.152ms, total 278.793ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 3.185ms, total 282.285ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:1867,Optimiz,Optimize,1867,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"- Get shape for an `NDArrayExpression`. While this should (and in some cases does) **not** actually perform computations like element wise maps or matmul's, it doesn't in all cases because not everything is worked into being deforested. I'm going to take a closer look at deforesting/shape calculation and tie up loose ends, but there are some subtleties and I think this is a good chunk.; - Changed IR nodes that are concerned with shape (`MakeNDArray`, `NDArrayReshape`) take shapes as hail tuples (instead of Scala sequences of IR, or IR with another parameter for length). The only thing Scala sequences were really useful for were statically knowing the number of dimensions which we can get with tuples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6077:100,perform,perform,100,https://hail.is,https://github.com/hail-is/hail/pull/6077,1,['perform'],['perform']
Performance,"- I left in pvc_size for backwards compatibility. We can rip it out at some point.; - I'm not happy with how the batch_worker_image seems slower now with the gsutil addition. Not sure if there's a better solution here. I tested a lot of this by hand on dev deploy. For example, making sure the flocks were right, the project quotas were right, the cache was working, the garbage collector, and the backwards compatibility with pvc_size was working. Let me know if you think there are other things I should test. If this is too much, I can think about splitting the storage and the cache into two PRs. I just figured since I almost had it all done together to push on that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9074:348,cache,cache,348,https://hail.is,https://github.com/hail-is/hail/pull/9074,2,['cache'],['cache']
Performance,"- Most of the code is actually the same, but I was intentionally not deforesting until now to get some benchmarks.; - Basically all you need to do is compose the `outputElement`s of your children (the body of the loops) and compute what the ultimate bounds (shape) of the nested loops should be.; - For Reindex, we statically reorder the loop variables used to index into the NDArray instead of permuting the shape/strides at runtime. Not a huge performance improvement but in the broadcasting case (adding dimensions) it's at least fewer multiplications at runtime in the loop body to compute the index.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6026:446,perform,performance,446,https://hail.is,https://github.com/hail-is/hail/pull/6026,1,['perform'],['performance']
Performance,"- On *deploys*, makes sure that whatever is in our third-party images is in our private registry before starting builds like hail-ubuntu that might depend on those images. This means that we can update our ubuntu base image without the australians needing to deploy any images by hand. However, this does not run in PRs because I 1) didn't want to add that kind of latency for PRs and 2) we don't do any kind of namespacing for our images so if we did include this for a PR that ultimately wasn't merged we would have to manually remove the image anyway so why not manually add it if you're going to PR it… I think point 2 is a little weak but I recall this being what we agreed on a couple months back when we discussed this. I'm wondering if we should just eat the minute or so latency at the beginning of PRs to be safe but it also feels like a shame for something that changes so infrequently. . - Again on deploys, upload the hailgenetics/* images to the private registry if they don't already exist there. This way any deployments that aren't hail team's GCP deployment can get these images automatically when they deploy a new SHA instead of uploading them manually. It won't backfill skipped versions, but we decided that was ok. This seems less relevant for testing on PRs as it will get triggered on releases and we can easily dev deploy to rectify the image if this breaks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12818:365,latency,latency,365,https://hail.is,https://github.com/hail-is/hail/pull/12818,2,['latency'],['latency']
Performance,- Refactored `exportRectangles` on BlockMatrix from a static function with an input and output file to an instance function with an output file that writes the instance.; - Added `BlockMatrixRectanglesWriter` so exporting rectangles happens through the IR.; - Updated tests and deleted parts of tests that dealt with invalid inputs that aren't applicable when exporting from an already loaded BlockMatrix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5478:386,load,loaded,386,https://hail.is,https://github.com/hail-is/hail/pull/5478,1,['load'],['loaded']
Performance,"- Removed `export_samples`, `export_variants`, `export_genotypes`. To perform same functionality, need to convert to KeyTable and then use `select` and `export`. - Changed Python interface for `select`, `drop`, and `key_by` to take varargs rather than a String or List of String. - Select is not backwards compatible with 0.1. To select a column name with periods in it, must use backticks now. - Not certain whether left hand side of named expression should be treated as an identifier or an annotationIdentifier. Right now, it's treated as an identifier. If it's an identifier, than `A.B = 5` will have a signature of `(""A.B"", TInt)`. If it's an annotationIdentifier path, than the signature would be `(""A"", TStruct((""B"", TInt)))`. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2279:70,perform,perform,70,https://hail.is,https://github.com/hail-is/hail/pull/2279,1,['perform'],['perform']
Performance,"- Signup page with web socket and spinner while waiting for account to create; - Upon account creation, a billing project named `{username}-trial` is created with $10 limit and a user `{username}`; - When deleting an account, the billing project is reopened if it's closed, then remove the user, and finally close the billing project. This behavior might be debatable. We may not need to remove the user from the billing project. I think it's better to be safe in case the billing project is reopened. Auth-driver is implicitly dependent on the batch front end, but this dependency isn't stated in build.yaml. An event queue is a future solution. I can get rid of my personal email being whitelisted, but I think it will be useful if we need to debug later and for a possible demo on Monday (although I'll probably just use some screenshots). If you want to test it in your namespace, make sure to comment out all the create and delete steps that are not related to billing projects.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9658:619,queue,queue,619,https://hail.is,https://github.com/hail-is/hail/pull/9658,1,['queue'],['queue']
Performance,"- Time execution stages in the backend (optimization, compilation, runtime); - Return a dictionary back to the front end of nanoseconds and formatted times for each stage.; - Add `hl.eval_timed` which propagates returns the evaluated IR as well as the timings dictionary. If called with `timed=False` (default), `Env.backend.execute()` drops the timings it received from the backend. Could change it to log them instead or always.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5857:40,optimiz,optimization,40,https://hail.is,https://github.com/hail-is/hail/pull/5857,1,['optimiz'],['optimization']
Performance,"- [ ] (@tpoterba) caf1e1e673 add fails_service_backend; - [ ] (@tpoterba, @cseed) a979dfba58 [hail] introduce and use mktemp and mktempd; - [ ] (@tpoterba) dcf026b01c [hail] make is.hail.expr.ir.functions threadsafe; - [ ] (@tpoterba) 807f38c20e [hail] fix use of row requiredness in lowerDistributedSort; - [ ] (@catoverdrive) 12df8eb456 [query-service] handle void-typed IRs in query-service; - [ ] (@catoverdrive) 03357ee83d [query-service] make user cache thread-safe; - [ ] (@tpoterba) 6c6734bc71 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) a3d2572ce7 [shuffler] log ShuffleCodecSpec anytime it is created; - [ ] (@daniel-goldstein) 8949dfec3c [scala-lsm] bugfix: least key may equal greatest key; - [ ] (@daniel-goldstein) 6067bd8e51 [services] discovered new transient error; - [ ] (@daniel-goldstein) c8356d30bb [shuffler] more assertions in ShuffleClient; - [ ] (@daniel-goldstein) 9991da90f0 [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [ ] (@daniel-goldstein) bc0140ab6f [query-service] move hail.jar earlier in Dockerfile; - [ ] (@daniel-goldstein) f96c28174d [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 6ae26339fe [query-service] simplify socket handling; - [ ] (@jigold) f3db30e23f [batch] teach JVMJob where to find the hail configuration files; - [ ] (@daniel-goldstein) b5c6d85554 [query-service] switch to services team approved logging; - [ ] (@tpoterba) 35a306c066 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 051c89b8e7 [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) ad9ea73d7a [query-service] run tests against query service",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10072:454,cache,cache,454,https://hail.is,https://github.com/hail-is/hail/pull/10072,1,['cache'],['cache']
Performance,- [ ] attributes PR (per-batch attributes); - [ ] per-job attributes; - [ ] use queue and concurrent worker pool for all k8s communication; - [ ] batch avoids scheduling more than 150k pods,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6493:80,queue,queue,80,https://hail.is,https://github.com/hail-is/hail/issues/6493,2,"['concurren', 'queue']","['concurrent', 'queue']"
Performance,"- [x] Remove TSample (it's just string); - [x] Remove count, or rework it to return a tuple; - [ ] Change concordance to use TDict; - [x] Filter_samples_list should take a list; - [x] TextTableConfig goes away; - [x] Annotate_samples_table and annotate_variants_table go away; - [x] Import_annotations_table goes away; - [x] KeyTable to VariantDataset conversion; - [ ] Precompiled binaries (!!!!!!!); - [ ] Fix log output to jupyter notebooks; - [ ] add all aggregator functions to sets and arrays (or fix aggregable scope issue); - [x] add python file-like objects so people can write to cloud file systems / HDFS; - [ ] ~~improved performance on python object conversion (or lazy evaluation at the least)~~ back compatible; - [x] annotate_samples_fam goes away; - [x] annotate VDS with interval keytable; - [x] read/write keytables to parquet; - [ ] rename logreg/ linreg / lmmreg to be more descriptive; - [x] no methods take a file; - [x] first-class object for Pedigree in python; - [x] make annotation-of-counts behavior consistent across the regression methods; - [x] make linreg / logreg / lmmreg consistent on whether they output count annotations. If anybody has other tasks, edit this post to add them here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1505:634,perform,performance,634,https://hail.is,https://github.com/hail-is/hail/issues/1505,1,['perform'],['performance']
Performance,"- [x] reduce number of database calls when creating a pod; - [x] put the source jobs of a batch on a queue which has 16-concurrent workers sending create_pod requests to k8s; - [x] modify the server API to /create, /create_jobs *, /close (prevents holding 12M jobs in memory on batch server); - [x] record pod status JSON in database for debugging purposes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6490:101,queue,queue,101,https://hail.is,https://github.com/hail-is/hail/issues/6490,2,"['concurren', 'queue']","['concurrent', 'queue']"
Performance,"- [x] set resource requests on input and output pods (https://github.com/hail-is/hail/pull/6507); - [x] request metrics (req/s, latency) for Hail services (at least batch) in Prometheus/Grafana; - [x] for both REST and web endpoints; - [ ] metrics for DB requests in batch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6491:128,latency,latency,128,https://hail.is,https://github.com/hail-is/hail/issues/6491,1,['latency'],['latency']
Performance,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2347:14,load,loadLength,14,https://hail.is,https://github.com/hail-is/hail/pull/2347,6,['load'],"['loadAddress', 'loadElement', 'loadInt', 'loadLength', 'loadX']"
Performance,"- automatically download catch.hpp from GitHub if it's missing; - remove builtin rules and suffix based rules to improve performance and ease debugging; - change Makefile variable definitions to match our standard `FOO OP VAL` (note: two spaces); - rely on `clang -MM` (see: [Clang command line reference](https://clang.llvm.org/docs/ClangCommandLineReference.html) and below explanation) to generate precise dependencies for object files (including source and header files); - explicitly specify which files depend on `NUMBER_OF_GENOTYPES_PER_ROW` to be set (namely the dependency file and the object file associated with `ibs.cpp`); - break `TEST_OBJECTS` into two steps so that we can have `_test.cpp` files which do not have corresponding `.cpp` files (consider, for example, a header-only file, which ApproximateQuantiles will be); - eliminate `$(BUILD)/headers` in favor of precise dependency tracking described above; - remove the target `$(BUILD)`, directories don't work the way you think in Make, it's better to have individual rules create the containing directories when necessary; - remove `wget` nonsense, standardize on `curl -sSL` (which produces useful error messages). ---. # clang -MM. This argument to clang allows us to generate ""depfile"" or ""dependency files"" which are valid `Makefile`s describing how object files depend on `c`, `cpp`, `h`, and `hpp` files. `clang -MM foo.cpp` writes to stdout a Makefile that indicates how `foo.o` depends on preprocessor includes of other *user* files. For example,. ```; # cat foo.cpp; #include<stdio.h>; #include ""bar.h""; # clang -MM foo.cpp; foo.o: foo.cpp bar.h; ```. The `-MT target` allows us to specify the target's name:; ```; # clang -MM foo.cpp -MT fiddle; fiddle: foo.cpp bar.h; ```. The `-MQ` argument asks `clang` to quote the variable before make sees it, so (nb, I first quote it for the shell so it doesn't get seen as an env var):; ```; # clang -MM foo.cpp -MQ '$fiddle'; $$fiddle: foo.cpp bar.h; ```. The `-MG` argument tel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5331:121,perform,performance,121,https://hail.is,https://github.com/hail-is/hail/pull/5331,1,['perform'],['performance']
Performance,"- create KeyTable from DataFrame; - to make above useful, we need a way to ""contract"" native types (opposite of expand_types), that, convert Struct with the appropriate fields to Variant, etc. One alternative is to use SparkAnnotationImpex and have the user specify the Hail type. (Also means we need a way to build Hail types in python.); - annotate variants or samples with keytable; - load fam file as keytable; - remove annotevariants table, vcf, vds and annotatesamples fam, table, list, vds. They can all be implemented with annotate with keytable.; - same with filter list",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1158:388,load,load,388,https://hail.is,https://github.com/hail-is/hail/issues/1158,1,['load'],['load']
Performance,- emit streams for `If` and `ReadPartition` IR. ; - delete `Emit.emitOldArrayIterator()` because all streamable IR's can be emitted with the new interface. before this is merged we probably want to make sure there are no significant performance regressions or anything of that sort,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7736:233,perform,performance,233,https://hail.is,https://github.com/hail-is/hail/pull/7736,1,['perform'],['performance']
Performance,- long running jobs are causing too much latency for interactive stuff (e.g. CI); - overscheduling core calculation wasn't behaving how we wanted,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7981:41,latency,latency,41,https://hail.is,https://github.com/hail-is/hail/pull/7981,1,['latency'],['latency']
Performance,"- put_on_ready should never be in an ensure_future, but I had to keep it there when used in Pod.create() because I didn't want to block the create/delete pool waiting for the pods to be on the ready queue; otherwise, it should be fixed everywhere else. - I added `Binds: None` in the Docker config as the default because if we specify a HostConfig, then I believe Docker uses a default of {} which might be allocating a volume unnecessarily. I can't find where I read that before. I can double check the performance of the change if you want. I'm pretty sure it helps.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7347:199,queue,queue,199,https://hail.is,https://github.com/hail-is/hail/pull/7347,2,"['perform', 'queue']","['performance', 'queue']"
Performance,"- set up tests that evaluate performance tradeoffs; - build interface for controlling compression levels in import / write; - also test parquet LZ4 / snappy on write with Mitja pipeline, with size",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/293:29,perform,performance,29,https://hail.is,https://github.com/hail-is/hail/issues/293,1,['perform'],['performance']
Performance,"- store globals, cols out of line, don't load when loading metadata; - removed MatrixLocalValue (also Table), just store in MatrixValue; - don't require vds, kt extensions; - /path/to/ds/cols is now a valid table fine format; - important bug fix: read_table was completely broken",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2854:41,load,load,41,https://hail.is,https://github.com/hail-is/hail/pull/2854,2,['load'],"['load', 'loading']"
Performance,"-- Optimize -- PruneDeadFields : 25.086ms, total 307.692ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- FoldConstants : 1.938ms, total 310.139ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ExtractIntervalFilters : 13.601ms, total 324.665ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- Simplify : 4.231ms, total 329.178ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardLets : 27.172ms, total 356.625ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- ForwardRelationalLets : 6.605ms, total 363.564ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize -- PruneDeadFields : 29.964ms, total 394.795ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- Optimize : 371.542ms, total 395.164ms(); 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 3.975ms, total 407.299ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- LoweringTransformation : 77.664ms, total 485.244ms, tagged coverage 0.0; 2019-11-06 18:44:11 root: INFO: Time taken for CompileAndEvaluate -- LowerMatrixToTable -- Verify : 1.780ms, total 487.281ms, tagged coverage 0.0; ...; ...; ...; 2019-11-06 18:44:11 root: INFO: Timer: aggregate:; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'Verify' (8): 8.109ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ConvertToSafeValue' (2): 12.828ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'InitializeCompiledFunction' (2): 14.536ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'ForwardRelationalLets' (7): 20.388ms; 2019-11-06 18:44:11 root: INFO: Time taken for tag 'FoldConst",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7476:2893,Optimiz,Optimize,2893,https://hail.is,https://github.com/hail-is/hail/pull/7476,1,['Optimiz'],['Optimize']
Performance,"--------- Captured log setup ------------------------------; INFO hailtop.aiocloud.aiogoogle.credentials:credentials.py:89 using credentials file /test-gsa-key/key.json: GoogleServiceAccountCredentials for test-665@hail-vdc.iam.gserviceaccount.com; ------------------------------ Captured log call -------------------------------; INFO hailtop.aiocloud.aiogoogle.credentials:credentials.py:89 using credentials file /test-gsa-key/key.json: GoogleServiceAccountCredentials for test-665@hail-vdc.iam.gserviceaccount.com; WARNING hailtop.aiocloud.aiogoogle.client.storage_client:storage_client.py:225 resumable upload chunk PUT request finished before writing data; WARNING hailtop.aiocloud.aiogoogle.client.storage_client:storage_client.py:117 dropping preempted task exception; Traceback (most recent call last):; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 258, in __step; result = coro.throw(exc); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/utils.py"", line 30, in feed; await self._queue.put(next); File ""/usr/lib/python3.9/asyncio/queues.py"", line 128, in put; await putter; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 196, in result; raise exc; asyncio.exceptions.CancelledError. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiocloud/aiogoogle/client/storage_client.py"", line 111, in __aexit__; value = await self._task; File ""/usr/lib/python3.9/asyncio/futures.py"", line 284, in __await__; yield self # This tells Task to wait for completion.; File ""/usr/lib/python3.9/asyncio/tasks.py"", line 328, in __wakeup; future.result(); File ""/usr/lib/python3.9/asyncio/futures.py"", line 196, in result; raise exc; asyncio.exceptions.CancelledError; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13742:4490,queue,queues,4490,https://hail.is,https://github.com/hail-is/hail/issues/13742,1,['queue'],['queues']
Performance,"-------------------------------------------------. I'm working through the GWAS tutorial and getting some strange errors with two different functions. On the 4th line calling hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True) I'm getting the following error:. ```; File ""gwas_tutorial.py"", line 12, in <module>; hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt',overwrite=True); File ""</Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/decorator.py:decorator-gen-946>"", line 2, in write; File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/matrixtable.py"", line 2494, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/backend/backend.py"", line 106, in execute; result = json.loads(Env.hail().backend.spark.SparkBackend.executeJSON(self._to_java_ir(ir))); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/username/miniconda3/envs/hail1/lib/python3.7/site-packages/hail/utils/java.py"", line 240, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: ScalaSigParserError: Unexpected failure. Java stack trace:; org.json4s.scalap.ScalaSigParserError: Unexpected failure; 	at org.json4s.scalap.Rules$$anonfun$expect$1.apply(Rules.scala:73); 	at org.json4s.scalap.scalasig.ClassFileParser$.parse(ClassFileParser.scala:95); 	at org.json4s.reflect.ScalaSigReader$.parseClassFileFromByteCode(ScalaSigReader.scala:178); 	at org.json4s.reflect.ScalaSigReader$.findScalaSig(ScalaSigReader.scala:172); 	at org.json4s.reflect.ScalaSigReader$.findClass(ScalaSigReader.scala:53); 	at org.json4s.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299:1237,load,loads,1237,https://hail.is,https://github.com/hail-is/hail/issues/6299,1,['load'],['loads']
Performance,"-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:7104,cache,cached,7104,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6538,cache,cached,6538,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --help false --version false --showHidden false --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --disable-tool-default-read-filters false"",Version=4.0.1.2,Date=""March 22, 2018 1:12:03 AM EDT"">; ##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##VEP=""v91"" time=""2018-03-22 04:15:25"" cache=""/media/SE5/.vep/homo_sapiens/91_GRCh38"" db=""homo_sapiens_core_91_38@ensembldb.ensembl.org"" ensembl-variation=91.c78d8b4 ensembl-funcgen=91.4681d69 ensembl-io=91.923d668 ensembl=91.18ee742 1000genomes=""phase3"" COSMIC=""82"" ClinVar=""201710"" ESP=""V2-SSA137"" HGMD-PUBLIC=""20172"" assembly=""GRCh38.p10"" dbSNP=""150"" gencode=""GENCODE 27"" genebuild=""2014-07"" gnomAD=""170228"" polyphen=""2.2.2"" regbuild=""16"" sift=""sift5.2.2""; ##INFO=<ID=CSQ,Number=.,Type=String,Description=""Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID|SIFT|PolyPhen|AF|AFR_AF|AMR_AF|EAS_AF|EUR_AF|SAS_AF|AA_AF|EA_AF|gnomAD_AF|gnomAD_AFR_AF|gnomAD_AMR_AF|gnomAD_ASJ_AF|gnomAD_EAS_AF|gnomAD_FIN_AF|gnomAD_NFE_AF|gn",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:24705,cache,cache,24705,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['cache'],['cache']
Performance,"-conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type im",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:1528,load,load,1528,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['load']
Performance,"-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4587,cache,cached,4587,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,-fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux ibs.cpp -MG -M -MF build/ibs.d -MT build/ibs.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux davies.cpp -MG -M -MF build/davies.d -MT build/davies.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Werror -Wextra -fPIC -ggdb -f,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659:1950,cache,cache-tests,1950,https://hail.is,https://github.com/hail-is/hail/issues/5659,1,['cache'],['cache-tests']
Performance,"-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5446,cache,cachetools,5446,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cachetools']
Performance,". The exceptions are:; - from batch-driver to batch workers; - from batch workers to internal-gateway; - to ukbb-rg; - from router to notebook workers; - letsencrypt (oh the irony). The major new build step is `create_certs` which creates a certificate, key, and; list of trusted ""principals"" for each ""principal"". ""Principal"" is a computer; security term referring to an authenticatable identity. In our system, the; services are each unique principals and every client (e.g. the test_batch CI; step) is also a principal. A principal's certificate is a unforgeable proof of; their identity. A principal's ""key"", in our system, is actually a public-private; (i.e. asymmetric) key pair which the client and server use to establish a; symmetric key for each new connection. A list of trusted principals is a list of; certificates. Every incoming connection must provide a certificate in the; trusted list or the server will drop the connection. Every service depends on the `create_certs` step because their deployment's load; secrets created by `create_certs`. The blog service is implemented by Ghost. Ghost only supports HTTP. As a result; we cannot make all network traffic in our cluster TLS-secured. However, we can; use an nginx sidecar on the blog pod which terminates TLS connections and sends; plaintext traffic on the loopback interface to Ghost. Thus, our goal is: no; plaintext traffic on non-loopback interfaces. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. We require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](https://github.com/kubernetes/kubernetes/pull/61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513:1102,load,load,1102,https://hail.is,https://github.com/hail-is/hail/pull/8513,1,['load'],['load']
Performance,"... in the sense that they always aggregate in partition order. This is achieved by a `AssociativeCombiner` which greedily combOp's aggregator state from adjacent partitions. I think this is the best you can do. Later we should have a `CommutativeCombiner` and choose between the two based on the whether the user-level aggregators are (duh) associative (like collect and prev_nonnull) or commutative (like collectAsSet or count). This is slightly conservative as I converted, with slavish consistency, all aggregators and reducers, included one only used by concordance, which I think is commutative. I'm OK with that mostly because this is safer and concordance really needs to get rewritten in Python (I think @tpoterba has a draft but it needed some performance work). FYI @chrisvittal this should fix any prev_nonnull aggregator/scan issues.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5420:754,perform,performance,754,https://hail.is,https://github.com/hail-is/hail/pull/5420,1,['perform'],['performance']
Performance,".0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (66",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4079,cache,cached,4079,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,".1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/java-native-access/jna/blob/master/CHANGES.md"">jna's changelog</a>.</em></p>; <blockquote>; <h1>Release 5.12.1</h1>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1447"">#1447</a>: Null-check cleanable in <code>c.s.j.Memory#close</code> - <a href=""https://github.com/dbwiddis""><code>@​dbwiddis</code></a>.</li>; </ul>; <h1>Release 5.12.0</h1>; <h2>Features</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1433"">#1433</a>: Add <code>CFEqual</code>, <code>CFDictionaryRef.ByReference</code>, <code>CFStringRef.ByReference</code> to <code>c.s.j.p.mac.CoreFoundation</code> - <a href=""https://github.com/shalupov""><code>@​shalupov</code></a></li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/978"">#978</a>: Remove use of finalizers in JNA and improve concurrency for <code>Memory</code>, <code>CallbackReference</code> and <code>NativeLibrary</code> - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1440"">#1440</a>: Support for LoongArch64 - <a href=""https://github.com/Panxuefeng-loongson""><code>@​Panxuefeng-loongson</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1444"">#1444</a>: Update embedded libffi to 1f14b3fa92d4442a60233e9596ddec428a985e3c and rebuild native libraries - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1438"">#1438</a>: Handle arrays in structures with differing size - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:1057,concurren,concurrency,1057,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['concurren'],['concurrency']
Performance,".4.1</a> (2022-01-21)</h3>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2><a href=""https://github.com/googleapis/google-auth-library-python/compare/v2.3.3...v2.4.0"">2.4.0</a> (2022-01-20)</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/919"">#919</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c99350455d0f7fd3aab950ac47b43000c73dd312"">c993504</a>)</li>; <li>add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/3b15092b3461278400e4683060f64a96d50587c4"">3b15092</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li><strong>deps:</strong> allow cachetools 5.0 for python 3.7+ (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/937"">#937</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/1eae37db7f6fceb32d6ef0041962ce1755d2116c"">1eae37d</a>)</li>; <li>fix the message format for metadata server exception (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/916"">#916</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/e756f08dc78616040ab8fbd7db20903137ccf0c7"">e756f08</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>fix intersphinx link for 'requests-oauthlib' (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/921"">#921</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/967be4f4e2a43ba7e240d7acb01b6b992d40e6ec"">967be4f</a>)</li>; <li>note Valu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:7698,cache,cachetools,7698,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['cache'],['cachetools']
Performance,".5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3379:3592,Load,LoadVCF,3592,https://hail.is,https://github.com/hail-is/hail/issues/3379,1,['Load'],['LoadVCF']
Performance,".7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/io/Serializable is/hail/asm4s/AsmFunction2 {. // access flags 0x1; public apply([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; L0; ALOAD 2; LDC 2; INVOKEVIRTUAL scala/collection/mutable/ArrayBuffer.apply (I)Ljava/lang/Object;; CHECKCAST org/apache/spark/sql/Row; ASTORE 3; ALOAD 3; IFNULL L1; ALOAD 3; LDC 5; INVOKEINTERFACE org/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:3017,load,load,3017,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['load']
Performance,".9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5769,cache,cached,5769,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:20,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:12267,concurren,concurrent,12267,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['concurren'],['concurrent']
Performance,".ContextRDD.collect(ContextRDD.scala:143); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:1179); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:454); at is.hail.expr.MatrixValue.write(Relational.scala:122); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.variant.MatrixTable$$anonfun$write$2.apply(MatrixTable.scala:2301); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:511); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2301); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: devel-4f13f27cd28d; Error summary: SparkException: Job 2 cancelled because SparkContext was shut down; [farrell@scc-hadoop ad.v1]$ Exception in thread ""Executor task launch worker for task 766"" java.lang.NullPointerException; at org.apache.spark.scheduler.Task.metrics$lzycompute(Task.scala:66); at org.apache.spark.scheduler.Task.metrics(Task.scala:65); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:473); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755:7859,concurren,concurrent,7859,https://hail.is,https://github.com/hail-is/hail/issues/4755,2,['concurren'],['concurrent']
Performance,.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.T,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:6637,concurren,concurrent,6637,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['concurren'],['concurrent']
Performance,.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790:6215,concurren,concurrent,6215,https://hail.is,https://github.com/hail-is/hail/issues/3790,1,['concurren'],['concurrent']
Performance,.ScalaValueWriter.doWriteScala(ScalaValueWriter.scala:63); 	at org.elasticsearch.spark.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:46); 	at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58); 	at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:168); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:5329,concurren,concurrent,5329,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['concurren'],['concurrent']
Performance,".ScalaValueWriter.doWriteScala(ScalaValueWriter.scala:63); 	at org.elasticsearch.spark.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:46); 	at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71); 	at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58); 	at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:168); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:107); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-408f188; Error summary: EsHadoopIllegalArgumentException: Spark SQL types are not handled through basic RDD saveToEs() calls; typically this is a mistake(as the SQL schema will be ignored). Use 'org.elasticsearch.spark.sql' package instead; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [ffc9fb0b99f64080b674ab7a07962df9] entered state [ERROR] while waiting for [DONE].; ```. Ideally it would get exported as nested objects: https://www.elastic.co/guide/en/elasticsearch/reference/current/nested.html#_using_literal_nested_literal_fields_for_arrays_of_objects. with elasticsearch mapping:; ```; u'vep': {'type': 'nested', 'properties': {u'category': {'type': 'keyword'}, u'major_consequence': {'type': 'keyword'}, u'gene_id': {'type': 'keyword'}, u'major_consequence_rank': {'type': 'in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4250:10883,concurren,concurrent,10883,https://hail.is,https://github.com/hail-is/hail/issues/4250,1,['concurren'],['concurrent']
Performance,.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13168,Load,LoadMatrix,13168,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,".apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGSched",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:5937,Load,LoadVCF,5937,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,".com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a"">a8eb4c8</a>)</li>; </ul>; <h2>v2.4.1</h2>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2>v2.4.0</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/919"">#919</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c99350455d0f7fd3aab950ac47b43000c73dd312"">c993504</a>)</li>; <li>add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/3b15092b3461278400e4683060f64a96d50587c4"">3b15092</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li><strong>deps:</strong> allow cachetools 5.0 for python 3.7+ (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/937"">#937</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/1eae37db7f6fceb32d6ef0041962ce1755d2116c"">1eae37d</a>)</li>; <li>fix the message format for metadata server exception (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/916"">#916</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/e756f08dc78616040ab8fbd7db20903137ccf0c7"">e756f08</a>)</li>; </ul>; <h3>Documentation</h3>; <ul>; <li>fix intersphinx link for 'requests-oauthlib' (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/921"">#921</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/967be4f4e2a43ba7e240d7acb01b6b992d40e6ec"">967be4f</a>)</li>; <li>note Valu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:2403,cache,cachetools,2403,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['cache'],['cachetools']
Performance,.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). java.lang.ClassFormatError: Too many arguments in method signature in class file __C2866stream; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:635); 	at is.hail.asm4s.HailClassLoader.liftedTree1$1(HailClassLoader.scala:10); 	at is.hail.asm4s.HailClassLoader.loadOrDefineClass(HailClassLoader.scala:6); 	at is.hail.asm4s.ClassesBytes.$anonfun$load$1(ClassBuilder.scala:64); 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(Contex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:12169,load,load,12169,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['load'],['load']
Performance,.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). 	Java stack trace:; 	java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 			at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 			at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 			at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 			at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); 			at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); 			at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); 			at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:6018,concurren,concurrent,6018,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748). Spark Worker Logs (truncated to crash):. 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 16 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; 2020-06-10 10:09:36 INFO ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 17 ms; [thread 46926922934016 also had an error][thread 46922053207808 also had an error][thread 46926901880576 also had an error][thread 46926888195840 also had an error][thread 46926887143168 also had an error][thread 46924854015744 also had an error]; [thread 46924847699712 also had an error]. 	#. 	# A fatal error has been detected by the Java Runtime Environment:. 	[thread 46926905038592 also had an error]#; 	# ; 	[thread 46926895564544 also had an error][thread 46926900827904 al,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:17938,concurren,concurrent,17938,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); 			at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 			at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 			at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 			at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 			at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 			at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1495); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2109); 			at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 			at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:10683,concurren,concurrent,10683,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['concurren'],['concurrent']
Performance,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3235:12793,concurren,concurrent,12793,https://hail.is,https://github.com/hail-is/hail/issues/3235,2,['concurren'],['concurrent']
Performance,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6187,Load,LoadVCF,6187,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11164,Load,LoadVCF,11164,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,".dependabot.com/plotly/plotly.py/pull/3765"">#3765</a> with thanks to <a href=""https://github.com/jvdd""><code>@​jvdd</code></a></li>; </ul>; <h3>Updated</h3>; <ul>; <li>Allow non-string extras in <code>flaglist</code> attributes, to support upcoming changes to <code>ax.automargin</code> in plotly.js <a href=""https://github-redirect.dependabot.com/plotly/plotly.js/pull/6193"">plotly.js#6193</a>, <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3749"">#3749</a></li>; </ul>; <h2>[5.8.2] - 2022-06-10</h2>; <h3>Fixed</h3>; <ul>; <li>Fixed a syntax error that caused rendering issues in Databricks notebooks and likely elsewhere. <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3763"">#3763</a> with thanks to <a href=""https://github.com/fwetdb""><code>@​fwetdb</code></a></li>; </ul>; <h2>[5.8.1] - 2022-06-08</h2>; <p>(no changes, due to a mixup with the build process!)</p>; <h2>[5.8.0] - 2022-05-09</h2>; <h3>Fixed</h3>; <ul>; <li>Improve support for type checking and IDE auto-completion by bypassing lazy-loading when type checking. <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3425"">#3425</a> with thanks to <a href=""https://github.com/JP-Ellis""><code>@​JP-Ellis</code></a></li>; <li>line dash-style validators are now correctly used everywhere so that values like <code>10px 2px</code> are accepted <a href=""https://github-redirect.dependabot.com/plotly/plotly.py/pull/3722"">#3722</a></li>; <li>Resolved various deprecation warning messages and compatibility issues with upstream dependencies and Python 3.11, plus removed dependency on <code>six</code>, with thanks to <a href=""https://github.com/maresb""><code>@​maresb</code></a>, <a href=""https://github.com/hugovk""><code>@​hugovk</code></a>, <a href=""https://github.com/tirkarthi""><code>@​tirkarthi</code></a>, <a href=""https://github.com/martinRenou""><code>@​martinRenou</code></a>, and <a href=""https://github.com/BjoernLudwigPTB""><code>@​BjoernLudwigPTB</code></a></li>; <l",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12113:3675,load,loading,3675,https://hail.is,https://github.com/hail-is/hail/pull/12113,1,['load'],['loading']
Performance,.hail.driver.FilterVariants$$anonfun$2.apply(FilterVariants.scala:45); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$5.apply(VariantSampleMatrix.scala:151); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:415); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/120:4068,concurren,concurrent,4068,https://hail.is,https://github.com/hail-is/hail/issues/120,1,['concurren'],['concurrent']
Performance,.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:18446,concurren,concurrent,18446,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['concurren'],['concurrent']
Performance,.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829); Caused by: is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	... 21 more. Driver stacktrace:; 	at org.apache.spark.sche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:8892,concurren,concurrent,8892,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['concurren'],['concurrent']
Performance,".json.gz when loading VDS, create with HailContext.write_partitioning. Java stack trace:; is.hail.utils.HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.variant.VariantDataset$.liftedTree1$1(VariantDataset.scala:89); 	at is.hail.variant.VariantDataset$.read(VariantDataset.scala:84); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:414); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:413); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:413); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2e237ca; Error summary: HailException: missing partitioner.json.gz when loading VDS, create with HailContext.write_partitioning.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683:4554,load,loading,4554,https://hail.is,https://github.com/hail-is/hail/issues/1683,1,['load'],['loading']
Performance,".p.win32.Crypt32</code> - <a href=""https://github.com/shalupov""><code>@​shalupov</code></a></li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1411"">#1411</a>: Do not throw <code>Win32Exception</code> on success for empty section in <code>Kernel32Util#getPrivateProfileSection</code> - <a href=""https://github.com/mkarg""><code>@​mkarg</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1414"">#1414</a>: Fix definition of <code>c.s.j.p.unix.X11.XK_Shift_R</code> - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1323"">#1323</a>. Fix crashes in direct callbacks on mac OS aarch64 - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1422"">#1422</a>: Load jawt library relative to <code>sun.boot.library.path</code> system on unix OSes - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1427"">#1427</a>: Rebuild all binaries with fix from <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1422"">#1422</a> and <a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1323"">#1323</a> - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; </ul>; <h1>Release 5.10.0</h1>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/java-native-access/jna/commit/3705b849892aa3c37e5608e640eff19047811a5c""><code>3705b84</code></a> Release 5.12.1</li>; <li><a href=""https://github.com/java-native-access/jna/commit/2f919e56bad203494fe9589206d6d23f27ef4f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:4827,Load,Load,4827,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['Load'],['Load']
Performance,".py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/usr/local/lib/python3.10/dist-packages/hail/backend/py4j_backend.py"", line 220, in _rpc; raise fatal_error_from_java_error_triplet(; hail.utils.java.FatalError: HailException: VCF spec does not support phased haploid calls. Java stack trace:; is.hail.utils.HailException: VCF spec does not support phased haploid calls.; at __C83collect_distributed_array_matrix_vcf_writer.apply_region154_245(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply_region133_246(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply_region1_250(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply(Unknown Source); at __C83collect_distributed_array_matrix_vcf_writer.apply(Unknown Source); at is.hail.backend.BackendUtils.$anonfun$collectDArray$19(BackendUtils.scala:142); at is.hail.utils.package$.using(package.scala:665); at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:170); at is.hail.backend.BackendUtils.$anonfun$collectDArray$18(BackendUtils.scala:141); at is.hail.backend.spark.SparkBackend$$anon$5.compute(SparkBackend.scala:474); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365); at org.apache.spark.rdd.RDD.iterator(RDD.scala:329); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:136); at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.127-bb535cd096c5; Error summary: HailException: VCF spec does not support phased haploid calls.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14330:3389,concurren,concurrent,3389,https://hail.is,https://github.com/hail-is/hail/issues/14330,2,['concurren'],['concurrent']
Performance,".py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/rwalters-hail-tmp/o?name=merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00&uploadType=resumable&upload_id=ADPycduMEzX6d_uX4CiP6_XItJKmP8UnUnYBfyPoselMbyLUkxs1wDL",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1692,concurren,concurrent,1692,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,".rdd.RDD$$anonfun$8.apply(RDD.scala:332); at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but ¯\_(ツ)_/¯). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hado",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:4354,load,load,4354,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['load'],['load']
Performance,.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:726); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$24(StorageImpl.java:574); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:26); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:239); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:235); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.120-f00f916faf78; Error summary: GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:9712,concurren,concurrent,9712,https://hail.is,https://github.com/hail-is/hail/issues/13409,3,['concurren'],['concurrent']
Performance,.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:2602,Load,LoadVCF,2602,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-bfea6715901c; Error summary: HailException: OrderedRVD error! Unexpected key in partition 7; Range bounds for partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Key should be in partition 7: ([0.8599223493342859]-[0.9976076885349009]]; Invalid key: [0.9986274705095608]; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4096:11103,concurren,concurrent,11103,https://hail.is,https://github.com/hail-is/hail/issues/4096,2,['concurren'],['concurrent']
Performance,.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	... 34 more; Caused by: org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: java.lang.ArrayIndexOutOfBoundsException; ```. And the actual informative trace nested in the `hail.log`:; ```; Caused by: java.lang.ArrayIndexOutOfBoundsException: 1; at scala.collection.mutable.WrappedArray$ofRef.apply(WrappedArray.scala:127); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$209.apply(FunctionRegistry.scala:1058); at org.broadinstitute.hail.expr.BinaryFun.apply(Fun.scala:108); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:143); at org.broadinstitute.hail.expr.FunctionRegistry$$anonfun$lookupMethod$1$$anonfun$36.apply(FunctionRegistry.scala:228); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLik,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202:7523,concurren,concurrent,7523,https://hail.is,https://github.com/hail-is/hail/issues/1202,1,['concurren'],['concurrent']
Performance,.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2276); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182); 	at scala.Option.foreach(Option.scala:407); 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:4866,concurren,concurrent,4866,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['concurren'],['concurrent']
Performance,".scala:36); 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); 	at is.hail.asm4s.ClassesBytes.load(ClassBuilder.scala:62); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:715); 	at is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:708); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1(Compile.scala:311); 	at is.hail.expr.ir.CompileIterator$.$anonfun$forTableStageToRVD$1$adapted(Compile.scala:310); 	at is.hail.expr.ir.lowering.TableStageToRVD$.$anonfun$apply$9(RVDToTableStage.scala:106); 	at is.hail.sparkextras.ContextRDD.$anonfun$cflatMap$2(ContextRDD.scala:211); 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1234); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1233); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.105-3f053140ad00; Error summary: ClassFormatError: Too many arguments in method signature in class file __C2866stream; ```. This used to work fine in earlier Hail versions, e.g. 0.2.85.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:13570,concurren,concurrent,13570,https://hail.is,https://github.com/hail-is/hail/issues/12532,2,['concurren'],['concurrent']
Performance,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901:6695,concurren,concurrent,6695,https://hail.is,https://github.com/hail-is/hail/issues/3901,2,['concurren'],['concurrent']
Performance,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465:8321,concurren,concurrent,8321,https://hail.is,https://github.com/hail-is/hail/issues/3465,5,['concurren'],['concurrent']
Performance,.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:1658,concurren,concurrent,1658,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['concurren'],['concurrent']
Performance,.service.ServiceBackendSocketAPI2$.$anonfun$main$6$adapted(ServiceBackend.scala:460); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$5(ServiceBackend.scala:460); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:460); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4$adapted(ServiceBackend.scala:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.113-0b5bc2eb0c95; Error summary: SocketException: Connection reset; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:25481,concurren,concurrent,25481,https://hail.is,https://github.com/hail-is/hail/issues/12982,6,['concurren'],['concurrent']
Performance,.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12604,load,loadClass,12604,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['load'],['loadClass']
Performance,.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4553,Load,LoadVCF,4553,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Ite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11366,Load,LoadVCF,11366,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4423,Load,LoadVCF,4423,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,".whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5128,cache,cached,5128,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,".write('data/subsetted_chr22.mt', overwrite=True)`. However this error keeps happening:; 2020-04-13 18:03:29 Hail: INFO: Number of BGEN files parsed: 1; 2020-04-13 18:03:29 Hail: INFO: Number of samples in BGEN files: 36; 2020-04-13 18:03:29 Hail: INFO: Number of variants across all BGEN files: 1255683; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""</home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/decorator.py:decorator-gen-1058>"", line 2, in write; File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py"", line 2508, in write; Env.backend().execute(MatrixWrite(self._mir, writer)); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1257, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/home/catarina.gouveia/miniconda3/envs/hail/lib/python3.7/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: Hail only supports 8-bit probabilities, found 16. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8545:1361,load,loads,1361,https://hail.is,https://github.com/hail-is/hail/issues/8545,1,['load'],['loads']
Performance,".zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3507:2473,load,loadAddress,2473,https://hail.is,https://github.com/hail-is/hail/issues/3507,1,['load'],['loadAddress']
Performance,"/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. Java stack trace:; java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1284); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCom",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:1760,concurren,concurrentGlobInternal,1760,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrentGlobInternal']
Performance,"/code></a>]</li>; <li>Support reading BC4U and DX10 BC1 images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/6486"">#6486</a> [<a href=""https://github.com/REDxEYE""><code>@​REDxEYE</code></a>]</li>; <li>Optimize ImageStat.Stat.extrema <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7593"">#7593</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Handle pathlib.Path in FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7578"">#7578</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use list comprehensions to create transformed lists <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7597"">#7597</a> [<a href=""https://github.com/hugovk""><code>@​hugovk</code></a>]</li>; <li>Added support for reading DX10 BC4 DDS images <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7603"">#7603</a> [<a href=""https://github.com/sambvfx""><code>@​sambvfx</code></a>]</li>; <li>Optimized ImageStat.Stat.count <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7599"">#7599</a> [<a href=""https://github.com/florath""><code>@​florath</code></a>]</li>; <li>Moved error from truetype() to FreeTypeFont <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7587"">#7587</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Correct PDF palette size when saving <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7555"">#7555</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed closing file pointer with olefile 0.47 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7594"">#7594</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>ruff: Minor optimizations of list comprehensions, x in set, etc. <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7524"">#7524</a> [<a href=""https://github.com/cclauss""><cod",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14191:8004,Optimiz,Optimized,8004,https://hail.is,https://github.com/hail-is/hail/pull/14191,3,['Optimiz'],['Optimized']
Performance,"/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053:3505,concurren,concurrent,3505,https://hail.is,https://github.com/hail-is/hail/issues/3053,1,['concurren'],['concurrent']
Performance,"/dist-packages/batch/worker/worker.py"", line 2150, in run; await self.jvm.execute(local_jar_location, self.scratch, self.log_file, self.jar_url, self.argv); File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 2629, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:224); at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:186); at is.hail.JVMEntryway.main(JVMEntryway.java:156); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; at is.hail.JVMEntryway$1.run(JVMEntryway.java:107); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); ... 7 more; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950:1471,concurren,concurrent,1471,https://hail.is,https://github.com/hail-is/hail/issues/12950,1,['concurren'],['concurrent']
Performance,"/github-redirect.dependabot.com/sveltejs/svelte/pull/7407"">#7407</a>)</li>; </ul>; <h2>3.46.5</h2>; <ul>; <li>Add TypeScript interfaces for typing actions (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/6538"">#6538</a>)</li>; <li>Do not generate <code>unused-export-let</code> warning inside <code>&lt;script context=&quot;module&quot;&gt;</code> blocks (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7055"">#7055</a>)</li>; <li>Do not collapse whitespace-only CSS vars (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7152"">#7152</a>)</li>; <li>Add <code>aria-description</code> to the list of allowed ARIA attributes (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7301"">#7301</a>)</li>; <li>Fix attribute escaping during SSR (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7327"">#7327</a>)</li>; <li>Prevent <code>.innerHTML</code> optimization from being used when <code>style:</code> directive is present (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7386"">#7386</a>)</li>; </ul>; <h2>3.46.4</h2>; <ul>; <li>Avoid <code>maximum call stack size exceeded</code> errors on large components (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/4694"">#4694</a>)</li>; <li>Preserve leading space with <code>preserveWhitespace: true</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/4731"">#4731</a>)</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/sveltejs/svelte/commit/52153dbce0237f0c36e4ff36377398d7f95276ef""><code>52153db</code></a> -&gt; v3.49.0</li>; <li><a href=""https://github.com/sveltejs/svelte/commit/3798808e7484b7eeee6acb2860c45bb2e59d84bd""><code>3798808</code></a> update changelog</li>; <li><a href=""https://github.com/sveltejs/svelte/commit/0fa0a38d5168a1767843fdb0a43c00a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12032:5627,optimiz,optimization,5627,https://hail.is,https://github.com/hail-is/hail/pull/12032,3,['optimiz'],['optimization']
Performance,"/index; 2023-09-22 19:11:12.656 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=656384, peakBytesReadable=641.00 KiB, chunks requested=4, cache hits=2; 2023-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""mes",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:9842,concurren,concurrent,9842,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"/microsoft/debugpy/issues/869"">#869</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/973"">#973</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/987"">#987</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/995"">#995</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1008"">#1008</a></p>; <p>Improvements: <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/951"">#951</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/1001"">#1001</a></p>; <h2>debugpy v1.6.2</h2>; <p>Fixes unintentional breaking change in public API in debugpy 1.6.1 (<a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/975"">#975</a>).</p>; <p>Other fixes: <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/969"">#969</a></p>; <h2>debugpy v1.6.1</h2>; <p>debugpy API now has type annotations.</p>; <p>Optimizations based on frame evaluation API are re-enabled by default.</p>; <p>Other improvements: <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/743"">#743</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/774"">#774</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/893"">#893</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/945"">#945</a></p>; <p>Bug fixes: <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/705"">#705</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/731"">#731</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/861"">#861</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/865"">#865</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/882"">#882</a>, <a href=""https://github-redirect.dependabot.com/microsoft/debugpy/issues/889"">#889</a>, <a href=""https://github-redirect.d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12103:1297,Optimiz,Optimizations,1297,https://hail.is,https://github.com/hail-is/hail/pull/12103,2,['Optimiz'],['Optimizations']
Performance,"/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 821, in unpack_url; hashes=hashes; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 659, in unpack_http_url; hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 882, in _download_http_url; _download_url(resp, link, content_file, hashes); File ""/usr/lib/python3/dist-packages/pip/download.py"", line 603, in _download_url; hashes.check_against_chunks(downloaded_chunks); File ""/usr/lib/python3/dist-packages/pip/utils/hashes.py"", line 46, in check_against_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 571, in written_chunks; for chunk in chunks:; File ""/usr/lib/python3/dist-packages/pip/utils/ui.py"", line 139, in iter; for x in it:; File ""/usr/lib/python3/dist-packages/pip/download.py"", line 560, in resp_read; decode_content=False):; File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 436, in stream; data = self.read(amt=amt, decode_content=decode_content); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 401, in read; raise IncompleteRead(self._fp_bytes_read, self.length_remaining); File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__; self.gen.throw(type, value, traceback); File ""/usr/share/python-wheels/urllib3-1.22-py2.py3-none-any.whl/urllib3/response.py"", line 307, in _error_catcher; raise ReadTimeoutError(self._pool, None, 'Read timed out.'); urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.; The command '/bin/sh -c python3 -m pip install --no-cache-dir -r requirements.txt -r dev-requirements.txt' returned a non-zero code: 2; [0m; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8390:3131,cache,cache-dir,3131,https://hail.is,https://github.com/hail-is/hail/issues/8390,1,['cache'],['cache-dir']
Performance,"/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API vers",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2267,Cache,CacheDir,2267,https://hail.is,https://github.com/hail-is/hail/issues/14513,8,['Cache'],['CacheDir']
Performance,"/worker.py"", line 634, in _localize_rootfs; await self._pull_image(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 587, in _pull_image; await pull(); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 566, in pull; raise ImageCannotBePulled from e; ImageCannotBePulled. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1887, in run_container; await container.run(on_completion); File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 920, in run; await on_completion(*args, **kwargs); File ""/usr/lib/python3.9/contextlib.py"", line 137, in __exit__; self.gen.throw(typ, value, traceback); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 1154, in step; yield; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 1873, in on_completion; await self.worker.fs.read(container.log_path),; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/fs/fs.py"", line 281, in read; async with await self.open(url) as f:; File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/router_fs.py"", line 76, in open; return await fs.open(url); File ""/usr/local/lib/python3.9/dist-packages/hailtop/aiotools/local_fs.py"", line 252, in open; f = await blocking_to_async(self._thread_pool, open, self._get_path(url), 'rb'); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 181, in blocking_to_async; return await asyncio.get_event_loop().run_in_executor(; File ""/usr/lib/python3.9/concurrent/futures/thread.py"", line 58, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 182, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); FileNotFoundError: [Errno 2] No such file or directory: '/batch/00a8b257731544b494247db2813c7a83/main/container.log'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13907:4182,concurren,concurrent,4182,https://hail.is,https://github.com/hail-is/hail/issues/13907,1,['concurren'],['concurrent']
Performance,"0,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1:90,2,0,0,0:30:40:0|1:113_GG_G:40,0,600,70,650,600,90,640,900,300,60,800,400,900,900:113:2,14,2,0	0/0:.:20:10	0/0:.:9:20	0/0:.:30:40	0/0:.:37:38		0/4:5,0,0,0,1:5:33:.:.:30,40,400,50,220,220,38,270,270,270,0,200,200,200,202:.:5,0,0,1	. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:7576,Load,LoadVCF,7576,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['Load'],['LoadVCF']
Performance,"0.128.0.126:36044: java.nio.channels.ClosedChannelException; java.nio.channels.ClosedChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2044,concurren,concurrent,2044,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,"0/site-packages/hail/backend/backend.py:192) value = None. File .../python3.10/site-packages/hail/backend/backend.py:188, in Backend.execute(self, ir, timed); [186](.../python3.10/site-packages/hail/backend/backend.py:186) payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); [187](.../python3.10/site-packages/hail/backend/backend.py:187) try:; --> [188](.../python3.10/site-packages/hail/backend/backend.py:188) result, timings = self._rpc(ActionTag.EXECUTE, payload); [189](.../python3.10/site-packages/hail/backend/backend.py:189) except FatalError as e:; [190](.../python3.10/site-packages/hail/backend/backend.py:190) raise e.maybe_user_error(ir) from None. File .../python3.10/site-packages/hail/backend/py4j_backend.py:220, in Py4JBackend._rpc(self, action, payload); [218](.../python3.10/site-packages/hail/backend/py4j_backend.py:218) if resp.status_code >= 400:; [219](.../python3.10/site-packages/hail/backend/py4j_backend.py:219) error_json = orjson.loads(resp.content); --> [220](.../python3.10/site-packages/hail/backend/py4j_backend.py:220) raise fatal_error_from_java_error_triplet(; [221](.../python3.10/site-packages/hail/backend/py4j_backend.py:221) error_json['short'], error_json['expanded'], error_json['error_id']; [222](.../python3.10/site-packages/hail/backend/py4j_backend.py:222) ); [223](.../python3.10/site-packages/hail/backend/py4j_backend.py:223) return resp.content, resp.headers.get('X-Hail-Timings', ''). FatalError: ClassTooLargeException: Class too large: __C67907collect_distributed_array_table_collect. Java stack trace:; is.hail.relocated.org.objectweb.asm.ClassTooLargeException: Class too large: __C67907collect_distributed_array_table_collect; 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(ClassWriter.java:599); 	at is.hail.lir.Emit$.apply(Emit.scala:234); 	at is.hail.lir.Classx.$anonfun$asBytes$4(X.scala:109); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461); 	at scala.collection.Iterato",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14362:6697,load,loads,6697,https://hail.is,https://github.com/hail-is/hail/issues/14362,1,['load'],['loads']
Performance,00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/dK3o5ZfXmYSkP5TA/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:37+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs BlockBlob Hot 1264 application/octet-stream 2023-06-09T12:43:34+00:00; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethod,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:3849,concurren,concurrent,3849,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,09883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2225,Load,LoadPlink,2225,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting Mar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:4945,cache,cached,4945,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"1. Add options to store the scores as sample annotations and the loadings as variant annotations (or, eventually, store by default and write out optionally); 2. Once LD-pruning is implemented, should it be performed first automatically? Probably not, but perhaps an option and the doc should mention the issue.; 3. PLINK has an option to use X-chromosome variants. What is it doing exactly? There are several decisions around encoding hemizygous sites for males. More importantly, does anyone use it? Should we support it?; 4. What about PCA of things other than genotypes, such as missingness? Analysts have mentioned applications to QC and flagged the latter specifically, which is implemented in GCTA.; 5. Extension to multiallelics? Probably not so important as few variants have more than two common alleles and each individual variant generally contributes little. If we did it, a good approach is probably a one-hot encoding although the variance normalization needs some care. For microsatellites/STRs a quantitative rather than categorical encoding may be better.; 6. Support for outlier detection a la SmartPCA and/or EIGENSTRAT?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/205:65,load,loadings,65,https://hail.is,https://github.com/hail-is/hail/issues/205,2,"['load', 'perform']","['loadings', 'performed']"
Performance,"1. I taught `get_or_add_user` to serialize loading of a single user's secrets; from kubernetes. The old code would issue multiple requests in parallel for; a user not yet in `users`. 2. The whole situation with the worker pool and the files in progress set did; not seem right to me. If a write comes in, we create a future corresponding; to the write and store the future in a dictionary. If a read comes in, we; first check redis, then we check if there is already a write or read of cloud; storage in progress. If there is no write nor read in progress, we initiate; a read. Before a read or write completes, it removes itself from the in-progress; files (future reads will now see the file in redis). Writes are permitted to; overwrite other writes that are already in progress.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11202:43,load,loading,43,https://hail.is,https://github.com/hail-is/hail/pull/11202,1,['load'],['loading']
Performance,"1. Lazily load the tokens on the end user machine because the first time someone logs in to the default namespace, they will have no tokens. 2. Teach `get_tokens_file` to default to the default end-user location. 3. Add a bunch of types which would have caught this error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11054:10,load,load,10,https://hail.is,https://github.com/hail-is/hail/pull/11054,1,['load'],['load']
Performance,"1. NativeModule manages the lifecycle of Scala-generated C++ functions. Generate the C++; source code as a Scala String, then construct ; NativeModule(compileOptions, sourceString, forceBuild); From that object you can do getKey(): String and getBinary(): Array[Byte] to get the compiled ; code in a serializable form which can be passed to other nodes in a cluster, and then used to; construct a local NativeModule(key, binary). 2. NativeCode.java now does a two-stage bootstrap on Linux to make sure that libhail.so is; loaded with the RTLD_GLOBAL flag so that dynamic-generated C++ can use functions; defined in libhail.so. On Mac, this works ok without the bootstrap. [We needed this for the; RowStore with generated-C++ decoders]. 3. Added a NativeStatus* parameter to all NativeLongFunc's, to encourage consistent handling; of errors.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973:522,load,loaded,522,https://hail.is,https://github.com/hail-is/hail/pull/3973,1,['load'],['loaded']
Performance,"1. Replace kaniko with buildkit.; 2. For any repository, the `cache` tag is a dump of layers that have been built before.; 3. Allow ci to start docker jobs that are ""unconfined"" by app armor and seccomp.; 4. Add `docker-build.sh` which enforces use of buildkit, knows about the `cache` tag, and ensures the cache is updated on each build.; 5. Remove the `gcloud` binary from Dockerfile.base. Replace uses with either the gcloud-sdk image or with ci-utils-image, which now contains the gcloud install.; 6. Move pyspark (which is huge, 100s of MB) before everything because its version rarely changes.; 7. Move requirements.txt to the end of base, since it changes more often than the rest.; 8. Move hailtop last in service-base because hailtop has a git SHA in it.; 9. Simplify make files: always use docker-build.sh, no explicit pushes (we almost always want to push), no explicit pulls (buildkit cache doesn't need it), none of this digest nonsense (it was never accurate anyway). When my namespace CI builds ci/test/resources/build.yaml, it finishes in 4 minutes. Still dominated by image building. Layer extraction (required when things change, e.g. hail top's SHA change or hello's python files) dominates our time. We might try collapsing the largely unchanging lower layers of service-base (pyspark, apt-get, gcs-connector, and catch2). That will hurt us when we *do* change one of those layers. Alternatively, we might make service-base based on hail-ubuntu instead of base. We could eliminate a bunch of build software like cmake, gcc, and the jdk. I based the create-certs image on hail-ubuntu to ensure its built early and doesn't hold up service deployment. The following is an as-cached-as-possible build. The service and hello images have to extract layers and build themselves because the SHA changed. <img width=""1920"" alt=""Screen Shot 2021-05-19 at 2 34 18 PM"" src=""https://user-images.githubusercontent.com/106194/118865766-4e74d800-b8af-11eb-8386-94a3782a2a45.png"">. I'm not even sur",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502:62,cache,cache,62,https://hail.is,https://github.com/hail-is/hail/pull/10502,4,['cache'],['cache']
Performance,"1. Separating the `apk update` from the `apk add` means that if the apk package repository metadata changes (say, the URL of some repository changes) and we change our `apk add` line (say we add a new package), then the `apk add` will fail (e.g. because it has an out of date URL for the repository that should contain the new package). The `apk add --no-cache ...` invocation is essentially the same as `apk update && apk add ... && rm -rf /path/to/repo/cache`. When using docker, it is good practice remove unnecessary files so that they do not get included in the ""image diff"" for that line of the Dockerfile. `apk add --no-cache ...` succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into the destination path (creating it if it does not exist) which must be a folder, it seems more clear to say `/batch/batch/`, indicating that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4791:355,cache,cache,355,https://hail.is,https://github.com/hail-is/hail/pull/4791,4,"['cache', 'perform']","['cache', 'performs']"
Performance,"1. Until we scale up the memory service's throughput, avoid use on the client; and the worker if there are more than 50 partitions. 2. On the driver, open no more than 100 concurrent connections to Google Cloud; Storage. 3. Set a timeout of five seconds to connect or read from Google Cloud Storage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11688:42,throughput,throughput,42,https://hail.is,https://github.com/hail-is/hail/pull/11688,2,"['concurren', 'throughput']","['concurrent', 'throughput']"
Performance,1. Use encoded bytes to transfer result from Scala to Python; 2. Use encoded bytes for RelationalLet literals; 3. Optimize after lifting relational operations to eliminate trivial lets; 4. Avoid the memory service for large jobs (eventually we need to scale memory),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11670:114,Optimiz,Optimize,114,https://hail.is,https://github.com/hail-is/hail/pull/11670,1,['Optimiz'],['Optimize']
Performance,"1. We currently do not do a per-variant variance normalization. There should be an option to include normalization, or even a partial normalization dividing by standard deviation raised to a power between zero and one.; 2. Include an option to compute and output the ""other"" singular vectors, corresponding to the loadings of the PCs on variants rather than samples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/100:314,load,loadings,314,https://hail.is,https://github.com/hail-is/hail/issues/100,1,['load'],['loadings']
Performance,"1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3,>=1.2.10; Using cached Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB); Collecting PyJWT; Using cached PyJWT-2.0.1-py3-none-any.whl (15 kB); Collecting numpy<2; Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:2169,cache,cached,2169,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:2534,Load,Loading,2534,https://hail.is,https://github.com/hail-is/hail/issues/2966,6,['Load'],['Loading']
Performance,10); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:3425,concurren,concurrent,3425,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['concurren'],['concurrent']
Performance,10); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:10251,concurren,concurrent,10251,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['concurren'],['concurrent']
Performance,109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13344,Load,LoadMatrix,13344,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,128 MiB partitions are a much more reasonable default than 1MB. This will result; in better file read/write performance.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7957:108,perform,performance,108,https://hail.is,https://github.com/hail-is/hail/pull/7957,1,['perform'],['performance']
Performance,"15; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK toplevel /opt/vep/src/ensembl-vep/vep:225; Date (localtime) = Mon Apr 29 23:53:34 2024; Ensembl API version = 95; ---------------------------------------------------. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 20 times, most recent failure: Lost task 8.19 in stage 1.0 (TID 2899) (hail-test-w-1.australia-southeast1-a.c.pb-dev-312200.internal executor 3): is.hail.utils.HailException: VEP command '/vep --format vcf --json --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh38 --fasta /opt/vep/.vep/homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open databa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:3721,cache,cache,3721,https://hail.is,https://github.com/hail-is/hail/issues/14513,1,['cache'],['cache']
Performance,"1894 force_bgz,; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2566,Load,LoadVCF,2566,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"1</a> [component: bokehjs] Save tool in gridplot initiates multiple downloads</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8684"">#8684</a> Allow at least partial alignment of fixed sized frames</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9113"">#9113</a> [component: bokehjs] Empty group widgets don't size properly once populated</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9133"">#9133</a> [BUG] Tabs ignore explicitly set dimensions</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9208"">#9208</a> [component: bokehjs] [BUG] sizing_mode='stretch_width' makes plot too wide if scrollbar is showing</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9320"">#9320</a> [BUG] Bokeh rendering performance</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9448"">#9448</a> [component: bokehjs] [BUG] Google Fonts not loading on Glyph on standalone HTML until interacting with Glyph</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9744"">#9744</a> [component: bokehjs] [BUG] bokeh server layout overlap on toggle visibility</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9763"">#9763</a> [BUG] <code>gridplot</code> <code>merge_tools</code> removes distinct tools it thinks are repeated, e.g., <code>xpan</code> and <code>ypan</code></li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9764"">#9764</a> [component: bokehjs] [BUG] MultiChoice placeholder text not displayed</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9992"">#9992</a> [component: bokehjs] [BUG] Select widget hiding tabs, when selecting a plot</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/10125"">#10125</a> [component: bokehjs] [BUG] widgets overlap each other</li>; <li><a href=""https://github-redirect.dependabot.com/b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12454:3561,load,loading,3561,https://hail.is,https://github.com/hail-is/hail/pull/12454,1,['load'],['loading']
Performance,"1_70_65,GAAGTCTCATCCCATC-1_45;AACGTCAGTGTTTCAG-1_74:1:19:255	0/1:2,4:6:60:101,0,60:.:.:.:.; chr1	12198	rs62635282	G	C	69.6	.	BaseQRankSum=0;ClippingRankSum=0;DB;ExcessHet=3.0103;FS=4.771;MQ=42;MQRankSum=-0.967;QD=23.2;ReadPosRankSum=0.967;SOR=2.225;CSQ=C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000450305|transcribed_unprocessed_pseudogene|2/6||||68|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|non_coding_transcript_exon_variant|MODIFIER|DDX11L1|ENSG00000223972|Transcript|ENST00000456328|processed_transcript|1/3||||330|||||||1||HGNC|HGNC:37102|||||||||||||||||||||||||||,C|downstream_gene_variant|MODIFIER|WASH7P|ENSG00000227232|Transcript|ENST00000488147|unprocessed_pseudogene|||||||||||2206|-1||HGNC|HGNC:38034|||||||||||||||||||||||||||;DP=3;AF=0.5;MLEAC=1;MLEAF=0.5;AN=2;AC=1	GT:AD:DP:GQ:PL	./.:.:.:.:.	0/1:1,2:3:37:77,0,37; ```. Getting this error message:; ```; INFO: [pid 11941] Worker Worker(salt=943636132, workers=1, host=seqr-loading-cluster-m, username=root, pid=11941) running SeqrVCFToMTTask(source_paths=gs://seqr-bw/merged_phased_3P5CH.split.vcf.gz, dest_path=gs://seqr-bw/merged_phased_3P5CH.mt, genome_version=38, vep_runner=VEP, reference_ht_path=gs://seqr-reference-data/GRCh38/all_reference_data/combined_reference_data_grch38.ht, clinvar_ht_path=gs://seqr-reference-data/GRCh38/clinvar/clinvar.GRCh38.2020-03-29.ht, hgmd_ht_path=None, sample_type=WGS, validate=False, dataset_type=VARIANTS, remap_path=, subset_path=, vep_config_json_path=); Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.5; SparkUI available at http://seqr-loading-cluster-m.c.seqr-project.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-914bd8a10ca2; LOGGING: writing to /tmp/c7e0443c47b54e91b295e2bff7b554b9/hail-20200405-1408-0.2.34-914bd8a10ca2.log; {'_Task__hash': -3818947167740532127,; 'clinvar_ht_path': 'gs://seqr-reference-data/GRCh38/clinva",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469:36112,load,loading-cluster-m,36112,https://hail.is,https://github.com/hail-is/hail/issues/8469,1,['load'],['loading-cluster-m']
Performance,"2 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidden""; }; ]; }; }. 	at is.hail.relocated.com.google.cloud.storage.StorageException.translate(St",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:10188,concurren,concurrent,10188,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.21-1d317a44e5fd; Error summary: NoSuchElementException: key not found: GRCh37; ```. ### Error No. 2; ```python; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. /usr/local/lib/python3.6/site-packages/hail/matrixtable.py in _repr_html_(self); 2524 ; 2525 def _repr_html_(self):; -> 2526 s = self.table_show._repr_html_(); 2527 if self.displayed_n_cols != self.actual_n_cols:; 2528 s += '<p style=""background: #fdd; padding: 0.4em;"">'. /usr/local/lib/python3.6/site-packages/hail/table.py in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:25945,concurren,concurrent,25945,https://hail.is,https://github.com/hail-is/hail/issues/7044,1,['concurren'],['concurrent']
Performance,2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.apply(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:19604,concurren,concurrent,19604,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,2); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GRCh37; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at is.hail.expr.ir.TypeParserEnvironment.getReferenceGenome(Parser.scala:136); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:363); 	at is.hail.expr.ir.IRParser$.type_field(Parser.scala:324); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$13.apply(Parser.scala:406); 	at is.hail.expr.ir.IRParser$.repsepUntil(Parser.scala:289); 	at is.hail.expr.ir.IRParser$.type_expr(Parser.scala:406); 	at is.hail.expr.ir.IRParser$$anonfun$parseType$1.appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7044:7881,concurren,concurrent,7881,https://hail.is,https://github.com/hail-is/hail/issues/7044,2,['concurren'],['concurrent']
Performance,"2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 20050, localhost): org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'; 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:247); 	at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:545); 	at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 6.0.0; 	at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:79); 	at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:613); 	at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:240); 	... 10 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4138:2494,concurren,concurrent,2494,https://hail.is,https://github.com/hail-is/hail/issues/4138,1,['concurren'],['concurrent']
Performance,"2.013327 ms; 2018-10-09 14:46:41 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 972 bytes result sent to driver; 2018-10-09 14:46:41 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 406 ms on localhost (executor driver) (1/1); 2018-10-09 14:46:41 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 14:46:41 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.420 s; 2018-10-09 14:46:41 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.669318 s; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: table7e606a8b83f4; 2018-10-09 14:46:41 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: CACHE TABLE `table7e606a8b83f4`; 2018-10-09 14:46:41 SparkSqlParser: INFO: Parsing command: `table7e606a8b83f4`; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 17.141549 ms; 2018-10-09 14:46:41 CodeGenerator: INFO: Code generated in 10.417049 ms; 2018-10-09 14:46:41 SparkContext: INFO: Starting job: sql at NativeMethodAccessorImpl.java:0; 2018-10-09 14:46:41 DAGScheduler: INFO: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 14:46:41 DAGScheduler: INFO: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions; 2018-10-09 14:46:41 DAGScheduler: INFO: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0); 2018-10-09 14:46:41 DAGScheduler: INFO: Parents of final stage: List(ShuffleMapStage 1); 2018-10-09 14:46:41 DAGScheduler: INFO: Missing parents: List(ShuffleMapStage 1); 2018-10-09 14:46:41 DAGScheduler: INFO: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents; 2018-10-09 14:46:41 MemoryStore: INFO: Bl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:35800,CACHE,CACHE,35800,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['CACHE'],['CACHE']
Performance,2.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3516:3318,concurren,concurrent,3318,https://hail.is,https://github.com/hail-is/hail/issues/3516,1,['concurren'],['concurrent']
Performance,"2.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Under ""Failed Stages"", these were the details for what I was running:; ```; org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.la",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508:6128,concurren,concurrent,6128,https://hail.is,https://github.com/hail-is/hail/issues/3508,1,['concurren'],['concurrent']
Performance,2/TopMed_8k.853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669:1215,Load,LoadVCF,1215,https://hail.is,https://github.com/hail-is/hail/issues/669,1,['Load'],['LoadVCF']
Performance,"23-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:10011,concurren,concurrent,10011,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). is.hail.utils.HailException: cannot set missing field for required type +PFloat64; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:18); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:18); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.annotations.RegionValueBuilder.setMissing(RegionValueBuilder.scala:207); 	at is.hail.io.vcf.VCFLine.parseAddInfoArrayDouble(LoadVCF.scala:1034); 	at is.hail.io.vcf.VCFLine.parseAddInfoField(LoadVCF.scala:1055); 	at is.hail.io.vcf.VCFLine.addInfoField(LoadVCF.scala:1075); 	at is.hail.io.vcf.VCFLine.parseAddInfo(LoadVCF.scala:1112); 	at is.hail.io.vcf.LoadVCF$.parseLineInner(LoadVCF.scala:1541); 	at is.hail.io.vcf.LoadVCF$.parseLine(LoadVCF.scala:1409); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1916); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.It,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:18968,Load,LoadVCF,18968,https://hail.is,https://github.com/hail-is/hail/issues/14102,1,['Load'],['LoadVCF']
Performance,"25 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/io/OutputBuffer;; 11330 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11331 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:5422,Load,LoadX,5422,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"25 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6169,cache,cached,6169,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"3-4x increase in job-scheduling throughput. ## Benchmarking; Below are before-and-after profiles of the same benchmark (30,000 1s jobs) under the proposed higher rate limit, showing CPU time:; <img width=""1889"" alt=""Screen Shot 2022-03-21 at 5 10 13 PM"" src=""https://user-images.githubusercontent.com/24440116/159364769-6fd60840-5745-40ab-802e-68b8d4f32078.png"">; <img width=""1885"" alt=""Screen Shot 2022-03-21 at 5 12 33 PM"" src=""https://user-images.githubusercontent.com/24440116/159364787-ca7ec307-877d-479c-9c19-8746b5e82eab.png"">. Looking at Wall time, the before profile is nearly identical because at the current rate limit the driver uses 100% of its CPU shares under this benchmark. On this branch, CPU utilization drops to 40-60%, giving the following wall time profile:; <img width=""1879"" alt=""Screen Shot 2022-03-21 at 5 30 39 PM"" src=""https://user-images.githubusercontent.com/24440116/159367182-0830d6ff-3b6f-4fa7-8004-0fc43283ec4a.png"">. So we can be confident that driver CPU is no longer a bottleneck even in the increased rate limit. ## So what's the bottleneck now?; Since the higher rate limit still leaves the driver plenty of CPU room (I've seen it peak at 60% of a vCPU), why not crank it higher? Well, we're increasing concurrency so latent deadlocks start to be a bigger issue again. We start to see tens of deadlocks per second in the proposed rate limit and hundreds at higher rate limits. As a result, we're spending more cycles repeating queries instead of actually scheduling faster. Next steps should focus on eliminating deadlocks before we can continue to max out CPU use. ## Miscellaneous; We've lumped in a few other monitoring changes that were helpful in this process and tried to leave the commits tidy. The one potentially rude change is enforcing a minimum wait time of half a second for `run_if_changed` loops. This dramatically reduced the number of scheduling loop invocations we were executing, greatly reducing the number of `compute_fair_share` queries, wh",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11638:1380,bottleneck,bottleneck,1380,https://hail.is,https://github.com/hail-is/hail/pull/11638,1,['bottleneck'],['bottleneck']
Performance,"3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3383,cache,cached,3383,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.readInt(ServiceBackend.scala:510); 	at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:561); 	at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4430,concurren,concurrent,4430,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"35); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.schedule",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:6161,Load,LoadVCF,6161,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,35006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:6082,Load,LoadPlink,6082,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,4 concurrent builds for 10 engineers with a couple branches each just isn't tenable.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7728:2,concurren,concurrent,2,https://hail.is,https://github.com/hail-is/hail/pull/7728,1,['concurren'],['concurrent']
Performance,"4.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Coll",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6077,cache,cached,6077,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,49); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iter,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11236,Load,LoadVCF,11236,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: foo: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; offending line: 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:20); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.utils.Context.wrapException(Context.scala:19); 	at is.hail.utils.WithContext.wrap(Context.scala:43); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:10554,Load,LoadMatrix,10554,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"4ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;); 31019 25774 (InsnX I2B; 31020 25775 (InsnX IOR; 31021 25776 (InsnX IOR; 31022 25777 (InsnX IOR; 31023 25778 (InsnX IOR; 31024 25779 (LdcX 0 I); 31025 25780 (InsnX ISHL; 31026 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2346null Z; 31027 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31028 25782 (LdcX 0 I))); 31029 25783 (InsnX ISHL; 31030 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2348null Z; 31031 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31032 25785 (LdcX 1 I))); 31033 25786 (InsnX ISHL; 31034 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2350null Z; 31035 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31036 25788 (LdcX 2 I))); 31037 25789 (InsnX ISHL; 31038 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2352null Z; 31039 (LoadX arg:1 L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 31040 25791 (LdcX 3 I))))); 31041 (ReturnX). # Elsewhere, this split method is called, then the resulting field is loaded and written to the output buffer. 11325 (MethodStmtX INVOKEVIRTUAL __C1527collect_distributed_array.__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616_region0_0 (L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)V; 11326 (LoadX arg:0 L__C1527collect_distributed_array;); 11327 (LoadX t489ae494/spills L__C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills;)); 11328 25772 (MethodStmtX INVOKEINTERFACE is/hail/io/OutputBuffer.writeByte (B)Vinterface; 11329 (GetFieldX GETFIELD __C2316__m1984ENCODE_SInsertFieldsStruct_TO_EBaseStruct___iruid_8616Spills.__f2354null Lis/hail/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328:5156,Load,LoadX,5156,https://hail.is,https://github.com/hail-is/hail/pull/11328,1,['Load'],['LoadX']
Performance,"5.8 GB; 17/01/17 09:24:46 INFO SparkEnv: Registering OutputCommitCoordinator; 17/01/17 09:24:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.; 17/01/17 09:24:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://129.94.72.55:4040; 17/01/17 09:24:46 INFO Executor: Starting executor ID driver on host localhost; 17/01/17 09:24:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37833.; 17/01/17 09:24:46 INFO NettyBlockTransferService: Server created on 129.94.72.55:37833; 17/01/17 09:24:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMasterEndpoint: Registering block manager 129.94.72.55:37833 with 15.8 GB RAM, BlockManagerId(driver, 129.94.72.55, 37833); 17/01/17 09:24:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 129.94.72.55, 37833); hail: info: running: read test.in.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: running: annotatevariants expr -c 'va = {}'; hail: info: running: write -o test.out.vds; [Stage 1:==> (1 + 24) / 25]hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.InsertInto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1260:2591,load,load,2591,https://hail.is,https://github.com/hail-is/hail/issues/1260,1,['load'],['load']
Performance,50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:26); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:239); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:235); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleCl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:6778,concurren,concurrent,6778,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['concurren'],['concurrent']
Performance,"50</a> from tylerjereddy/treddy_scipy_192_more_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/6b098c25223e224ff44101f86bbc86efecffe1d9""><code>6b098c2</code></a> TST: optimize.milp: remove problematic timeout/iteration test</li>; <li><a href=""https://github.com/scipy/scipy/commit/24dce9760b87934f1be046ec817c758b0f3952dc""><code>24dce97</code></a> DOC: stats.pearsonr: typo in coeffic<em>i</em>ent (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17153"">#17153</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/a6ba7cad3b54c35d2ccb55c595691689004742c1""><code>a6ba7ca</code></a> MAINT: misc 1.9.2 updates</li>; <li><a href=""https://github.com/scipy/scipy/commit/ed9760e60a28b8f13e5644494033e2dab9aafbcd""><code>ed9760e</code></a> MAINT: stats.pearson3: fix ppf for negative skew (<a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17055"">#17055</a>)</li>; <li><a href=""https://github.com/scipy/scipy/commit/6fb67007dd7105755057f3379fb7ef423eae524e""><code>6fb6700</code></a> FIX: optimize.milp: return feasible solution if available on timeout/node lim...</li>; <li><a href=""https://github.com/scipy/scipy/commit/bcfce27fc061cbde6ac6531799362e0420ea4796""><code>bcfce27</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/scipy/scipy/issues/17132"">#17132</a> from tylerjereddy/treddy_192_backports</li>; <li><a href=""https://github.com/scipy/scipy/commit/2bc973a2c28c4b6b5bea0e288631834fe34b526e""><code>2bc973a</code></a> BLD: set version to 1.9.2.dev0 (and trigger wheel build CI)</li>; <li>Additional commits viewable in <a href=""https://github.com/scipy/scipy/compare/v1.2.1...v1.9.2"">compare view</a></li>; </ul>; </details>; <br />. Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12352:2788,optimiz,optimize,2788,https://hail.is,https://github.com/hail-is/hail/pull/12352,1,['optimiz'],['optimize']
Performance,6); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:3820,concurren,concurrent,3820,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,6); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:9978,concurren,concurrent,9978,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,6); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12116,concurren,concurrent,12116,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,"6456</a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/psf/requests/blob/main/HISTORY.md"">requests's changelog</a>.</em></p>; <blockquote>; <h2>2.32.0 (2024-05-20)</h2>; <p><strong>Security</strong></p>; <ul>; <li>Fixed an issue where setting <code>verify=False</code> on the first request from a; Session will cause subsequent requests to the <em>same origin</em> to also ignore; cert verification, regardless of the value of <code>verify</code>.; (<a href=""https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56"">https://github.com/psf/requests/security/advisories/GHSA-9wx4-h78v-vm56</a>)</li>; </ul>; <p><strong>Improvements</strong></p>; <ul>; <li><code>verify=True</code> now reuses a global SSLContext which should improve; request time variance between first and subsequent requests. It should; also minimize certificate load time on Windows systems when using a Python; version built with OpenSSL 3.x. (<a href=""https://redirect.github.com/psf/requests/issues/6667"">#6667</a>)</li>; <li>Requests now supports optional use of character detection; (<code>chardet</code> or <code>charset_normalizer</code>) when repackaged or vendored.; This enables <code>pip</code> and other projects to minimize their vendoring; surface area. The <code>Response.text()</code> and <code>apparent_encoding</code> APIs; will default to <code>utf-8</code> if neither library is present. (<a href=""https://redirect.github.com/psf/requests/issues/6702"">#6702</a>)</li>; </ul>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Fixed bug in length detection where emoji length was incorrectly; calculated in the request content-length. (<a href=""https://redirect.github.com/psf/requests/issues/6589"">#6589</a>)</li>; <li>Fixed deserialization bug in JSONDecodeError. (<a href=""https://redirect.github.com/psf/requests/issues/6629"">#6629</a>)</li>; <li>Fixed bug where an ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14555:4915,load,load,4915,https://hail.is,https://github.com/hail-is/hail/pull/14555,1,['load'],['load']
Performance,667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:17); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:498); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:10,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:4640,Load,LoadVCF,4640,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['Load'],['LoadVCF']
Performance,667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:557); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:10506,Load,LoadVCF,10506,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['Load'],['LoadVCF']
Performance,68); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:68); at is.hail.keytable.KeyTable$$anonfun$8.apply(KeyTable.scala:65); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1275:9092,concurren,concurrent,9092,https://hail.is,https://github.com/hail-is/hail/issues/1275,2,['concurren'],['concurrent']
Performance,"6_64.whl (15.4 MB); Collecting aiohttp-session<2.8,>=2.7; Using cached aiohttp_session-2.7.0-py3-none-any.whl (14 kB); Collecting dill<0.4,>=0.3.1.1; Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB); Collecting humanize==1.0.0; Using cached humanize-1.0.0-py2.py3-none-any.whl (51 kB); Collecting hurry.filesize==0.9; Using cached hurry.filesize-0.9-py3-none-any.whl; Collecting scipy<1.7,>1.2; Using cached scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3206,cache,cached,3206,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-query-ger0g_jars_b115f6a6ec23f111a4512b562b52d9f8a52ec41c.jar.jar:0.0.1-SNAPSHOT]; 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 		at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 		at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 		at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 		at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; 	Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""errors"": [; {; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist)."",; ""domain"": ""global"",; ""reason"": ""forbidde",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:25428,concurren,concurrent,25428,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"7,.,2.144,8.001;AS_MQ=55.75,.,38.98,40.20;AS_MQRankSum=0.200,.,-1.050,-0.500;AS_QD=0.50,0.00,0.25,0.52;AS_ReadPosRankSum=-0.200,.,0.500,-0.220;AS_SOR=2.300,.,1.600,3.000;BaseQRankSum=0.200;DP=600000;ExcessHet=0.0477;FS=0.900;MQ=55.02;MQRankSum=-0.553;QD=1.00;ReadPosRankSum=-0.162;SOR=0.792;VarDP=650	GT:AD:DP:GQ:PGT:PID:PL:PS:SB	0/0:.:21:30	0/0:.:300:20	0/0:.:30:72	0/0:.:31:98	0|1:29,3,0,0,0:33:78:0|1:113_GG_G:78,0,1100,140,1400,1200,172,1600,1200,1000,175,1100,1100,1300,1000:113:19,19,2,1	0/0:.:20:19	0/0:.:19:20	0/0:.:25:50		0|1:90,2,0,0,0:30:40:0|1:113_GG_G:40,0,600,70,650,600,90,640,900,300,60,800,400,900,900:113:2,14,2,0	0/0:.:20:10	0/0:.:9:20	0/0:.:30:40	0/0:.:37:38		0/4:5,0,0,0,1:5:33:.:.:30,40,400,50,220,220,38,270,270,270,0,200,200,200,202:.:5,0,0,1	. 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:22); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:22); 	at is.hail.utils.package$.fatal(package.scala:81); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7(LoadVCF.scala:1921); 	at is.hail.io.vcf.MatrixVCFReader.$anonfun$executeGeneric$7$adapted(LoadVCF.scala:1909); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at __C678stream_Let.apply(Emit.scala); 	at is.hail.expr.ir.CompileIterator$$anon$2.step(Compile.scala:302); 	at is.hail.expr.ir.CompileIterator$LongIteratorWrapper.hasNext(Compile.scala:155); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2(RVD.scala:1030); 	at is.hail.rvd.RVD$.$anonfun$getKeyInfo$2$adapted(RVD.scala:1029); 	at is.hail.sparkextras.ContextRDD.$anonfun$crunJobWithIndex$1(ContextRDD.scala:242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102:7486,Load,LoadVCF,7486,https://hail.is,https://github.com/hail-is/hail/issues/14102,2,['Load'],['LoadVCF']
Performance,"7/python/pyspark/java_gateway.py in launch_gateway(conf); 75 def preexec_func():; 76 signal.signal(signal.SIGINT, signal.SIG_IGN); ---> 77 proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); 78 else:; 79 # preexec_fn not supported on Windows. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags); 388 p2cread, p2cwrite,; 389 c2pread, c2pwrite,; --> 390 errread, errwrite); 391 except Exception:; 392 # Preserve original exception in case os.close raises. /scratch/PI/dpwall/computeEnvironments/miniconda2/lib/python2.7/subprocess.pyc in _execute_child(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite); 1022 raise; 1023 child_exception = pickle.loads(data); -> 1024 raise child_exception; 1025; 1026. OSError: [Errno 2] No such file or directory. and the second error we would get would be. ------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) <ipython-input-6-93fa734a63bb> in <module>() ----> 1 hc_nate = HailContext() /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir) 60 self._jhc = scala_object(self._hail, 'HailContext').apply( 61 jsc, appName, joption(master), local, log, quiet, append, ---> 62 parquet_compression, min_block_size, branching_factor, tmp_dir) 63 64 self._jsc = self._jhc.sc() /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_clie",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:3058,load,loads,3058,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['load'],['loads']
Performance,"701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing pro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053:3652,concurren,concurrent,3652,https://hail.is,https://github.com/hail-is/hail/issues/3053,1,['concurren'],['concurrent']
Performance,"79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2407:4961,concurren,concurrent,4961,https://hail.is,https://github.com/hail-is/hail/issues/2407,1,['concurren'],['concurrent']
Performance,"790a4d87-9035-41ae-afc6-326f710d9a89; 2023-09-24 01:58:51.513 : INFO: TaskReport: stage=0, partition=9571, attempt=0, peakBytes=4507648, peakBytesReadable=4.30 MiB, chunks requested=51, cache hits=0; 2023-09-24 01:58:51.513 : INFO: RegionPool: FREE: 4.3M allocated (2.2M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guplo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4391,concurren,concurrent,4391,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,"7e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (Let g; (ArrayRef; (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-08 18:19:48 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (CastTableToMatrix `the entries! [877f12a8827e18f61222c6c8c5fb04a8]` __cols (s); (TableMapRows; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixMapRows; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/Users/dking/projects/hail-data/caitlin/ukb_imp_chr22_v3.bgen\""],\""indexFileMap\"":{},\""blockSizeInMB\"":128}""); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va)))))); (InsertFields; (Ref row); (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap i; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref row)); (Ref i))))))); (MakeStruct; (locus; (GetField locus; (Ref va))); (alleles; (GetField alleles; (Ref va))))))); ```; ### What went wrong (all error messages here, including the full java stack trace):; This wasn't optimized to a read of the metadata.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5100:2542,optimiz,optimized,2542,https://hail.is,https://github.com/hail-is/hail/issues/5100,1,['optimiz'],['optimized']
Performance,"8469</a> Modifying a child element in a tab causes the whole tab to rerender</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8531"">#8531</a> [component: bokehjs] Save tool in gridplot initiates multiple downloads</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/8684"">#8684</a> Allow at least partial alignment of fixed sized frames</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9113"">#9113</a> [component: bokehjs] Empty group widgets don't size properly once populated</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9133"">#9133</a> [BUG] Tabs ignore explicitly set dimensions</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9208"">#9208</a> [component: bokehjs] [BUG] sizing_mode='stretch_width' makes plot too wide if scrollbar is showing</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9320"">#9320</a> [BUG] Bokeh rendering performance</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9448"">#9448</a> [component: bokehjs] [BUG] Google Fonts not loading on Glyph on standalone HTML until interacting with Glyph</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9744"">#9744</a> [component: bokehjs] [BUG] bokeh server layout overlap on toggle visibility</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9763"">#9763</a> [BUG] <code>gridplot</code> <code>merge_tools</code> removes distinct tools it thinks are repeated, e.g., <code>xpan</code> and <code>ypan</code></li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9764"">#9764</a> [component: bokehjs] [BUG] MultiChoice placeholder text not displayed</li>; <li><a href=""https://github-redirect.dependabot.com/bokeh/bokeh/issues/9992"">#9992</a> [component: bokehjs] [BUG] Select widget hiding tabs, when selecting a plot</li>; <li><a href=""https://github-redirect.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12454:3412,perform,performance,3412,https://hail.is,https://github.com/hail-is/hail/pull/12454,1,['perform'],['performance']
Performance,853.vcf.bgz \ splitmulti \; write -o TOPMed.6998.chr22.vds`. `[Stage 0:====================================================> (52 + 4) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669:1230,Load,LoadVCF,1230,https://hail.is,https://github.com/hail-is/hail/issues/669,1,['Load'],['LoadVCF']
Performance,87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13306,Load,LoadMatrix,13306,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"88 Any; 189 """"""; --> 190 return eval_timed(expression)[0]; 191 ; 192 . <decorator-gen-714> in eval_timed(expression). ~/projects/hail/hail/python/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 612 def wrapper(__original_func, *args, **kwargs):; 613 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 614 return __original_func(*args_, **kwargs_); 615 ; 616 return wrapper. ~/projects/hail/hail/python/hail/expr/expressions/expression_utils.py in eval_timed(expression); 154 if ir_type != expression.dtype:; 155 raise ExpressionException(f'Expression type and IR type differed: \n{ir_type}\n vs \n{expression_type}'); --> 156 return Env.backend().execute(expression._ir, True); 157 else:; 158 uid = Env.get_uid(). ~/projects/hail/hail/python/hail/backend/spark_backend.py in execute(self, ir, timed); 294 jir = self._to_java_value_ir(ir); 295 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); --> 296 result = json.loads(self._jhc.backend().executeJSON(jir)); 297 value = ir.typ._from_json(result['value']); 298 timings = result['timings']. ~/miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/projects/hail/hail/python/hail/backend/spark_backend.py in deco(*args, **kwargs); 39 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 40 'Hail version: %s\n'; ---> 41 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 42 except pyspark.sql.utils.CapturedException as e:; 43 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: Index 5 is out of bounds for axis 0 with size 2. Java stack trace:; is.hail.utils.HailException: Index 5 is out of bounds for axis 0 with size 2; 	at __C889Compiled.apply(Unknown Source); 	at is.hail.expr.ir.Com",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9226:1772,load,loads,1772,https://hail.is,https://github.com/hail-is/hail/issues/9226,1,['load'],['loads']
Performance,9); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:247); at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1275:3523,concurren,concurrent,3523,https://hail.is,https://github.com/hail-is/hail/issues/1275,1,['concurren'],['concurrent']
Performance,95); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	... 30 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803); 	at sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:5745,Load,LoadMatrix,5745,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1.applyOrElse(HeartbeatReceiver.scala:119); at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:105); at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205); at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101); at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:216); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); java.lang.OutOfMemoryError: GC overhead limit exceeded; at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:170); at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:3386,concurren,concurrent,3386,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['concurren'],['concurrent']
Performance,"9:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1884,Load,LoadPlink,1884,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,": 'E'}` for machine types in Azure. This corresponds to 2Gi/core, 4Gi/core, and 8Gi/core.; - Spot price is set to -1 for now until we figure out a better billing strategy; - We look for existing network security groups to tell if a VM has been fully cleaned up already in the garbage collection loop. # To-Do:. ## Services. - Use global config and make an `AzureConfig` (@daniel-goldstein not sure if you're already doing this) instead of optional environment variables; - Azure user disks are not implemented; There's a maximum number of disks that can be mounted per machine type with a maximum of 32 along with figuring out the API calls. We'll need a semaphore of some sort.; - No activity logs loop. Not necessary for initial development and preemption billing is not working how intended anyways (will add to the list to fix!). We also don't track vm creation success rates per zone like we do with GCP. It might be good to look for VM deletion events to remove instances that are no longer present and then do a deep delete as then we'll have some redundancy and faster response times.; - Figure out how to do a deep-delete as much as possible for VMs when using the create VM REST API. This is essential for cleaning up resources for idled out workers when the driver is down for a long period of time.; - User billing based on resources used based on the `AzureInstanceConfig`; - Spot billing strategy; - Check network IP settings in the worker; - Add garbage collection CLI commands to build.yaml to clean up VMs, disks, nics, public ip addresses, and network security groups based on a tag; - Fix batch tests to be cloud agnostic. ## Infrastructure. - Create a shared SSH public key on the VMs for development purposes; - Consider having every PR / namespace deploy resources in a separate resource group rather than having one resource group for all Batch resources! We'd need something to name the resource groups something like `hail-dev_jigold` and `jigold_jigold` for example to handle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10970:2100,response time,response times,2100,https://hail.is,https://github.com/hail-is/hail/pull/10970,1,['response time'],['response times']
Performance,": Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4810 bytes); 2018-10-09 15:04:36 Executor: INFO: Running task 0.0 in stage 0.0 (TID 0); 2018-10-09 15:04:36 Executor: INFO: Fetching spark://10.32.119.167:61887/jars/sparklyr-2.2-2.11.jar with timestamp 1539122673186; 2018-10-09 15:04:36 TransportClientFactory: INFO: Successfully created connection to /10.32.119.167:61887 after 11 ms (0 ms spent in bootstraps); 2018-10-09 15:04:36 Utils: INFO: Fetching spark://10.32.119.167:61887/jars/sparklyr-2.2-2.11.jar to /private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-4d23a45e-e197-4f14-ac11-3973337df8a3/userFiles-33b96853-73f9-423a-ac6a-bcdb9106012a/fetchFileTemp414690014855588879.tmp; 2018-10-09 15:04:36 Executor: INFO: Adding file:/private/var/folders/w4/9k0my8pd6113d61pq05fvqlr0000gn/T/spark-4d23a45e-e197-4f14-ac11-3973337df8a3/userFiles-33b96853-73f9-423a-ac6a-bcdb9106012a/sparklyr-2.2-2.11.jar to class loader; 2018-10-09 15:04:36 CodeGenerator: INFO: Code generated in 140.241861 ms; 2018-10-09 15:04:36 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 1015 bytes result sent to driver; 2018-10-09 15:04:36 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 395 ms on localhost (executor driver) (1/1); 2018-10-09 15:04:36 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 2018-10-09 15:04:36 DAGScheduler: INFO: ResultStage 0 (collect at utils.scala:44) finished in 0.412 s; 2018-10-09 15:04:36 DAGScheduler: INFO: Job 0 finished: collect at utils.scala:44, took 0.679005 s; 2018-10-09 15:04:36 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: table8508c46074; 2018-10-09 15:04:36 SparkSession$Builder: WARN: Using an existing SparkSession; some configuration may not take effect.; 2018-10-09 15:04:36 SparkSqlParser: INFO: Parsing command: CACHE TABLE `table",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:17272,load,loader,17272,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['load'],['loader']
Performance,": six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, numpy, Jinja2, ipython-genutils, google-resumable-media, google-cloud-core, google-auth-oauthlib, fsspec, decorator, aiohttp, traitlets, tqdm, tabulate, scipy, python-json-logger, pyspark, PyJWT, pygments, prompt-toolkit, pickleshare, pexpect, parsimonious, pandas, nest-asyncio, jedi, hurry.filesize, humanize, google-cloud-storage, gcsfs, dill, Deprecated, bokeh, backcall, asyncinit, aiohttp-session, ipython, hail; Successfully installed Deprecated-1.2.12 Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 PyYAML-5.4.1 aiohttp-3.7.4 aiohttp-session-2.7.0 async-timeout-3.0.1 asyncinit-0.2.4 attrs-20.3.0 backcall-0.2.0 bokeh-1.4.0 cachetools-4.2.1 certifi-2020.12.5 chardet-3.0.4 decorator-4.4.2 dill-0.3.3 fsspec-0.8.7 gcsfs-0.7.2 google-api-core-1.26.1 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-cloud-core-1.6.0 google-cloud-storage-1.25.0 google-resumable-media-0.5.1 googleapis-common-protos-1.53.0 hail-0.2.64 humanize-1.0.0 hurry.filesize-0.9 idna-2.8 ipython-7.21.0 ipython-genutils-0.2.0 jedi-0.18.0 multidict-5.1.0 nest-asyncio-1.5.1 numpy-1.20.1 oauthlib-3.1.0 packaging-20.9 pandas-1.1.4 parsimonious-0.8.1 parso-0.8.1 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 prompt-toolkit-3.0.17 protobuf-3.15.6 ptyprocess-0.7.0 py4j-0.10.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pygments-2.8.1 pyparsing-2.4.7 pyspark-2.4.1 python-dateutil-2.8.1 python-json-logger-0.1.11 pytz-2021.1 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.7.2 scipy-1.6.1 six-1.15.0 tabulate-0.8.3 tornado-6.1 tqdm-4.42.1 traitlets-5.0.5 typing-extensions-3.7.4.3 urllib3-1.25.11 wcwidth-0.2.5 wrapt-1.12.1 yarl-1.6.3; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:8465,cache,cachetools-,8465,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cachetools-']
Performance,"://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/d15a5bacbb5ed54f1a474aede9a2c3cb9d8832fb""><code>d15a5ba</code></a> Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/489c4192dd9682a9d1e53f4c8f6f7bb826e33589""><code>489c419</code></a> Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/f461401e38fe95362a6d3c5afd8b592964b4bd29""><code>f461401</code></a> Silence AsciiLineReader warning when creating a FASTA sequence index (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1559"">#1559</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/1449dec45b4e95293db14595ec0d11a3839bac23""><code>1449dec</code></a> Support loading of CSI from URLs/streams. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1507"">#1507</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1595"">#1595</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/22aec6782b33f8d169a5d1cf63e952126a3f09e0""><code>22aec67</code></a> Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/70e42597ee8e2db6241f7b147f1356a1f8a846bc""><code>70e4259</code></a> Remove unnecessary println in test (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1602"">#1602</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/6507249a4422d021b984e710e8f031816f6d8da2""><code>6507249</code></a> Make the CRAM MD5 failure message more user friendly. (<a href=""https://github-redirect.dependab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12229:7755,load,loading,7755,https://hail.is,https://github.com/hail-is/hail/pull/12229,1,['load'],['loading']
Performance,"://github-redirect.dependabot.com/samtools/htsjdk/issues/1616"">#1616</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/d15a5bacbb5ed54f1a474aede9a2c3cb9d8832fb""><code>d15a5ba</code></a> Added ULTIMA and ELEMENT as valid value for RG-PL according to SAM spec. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1619"">#1619</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/489c4192dd9682a9d1e53f4c8f6f7bb826e33589""><code>489c419</code></a> Support CRAM reference regions. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1605"">#1605</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/f461401e38fe95362a6d3c5afd8b592964b4bd29""><code>f461401</code></a> Silence AsciiLineReader warning when creating a FASTA sequence index (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1559"">#1559</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/1449dec45b4e95293db14595ec0d11a3839bac23""><code>1449dec</code></a> Support loading of CSI from URLs/streams. <a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1507"">#1507</a> (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1595"">#1595</a>)</li>; <li><a href=""https://github.com/samtools/htsjdk/commit/22aec6782b33f8d169a5d1cf63e952126a3f09e0""><code>22aec67</code></a> Fix decoding of CRAM Scores read feature during normalization. (<a href=""https://github-redirect.dependabot.com/samtools/htsjdk/issues/1592"">#1592</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/samtools/htsjdk/compare/2.24.1...3.0.2"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.github.samtools:htsjdk&package-manager=gradle&previous-version=2.24.1&new-version=3.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#abo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12310:8103,load,loading,8103,https://hail.is,https://github.com/hail-is/hail/pull/12310,1,['load'],['loading']
Performance,:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at java.util.concurrent.AbstractExecutorService.doInvokeAny(AbstractExecutorService.java:193); 	at java.util.concurrent.AbstractExecutorService.invokeAny(AbstractExecutorService.java:215); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1282); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1229); 	at is.hail.io.fs.HadoopFS.listStatus(HadoopFS.scala:104); 	at is.hail.utils.Py4jUtils$class.ls(Py4jUtils.scala:55); 	at is.hail.utils.package$.ls(package.scala:77); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:3093,concurren,concurrent,3093,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['concurren'],['concurrent']
Performance,":1153, in Table.export(self, output, types_file, header, parallel, delimiter); 1150 hl.current_backend().validate_file(output); 1152 parallel = ir.ExportType.default(parallel); -> 1153 Env.backend().execute(; 1154 ir.TableWrite(self._tir, ir.TableTextWriter(output, types_file, header, parallel, delimiter))). File ~/.local/lib/python3.10/site-packages/hail/backend/backend.py:178, in Backend.execute(self, ir, timed); 176 payload = ExecutePayload(self._render_ir(ir), '{""name"":""StreamBufferSpec""}', timed); 177 try:; --> 178 result, timings = self._rpc(ActionTag.EXECUTE, payload); 179 except FatalError as e:; 180 raise e.maybe_user_error(ir) from None. File ~/.local/lib/python3.10/site-packages/hail/backend/py4j_backend.py:210, in Py4JBackend._rpc(self, action, payload); 208 path = action_routes[action]; 209 port = self._backend_server_port; --> 210 resp = self._requests_session.post(f'http://localhost:{port}{path}', data=data); 211 if resp.status_code >= 400:; 212 error_json = orjson.loads(resp.content). File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:635, in Session.post(self, url, data, json, **kwargs); 624 def post(self, url, data=None, json=None, **kwargs):; 625 r""""""Sends a POST request. Returns :class:`Response` object.; 626 ; 627 :param url: URL for the new :class:`Request` object.; (...); 632 :rtype: requests.Response; 633 """"""; --> 635 return self.request(""POST"", url, data=data, json=json, **kwargs). File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:587, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json); 582 send_kwargs = {; 583 ""timeout"": timeout,; 584 ""allow_redirects"": allow_redirects,; 585 }; 586 send_kwargs.update(settings); --> 587 resp = self.send(prep, **send_kwargs); 589 return resp. File /opt/conda/lib/python3.10/site-packages/requests/sessions.py:701, in Session.send(self, request, **kwargs); 698 start = preferred_cloc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960:11727,load,loads,11727,https://hail.is,https://github.com/hail-is/hail/issues/13960,1,['load'],['loads']
Performance,":323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13020,Load,LoadVCF,13020,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,":458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.115-71fc978b5c22; Error summary: SocketException: Connection reset. -------------------. Some more content from the failing worker job:. ...; 2023-05-04 01:04:35.959 : INFO: executing D-Array [shuffle_initial_write] with 1 tasks; 2023-05-04 01:04:35.960 : INFO: RegionPool: initialized for thread 8: pool-1-thread-1; 2023-05-04 01:04:35.965 GoogleStorageFS$: INFO: createNoCompression: gs://cpg-acute-care-hail/batch-tmp/tmp/hail/pV2Mgy4FVKSGKMwZGafyTh/hail_shuffle_temp_initial-ktRgTs8RfA9fHie5JKHmUy0e020450-e61c-4fa9-9419-2278528f3c86; 2023-05-04 01:04:37.559 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=132096, peakBytesReadabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12983:21656,concurren,concurrent,21656,https://hail.is,https://github.com/hail-is/hail/issues/12983,1,['concurren'],['concurrent']
Performance,:458); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:458); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:124); 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:458); 	at is.hail.backend.service.Main$.main(Main.scala:33); 	at is.hail.backend.service.Main.main(Main.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:464); 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:237); 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190); 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109); 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1400); 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1368); 	at sun.security.ssl.SSLSo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12982:13942,concurren,concurrent,13942,https://hail.is,https://github.com/hail-is/hail/issues/12982,3,['concurren'],['concurrent']
Performance,:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Opt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4488,Load,LoadVCF,4488,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,; 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:24); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$cl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:6183,Optimiz,Optimize,6183,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,; 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.fs.RouterFS.readNoCompression(RouterFS.scala:26); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:239); 	at is.hail.backend.service.ServiceBackend$$anon$4.call(ServiceBackend.scala:235); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750). com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; GET https://storage.googleapis.com/download/storage/v1/b/wes-bipolar-tmp-4day/o/bge-wave-1-VQSR%2FparallelizeAndComputeWithIndex%2FgCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=%2Fresult.2706?alt=media; No such object: wes-bipolar-tmp-4day/bge-wave-1-VQSR/parallelizeAndComputeWithIndex/gCyfD7XOt_MQrrCGc4Q-RrrWPb3cTAbhhcV28BCntiU=/result.2706; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonC,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:6693,concurren,concurrent,6693,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['concurren'],['concurrent']
Performance,; 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-74bf1eb; Error summary: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:14317,concurren,concurrent,14317,https://hail.is,https://github.com/hail-is/hail/issues/3760,2,['concurren'],['concurrent']
Performance,; 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806:2413,concurren,concurrent,2413,https://hail.is,https://github.com/hail-is/hail/issues/1806,1,['concurren'],['concurrent']
Performance,; 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:834). is.hail.utils.HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.variant.ReferenceGenome.checkLocus(ReferenceGenome.scala:210); 	at is.hail.variant.Locus$.apply(Locus.scala:18); 	at is.hail.variant.Locus$.annotation(Locus.scala:24); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3(LoadPlink.scala:43); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$3$adapted(LoadPlink.scala:37); 	at is.hail.utils.WithContext.foreach(Context.scala:49); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.ut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:5783,Load,LoadPlink,5783,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,; 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.FoldConstants$.is$hail$expr$ir$FoldConstants$$foldConstants(FoldConstants.scala:13); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:9); 	at is.hail.expr.ir.FoldConstants$$anonfun$apply$1.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:28); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scopedNewRegion$1.apply(ExecuteContext.scala:25); 	at is.hail.utils.package$.using(package.scala:602); 	at is.hail.annotations.Region$.scoped(Region.scala:18); 	at is.hail.expr.ir.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:25); 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:8); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1$$anonfun$apply$mcV$sp$1.apply(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$is$hail$expr$ir$Optimize$$runOpt$1$1.apply(Optimize.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.is$hail$expr$ir$Optimize$$runOpt$1(Optimize.scala:20); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply$mcV$sp(Optimize.scala:26); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.expr.ir.Optimize$$anonfun$apply$1.apply(Optimize.scala:24); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:23); 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:27); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(Lowerin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128:5353,Optimiz,Optimize,5353,https://hail.is,https://github.com/hail-is/hail/issues/9128,1,['Optimiz'],['Optimize']
Performance,; 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.RewriteBottomUp$.is$hail$expr$ir$RewriteBottomUp$$rewrite$1(RewriteBottomUp.scala:6); 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:25); 	at is.hail.expr.ir.ExtractIntervalFilters$.apply(ExtractIntervalFilters.scala:254); 	at is.hail.expr.ir.Optimize$.optimize(Optimize.scala:19); 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:49); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$$anonfun$optimizeIR$1$1.apply(CompileAndEvaluate.scala:20); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.optimizeIR$1(CompileAndEvaluate.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:24); 	at is.hail.backend.Backend.execute(Backend.scala:86); 	at is.hail.backend.Backend.executeJSON(Backend.scala:92); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2.16-e95038bbed35; Error summary: MatchError: locus<GRCh,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:5778,optimiz,optimizeIR,5778,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['optimiz'],['optimizeIR']
Performance,; 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3074:5162,concurren,concurrent,5162,https://hail.is,https://github.com/hail-is/hail/issues/3074,2,['concurren'],['concurrent']
Performance,; 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790:6130,concurren,concurrent,6130,https://hail.is,https://github.com/hail-is/hail/issues/3790,1,['concurren'],['concurrent']
Performance,"; -> 1895 force; 1896 ); 1897 return MatrixTable(jmt). ~/bin/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258 ; 1259 for temp_arg in temp_args:. ~/bin/anaconda3/lib/python3.6/site-packages/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:633); 	at is.hail.io.vcf.MatrixVCFReader.<init>(LoadVCF.scala:894); 	at is.hail.io.vcf.LoadVCF$.pyApply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF.pyApply(LoadVCF.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.2-a2eaf89baa0c; Error summary: HailException: arguments refer to no files; ```. Ba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775:2587,Load,LoadVCF,2587,https://hail.is,https://github.com/hail-is/hail/issues/4775,1,['Load'],['LoadVCF']
Performance,"; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pytest-dev/pytest-asyncio/blob/master/CHANGELOG.rst"">pytest-asyncio's changelog</a>.</em></p>; <blockquote>; <h1>0.20.1 (22-10-21)</h1>; <ul>; <li>Fixes an issue that warned about using an old version of pytest, even though the most recent version was installed. <code>[#430](https://github.com/pytest-dev/pytest-asyncio/issues/430) &lt;https://github.com/pytest-dev/pytest-asyncio/issues/430&gt;</code>_</li>; </ul>; <h1>0.20.0 (22-10-21)</h1>; <ul>; <li>BREAKING: Removed <em>legacy</em> mode. If you're upgrading from v0.19 and you haven't configured <code>asyncio_mode = legacy</code>, you can upgrade without taking any additional action. If you're upgrading from an earlier version or you have explicitly enabled <em>legacy</em> mode, you need to switch to <em>auto</em> or <em>strict</em> mode before upgrading to this version.</li>; <li>Deprecate use of pytest v6.</li>; <li>Fixed an issue which prevented fixture setup from being cached. <code>[#404](https://github.com/pytest-dev/pytest-asyncio/issues/404) &lt;https://github.com/pytest-dev/pytest-asyncio/pull/404&gt;</code>_</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/c8d017407d39dd81d6864fa9a58ba1240d54be9f""><code>c8d0174</code></a> fix: Do not warn about outdated pytest version when pytest&gt;=7 is installed. (...</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/6450ddbe974f5359d56317ba8bdda8b2ab48655a""><code>6450ddb</code></a> Prepare release of v0.20.0. (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest-asyncio/issues/428"">#428</a>)</li>; <li><a href=""https://github.com/pytest-dev/pytest-asyncio/commit/150f29c107fbd76641de47e040d43840769ef92c""><code>150f29c</code></a> Build(deps): Bump hypothesis in /dependencies/default (<a href=""https://github-redirect.dependabot.com/pytest-dev/pytest",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12390:3610,cache,cached,3610,https://hail.is,https://github.com/hail-is/hail/pull/12390,1,['cache'],['cached']
Performance,; Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4527,Load,LoadVCF,4527,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,; E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$3(LocalBackend.scala:150); E 	at is.hail.utils.package$.$anonfun$runAll$2(package.scala:1038); E 	at is.hail.CancellingExecutorService.$anonfun$newTaskFor$2(package.scala:1090); E 	at is.hail.CancellingExecutorService$CancellingTask.run(package.scala:1067); E 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515); E 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264); E 	at is.hail.relocated.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:327); E 	at is.hail.CancellingExecutorService.execute(package.scala:1111); E 	at java.base/java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:184); E 	at is.hail.utils.package$.$anonfun$runAll$1(package.scala:1038); E 	at scala.collection.Iterator.foreach(Iterator.scala:943); E 	at scala.collection.Iterator.foreach$(Iterator.scala:943); E 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431); E 	at scala.collection.IterableLike.foreach(IterableLike.scala:74); E 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73); E 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56); E 	at is.hail.utils.package$.runAll(package.scala:1038); E 	at is.hail.utils.package$.$anonfun$runAl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:4115,concurren,concurrent,4115,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['concurren'],['concurrent']
Performance,; at is.hail.variant.VariantDatasetFunctions.filterGenotypes(VariantDataset.scala:449); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2966:13657,load,loadClass,13657,https://hail.is,https://github.com/hail-is/hail/issues/2966,1,['load'],['loadClass']
Performance,"; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(Ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3361:13354,Load,LoadVCF,13354,https://hail.is,https://github.com/hail-is/hail/issues/3361,1,['Load'],['LoadVCF']
Performance,; at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.U,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:3904,concurren,concurrent,3904,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,; at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.Torren,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:10062,concurren,concurrent,10062,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,; at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.Ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342:12200,concurren,concurrent,12200,https://hail.is,https://github.com/hail-is/hail/issues/3342,1,['concurren'],['concurrent']
Performance,; batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/bunch/eOrFpVrN98GBIizi/specs.idx BlockBlob Hot 16 application/octet-stream 2023-06-09T12:43:34+00:00; ```. I looked at the status:. ```; az storage blob download --account-name haildevtest --container test --name batch/logs/we5a79QlczzdluUx8kT2Vh/batch/1148/2/31Owgv/status.json | jq '.' | less; ```. which contained an error (I un-escaped the string here):. ```; JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: is.hail.backend.service.EndOfInputException; 	at is.hail.backend.service.ServiceBackendSocketAPI2.read(ServiceBackend.scala:497); 	at is.hail.backend.service.ServiceBackendSocketAPI2.re,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13160:4291,concurren,concurrent,4291,https://hail.is,https://github.com/hail-is/hail/pull/13160,1,['concurren'],['concurrent']
Performance,"</p>; <ul>; <li>Support for Jinja2 version 3</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.6/"">https://pypi.org/project/nbsphinx/0.8.6/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.5...0.8.6"">https://github.com/spatialaudio/nbsphinx/compare/0.8.5...0.8.6</a></p>; <p>Version 0.8.5 -- 2021-05-12 -- PyPI__ -- diff__</p>; <ul>; <li>Freeze Jinja2 version to 2.11 (for now, until a bugfix is found)</li>; <li>Add <code>theme_comparison.py</code> tool for creating multiple versions; (with different HTML themes) of the docs at once</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.5/"">https://pypi.org/project/nbsphinx/0.8.5/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.4...0.8.5"">https://github.com/spatialaudio/nbsphinx/compare/0.8.4...0.8.5</a></p>; <p>Version 0.8.4 -- 2021-04-29 -- PyPI__ -- diff__</p>; <ul>; <li>Support for <code>mathjax3_config</code> (for Sphinx &gt;= 4)</li>; <li>Force loading MathJax on HTML pages generated from notebooks; (can be disabled with <code>nbsphinx_assume_equations = False</code>)</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.4/"">https://pypi.org/project/nbsphinx/0.8.4/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.3...0.8.4"">https://github.com/spatialaudio/nbsphinx/compare/0.8.3...0.8.4</a></p>; <p>Version 0.8.3 -- 2021-04-09 -- PyPI__ -- diff__</p>; <ul>; <li>Increase <code>line_length_limit</code> (for <code>docutils</code> 0.17+)</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.3/"">https://pypi.org/project/nbsphinx/0.8.3/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.2...0.8.3"">https://github.com/spatialaudio/nbsphinx/compare/0.8.2...0.8.3</a></p>; <p>Version 0.8.2 -- 2021-02-28 -- PyPI__ -- diff__</p>; <ul>; <li>Add support for <code>data-footcite</code> HTML attribute</li>; <li>Disable automatic highlighting in notebooks,; setting <code>highlight_language</code> is no",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11477:3687,load,loading,3687,https://hail.is,https://github.com/hail-is/hail/pull/11477,1,['load'],['loading']
Performance,"<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collectin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5301,cache,cached,5301,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"<a href=""https://pypi.org/project/nbsphinx/0.8.7/"">https://pypi.org/project/nbsphinx/0.8.7/</a></p>; <ul>; <li>Fix assertion error in LaTeX build with Sphinx 4.1.0+</li>; </ul>; <h2>nbsphinx 0.8.6</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.6/"">https://pypi.org/project/nbsphinx/0.8.6/</a></p>; <ul>; <li>Support for Jinja2 version 3</li>; </ul>; <h2>nbsphinx 0.8.5</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.5/"">https://pypi.org/project/nbsphinx/0.8.5/</a></p>; <ul>; <li>Freeze Jinja2 version to 2.11 (for now, until a bugfix is found)</li>; <li>Add <code>theme_comparison.py</code> tool for creating multiple versions (with different HTML themes) of the docs at once</li>; </ul>; <h2>nbsphinx 0.8.4</h2>; <p><a href=""https://pypi.org/project/nbsphinx/0.8.4/"">https://pypi.org/project/nbsphinx/0.8.4/</a></p>; <ul>; <li>Support for <code>mathjax3_config</code> (for Sphinx &gt;= 4)</li>; <li>Force loading MathJax on HTML pages generated from notebooks (can be disabled with <code>nbsphinx_assume_equations = False</code>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/spatialaudio/nbsphinx/blob/master/NEWS.rst"">nbsphinx's changelog</a>.</em></p>; <blockquote>; <p>Version 0.8.8 -- 2021-12-31 -- PyPI__ -- diff__</p>; <ul>; <li>Support for the <code>sphinx_codeautolink</code> extension</li>; <li>Basic support for the <code>text</code> builder</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.8/"">https://pypi.org/project/nbsphinx/0.8.8/</a>; __ <a href=""https://github.com/spatialaudio/nbsphinx/compare/0.8.7...0.8.8"">https://github.com/spatialaudio/nbsphinx/compare/0.8.7...0.8.8</a></p>; <p>Version 0.8.7 -- 2021-08-10 -- PyPI__ -- diff__</p>; <ul>; <li>Fix assertion error in LaTeX build with Sphinx 4.1.0+</li>; </ul>; <p>__ <a href=""https://pypi.org/project/nbsphinx/0.8.7/"">https://pypi.org/project/nbsphinx/0.8.7/</a>; __ <a href=""https://github.com/spatialaudio/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11477:1477,load,loading,1477,https://hail.is,https://github.com/hail-is/hail/pull/11477,1,['load'],['loading']
Performance,"<code>detect_all</code> return child prober confidences (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/210"">#210</a>)</li>; <li>Updated examples in docs (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/223"">#223</a>, <a href=""https://github.com/domdfcoding""><code>@​domdfcoding</code></a>)</li>; <li>Documentation fixes (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/212"">#212</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/224"">#224</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/225"">#225</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/226"">#226</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/220"">#220</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/221"">#221</a>, <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/244"">#244</a> from too many to mention)</li>; <li>Minor performance improvements (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/252"">#252</a>, <a href=""https://github.com/deedy5""><code>@​deedy5</code></a>)</li>; <li>Add support for Python 3.10 when testing (<a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/232"">#232</a>, <a href=""https://github.com/jdufresne""><code>@​jdufresne</code></a>)</li>; <li>Lots of little development cycle improvements, mostly thanks to <a href=""https://github.com/jdufresne""><code>@​jdufresne</code></a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/chardet/chardet/commit/ff5dcb25a59990e43683b8e9057f6f746bfb2658""><code>ff5dcb2</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/chardet/chardet/issues/254"">#254</a> from chardet/master</li>; <li><a href=""https://github.com/chardet/chardet/commit/322229573173307e1380eb151ea446b8c6fe2c3b""><code>3222295</code></a> Linte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12107:2456,perform,performance,2456,https://hail.is,https://github.com/hail-is/hail/pull/12107,1,['perform'],['performance']
Performance,"<code>wheel</code>), and specify their; own version of OpenSSL. For those users, the <code>CFLAGS</code>, <code>LDFLAGS</code>,; <code>INCLUDE</code>, <code>LIB</code>, and <code>CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS</code> environment; variables will no longer be respected. Instead, users will need to; configure their builds <code>as documented here</code>_.</li>; <li>Added support for; :ref:<code>disabling the legacy provider in OpenSSL 3.0.x&lt;legacy-provider&gt;</code>.</li>; <li>Added support for disabling RSA key validation checks when loading RSA; keys via; :func:<code>~cryptography.hazmat.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/d6951dca25de45abd52da51b608055371fbcde4e""><code>d6951dc</code></a> changelog + security fix backport (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8231"">#8231</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/138da90c8450446b19619e3faa77b9da54c34be3""><code>138da90</code></a> workaround scapy bug in downstream tests (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8218"">#8218</a>) (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8228"">#8228</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/69527bc79095c9646d7e839121f0783477892ecc""><code>69527bc</code></a> bookworm is py311 now (<a href=""https:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:3117,perform,performance,3117,https://hail.is,https://github.com/hail-is/hail/pull/12668,4,['perform'],['performance']
Performance,"<li>The project’s sdist now includes all needed files to run the test suite (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/49"">#49</a>, <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/pull/50"">#50</a>). Thanks to <a href=""https://github.com/jayvdb""><code>@​jayvdb</code></a>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/20d279d3cba11d64529ee88c5a2092c5c09919b6""><code>20d279d</code></a> Release v0.13.1</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/5bd2922633791277f3073135f779fee3e6684bb4""><code>5bd2922</code></a> Update <code>patch_click</code> to fix compatibility issue with click 8.1.0. Fix <a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/132"">#132</a> (#...</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/e16056828523d9af3e13b67243d62830ff03d89d""><code>e160568</code></a> chore(deps): update actions/cache action to v3</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/bce95021c33e9206104512c412751ee435a6606b""><code>bce9502</code></a> chore(deps): update dependency prettier to v2.6.1</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/c7ebbaedf5bc6a8c562a46941681d7bc8598497b""><code>c7ebbae</code></a> chore(deps): update dependency prettier to v2.6.0</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/efd710b42cf0982582bb8a4f345ccfa967866b97""><code>efd710b</code></a> chore(deps): update dependency coverage to v6.3.2</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/438cf6131c1784de8bc9b34970beace1ec7c52af""><code>438cf61</code></a> Update tested Python versions on GitHub (<a href=""https://github-redirect.dependabot.com/thibaudcolas/curlylint/issues/122"">#122</a>)</li>; <li><a href=""https://github.com/thibaudcolas/curlylint/commit/b64ec22effafffc6a1371e544c560e6bfc24b56e""><code>b64",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11713:7472,cache,cache,7472,https://hail.is,https://github.com/hail-is/hail/pull/11713,1,['cache'],['cache']
Performance,==============================================>(1049 + 1) / 1050]hail: info: Ordering unsorted dataset with network shuffle[A^[[A; [Stage 1:====================================================>(1043 + 7) / 1050]hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/673:1219,Load,LoadVCF,1219,https://hail.is,https://github.com/hail-is/hail/issues/673,1,['Load'],['LoadVCF']
Performance,==================================>(1049 + 1) / 1050]hail: info: Ordering unsorted dataset with network shuffle[A^[[A; [Stage 1:====================================================>(1043 + 7) / 1050]hail: importvcf: caught exception: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer; at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:143); at org.apache.spark.rdd.OrderedRDD$$anonfun$calculateKeyRanges$1.apply(OrderedRDD.scala:142); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.rdd.OrderedRDD$.calculateKeyRanges(OrderedRDD.scala:142); at org.apache.spark.rdd.OrderedRDD$.apply(OrderedRDD.scala:117); at org.broadinstitute.hail.RichPairRDD$.toOrderedRDD$extension(Utils.scala:482); at org.broadinstitute.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:267); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:85); at org.broadinstitute.hail.driver.ImportVCF$.run(ImportVCF.scala:31); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/673:1234,Load,LoadVCF,1234,https://hail.is,https://github.com/hail-is/hail/issues/673,1,['Load'],['LoadVCF']
Performance,"==========> (1 + 1) / 2]hail: info: running: vep --force --config /home/users/cseed/vep.properties; [Stage 1:======================================> (12 + 6) / 18]hail: vep: caught exception: Job aborted due to stage failure: Task 17 in stage 1.0 failed 4 times, most recent failure: Lost task 17.3 in stage 1.0 (TID 22, nid00019.urika.com): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:836); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:125); at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:113); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:127); at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:134); at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:512); at org.apache.spark.storage.BlockManager.getLocal(BlockManager.scala:429); at org.apache.spark.storage.BlockManager.get(BlockManager.scala:618); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44); at org.apache.spark.rdd.RDD.iterator(RDD.scala:262); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/430:1214,Cache,CacheManager,1214,https://hail.is,https://github.com/hail-is/hail/issues/430,4,"['Cache', 'concurren']","['CacheManager', 'concurrent']"
Performance,"> OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743:2197,load,loadLength,2197,https://hail.is,https://github.com/hail-is/hail/issues/2743,1,['load'],['loadLength']
Performance,"> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h2>3.9.11 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d2c06ce06c6e63981bf0276e4b7c74e5845e""><code>b0e4d2c</code></a> yyjson 0eca326, recursion limit</li>; <li><a href=""https://github.com/ijl/orjson/commit/5067eadc84cf516e4eb33bcb09ad756bb59dc42e""><code>5067ead</code></a> impl_esca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:2419,optimiz,optimization,2419,https://hail.is,https://github.com/hail-is/hail/pull/14357,3,['optimiz'],['optimization']
Performance,">; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/kjd/idna/blob/master/HISTORY.rst"">idna's changelog</a>.</em></p>; <blockquote>; <p>3.7 (2024-04-11); ++++++++++++++++</p>; <ul>; <li>Fix issue where specially crafted inputs to encode() could; take exceptionally long amount of time to process. [CVE-2024-3651]</li>; </ul>; <p>Thanks to Guido Vranken for reporting the issue.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/kjd/idna/commit/1d365e17e10d72d0b7876316fc7b9ca0eebdd38d""><code>1d365e1</code></a> Release v3.7</li>; <li><a href=""https://github.com/kjd/idna/commit/c1b3154939907fab67c5754346afaebe165ce8e6""><code>c1b3154</code></a> Merge pull request <a href=""https://redirect.github.com/kjd/idna/issues/172"">#172</a> from kjd/optimize-contextj</li>; <li><a href=""https://github.com/kjd/idna/commit/0394ec76ff022813e770ba1fd89658790ea35623""><code>0394ec7</code></a> Merge branch 'master' into optimize-contextj</li>; <li><a href=""https://github.com/kjd/idna/commit/cd58a23173d2b0a40b95ee680baf3e59e8d33966""><code>cd58a23</code></a> Merge pull request <a href=""https://redirect.github.com/kjd/idna/issues/152"">#152</a> from elliotwutingfeng/dev</li>; <li><a href=""https://github.com/kjd/idna/commit/5beb28b9dd77912c0dd656d8b0fdba3eb80222e7""><code>5beb28b</code></a> More efficient resolution of joiner contexts</li>; <li><a href=""https://github.com/kjd/idna/commit/1b121483ed04d9576a1291758f537e1318cddc8b""><code>1b12148</code></a> Update ossf/scorecard-action to v2.3.1</li>; <li><a href=""https://github.com/kjd/idna/commit/d516b874c3388047934938a500c7488d52c4e067""><code>d516b87</code></a> Update Github actions/checkout to v4</li>; <li><a href=""https://github.com/kjd/idna/commit/c095c75943413c75ebf8ac74179757031b7f80b7""><code>c095c75</code></a> Merge branch 'master' into dev</li>; <li><a href=""https://github.com/kjd/idna/commit/60a0a4cb61ec6834d74306bd8a1f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14464:1642,optimiz,optimize-contextj,1642,https://hail.is,https://github.com/hail-is/hail/pull/14464,7,['optimiz'],['optimize-contextj']
Performance,">; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13 - 2024-02-03</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12 - 2024-01-18</h2>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/ijl/orjson/commit/11c7de8e5846fa65449aa1f6ffc05c5a1090df03""><code>11c7de8</code></a> 3.10.0</li>; <li><a href=""https://github.com/ijl/orjson/commit/1fc3ed80c24864607be709d29e0d5f47fc507626""><code>1fc3ed8</code></a> Support numpy.float16</li>; <li><a href=""https://github.com/ijl/orjson/commit/56c1a03216426c54dfbe9a4b6c3f70013c65a1f8""><code>56c1a03</code></a> cargo update, build misc</li>; <li><a href=""https://github.com/ijl/orjson/commit/a348f59f0b55d92a1364523560f52f5b3cf9c12a""><code>a348f59</code></a> 3.9.15</li>; <li><a href=""https://github.com/ijl/orjson/commit/b0e4d",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:2874,optimiz,optimization,2874,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['optimiz'],['optimization']
Performance,">; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/da6d0d034822f66966e4a84a3a1e2f37cc83e3b0""><code>da6d0d0</code></a> Remove unneeded &quot;update order&quot; consistency test</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/e85d8659733cb3e28d539a28db0fdd71672ab2e4""><code>e85d865</code></a> Simplify &quot;update order&quot; consistency test</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/7dc426c95a0c329d5514e6198d92080f1ffc1e5e""><code>7dc426c</code></a> Fix update() ordering to be more consistent with add() ordering (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/159"">#159</a>)</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/13d30bc654eb9e6be092282ca502967fcb7f0113""><code>13d30bc</code></a> Bump version to 2.2.2</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/4997d0e849f2275d1931772a5432163ecc20e0b0""><code>4997d0e</code></a> Refactor small slice optimization in SortedList.<strong>getitem</strong></li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/6ee5d57fc8d691fbab4972b853a60348d0f922ef""><code>6ee5d57</code></a> improve SortedList.<strong>getitem</strong>() performance for small slices</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/ac80254fb6a08045ced7d9704412878ff8000fa7""><code>ac80254</code></a> suppress warning in test of deprecated function (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/118"">#118</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/grantjenks/python-sortedcontainers/compare/v2.1.0...v2.4.0"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=sortedcontainers&package-manager=pip&previous-version=2.1.0&new-version=2.4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11476:3017,optimiz,optimization,3017,https://hail.is,https://github.com/hail-is/hail/pull/11476,1,['optimiz'],['optimization']
Performance,">; <li>Added PyPy 3.10 and removed PyPy 3.8 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7216"">#7216</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Added in_place argument to ImageOps.exif_transpose() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7092"">#7092</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Corrected error code <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7177"">#7177</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Use &quot;not in&quot; <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7174"">#7174</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Only call text_layout once in getmask2 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7206"">#7206</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed calling putpalette() on L and LA images before load() <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7187"">#7187</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Removed unused INT64 definition <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7180"">#7180</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Updated xz to 5.4.3 <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7136"">#7136</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Fixed saving TIFF multiframe images with LONG8 tag types <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7078"">#7078</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]</li>; <li>Do not set size unnecessarily if image fails to open <a href=""https://redirect.github.com/python-pillow/Pillow/issues/7056"">#7056</a> [<a href=""https://github.com/radarhere""><code>@​radarhere</code></a>]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13321:4724,load,load,4724,https://hail.is,https://github.com/hail-is/hail/pull/13321,1,['load'],['load']
Performance,">; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/0f43ce67de72bd511d849c07bd7728c0d6f2e6dd""><code>0f43ce6</code></a> Document path and relativePath properties</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a8504f9d60d0264808894e4bb80d4a73b8086a3e""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12345:2448,cache,cached,2448,https://hail.is,https://github.com/hail-is/hail/pull/12345,1,['cache'],['cached']
Performance,">; <ul>; <li>Update dependencies</li>; </ul>; <h2>5.2.0</h2>; <p>New features:</p>; <ul>; <li>Add <code>eachFile</code> method that adds an action to be applied to each source URL before it is downloaded. The action can be used to modify the filename of the target file.</li>; <li>Add <code>runAsync</code> method to download extension. This allows multiple files to be downloaded in parallel if the download extension is used. For normal download tasks, multiple files were downloaded in parallel already.</li>; </ul>; <h2>5.1.3</h2>; <p>Bug fixes:</p>; <ul>; <li>Initialize progress logger just before the download starts (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/243"">#243</a>)</li>; </ul>; <h2>5.1.2</h2>; <p>Bug fixes:</p>; <ul>; <li>Do not include default HTTP and HTTPS ports in <code>Host</code> header unless explicitly specified by the user</li>; </ul>; <h2>5.1.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Correctly update cached sources</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.5 and 7.5.1</li>; <li>Update dependencies</li>; </ul>; <h2>5.1.0</h2>; <p>New features:</p>; <ul>; <li>Add possibility to enable preemptive Basic authentication (through the new <code>preemptiveAuth</code> flag)</li>; <li>Warn if server does not send <code>WWW-Authenticate</code> header in 401 response</li>; <li>Log request and response headers in debug mode</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Add integration tests for Gradle 7.4.1 and 7.4.2</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.5</h2>; <p>Maintenance:</p>; <ul>; <li>Publish signed artifacts to Gradle plugin portal</li>; <li>Update dependencies</li>; </ul>; <h2>5.0.4</h2>; <p>Bug fixes:</p>; <ul>; <li>Fix deadlock in <code>DownloadExtension</code> if <code>max-workers</code> equals 1 (thanks to <a href=""https://github.com/beatbrot""><code>@​beatbrot</code></a> for spotting this, see <a href=""https://github-redirect.dependabot.com/michel-kraeme",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12332:1729,cache,cached,1729,https://hail.is,https://github.com/hail-is/hail/pull/12332,1,['cache'],['cached']
Performance,"><code>@​akayunov</code></a></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jmoiron/humanize/commit/a1514eb521c2befe40274674d61aba4f0fbf6137""><code>a1514eb</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/239"">#239</a> from hugovk/rm-3.6</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/48506d434fd315a976bbdc058a791b80086f7e7e""><code>48506d4</code></a> pre-commit autoupdate</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/8f2c8551e5e20cc6cc3bcaa241fa2c1760d07926""><code>8f2c855</code></a> Remove unused import</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/04bf8872908178b3d7d9fb4b316da8ce72916209""><code>04bf887</code></a> Drop support for Python 3.6</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/0f2ff42cbe632c47ddb6ac255c61890ab8a46fd4""><code>0f2ff42</code></a> Use actions/setup-python's pip cache and update other CI config</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/464de5965692765d29d1c3cfde1f87c4ceece440""><code>464de59</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/253"">#253</a> from hugovk/rm-VERSION</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/66b8a6322fbda9bffb2882500c6a9b6c96271401""><code>66b8a63</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/250"">#250</a> from carterbox/no-overflow-naturaldelta</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/e89c8c8e325ccb2b3ee78ef507e9d6805c47a175""><code>e89c8c8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jmoiron/humanize/issues/241"">#241</a> from samueljsb/remove-deprecated-private-function-ali...</li>; <li><a href=""https://github.com/jmoiron/humanize/commit/ffe4bcfaa6cfbd95ba47315f8f71a206485af6ae""><code>ffe4bc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11517:5341,cache,cache,5341,https://hail.is,https://github.com/hail-is/hail/pull/11517,2,['cache'],['cache']
Performance,"><em>Sourced from <a href=""https://github.com/boto/boto3/blob/develop/CHANGELOG.rst"">boto3's changelog</a>.</em></p>; <blockquote>; <h1>1.26.15</h1>; <ul>; <li>bugfix:Endpoints: [<code>botocore</code>] Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botocore</code>] Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/late",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12498:1149,latency,latency,1149,https://hail.is,https://github.com/hail-is/hail/pull/12498,1,['latency'],['latency']
Performance,">= 354 partitions using an external spark cluster (i.e. specifying `master` in `hail.init`), the spark worker crashes with a SIGSEGV. The issue does not occur with `variant_qc` but we do not know know the extent of what specific operations trigger it. Below is a test that consistently triggers the issue:. Setup:. $ $SPARK_HOME/sbin/start-master.sh --host localhost --port 7077; $ $SPARK_HOME/sbin/start-shuffle-service.sh; $ $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077 --work-dir /scratch/local/. Test:. import hail; hail.init(master=""spark://localhost:7077""); P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = hail.sample_qc(mt); mt = mt.filter_cols(mt.sample_qc.n_hom_var > V*0.32); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception:; print(""\n[FAIL] with "", N, ""partitions""); break. Test Output (SIGSEGV is reported in Spark worker logs, see end):. 2020-06-10 10:29:56 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 	Setting default log level to ""WARN"".; 	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 	Running on Apache Spark version 2.4.5; 	SparkUI available at http://US0HPN0036.cm.cluster:4047; 	Welcome to; 		 __ __ <>__; 		/ /_/ /__ __/ /; 	 / __ / _ `/ / /; 	 /_/ /_/\_,_/_/_/ version 0.2.44-6cfa355a1954; 	LOGGING: writing to /bmrn/apps/bmrn-hugelib/0.3.0/test/hail-20200610-1029-0.2.44-6cfa355a1954.log; 	2020-06-10 10:29:59 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 1:==========================> (171 + 80) / 350]; 	[PASS] with 350 partitions: (50000, 984); 	2020-06-10 10:30:08 Hail: INFO: balding_nichols_model: generating genotypes for 1 populations, 1000 samples, and 50000 variants...; 	[Stage 3:==========================> (169 + 80) / 351]; 	[PASS] with 351 partitions: (500",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944:1219,load,load,1219,https://hail.is,https://github.com/hail-is/hail/issues/8944,1,['load'],['load']
Performance,">=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5476,cache,cached,5476,https://hail.is,https://github.com/hail-is/hail/issues/10197,2,['cache'],"['cached', 'cachetools-']"
Performance,">Pandas 1.5.0rc0</h2>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pandas-dev/pandas/commit/8dab54d6573f7186ff0c3b6364d5e4dd635ff3e7""><code>8dab54d</code></a> RLS: 1.5.2</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/d78c5e624936ea5bc30568fd7d6fc9b5f42d0beb""><code>d78c5e6</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49806"">#49806</a> on branch 1.5.x (DOC: Update what's new notes for 1.5.2 re...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/98c6139ff12107b9aa34441d25ef1593b6a0adca""><code>98c6139</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49579"">#49579</a> on Branch 1.5.x (BUG: Behaviour change in 1.5.0 when using...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/9196f8d545d1118f1233c1b45e7b740cb95c370c""><code>9196f8d</code></a> Backport PR STYLE enable pylint: method-cache-max-size-none (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49784"">#49784</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/8c4b559c87561ca68ccdc3e81ff3c5218c7b4db7""><code>8c4b559</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49776"">#49776</a> on branch 1.5.x (REGR: arithmetic ops recursion error with...</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/1616fb3d2c00905a5f3af510db893206ae00ea09""><code>1616fb3</code></a> Backport PR Revert &quot;Add color and size to arguments (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/44856"">#44856</a>)&quot; (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/49752"">#49752</a>)</li>; <li><a href=""https://github.com/pandas-dev/pandas/commit/6f8e1745472c9d107367da1e38494425c3938234""><code>6f8e174</code></a> Backport PR <a href=""https://github-redire",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12564:3490,cache,cache-max-size-none,3490,https://hail.is,https://github.com/hail-is/hail/pull/12564,1,['cache'],['cache-max-size-none']
Performance,"@cseed I've implemented `readPartitions` on `HailContext` and `writePartitions` on `RichRDD`, re-implemented read/write of blocks on `BlockMatrix` to preserve partitioning using these, and moved MatrixValue and KeyTable row read/write to use these. readPartitions and writePartitions use `unsafeReadPartition` and `writePartition` in RichHadoopConfig. `readRow` includes `close()`, and I've added `close()` to `readBlock` as well. At first I used `using` (see `readPartition` which is remarked out), but this closes the resource before the iterator iterated in the readRows case. Yet somehow, `testWriteRead` in VSMSuite passes even with `readPartition`. This test just creates a random VDS, writes, reads, compares. I added `testWriteRead2` which imports `sample.vcf`, writes, reads, compares. And indeed, the latter fails with `readPartition`. I have no explanation for why the behavior would differ (could the former be somehow cached?). I'm not happy with the asymmetry or lake of read safety if closing isn't propagated backward (I think it is for DataInputStream), but would like to get your feedback before messing around with these functions further. I've left the filename that had ReadRowsRDD with that name so it's easier to compare, but will change prior to merging since that class is gone. No class jumps out, should I just use the first one, `ArrayInputStream`, or a name that's not a class?. `ReadRDDPartition` in file ReadRowsRDDs and `IntPartition` in `BlockMatrix` have the same definition (the latter is used for more than just reading). I'm thinking of just having IntPartition located somewhere other than BlockMatrix. Any thoughts on where?. I added `sqrt` on blockSize in `blockMatrixGen` in order to get more cases with `blockSize < min(rows, cols)`. Only 1 of 10 had this property in `readWriteIdentityRandom` and its an important case to check with respect to the GridPartitioner dealing with block indices corrected (particularly with transpose in the mix).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2367:931,cache,cached,931,https://hail.is,https://github.com/hail-is/hail/pull/2367,1,['cache'],['cached']
Performance,"@danking, I started playing with your ASM experiment and wrote a library for lightweight bytecode generation. The primary abstractions are `FunctionBuilder` and `Code[T]`. The latter is an object that can generate bytecode to produce a value of type `T` on the top of the stack. I'm reasonably happy with the interface, see this example for factorial:. https://github.com/cseed/hail/commit/93d95982bccd16ffa531f67fa47163f3fc8cbdde#diff-e434fa9004c38142a8f6f64ffa73b48eR109. No ClassBuilder yet. Apart from that, all the major features are there. There are a bunch of missing operations (type conversions, for example) and I only have wrapper classes for `Int` and `Double`. Once we fill it out I think it will make an excellent stand-alone library. While I optimized conditional generation to be smart about converting between indicator values (0, 1) and branch targets, it still emits some unnecessary GOTOs for fall through and could be improved. There are two double comparison bytecodes (DCMPG and DCMPL) that treat NAN differently. I wasn't sure which one to use. We should probably emulate Java/Scala. I can't tell if ASM is generating short bytecodes for load from small local indices (e.g. ILOAD_2) or small constants (e.g., ICONST_3). It isn't clear if the pretty printer that comes with ASM makes a distinction. We probably need to dump to a file and run `javap`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/921:757,optimiz,optimized,757,https://hail.is,https://github.com/hail-is/hail/pull/921,2,"['load', 'optimiz']","['load', 'optimized']"
Performance,"@huy-nguyen is getting a segfault on on current release (0.2.33-5d8cae649505):; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa4b25e18cd, pid=6637, tid=0x00007f9a4f1fc700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-1~deb9u1-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # J 8451 C2 is.hail.annotations.Region$.loadBit(JJ)Z (33 bytes) @ 0x00007fa4b25e18cd [0x00007fa4b25e18a0+0x2d]; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /tmp/cac7924b3c14494b9702ac2689c0c52d/hs_err_pid6637.log; ```; with this pipeline:; ```; def normalize_contig(input_contig: hl.expr.StringExpression) -> hl.expr.StringExpression:; return input_contig.replace(""^chr"", """"). def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; ​ mt = mt.choose_cols(list(range(10))); ​; x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue); ​; downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.N,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ); ​; downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240:473,load,loadBit,473,https://hail.is,https://github.com/hail-is/hail/issues/8240,1,['load'],['loadBit']
Performance,"@jigold I screwed up the stacked PRs so I'll need reapprovals. Sorry :(; ---; 1. Separating the `apk update` from the `apk add` means that; if the apk package repository metadata changes (say, the; URL of some repository changes) and we change our `apk add`; line (say we add a new package), then the `apk add` will; fail (e.g. because it has an out of date URL for the; repository that should contain the new package). The; `apk add --no-cache ...` invocation is essentially the same; as `apk update && apk add ... && rm -rf /path/to/repo/cache`.; When using docker, it is good practice remove unnecessary; files so that they do not get included in the ""image diff""; for that line of the Dockerfile. `apk add --no-cache ...`; succinctly performs exactly what we want. 2. Keeping each package on a separate line and sorting those; lines makes diffs easy-to-read with one line per new package. 3. Because `COPY` moves all the *contents* of a source folder into; the contents of the destination path (creating it if it does not; exist), it seems more clear to say `/batch/batch/`, indicating; that we are moving data into a folder.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4828:439,cache,cache,439,https://hail.is,https://github.com/hail-is/hail/pull/4828,4,"['cache', 'perform']","['cache', 'performs']"
Performance,"@johnc1231 Had this idea for dealing with registry flakiness. I was hesitant at first because really I want buildkit to retry more transient errors, but maybe with its local cache it would be quick?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11666:174,cache,cache,174,https://hail.is,https://github.com/hail-is/hail/pull/11666,1,['cache'],['cache']
Performance,"@konradjk asked for weighted OLS, which is just a transformation of `x` and `y` by `sqrt(w)`. Currently `sqrt` is done `1 + len(x)` per record rather than once because you can't bind inside an aggregate. If that's a bottleneck, I could rework the aggregator to pass the w through to scala and avoid taking sqrt altogether. But for now this simple change at the Python level seems reasonable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4146:216,bottleneck,bottleneck,216,https://hail.is,https://github.com/hail-is/hail/pull/4146,1,['bottleneck'],['bottleneck']
Performance,"@tomwhite @jkeebler Added a parquet_genotypes flag to VDS.write:. ```; import hail; hc = hail.HailContext(); vds = hc.import_vcf('sample.vcf'); vds.count(); vds.write('sample.pq.vds', overwrite=True, parquet_genotypes=True); ```. Then in the Spark shell:. ```; scala> val df = spark.read.parquet(""sample.pq.vds/rdd.parquet""); scala> df.printSchema(); root; |-- variant: struct (nullable = true); [variant and annotation fields elided]; |-- gs: array (nullable = true); | |-- element: struct (containsNull = true); | | |-- gt: integer (nullable = true); | | |-- ad: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- dp: integer (nullable = true); | | |-- gq: integer (nullable = true); | | |-- px: array (nullable = true); | | | |-- element: integer (containsNull = true); | | |-- fakeRef: boolean (nullable = true); | | |-- isDosage: boolean (nullable = true); ```. I added correctness tests, but no performance testing on the Hail side yet. Note, the `px` field is the `PL` in the case of sequence data and 16-bit fixed-point dosages in the case of array data (See `Gentoype` for more details.) We know if we have dosage or not globally (`VariantMetadata.isDosage`), so I can customize the resulting schema in v2. Finally, I'm seeing `containsNull = true` here, but I set it to `containsNull = false` when I constructed the schema programatically. Spark/Parquet seem to be consistently ignoring my non-missing hints. Have you seen this before? Any idea why it is happening?. From `Genotype.schema`:. ```; def schema: DataType = StructType(Array(; StructField(""gt"", IntegerType),; StructField(""ad"", ArrayType(IntegerType, containsNull = false)),; StructField(""dp"", IntegerType),; StructField(""gq"", IntegerType),; StructField(""px"", ArrayType(IntegerType, containsNull = false)),; StructField(""fakeRef"", BooleanType, nullable = false),; StructField(""isDosage"", BooleanType, nullable = false))); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1421:933,perform,performance,933,https://hail.is,https://github.com/hail-is/hail/pull/1421,1,['perform'],['performance']
Performance,"@tpoterba , can you take a look as well?. Notes:. 1. Azure uses Spark 3.0.2, so I need to build and publish a wheel for Spark 3.0.2.; 2. Azure provides Jupyter Notebooks already.; 3. hail/Makefile (for manual deploys) was missing some changes for deploy.sh, so I updated it.; 4. Azure sets the `AZURE_SPARK` environment variable inside hosted Jupyter Notebooks. 5. In Azure's Jupyter, if you set `extraClassPath` you break the extant classpath (e.g. you cannot load Scala stdlib classes). However, the JARs specified in `spark.jars` are added to the classpath properly, so, in Azure, it suffices to specify `spark.jars`. 6. Azure lacks requester pays, so I require Azure users download, untar, and upload the VEP files to their own bucket. 7. Instead of ""submit"", Azure installs Livy, a Java job-queue system. I have no idea how to set environment variables in Livy and Azure does not set AZURE_SPARK in Livy jobs; therefore, I search for `hdinsight` in the CLASSPATH.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187:461,load,load,461,https://hail.is,https://github.com/hail-is/hail/pull/11187,2,"['load', 'queue']","['load', 'queue']"
Performance,"@tpoterba per your comments have largely left the PStruct varargs constructor in use in the PCanonicalStruct implementation. I think it would be simpler to just use the normal IndexedSeq constructor, and slightly more performant, and if you're interested in that could issue a separate PR. The only change in the implementation of PCanonicalStruct from the master version of PStruct is that I pass through requiredeness in all construction operations. Previously a few, like rename would not do this. Notably this only happened when they used the more complex varargs constructor, and seemed like a bug. The empty constructor was removed because it wasn't necessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7733:218,perform,performant,218,https://hail.is,https://github.com/hail-is/hail/pull/7733,1,['perform'],['performant']
Performance,"@tpoterba this builds on your recent PR https://github.com/hail-is/hail/pull/3882. partition counts are computed through Interpret on demand and memoized in the IR. fastPartitionCounts means get the partition counts if you have them (either because the IR know their partition counts, or they were previously computed by counting the RVD partitions). partitionCounts means compute (and memoize) if they aren't available the fast way. I think this is now optimal except that MatrixTable.count potentially runs things twice. We might be able to fix this with a MatrixLet in the case you're calling MatrixTable.count(). Next Table.index/MatrixTable.indexRows should use partitionCounts instead of zipWithIndex because computing the partition counts via the optimizer will potentially be much faster.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3891:754,optimiz,optimizer,754,https://hail.is,https://github.com/hail-is/hail/pull/3891,1,['optimiz'],['optimizer']
Performance,@tpoterba you were in here recently for performance so your eyes are appreciated. I simplified things a bit and localized almost all the parsing logic to `BgenRecord`. The contract for `advance` is that it is always called when `bfis` is pointing at the start of a record _or_ at or past the `end`. Advance will return the position to the start of a record or at or past the `end`. It returns true if there was a new record found. False otherwise. I avoided a couple allocating patterns. The rest of the diffs are copy pastes and some indentation changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3783:40,perform,performance,40,https://hail.is,https://github.com/hail-is/hail/pull/3783,1,['perform'],['performance']
Performance,"A [daemonset](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) which, given a list of images, constantly tries to keep those images cached on the nodes docker container. Because the preemptible nodes that execute CI builds are tainted, this will only execute on the main nodes and the non-preemptibles, which is the functionality we desire for now because this tool will be used to ensure `notebook` worker images are always cached. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4647:153,cache,cached,153,https://hail.is,https://github.com/hail-is/hail/pull/4647,2,['cache'],['cached']
Performance,"A lot of changes here. A summary:; - This subsumes notebook, so I deleted notebook and renamed notebook2 => notebook. Apologies, this makes the diff slightly harder to read.; - Added a simple messaging framework, stored in aiohttp session cookie, set message with `set_message`, handled by web_common by `base_context` by the default layout,; - Added notebook.hail.is/workshop-admin to manage and enable/disabled workshops. Workshops stored in the database.; - Workshop will be located at notebook.hail.is/workshop (I will move to workshop.hail.is as a later step); - Meta change: don't try to track dependencies on `make check` everywhere, it isn't really needed and it wasn't correct; - Rewrote code to monitor the spin up of notebooks: store notebook state in the database. I'm happy with how it turned out, it will be simpler and more reliable.; - I refactored the auth code to support the needs of workshops. I think it is also improved: simpler. Things left to do:; - ~~Port the load test code. And load test!~~; - The notebook link shouldn't be click-able if the notebook isn't ready. (Even better: If you click, launch the notebook when it is ready.); - ~~Didn't test the error case (when the notebook isn't actually available). This probably needs some work, and should get integrated into the message framework.~~; - The workshop header is a bit spare. Maybe add a slash (/) link. What would it link to?; - ~~Move notebook.hail.is/workshop to workshop.hail.is~~; - (low-prio) Finally, when the notebook state changes, we just refresh the page. Might be nice to just dynamically update HTML. Maybe react?; - (unrelated) The message framework should get used by the other services. @tpoterba I'm assigning this to you since you're point for the workshop. @akotlar knows this code if you want to re-assign. I gave you an account in my namespace, so you should be able to see/play with this at internal.hail.is/cseed/notebook. FYI @akotlar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112:985,load,load,985,https://hail.is,https://github.com/hail-is/hail/pull/7112,2,['load'],['load']
Performance,"A number of people have observed the same behavior, but the prometheus team closes it as not a bug. It appears to be related to suddenly adding a very large number of metrics. This started happening when I started the scale tests, which keeps the k8s system operating with about 2000 pods, working through 30k pods over time. It's possible all the added pod information is bringing down prometheus. It appears k8s is restarting prometheus between every ten and twenty minutes. It seems likely that prometheus is spending more than ten minutes to load its database. This is surprising given that the database is a mere 31 GB:. ```; Filesystem Size Used Available Use% Mounted on; overlay 94.3G 46.4G 47.9G 49% /; tmpfs 64.0M 0 64.0M 0% /dev; tmpfs 14.7G 0 14.7G 0% /sys/fs/cgroup; /dev/sdd 49.0G 31.2G 17.8G 64% /prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/prometheus; /dev/sda1 94.3G 46.4G 47.9G 49% /dev/termination-log; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/resolv.conf; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hostname; /dev/sda1 94.3G 46.4G 47.9G 49% /etc/hosts; shm 64.0M 0 64.0M 0% /dev/shm; tmpfs 14.7G 12.0K 14.7G 0% /var/run/secrets/kubernetes.io/serviceaccount; tmpfs 14.7G 0 14.7G 0% /proc/acpi; tmpfs 64.0M 0 64.0M 0% /proc/kcore; tmpfs 64.0M 0 64.0M 0% /proc/keys; tmpfs 64.0M 0 64.0M 0% /proc/timer_list; tmpfs 14.7G 0 14.7G 0% /proc/scsi; tmpfs 14.7G 0 14.7G 0% /sys/firmware; ```. Which isn't much larger than it was before the scaling tests. It appears to slowly increase the amount of memory it needs:; ```; 1 0 nobody S 30.9g103.7 1 11.5 /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --web.console.libraries=/usr/share/prometheus/console_libraries --web.console.templates=/usr/share/prometheus/consoles --web.external; ```. caping out at 31.5 GB (the disk is 31.2 GB). Now, it is presumably trying to recover. It's been up for about 7 minutes. Still unavailable:; ```; /prometheus $ wget localhost:9090/monitoring/prometheus; Connecti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773:546,load,load,546,https://hail.is,https://github.com/hail-is/hail/issues/6773,1,['load'],['load']
Performance,"A small PR for you, @cseed. 😀. If we are immediately looking at the rows or cols table of; a MatrixTable, then we need not load the entries in either case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3626:123,load,load,123,https://hail.is,https://github.com/hail-is/hail/pull/3626,1,['load'],['load']
Performance,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/542:407,load,load,407,https://hail.is,https://github.com/hail-is/hail/pull/542,3,['load'],['load']
Performance,"A summary of major changes:; - The genotype schema has changed from pl to px, where px is an Array[Int] that stores probabilifrom pl to px, where px is an Array[Int] that stores probabilities (phred or linear scaled). g.pl and g.dosage are used for accessing the PLs and/or dosages.; - The VariantMetadata includes information about whether the dataset is dosage data; - Can use indexbgen and importbgen to load BGEN files; - Can use importplink to load PLINK binary files; - Can use importgen to load GEN files and exportgen to export data in GEN format; - Reorganized the VCF import/export scripts to the io folder. Fix indentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/573:407,load,load,407,https://hail.is,https://github.com/hail-is/hail/pull/573,3,['load'],['load']
Performance,"A user reported this error `concurrent.futures._base.TimeoutError` with no stack trace while copying files in a batch job. There's a comment in `is_transient_error` that we should catch this error, but I did not see it caught in the existing function.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817:28,concurren,concurrent,28,https://hail.is,https://github.com/hail-is/hail/pull/11817,1,['concurren'],['concurrent']
Performance,"A very uninteresting PR but just to show you more bits of the codebase that are relevant to our earlier `python-dill` bug. We publish a small collection of images in DockerHub that users can use, like `python-dill` and a `hail` image that includes the whole hail pip package. You can use these like `j.image('hailgenetics/hail')`. However, DockerHub sets severe rate limits that would throttle a large batch from pulling those images on N workers for sufficiently large N. So, we mirror these images in our private image registry in GCP / Azure. If a user submits a job with one of these images, we instead pull from our own registry instead. This script does the mirroring from DockerHub -> internal registry. All I did in this PR is refactor the script. I don't honestly know why I used two lists instead of one, there was probably at one point some difference in how these images were handled that got deleted.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12231:385,throttle,throttle,385,https://hail.is,https://github.com/hail-is/hail/pull/12231,1,['throttle'],['throttle']
Performance,AbstractChannelHandlerContext.java:336); 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); 	Error: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.; 	This stopped SparkContext was created at:; 	; 	org.apache.spark.SparkContext.getOrCreate(SparkContext.scala); 	sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	java.lang.reflect.Method.invoke(Method.java:498); 	sparklyr.Invoke.invoke(invoke.scala:139); 	sparklyr.StreamHandler.handleMethodCall(stream.scala:123); 	sparklyr.StreamHandler.read(stream.scala:66); 	sparklyr.BackendHandler.channelRead0(handler.scala:51); 	sparklyr.BackendHandler.channelRead0(handler.scala:4); 	io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105); 	io.netty.channel.AbstractCha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:6598,concurren,concurrent,6598,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['concurren'],['concurrent']
Performance,"AbstractChannelHandlerContext.java:336); 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357); 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); </details>. <details>; <summary>Working hail.log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletCon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:13216,concurren,concurrent,13216,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['concurren'],['concurrent']
Performance,AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:439); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:525); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:466); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:490); 	at com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:6523); 	at is.hail.relocated.com.google.cloud.storage.spi.v1.HttpStorageRpc.load(HttpStorageRpc.java:726); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.lambda$readAllBytes$24(StorageImpl.java:574); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:60); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.run(StorageImpl.java:1476); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:574); 	at is.hail.relocated.com.google.cloud.storage.StorageImpl.readAllBytes(StorageImpl.java:563); 	at is.hail.io.fs.GoogleStorageFS.$anonfun$readNoCompression$1(GoogleStorageFS.scala:288); 	at is.hail.services.package$.retryTransientErrors(package.scala:163); 	at is.hail.io.fs.GoogleStorageFS.readNoCompression(GoogleStorageFS.scala:286); 	at is.hail.io.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409:8472,load,load,8472,https://hail.is,https://github.com/hail-is/hail/issues/13409,1,['load'],['load']
Performance,Add ForwardLets optimizer pass.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5245:16,optimiz,optimizer,16,https://hail.is,https://github.com/hail-is/hail/pull/5245,1,['optimiz'],['optimizer']
Performance,Add Maximize and MinimizeLets optimizer passes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041:30,optimiz,optimizer,30,https://hail.is,https://github.com/hail-is/hail/pull/5041,1,['optimiz'],['optimizer']
Performance,Add PValue load/store.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8247:11,load,load,11,https://hail.is,https://github.com/hail-is/hail/pull/8247,1,['load'],['load']
Performance,Add Python type parser. Speed up Python API by ~3x with optimizations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2979:56,optimiz,optimizations,56,https://hail.is,https://github.com/hail-is/hail/pull/2979,1,['optimiz'],['optimizations']
Performance,Add `ExtractIntervalFilters` optimizer pass.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5979:29,optimiz,optimizer,29,https://hail.is,https://github.com/hail-is/hail/pull/5979,1,['optimiz'],['optimizer']
Performance,"Add a code cache. 50 is was chosen somewhat randomly. Normalize incoming IR so name differences don't case a recompile. Move ApplyIR `conversion` since it shouldn't be involved in equality. Add hashCode to GR because you should always define hashCode, equals as a pair (and it was behaving very strangely without it). @chrisvittal I think this resolves the last of the issues you ran into on Friday.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5426:11,cache,cache,11,https://hail.is,https://github.com/hail-is/hail/pull/5426,1,['cache'],['cache']
Performance,"Add a codegen method `SNDArray.coiterate`, with signature; ```; def coiterate(cb: EmitCodeBuilder, region: Value[Region], indexVars: IndexedSeq[String], arrays: IndexedSeq[(SNDArrayCode, IndexedSeq[Int], String)], body: IndexedSeq[SSettable] => Unit, deepCopy: Boolean): Unit; ```; For example, the index expression `A[i, j] += B[j]` would be written; ```; coiterate(cb, region, Seq(""i"", ""j""), Seq((A, Seq(0, 1), ""A""), (B, Seq(1), ""B"")), {; case Seq(a, b) => cb.assign(a, SCode.add(cb, a, b)); }); ```; This generates a loop nest, with one loop per variable in `indexVars`. The inner loop is `indexVars(0)`, so that column major traversal is when index variables are increasing, as in `(A, Seq(0, 1), ""A"")`. Each index variable iterates over a dimension, with the size of the dimension inferred from its use. In the inner loop, each index variable `i0, i1, ...` has a value; `body` is run, with each of the `SSettable`s bound to an element of the corresponding input in `arrays`. For example, if the first element of `arrays` is `(A, IndexedSeq(1, 3), ""A"")`, then the `SSettable` will be the element of `A` at index `(i1, i3)`. However, it avoids computing the address of each element from the indices from scratch in the inner loop. This was motivated by the need to generate operations on ndarrays in the local whitening aggregator. I replaced a few uses of `forEachIndex` with `coiterate`, which may give a performance boost since it avoids index math in the inner loop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10583:1410,perform,performance,1410,https://hail.is,https://github.com/hail-is/hail/pull/10583,1,['perform'],['performance']
Performance,"Add an extra (currently unused) flag on PartitionNativeWriter and an; extra field on its return type. When the flag is true, we compute the; sum of the byte sizes of every row and return it as the extra field; `partitionByteSize`. We will use this to compute branch factors in LowerDistributedSort and; depending on computational cost, possible turn this on by default to; allow more optimization around deserializing/processing more than one; row at a time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11829:384,optimiz,optimization,384,https://hail.is,https://github.com/hail-is/hail/pull/11829,1,['optimiz'],['optimization']
Performance,"Add components and list of reviewers for those components to the index.html page. It looks like this:. <img width=""667"" alt=""screen shot 2019-02-14 at 11 07 43 am"" src=""https://user-images.githubusercontent.com/1244990/52800012-ea686700-3048-11e9-84bd-33adb86e4820.png"">. The order of the names is random each time the page is loaded. Idea is to take the first name for the component you're reviewing when creating PRs. @danking I removed you from Hail front-end. Now everyone appears on 2-3 components. @akotlar FYI, I ripped out the JSON endpoints since I'm happy with the Flask implementation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5353:327,load,loaded,327,https://hail.is,https://github.com/hail-is/hail/pull/5353,1,['load'],['loaded']
Performance,Add option to import_bgen to load only row fields,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3448:29,load,load,29,https://hail.is,https://github.com/hail-is/hail/issues/3448,1,['load'],['load']
Performance,Add rewrite rule to optimize a common case of Literal array contains,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5660:20,optimiz,optimize,20,https://hail.is,https://github.com/hail-is/hail/pull/5660,1,['optimiz'],['optimize']
Performance,Add selection to PCA to push down optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3888:34,optimiz,optimization,34,https://hail.is,https://github.com/hail-is/hail/pull/3888,1,['optimiz'],['optimization']
Performance,Add some optimizations for Ryan's pipeline,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4504:9,optimiz,optimizations,9,https://hail.is,https://github.com/hail-is/hail/pull/4504,1,['optimiz'],['optimizations']
Performance,"Add the following transformations:; ```scala; MakeStruct(""a"" -> GetField(o, ""x""), ...) -> CastRename(SelectFields(o, [""x""..]), newtype); If(IsNA(x), NA(x.typ), x) -> x; ```. The changes to `SStructView` (nee `SSubsetStruct`) were as a result of a bud that prevented subsetting and then renaming to an excluded field, ie `{x, y, z} subset {z} rename {x}`. Now `SStructView` leaves its parent `SType` unmodified and casts loads through the parent to the appropriate `SValue`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14231:420,load,loads,420,https://hail.is,https://github.com/hail-is/hail/pull/14231,1,['load'],['loads']
Performance,Added Chunk Cache to facilitate faster chunk interactions through less use of malloc and free.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10618:12,Cache,Cache,12,https://hail.is,https://github.com/hail-is/hail/pull/10618,1,['Cache'],['Cache']
Performance,"Added a panel for it [here](https://grafana.hail.is/d/TVytxvb7z/batch-driver-performance?orgId=1&var-namespace=dgoldste&from=1646327460490&to=1646328244432). I was kind of surprised to see that we don't have a queue building up, but the back-pressure could be somewhere else. I can also bring up the rate limit now that the DB can handle it",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11472:77,perform,performance,77,https://hail.is,https://github.com/hail-is/hail/pull/11472,2,"['perform', 'queue']","['performance', 'queue']"
Performance,Added additional support for loading (implicit parent data) and exporting (in the context of exportplink) fam files,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/483:29,load,loading,29,https://hail.is,https://github.com/hail-is/hail/pull/483,2,['load'],['loading']
Performance,"Added cache, persist, coalesce, count, exportGenotypes to GDS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1584:6,cache,cache,6,https://hail.is,https://github.com/hail-is/hail/pull/1584,1,['cache'],['cache']
Performance,Added detailed PCA docs in Pandoc format (Markdown+LaTeX).; Rewrote PCA command and SamplePCA method accordingly.; Passes preliminary testing including handling of missingness. Tests still needed.; Variant sorting in loadings output still needed.; Further issues flagged in the doc.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/202:217,load,loadings,217,https://hail.is,https://github.com/hail-is/hail/pull/202,1,['load'],['loadings']
Performance,Added example of loading dict from file for rename_samples,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2018:17,load,loading,17,https://hail.is,https://github.com/hail-is/hail/pull/2018,1,['load'],['loading']
Performance,Added ir.Optimize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3330:9,Optimiz,Optimize,9,https://hail.is,https://github.com/hail-is/hail/pull/3330,1,['Optimiz'],['Optimize']
Performance,"Added new load and extract files for additional datasets such as GERP++, variant_summary",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6803:10,load,load,10,https://hail.is,https://github.com/hail-is/hail/pull/6803,1,['load'],['load']
Performance,Added optimizer to prune dead fields,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3470:6,optimiz,optimizer,6,https://hail.is,https://github.com/hail-is/hail/pull/3470,1,['optimiz'],['optimizer']
Performance,"Added optional parameters to the StartChangeSet API to support tagging a resource while making a request to create it.</li>; <li>api-change:<code>rekognition</code>: [<code>botocore</code>] Adding support for ImageProperties feature to detect dominant colors and image brightness, sharpness, and contrast, inclusion and exclusion filters for labels and label categories, new fields to the API response, &quot;aliases&quot; and &quot;categories&quot;</li>; <li>api-change:<code>securityhub</code>: [<code>botocore</code>] Documentation updates for Security Hub</li>; <li>api-change:<code>ssm-incidents</code>: [<code>botocore</code>] RelatedItems now have an ID field which can be used for referencing them else where. Introducing event references in TimelineEvent API and increasing maximum length of &quot;eventData&quot; to 12K characters.</li>; </ul>; <h1>1.26.7</h1>; <ul>; <li>api-change:<code>autoscaling</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ec2</code>: [<code>botocore</code>] This release adds a new price capacity optimized allocation strategy for Spot Instances to help customers optimize provisioning of Spot Instances via EC2 Auto Scaling, EC2 Fleet, and Spot Fleet. It allocates Spot Instances based on both spare capacity availability and Spot Instance price.</li>; <li>api-change:<code>ecs</code>: [<code>botocore</code>] This release adds support for task scale-in protection with updateTaskProtection and getTaskProtection APIs. UpdateTaskProtection API can be used to protect a service managed task from being terminated by scale-in events and getTaskProtection API to get the scale-in protection status of a task.</li>; <li>api-change:<code>es</code>: [<code>botocore</c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12458:1989,optimiz,optimized,1989,https://hail.is,https://github.com/hail-is/hail/pull/12458,4,['optimiz'],"['optimize', 'optimized']"
Performance,"Adds BlockMatrix sparsify functions for:; - band matrix; - upper/lower triangle (special case of band); - a collection of rectangles. For diagonal band, I switched GridPartitioner.filterBand to go from lower to upper diagonal index, rather than taking a lower and upper bandwidth. This is more general, e.g. the diagonal itself need not be in the band. Band and triangle zero out elements in partially overlapping blocks by default. Rectangles currently only supports dropping whole blocks. Also adds `export_rectangles` for exporting rectangular regions to TSV in parallel.; I use parameters `path_in` and `path_out`, and switched `BlockMatrix.export` to this convention as well from `input` and `output` to avoid using the reserved word `input`. I have not exposed export methods directly on BlockMatrix for now as it'd be very easy for users to needlessly write and read an already written BlockMatrix. I could add these in a later PR with a warning, or we can wait until we've moved to IR and can optimize read followed by export to export on the file. It'd also be good to add compression options (and float formatting options to `export` and `export_rectanges`). Along the way I fixed NaN checking (due to Double.NaN != Double.NaN) on scalar and vector `/` sparse block ops and added NaN and Infinity checking to scalar and vector `*` sparse ops. Together with `sparsify_row_intervals` in the first sparse matrix PR, this PR exposes all the BlockMatrix functionality needed for big LD applications of Kate/Ran and Jacob/Masa.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3539:1001,optimiz,optimize,1001,https://hail.is,https://github.com/hail-is/hail/pull/3539,1,['optimiz'],['optimize']
Performance,Adds optimization available in lowering process if number of rows per partition from child TableIR is known,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10798:5,optimiz,optimization,5,https://hail.is,https://github.com/hail-is/hail/pull/10798,1,['optimiz'],['optimization']
Performance,"Adds the ability to rerun/retry queries from the nearest `CollectDistributedArray` (`CDA`) IR site. Computes a ""Semantic Hash"" of the top-level IR, which is split and shared among the various constituent `CDA` calls in a query. The `CDA` procedure looks in an execution cache for the results of each partition for that call and uses/updates the cache with successful partition computations. . The nature of the staged- lower and execute model means we don't know how many `CDA` calls that will be generated ahead of time. Thus we treat the ""Semantic Hash"" in a similar way to an RNG state variable and generate a key from the Semantic Hash every time every time we encounter a `CDA`. Since an `ExecutionContext` is re-used for multiple queries in tests while a `SemanticHash` is coupled to one query, the two were kept separate. To minimise the amount of manual state handling, the code was transformed to use a ""State"" monad (abstracted as `MonadLower`). Since the `ExecuteContext` is used nearly everywhere the semantic hash is required, the `ExecuteContext` was absorbed into the `MonadLower` interface. `Lower` is a simple, concrete instance of `MonadLower`, and is used to adapt statements into `MonadLower` expressions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13194:270,cache,cache,270,https://hail.is,https://github.com/hail-is/hail/pull/13194,2,['cache'],['cache']
Performance,"Adds the ability to rerun/retry queries from the nearest `CollectDistributedArray` (`CDA`) `IR` site. Computes a ""Semantic Hash"" of the top-level `IR` which is used to generate a key for the various constituent `CDA` calls in a query. The implementation for CDA, `BackendUtils.collectDArray`, uses that key to look into an the execution cache for the results of each partition for that call and uses/updates the cache with successful partition computations. The nature of the staged- lower and execute model means we don't know how many `CDA` calls that will be generated ahead of time. Thus we treat the ""Semantic Hash"" in a similar way to an RNG state variable and generate a key from the Semantic Hash every time every time we encounter a `CDA`. The execution cache is implemented on-top of a local or remote filesystem (configurable via the `HAIL_CACHE_DIR` environment variable). This defaults to `{tmpdir}/hail/{pip-version}`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12954:337,cache,cache,337,https://hail.is,https://github.com/hail-is/hail/pull/12954,3,['cache'],['cache']
Performance,"Adds the available [GIANT 2018 Exome Array Summary Statistics](https://portals.broadinstitute.org/collaboration/giant/index.php/GIANT_consortium_data_files#2018_Exome_Array_Summary_Statistics) datasets for WHR, BMI, and height as Hail Tables. For reproducibility, I added the notebook I used to generate the tables and schemas. The datasets were small in this case, and I ended up doing things locally on my machine. It didn't seem to make sense to try to redo things to fit into the older extract/load workflow once everything had already been generated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10235:498,load,load,498,https://hail.is,https://github.com/hail-is/hail/pull/10235,1,['load'],['load']
Performance,"Adds unphased versions to `1000_Genomes_HighCov_autosomes` and `1000_Genomes_HighCov_chrX` datasets. Unphased versions contained multiallelic variants, so these were split with `split_multi_hts`. The `chrY` dataset had not had multiallelic variants split before, so this fixes that as well. The dataset `load` scripts were replaced with a notebook that contains more detail about the process (for both phased and unphased versions).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10607:304,load,load,304,https://hail.is,https://github.com/hail-is/hail/pull/10607,1,['load'],['load']
Performance,"After do_handshake, [schedule_loop_body](https://github.com/hail-is/hail/blob/2d019337114a972016ad843baabe76814dc8ad10/batch/batch/driver/instance_collection/pool.py#L371) is our biggest offender on the profiler. That in itself is not a bad thing, ideally we want the scheduling loop to be running as much as possible to give us the highest throughput, but the loop should still be efficient. All the queries in `user_runnable_jobs` show up the same on the profiler, since they are the same function call-stack just with different arguments. This should give us finer granularity into what's taking up our time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11358:341,throughput,throughput,341,https://hail.is,https://github.com/hail-is/hail/pull/11358,1,['throughput'],['throughput']
Performance,"After profiling read/count on a 600M ExAC sites KeyTable, cached types/requiredness (requiredness was the big one, calling a virtual function) in arrays. ```; MASTER; In [4]: %timeit -n 10 df.count(); 4.41 s per loop; In [3]: %timeit -n 10 df.count(); 10 loops, best of 3: 4.63 s per loop. avg of two: 4.525 s per loop; ```; ```; THIS BRANCH; In [7]: %timeit -n 10 df.count(); 10 loops, best of 3: 2.94 s per loop; In [3]: %timeit -n 10 df.count(); 10 loops, best of 3: 3.34 s per loop. avg of two: 3.14 s per loop; ```. 44% increase in decoding throughput for KeyTable, can't imagine VDS would lag far behind.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2762:58,cache,cached,58,https://hail.is,https://github.com/hail-is/hail/pull/2762,2,"['cache', 'throughput']","['cached', 'throughput']"
Performance,"Again, [looking at utilization](https://console.cloud.google.com/monitoring/dashboards/builder/982ec67a-4b20-4901-a0aa-af418813a9c4?project=hail-vdc&dashboardBuilderState=%257B%2522editModeEnabled%2522:false%257D&timeDomain=1m&f.rlabel.namespace_name=default&f.umlabel.app=batch-driver), the driver is generally not using its full request. The Python spikes are maybe 35% of utilization and the nginx spikes are maybe 15%. I set the requests to around the top of these spikes. That should ensure that normal daily load is handled without scale up, but during low periods we can pack much better. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12014:514,load,load,514,https://hail.is,https://github.com/hail-is/hail/pull/12014,1,['load'],['load']
Performance,"Allow -b 0.; Added typeclasses for converting to/from JSON.; Implemented for OrderedPartitioner and Locus.; Use in VSM.{read, write}.; Remove OrderedPartitioner.ascending. It wasn't properly implemented. Still try to load serialized .vds/partitioner if partitioner.json.gz; isn't there.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1139:217,load,load,217,https://hail.is,https://github.com/hail-is/hail/pull/1139,1,['load'],['load']
Performance,Allows for local computation of ld matrix when the matrix is relatively small to improve performance.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1937:89,perform,performance,89,https://hail.is,https://github.com/hail-is/hail/pull/1937,1,['perform'],['performance']
Performance,"Alright, so the goal of this PR is to make this work:. ```; HAIL_QUERY_BACKEND=service \; python3 -c 'import hail as hl; hl.utils.range_table(10).write(""gs://foo/bar.t"")`; ```. In particular, the Hail Query JAR is stored in a well known location. If we know the desired git revision, (we do, it should be the same git revision as our wheel), then we can deduce the JAR URL for the user. Moreover, if we're pointed at a namespace, we can still determine the correct location [1]. This PR provides three escape hatches to this behavior:; 1. Specify the `jar_url` parameter to `ServiceBackend`.; 2. Specify the `HAIL_JAR_URL` environment variable.; 3. Specify a JAR url in the user config: `hailctl config set query/jar_url gs://...`. This PR is unfortunately entangled with one other minor change. In `main`, we send the git revision *and* the JAR URL to the driver and the worker as a part of the ""command string"" (the JVMEntryway passes this array of strings to the `main` method of `ServiceBackendSocketAPI2` or `Worker`. After this change, the backend does not necessarily know the git revision. That's fine. The git revision was only ever used as:; 1. a cache key for the JAR cache, and; 2. a unique name for the JAR; Both of these uses are buggy anyway! If you re-use a HAIL_SHA with a different HAIL_JAR_URL and you land on a worker that previously pulled that HAIL_SHA, you'll get the previously pulled JAR, not the newly specified one. Instead I use the JAR URL directly as a cache key and unique name. ---. [1] Odds are good that the developer has not uploaded a JAR to this location, but they can do so by dev deploying `upload_query_jar` or by running `make -C query push-jar`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11643:1157,cache,cache,1157,https://hail.is,https://github.com/hail-is/hail/pull/11643,3,['cache'],['cache']
Performance,Also add my extremely specific conda installation location to `loadconda`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5576:63,load,loadconda,63,https://hail.is,https://github.com/hail-is/hail/pull/5576,1,['load'],['loadconda']
Performance,Also added it to third-party images so we're not pulling from DockerHub. Turns out the \ufeff bug we were seeing is hitting a lot of people and is addressed in this release. I put this up in my namespace to see that I can load it without error (though didn't try copying over dashboards and such).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12300:222,load,load,222,https://hail.is,https://github.com/hail-is/hail/pull/12300,1,['load'],['load']
Performance,"Also fixed getting the logs for a job. - I didn't realize the context manager for asyncio_timeout was throwing an asyncio.TimeoutError. Now, I handle the TimeoutError exception and then throw our own exception after we've uploaded the logs and cleaned up the container. This way it still shows up as an error. - I noticed the logs were being cached when a user gets the logs while the job is running and we don't update the cache until the job is complete. Therefore, I think from the code, if the user asks for the logs part-way through the job running, they wouldn't see any updates until the job is completed. I'm not sure why no-one has complained about this yet, so might be good to double check that this is indeed a bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8280:342,cache,cached,342,https://hail.is,https://github.com/hail-is/hail/pull/8280,2,['cache'],"['cache', 'cached']"
Performance,Also rename so that these don't pop up as spurious performance changes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10685:51,perform,performance,51,https://hail.is,https://github.com/hail-is/hail/pull/10685,1,['perform'],['performance']
Performance,"Any page that was not a root page did not render properly because it; pointed to relative locations for the css and js resources. Moreover,; the 404 page incorrectly used a relative load for the navbar. These; changes change the template.xslt to use an absolute; protocol-agnostic (useful for testing locally without TLS/SSL) URL and; change 404.xslt to use an absolute load of the navbar. Currently, 404 is the only page that might appear at a non-root URL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4936:182,load,load,182,https://hail.is,https://github.com/hail-is/hail/pull/4936,2,['load'],['load']
Performance,"Anyone using recent versions of the hail-base image to connect to Google Storage has encountered MethodNotFound errors like this:; ```; Activated service account credentials for: [dpalmer-o8fe7@hail-vdc.iam.gserviceaccount.com]; 2020-03-23 20:00:58 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Initializing Spark and Hail with default parameters...; Running on Apache Spark version 2.4.0; SparkUI available at http://59dd09c396e8:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.34-2684f0214a05; LOGGING: writing to /hail-20200323-2000-0.2.34-2684f0214a05.log; Traceback (most recent call last):; File ""/scripts/hail_test.py"", line 3, in <module>; bam = hl.import_table('gs://dalio_bipolar_w1_w2_hail_02/analysis/gene_sets/BP_including_BPSCZ_MAC5_gene_set_counts_per_sample.tsv'); File ""</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-1276>"", line 2, in import_table; File ""/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/hail/python/hail/methods/impex.py"", line 1511, in import_table; t = Table(TableRead(tr)); File ""/hail/python/hail/table.py"", line 334, in __init__; self._type = self._tir.typ; File ""/hail/python/hail/ir/base_ir.py"", line 303, in typ; self._compute_type(); File ""/hail/python/hail/ir/table_ir.py"", line 215, in _compute_type; self._type = Env.backend().table_type(self); File ""/hail/python/hail/backend/backend.py"", line 121, in table_type; jir = self._to_java_ir(tir); File ""/hail/python/hail/backend/backend.py"", line 105, in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); File ""/hail/python/hail/ir/base_ir.py"", line 311, in parse; return Env.hail().expr.ir.IRParser.parse_table_ir(code, ref_map, ir_map); File ""/spark-2.4.0-b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8343:286,load,load,286,https://hail.is,https://github.com/hail-is/hail/issues/8343,1,['load'],['load']
Performance,"Apparently distutils isn't a core python package anymore. Here's an example where I tried to build the hail ubuntu image on main https://ci.hail.is/batches/2214931. ```; #13 59.80 update-alternatives: using /usr/bin/python3.7 to provide /usr/bin/python3 (python3) in auto mode; #13 59.87 Traceback (most recent call last):; #13 59.87 File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; #13 59.87 ""__main__"", mod_spec); #13 59.87 File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; #13 59.87 exec(code, run_globals); #13 59.87 File ""/usr/lib/python3/dist-packages/pip/__main__.py"", line 16, in <module>; #13 59.87 from pip._internal.cli.main import main as _main # isort:skip # noqa; #13 59.87 File ""/usr/lib/python3/dist-packages/pip/_internal/cli/main.py"", line 10, in <module>; #13 59.87 from pip._internal.cli.autocompletion import autocomplete; #13 59.87 File ""/usr/lib/python3/dist-packages/pip/_internal/cli/autocompletion.py"", line 9, in <module>; #13 59.87 from pip._internal.cli.main_parser import create_main_parser; #13 59.87 File ""/usr/lib/python3/dist-packages/pip/_internal/cli/main_parser.py"", line 7, in <module>; #13 59.87 from pip._internal.cli import cmdoptions; #13 59.87 File ""/usr/lib/python3/dist-packages/pip/_internal/cli/cmdoptions.py"", line 19, in <module>; #13 59.87 from distutils.util import strtobool; #13 59.87 ModuleNotFoundError: No module named 'distutils.util'; ```. My guess is that we haven't actually built this layer in a while and we've been riding the cache, but my recent experiments with our docker images might have invalidated it? Seems like it hasn't been a core module for a while so I'm surprised we haven't hit this issue sooner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11791:1519,cache,cache,1519,https://hail.is,https://github.com/hail-is/hail/pull/11791,1,['cache'],['cache']
Performance,"Applies on top of #4713 . Compared to previous versions, this uses a priority queue rather than a linear search to create the list of returned items. We return tuples so that the caller can construct arrays as appropriate rather than constructing an array of values here.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4719:78,queue,queue,78,https://hail.is,https://github.com/hail-is/hail/pull/4719,1,['queue'],['queue']
Performance,Apply the same logic as used in batch to gateway to get better cache behavior.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5807:63,cache,cache,63,https://hail.is,https://github.com/hail-is/hail/pull/5807,1,['cache'],['cache']
Performance,"As discussed in Zulip, `BlockMatrix.write_from_entry_expr` throws OOM error when running on a cluster without `--properties 'core:fs.gs.outputstream.upload.chunk.size=1048576'`. The reason is as documented in [the Hail doc](https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html#hail.linalg.BlockMatrix.from_entry_expr), but I hope either Hail 1) makes this property as default, or 2) throws more appropriate error/warning message. > This method opens n_cols / block_size files concurrently per task. To not blow out memory when the number of columns is very large, limit the Hadoop write buffer size. Error:; ```20/03/03 21:39:46 ERROR org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1213,5,main]; java.lang.OutOfMemoryError: GC overhead limit exceeded; 20/03/03 21:39:50 ERROR org.apache.spark.executor.Executor: Exception in task 55.0 in stage 3.0 (TID 1197); java.lang.NullPointerException; at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:515); at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.create(GoogleCloudStorageFileSystem.java:261); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.createChannel(GoogleHadoopOutputStream.java:82); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopOutputStream.<init>(GoogleHadoopOutputStream.java:74); at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.create(GoogleHadoopFileSystemBase.java:797); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1067); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1048); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:937); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:925); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:91); at is.hail.io.fs.HadoopFS.unsafeWriter(HadoopFS.scala:445); at is.hail.linalg.WriteBlocksRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8239:484,concurren,concurrently,484,https://hail.is,https://github.com/hail-is/hail/issues/8239,1,['concurren'],['concurrently']
Performance,"As per the discussion on Slack, Option should be the default and null is allowed in two places:; 1. At legacy/Java APIs that expect null. (Possibly) null values should be immediately converted to Option with `Option(expr)` and Options should be converted to possibly null values with `o.orNull`.; 2. When the scope of null values is well-defined and significant performance gains justify the added danger. Immediately two instances come to mind that can be improved:; - State has an nullable vds; - Args4j encourages the use of null and needs to go (for this and many reasons). We need to examine every instance of `null` and `= _` in the code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/236:362,perform,performance,362,https://hail.is,https://github.com/hail-is/hail/issues/236,1,['perform'],['performance']
Performance,"As written there are at least two issues:. 1. This function is not idempotent. If we retry the request while the first is still running, the second request will receive nothing from the first query and attempt to insert records into that empty space. 2. Intentionally concurrent updates will try to use the same update id because we do not take a FOR UPDATE lock on the gap after the last row. We should verify that what I have written here takes that gap lock.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14367:268,concurren,concurrent,268,https://hail.is,https://github.com/hail-is/hail/pull/14367,1,['concurren'],['concurrent']
Performance,"Aside from removing these for being unused, I think it makes more sense when we want to bring something of the like back for the combiner just to have an async eval that the client (the combiner) can then use to evaluate many IRs concurrently. I don't much see the utility of the *_many methods. cc: @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12490:230,concurren,concurrently,230,https://hail.is,https://github.com/hail-is/hail/pull/12490,1,['concurren'],['concurrently']
Performance,Assigning Cotton for context since he implemented ArrayAgg back in January. It looks to me like this stuff was never actually getting optimized...,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5765:134,optimiz,optimized,134,https://hail.is,https://github.com/hail-is/hail/pull/5765,1,['optimiz'],['optimized']
Performance,"Associate a region with each ptype, and remove region parameterizations of ptype methods, including load*, is*Missing, allocate. Reference from Zulip:. Alex Kotlar: What is our long term plan for load* methods, and do we need their region parameterizations? I would love to understand the design proposal for these methods, in part because I want to document our allocation strategy in the ptype design doc (or maybe in a new design doc for regions). Observations:. Methods like loadElement (PContainer and inheritors) and loadField (PBaseStruct and inheritors) have region-taking parameterizations, but these methods are always wrappers for non-region parameterization (e.g loadElement(region, offset, idx) = loadElement(offset, idx)), which makes sense since our ""offsets"" are now memory addresses in these cases (can be read without knowledge of the region that allocated that memory). I believe historically these were really offsets into a region, requiring that region to load it. I believe the remaining use case, now that these offsets are absolute, is to allow for off-heap allocation. This seems slightly odd for a load operation/getter, but I am probably not seeing the intention. Thanks!. daniel king: The history is correct. daniel king: You may want a load to do allocation if you're loading from a lazy datastructure, like a lazily decoded BGEN genotype row. Alex Kotlar: ok, thanks Dan, will keep that parameterization as is. daniel king: You should check-in with Tim though, not clear that load is the place to do this. Tim Poterba: Yep, agree with Dan here. This was the reason I pushed back on your pr to remove the region args in December. Patrick Schultz: How would a lazily decoded datastructure work? Would it mutate to record the fact that some lazy value has already been computed? Or would it recompute every time that value is accessed?. Patrick Schultz: We probably want the former for performance, but we should figure out what the memory management for that looks like. P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826:100,load,load,100,https://hail.is,https://github.com/hail-is/hail/issues/7826,7,['load'],"['load', 'loadElement', 'loadField']"
Performance,"At some point (highly likely that it was the Ubuntu 20.04 -> 22.04 upgrade) Batch went from using cgroups v1 to cgroups v2 for setting containers' CPU and memory limits. We mostly don't touch cgroups, the container runtime handles that for us, but we poll the `cgroupfs` for recoding memory usage and CPU utilization. The accounting mechanism changed between v1 and v2 so batch was silently failing to collect these metrics. Deploying these changes into my namespace got me back the following plots (compiling hail):. <img width=""701"" alt=""Screenshot 2023-09-14 at 5 47 24 PM"" src=""https://github.com/hail-is/hail/assets/24440116/0f470e5a-7feb-4b9e-bac6-f560c8366d8e"">. The reason why we fail silently when the file doesn't exist is because we are letting the container runtime manage the cgroup, and there is a race condition between the container exiting + the cgroup getting destroyed and our polling of this file. We could probably do a better job reporting an error, like this though, perhaps logging errors if we fail to read this file more than X number of times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13626:812,race condition,race condition,812,https://hail.is,https://github.com/hail-is/hail/pull/13626,1,['race condition'],['race condition']
Performance,"At some point we started optimizing the MakeStruct to a SelectFields,; which is great, but not if it breaks important optimizations like the; avoid-a-shuffle rewrite rule!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7073:25,optimiz,optimizing,25,https://hail.is,https://github.com/hail-is/hail/pull/7073,2,['optimiz'],"['optimizations', 'optimizing']"
Performance,"Autosummary now documents only the members specified in a module's; <code>__all__</code> attribute if :confval:<code>autosummary_ignore_module_all</code> is set to; <code>False</code>. The default behaviour is unchanged. Autogen also now supports; this behavior with the <code>--respect-module-all</code> switch.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9555"">#9555</a>: autosummary: Improve error messages on failure to load target object</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9800"">#9800</a>: extlinks: Emit warning if a hardcoded link is replaceable; by an extlink, suggesting a replacement.</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9961"">#9961</a>: html: Support nested <!-- raw HTML omitted --> HTML elements in other HTML builders</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/10013"">#10013</a>: html: Allow to change the loading method of JS via <code>loading_method</code>; parameter for :meth:<code>Sphinx.add_js_file()</code></li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9551"">#9551</a>: html search: &quot;Hide Search Matches&quot; link removes &quot;highlight&quot; parameter; from URL</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9815"">#9815</a>: html theme: Wrap sidebar components in div to allow customizing their; layout via CSS</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9827"">#9827</a>: i18n: Sort items in glossary by translated terms</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9899"">#9899</a>: py domain: Allows to specify cross-reference specifier (<code>.</code> and; <code>~</code>) as <code>:type:</code> option</li>; <li><a href=""https://github-redirect.dependabot.com/sphinx-doc/sphinx/issues/9894"">#9894</a>: linkcheck: add option <code>linkcheck_excl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11522:2453,load,loading,2453,https://hail.is,https://github.com/hail-is/hail/pull/11522,2,['load'],['loading']
Performance,Avoid query-service races when looking at the cache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10309:46,cache,cache,46,https://hail.is,https://github.com/hail-is/hail/pull/10309,1,['cache'],['cache']
Performance,Azure seems to have pervasively higher latency than GCP. This should reduce the amount of warning logs we receive.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12473:39,latency,latency,39,https://hail.is,https://github.com/hail-is/hail/pull/12473,1,['latency'],['latency']
Performance,"B); Collecting asyncinit<0.3,>=0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting decorator<5; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting aiohttp==3.7.4; Using cached aiohttp-3.7.4-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB); Collecting google-cloud-storage==1.25.*; Using cached google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); Collecting yarl<2.0,>=1.0; Using cached yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB); Collecting typing-extensions>=3.6.5; Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB); Collecting attrs>=17.3.0; Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB); Collecting chardet<4.0,>=2.0; Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB); Collecting async-timeout<4.0,>=3.0; Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB); Collecting multidict<7.0,>=4.5; Using cached multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB); Collecting google-auth>=1.2; Using cached google_auth-1.27.1-py2.py3-none-any.whl (136 kB); Collecting google-auth-oauthlib; Using cached google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB); Collecting fsspec>=0.8.0; Using cached fsspec-0.8.7-py3-none-any.whl (103 kB); Collecting google-cloud-core<2.0dev,>=1.2.0; Using cached google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB); Collecting google-resumable-media<0.6dev,>=0.5.0; Using cached google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB); Requirement already satisfied: setuptools in ./venv/3.8/lib/python3.8/site-packages (from hurry.filesize==0.9->hail) (54.1.2); Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1; Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB); Collecting idna<2.9,>=2.5; Using cached idna-2.8-py2.py3-none-any.whl (58 kB); Collecting certifi>=2017.4.17; Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB); Collecting tornado>=4.3; Using cached tornado-6.1-cp38-cp38-manylinux2010_x86_64.whl (427 kB); Collecting six>=1.5.2; Usin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:3672,cache,cached,3672,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,BGEN performance improvements,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1917:5,perform,performance,5,https://hail.is,https://github.com/hail-is/hail/pull/1917,1,['perform'],['performance']
Performance,BGEN performance on par with 0.1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4020:5,perform,performance,5,https://hail.is,https://github.com/hail-is/hail/issues/4020,1,['perform'],['performance']
Performance,"BackendSocketAPI2.$anonfun$parseInputToCommandThunk$3(ServiceBackend.scala:650); at is.hail.backend.service.ServiceBackendSocketAPI2.executeOneCommand(ServiceBackend.scala:822); at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:447); at is.hail.backend.service.Main$.main(Main.scala:15); at is.hail.backend.service.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.124-87398e1b514e; Error summary: HailException: file already exists: gs://aou_analysis/250k/data/utils/aou_mt_sample_qc_250k.ht; ```; </details>. The code is simple and clearly is running against a path that does not already exist:; ```; if not hl.hadoop_exists(get_aou_util_path('mt_sample_qc')):; print('Run sample qc MT.....'); mt = hl.read_matrix_table(ACAF_MT_PATH); mt = mt.filter_rows(mt.locus.in_autosome()); # mt = mt.filter_rows(mt.locus.contig == 'chr1'); ht = hl.sample_qc(mt, name='mt_sample_qc'); ht.write(get_aou_util_path('mt_sample_qc'), overwrite=args.overwrite); ```. Job log: https://batch.hail.is/batches/8058522/jobs/171029. <details>; <summary>The last TableIR logged</summary>. ```; 2023-10-13 02:14:44.213 : INFO: after optimize: darrayLowerer, after LowerAndExecute",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13809:6762,concurren,concurrent,6762,https://hail.is,https://github.com/hail-is/hail/issues/13809,1,['concurren'],['concurrent']
Performance,"Based off of discussion in #11907, this aims to avoid separate PRs from clobbering the image cache tag and sets up PR-specific cache tags per image. Note that using `ci-intermediate` was also detrimental to the image cache and I don't think different images sharing layers under the common name holds much value. I think we should ultimately get rid of `ci-intermediate` entirely and explicitly name our images so that they don't ruin each other's caches. I tested this in my namespace's CI. Here's the image build times from two consecutive dev deploys:. Before | After; :-------------------------:|:-------------------------:; ![Screen Shot 2022-07-05 at 6 14 36 PM](https://user-images.githubusercontent.com/24440116/177426924-5d5ade8c-0cee-4a0e-b477-2156d4e01e78.png) | ![Screen Shot 2022-07-05 at 6 14 45 PM](https://user-images.githubusercontent.com/24440116/177426882-c0029760-42ae-471d-b48c-daa0eadea448.png). I don't personally see the need for adding more SHAs to the cache as mentioned in #11907, a per-PR cache seems like exactly what you would want. The one drawback I can think of here is that a deploy won't make use of the cache from the PR that resulted in the commit to main. I believe the commit SHAs would be different because we squash so other than devising a way to trace the commit back to the PR I don't see how we can easily connect the two. Still, I feel like it's not a big deal since it will still use the previously deployed commit as a cache, so most deploys will still be very fast and no one's waiting on deploys in the same way as we wait on PRs and dev deploys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11999:93,cache,cache,93,https://hail.is,https://github.com/hail-is/hail/pull/11999,8,['cache'],"['cache', 'caches']"
Performance,"Based on #9076. This PR caches input files at `/cache` on the worker. I do not support wildcard characters in filenames. I had to change the Flock implementation to only lock the directories and not a file if a file name is given. This is because we don't know if a user means a file or directory a priori. For example, `gs://jigold/test`. Is that a directory test or a file test? I figured more coarse-grained locking was fine.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095:24,cache,caches,24,https://hail.is,https://github.com/hail-is/hail/pull/9095,2,['cache'],"['cache', 'caches']"
Performance,"Basic abstraction of element-wise operations between BlockMatrix objects and ""small"" values (scalars, NDArrays). Major additions include:; - Addition of element type to BlockMatrixType; - BlockMatrixMap2 which applies a binary op to two BlockMatrixIR nodes of the same shape. This sets up groundwork for element-wise operations, though in the long term can be phased out and replaced with an OuterProduct/Map.; - BlockMatrixBroadcast which wraps a BlockMatrixIR to give it a new shape. Broadcasts are never actually realized and are matched against in the execute of BlockMatrixMap2 so the appropriate BlockMatrix method can be called to perform the broadcast operation. Since the only supported values that can be broadcast are scalars, row vectors and column vectors, there is a corresponding enumeration to indicate the direction of broadcast. This can be generalized to an arbitrary index expression when higher-dimensional broadcasts/transformations are supported.; - ValueToBlockMatrix node that takes any IR to a BlockMatrixIR. Currently only supports a F64 and MakeArray. As this could generalize to any IR node that reduces to a tensor, all BlockMatrixType fields are not expected to be derived values of the interpreted IR and left as inputs to the ValueToBlockMatrix IR node. ### Workarounds; - MakeArrays are used to wrap vector values that came from NDArrays. Since ValueToBlockMatrix requires a shape and the BlockMatrix interface can only construct matrices given a 1-D array of data, I just flatten 2D arrays on the python side.; - To satisfy the BlockMatrix interface, some row/col vectors need to be interpreted to arrays and some need to be further constructed into BlockMatrix instances. ### Remaining tasks; - Implement a BlockMatrixMap for Unary ops; - Use IR instead of ApplyBinaryPrimOp for BlockMatrixMap2; - Update Typecheck for IR in Map nodes and children IR nodes in ValueToBlockMatrix; - Test methods to check evaluation of Apply*Op on BlockMatrices",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5220:638,perform,perform,638,https://hail.is,https://github.com/hail-is/hail/pull/5220,1,['perform'],['perform']
Performance,"Batches that have a `pr` attribute now link back to the corresponding PR page. Also fixed a latent bug where we were only loading batches for a PR based on the PR number, so if we had multiple watched branches you could display batches for PRs of the same number across branches.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10295:122,load,loading,122,https://hail.is,https://github.com/hail-is/hail/pull/10295,1,['load'],['loading']
Performance,"Before open batches, the `n_jobs` of a batch was a constant known before any jobs were added. Moreover, we did not start scheduling jobs until all the jobs were added to the database. Therefore, it was always safe to assume that the final ""bunch"" of jobs in the database was the last ""bunch"" ergo it spanned from its `start_job_id` to the job with id `n_jobs` (nb: job ids are 1-indexed). When open batches were added, the `n_jobs` became a mutable value. Moreover, `n_jobs` includes jobs in bunches *which have not yet been added to the database*. In particular, suppose two clients are each submitting a bunch of size 10. Each client independently ""reserves"" 10 job slots by atomically incrementing `n_jobs` by ten. `n_jobs` is now 20. Further suppose that the first bunch is added to the database and begins scheduling before the second bunch is added to the database. In this case, when calculating the size of this bunch (for use in the bunch cache, and *only* in the bunch cache), we see that this is the last (and only) bunch in the database and assume that `n_jobs` is the last job id in this bunch. This is incorrect because `n_jobs` includes the not-yet-visible second bunch.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13399:948,cache,cache,948,https://hail.is,https://github.com/hail-is/hail/pull/13399,2,['cache'],['cache']
Performance,"Ben submitted a pipeline where the first 85% of jobs run in us-central1 while the last 15% run in us-east1. The autoscaler only looks at the head of the job queue and then sorts the result set to figure out the regions to spin up instances in. The scheduler looks at the entire job queue and then sorts the result set to figure out the regions to spin up instances in. The sort order placed us-east1 before us-central1. Concretely, the autoscaler is spinning up instances in us-central1 only while the scheduler is trying to schedule jobs in us-east1. This PR attempts to solve this problem by placing bounds on which jobs the scheduler can look at based on the records the autoscaler actually considered. This is a bit of a hack and I'm worried about the performance implications. On Ben's pipeline with 100K jobs, this will add 0.3 seconds per user considered by the autoscaler. However, the scheduler query got 5x faster with the bounds in place (0.05 seconds vs 0.25 seconds). ```; mysql> EXPLAIN SELECT jobs.job_id, spec, cores_mcpu, regions_bits_rep, time_ready; -> FROM jobs FORCE INDEX(jobs_batch_id_state_always_run_cancelled); -> LEFT JOIN jobs_telemetry ON jobs.batch_id = jobs_telemetry.batch_id AND jobs.job_id = jobs_telemetry.job_id; -> WHERE jobs.batch_id = BATCH_ID AND (jobs.batch_id < BATCH_ID OR (jobs.batch_id = BATCH_ID AND jobs.job_id <= 15000)) AND inst_coll = ""standard"" AND jobs.state = 'Ready' AND always_run = 0 AND cancelled = 0; -> ORDER BY jobs.batch_id, inst_coll, state, always_run, -n_regions DESC, regions_bits_rep, jobs.job_id; -> LIMIT 300;; +----+-------------+----------------+------------+--------+------------------------------------------+------------------------------------------+---------+-------------------------+-------+----------+----------------------------------------------------+; | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |; +----+-------------+----------------+------------+---",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13268:157,queue,queue,157,https://hail.is,https://github.com/hail-is/hail/pull/13268,3,"['perform', 'queue']","['performance', 'queue']"
Performance,Big aggregator performance gainz,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/954:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/954,1,['perform'],['performance']
Performance,"Bigger than I expected, but:; 1. Re-enable the FS tests and create a Gradle target for them so they can be run locally.; 2. Allow the FS tests to be easily used locally by not hardcoding a particular key file path.; 3. Skip GoogleStorageFSSuite when `CLOUD` is not `gcp`; 4. Remove irrelevant env vars from non-FS Scala tests.; 5. Eliminate the ""hail_repl"" image and deployment which was scoped dev anyway and never used.; 6. Add hail_pip_installed_image which can be used to execute `hailtop.aiotools.copy`.; 7. Use copy in two places in build.yaml.; 8. Add a command line argument for configuring the number of concurrent transfers which sets an upper bound on the number of open source files (and, additionally, open destination files). On my MacBook, I can't seem to open 100 local files simultaneously. I set the default low enough that local use should work by default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11089:613,concurren,concurrent,613,https://hail.is,https://github.com/hail-is/hail/pull/11089,1,['concurren'],['concurrent']
Performance,"Bit packing is not limited to biallelic datasets. In the biallelic case, we can represent the three states as:. | state | bits |; | ----- | -----|; | homRef | `0`/`0` |; | het | `0`/`1` |; | homVar | `1`/`1` |. We can play the same trick with two bits per allele:. | state | bits |; | --- | --- |; | A/A | `00`/`00` |; | A/G | `00`/`01` |; | A/C | `00`/`10` |; | A/T | `00`/`11` |; | | |; | G/G | `01`/`01` |; | G/C | `01`/`10` |; | G/T | `01`/`11` |; | | |; | C/C | `10`/`10` |; | C/T | `10`/`11` |; | | |; | T/T | `11`/`11` |; | | |; | NA | `01`/`00` or `10`/`00` or `11`/`00` |. We can't play the same trick with three bits per allele (six per genotype) because the SSE/AVX registers cannot perform the shift appropriately for non-byte-aligned data. So the next step would be four bits per allele (eight per genotype) which allows 16 total alleles. For each bit-count, the algorithm is exactly the same, but with shifts changed. We should write a little DSL for specifying these bit operations which automatically generates C code for various allele counts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1082:694,perform,perform,694,https://hail.is,https://github.com/hail-is/hail/issues/1082,1,['perform'],['perform']
Performance,"Both of these were noted by Bernick as a part of his security review. 1. `local_infile`: if this is on, a client could in theory read any file on the instance by loading it into a table.; 2. `skip_show_database`: this disables `SHOW DATABASES` by default; we can still grant certain users (e.g. the admin-pod) the `SHOW DATABASES` privilege. A bit of security by obscurity, IMO, but it does not bother me much. I can always see the list of databases via the GCP console.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12835:162,load,loading,162,https://hail.is,https://github.com/hail-is/hail/pull/12835,1,['load'],['loading']
Performance,"Brought up by TJ in a recent conversation. He wants to not use the browser to work on Jupyter notebooks for performance / IDE convenience reasons. From a brief look, there appear to be two issues in getting this to work. First, VS Code will need to be started with proxy flags. As its runtime is Electron, all Chromium flags will work, so could almost specify HAILCTL_CHROME=code hailctl connect ... , but this doesn't directly work because VS Code also needs a workspace (so the cli invocation will need to be slightly different). Second, password-less may not work without `disable_xsrf_check`. Relevant issue: https://github.com/microsoft/vscode-python/issues/7137. There may be ways to hijack a proxied localhost connection, so unless we fully understand those issues, if disable_xsrf_check is necessary to enable password-less, it would be better to generate a token.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9067:108,perform,performance,108,https://hail.is,https://github.com/hail-is/hail/issues/9067,1,['perform'],['performance']
Performance,"Building on #4487, removes `IntervalTree`, replacing its remaining uses (mostly in `RVDPartitioner`) with an optimized generic binary search implementation. The key observation is that standard binary search works on an array of intervals such that all left endpoints are non-decreasing, as are all left endpoints, using in the binary search the non-standard ordering on intervals that compares I < J if I is completely below J (without overlap). In this ordering ""equality"" (defined as neither less than nor greater) means overlapping. This should speed up all partitioner queries.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4511:109,optimiz,optimized,109,https://hail.is,https://github.com/hail-is/hail/pull/4511,1,['optimiz'],['optimized']
Performance,"Builds on #2236. . Implements a LoadMatrix function that loads a VariantSampleMatrix from TSV of [rowID, ints... ] with a header containing column IDs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2246:32,Load,LoadMatrix,32,https://hail.is,https://github.com/hail-is/hail/pull/2246,2,"['Load', 'load']","['LoadMatrix', 'loads']"
Performance,"Builds on @jigold's PR: https://github.com/hail-is/hail/pull/4582. First pipeline executed via API!. Start the server:. ```; $ hail hail/python/hail-apiserver/hail-apiserver.py; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-cc8ca5cfae35; * Serving Flask app ""hail-apiserver"" (lazy loading); * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit); ```. Run a hail pipeline:. ```; $ hail; >>> import hail as hl; >>> hl.init(_backend=hl.backend.ServiceBackend('localhost', 5000)); Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-cc8ca5cfae35; >>> t = hl.Table.parallelize([hl.struct(a=1), hl.struct(a=5)]); >>> t.count(); 2; >>> ; ```. and on the server logs:. ```; 2018-10-19 22:36:18 Hail: INFO: execute: (TableCount (TableParallelize None (MakeArray None (MakeStruct (a (I32 1))) (MakeStruct (a (I32 5)))))); 2018-10-19 22:36:18 Hail: INFO: result: {'type': 'int64', 'value': '2'}; 127.0.0.1 - - [19/Oct/2018 22:36:18] ""POST /execute HTTP/1.1"" 200 -; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4589:322,load,loading,322,https://hail.is,https://github.com/hail-is/hail/pull/4589,1,['load'],['loading']
Performance,"Builds on: https://github.com/hail-is/hail/pull/2074. Added optimized unsafe row add to region value builder. Tests are faster than toward_fullgeneric_4, 0.1 (8m2s vs 9m18s, 0.1: 8m20s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2081:60,optimiz,optimized,60,https://hail.is,https://github.com/hail-is/hail/pull/2081,1,['optimiz'],['optimized']
Performance,"Builds on: https://github.com/hail-is/hail/pull/2228. It is smaller, cleaner and more self-contained, but I could still break it into more pieces if needed. Some remarks:; - I removed the BroadcastTypeTree nonsense. This will kill KeyTable joins but we can fix that later.; - I sample keys as part of collecting the partition key info.; - I left off two features off the key ranges sampler vs OrderedRDD: I don't resample large partitions and I don't use weights to calculate the ranges.; - I included the minimal LoadVCF interface changes. VCF parser will come as a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2236:514,Load,LoadVCF,514,https://hail.is,https://github.com/hail-is/hail/pull/2236,1,['Load'],['LoadVCF']
Performance,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2301:261,Cache,Cache,261,https://hail.is,https://github.com/hail-is/hail/pull/2301,1,['Cache'],['Cache']
Performance,Builds on: https://github.com/hail-is/hail/pull/2563. Didn't stack (yet). Had to remove the requiredness on gs since I can't produce it in the IR (minor). Fixed two bugs along the way: wrong TypeInfo in ArrayMap and loadPrimitive wasn't loading the array address.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2564:216,load,loadPrimitive,216,https://hail.is,https://github.com/hail-is/hail/pull/2564,2,['load'],"['loadPrimitive', 'loading']"
Performance,"Builds on: https://github.com/hail-is/hail/pull/5004. Convert all operations in table.py to IR (if possible). Here are the things that remain in order to get rid of Table._jt in table.py. Rewrite in Python:; - expandTypes; - flatten; - collectJSON: use aggregate/collect (@tpoterba, do you feel this will be significantly slower now?); - showString: rewrite in Python in terms of collect. Add IR:; - intervalJoin; - same; - groupByKey. Should only work with SparkBackend:; - toDF. Hmm:; - forceCount: remove? add force option to TableCount that disables optimization?; - nPartitions; - filterPartitions; - persist, unpersist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5015:554,optimiz,optimization,554,https://hail.is,https://github.com/hail-is/hail/pull/5015,1,['optimiz'],['optimization']
Performance,"Bumps [aiohttp](https://github.com/aio-libs/aiohttp) from 3.9.3 to 3.9.4.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiohttp/releases"">aiohttp's releases</a>.</em></p>; <blockquote>; <h2>3.9.4</h2>; <h2>Bug fixes</h2>; <ul>; <li>; <p>The asynchronous internals now set the underlying causes; when assigning exceptions to the future objects; -- by :user:<code>webknjaz</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8089"">#8089</a>.</p>; </li>; <li>; <p>Treated values of <code>Accept-Encoding</code> header as case-insensitive when checking; for gzip files -- by :user:<code>steverep</code>.</p>; <p><em>Related issues and pull requests on GitHub:</em>; <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8104"">#8104</a>.</p>; </li>; <li>; <p>Improved the DNS resolution performance on cache hit -- by :user:<code>bdraco</code>.</p>; <p>This is achieved by avoiding an :mod:<code>asyncio</code> task creation in this case.</p>; <p><em>Related issues and pull requests on GitHub:</em>; <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/8163"">#8163</a>.</p>; </li>; <li>; <p>Changed the type annotations to allow <code>dict</code> on :meth:<code>aiohttp.MultipartWriter.append</code>,; :meth:<code>aiohttp.MultipartWriter.append_json</code> and; :meth:<code>aiohttp.MultipartWriter.append_form</code> -- by :user:<code>cakemanny</code></p>; <p><em>Related issues and pull requests on GitHub:</em>; <a href=""https://redirect.github.com/aio-libs/aiohttp/issues/7741"">#7741</a>.</p>; </li>; <li>; <p>Ensure websocket transport is closed when client does not close it; -- by :user:<code>bdraco</code>.</p>; <p>The transport could remain open if the client did not close it. This; change ensures the transport is closed when the client does not close; it.</p>; </li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14477:925,perform,performance,925,https://hail.is,https://github.com/hail-is/hail/pull/14477,12,"['cache', 'perform']","['cache', 'performance']"
Performance,"Bumps [aiorwlock](https://github.com/aio-libs/aiorwlock) from 1.0.0 to 1.3.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiorwlock/releases"">aiorwlock's releases</a>.</em></p>; <blockquote>; <h2>aiorwlock 1.2.0</h2>; <h1>Changes</h1>; <ul>; <li>Fix a bug that makes concurrent writes possible under some (rare) conjunctions (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/235"">#235</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/aio-libs/aiorwlock/blob/master/CHANGES.rst"">aiorwlock's changelog</a>.</em></p>; <blockquote>; <p>1.3.0 (2022-1-18); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Dropped Python 3.6 support</li>; <li>Python 3.10 is officially supported</li>; <li>Drop deprecated <code>loop</code> parameter from <code>RWLock</code> constructor</li>; </ul>; <p>1.2.0 (2021-11-09); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Fix a bug that makes concurrent writes possible under some (rare) conjunctions (<a href=""https://github-redirect.dependabot.com/aio-libs/aiorwlock/issues/235"">#235</a>)</li>; </ul>; <p>1.1.0 (2021-09-27); ^^^^^^^^^^^^^^^^^^</p>; <ul>; <li>Remove explicit loop usage in <code>asyncio.sleep()</code> call, make the library forward; compatible with Python 3.10</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/6599d10ba16f95f19d5b5963a00aa857bc98f656""><code>6599d10</code></a> Bump to 1.3.0</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/d4b41f54b57caf316c41c3973ab82bd53a418ff8""><code>d4b41f5</code></a> Drop deprecated 'loop' parameter from RWLock constructor</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/3edb2a1bc1636832df12671f035e21dd74440824""><code>3edb2a1</code></a> Fix tests</li>; <li><a href=""https://github.com/aio-libs/aiorwlock/commit/45a7418474a55defe9c53fd8e38df60af514cf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11514:326,concurren,concurrent,326,https://hail.is,https://github.com/hail-is/hail/pull/11514,1,['concurren'],['concurrent']
Performance,"Bumps [botocore](https://github.com/boto/botocore) from 1.24.13 to 1.24.14.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/boto/botocore/blob/develop/CHANGELOG.rst"">botocore's changelog</a>.</em></p>; <blockquote>; <h1>1.24.14</h1>; <ul>; <li>api-change:<code>chime-sdk-meetings</code>: Adds support for Transcribe language identification feature to the StartMeetingTranscription API.</li>; <li>api-change:<code>ecs</code>: Amazon ECS UpdateService API now supports additional parameters: loadBalancers, propagateTags, enableECSManagedTags, and serviceRegistries</li>; <li>api-change:<code>migration-hub-refactor-spaces</code>: AWS Migration Hub Refactor Spaces documentation update.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/botocore/commit/5c6f8c8d8e6c5ed05b05302ba9ef83cc2f0c420f""><code>5c6f8c8</code></a> Merge branch 'release-1.24.14'</li>; <li><a href=""https://github.com/boto/botocore/commit/3042265ca9488b8d73c6442f703337309d6733a4""><code>3042265</code></a> Bumping version to 1.24.14</li>; <li><a href=""https://github.com/boto/botocore/commit/ba0d095eeb62a2a293abadb54111df5fc0e2f0c8""><code>ba0d095</code></a> Update to latest models</li>; <li><a href=""https://github.com/boto/botocore/commit/a8c5cc855ecb91f5f64d73f2a15dfebc9e5e20e0""><code>a8c5cc8</code></a> Merge branch 'release-1.24.13' into develop</li>; <li>See full diff in <a href=""https://github.com/boto/botocore/compare/1.24.13...1.24.14"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=botocore&package-manager=pip&previous-version=1.24.13&new-version=1.24.14)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11534:532,load,loadBalancers,532,https://hail.is,https://github.com/hail-is/hail/pull/11534,1,['load'],['loadBalancers']
Performance,"Bumps [cryptography](https://github.com/pyca/cryptography) from 41.0.2 to 41.0.3.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pyca/cryptography/blob/main/CHANGELOG.rst"">cryptography's changelog</a>.</em></p>; <blockquote>; <p>41.0.3 - 2023-08-01</p>; <pre><code>; * Fixed performance regression loading DH public keys.; * Fixed a memory leak when using; :class:`~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305`.; * Updated Windows, macOS, and Linux wheels to be compiled with OpenSSL 3.1.2.; <p>.. _v41-0-2:; </code></pre></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/b22271cf3c3dd8dc8978f8f4b00b5c7060b6538d""><code>b22271c</code></a> bump for 41.0.3 (<a href=""https://redirect.github.com/pyca/cryptography/issues/9330"">#9330</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/774a4a16cbd22a89fdb4195ade9e4fcee27a7afa""><code>774a4a1</code></a> Only check DH key validity when loading a private key. (<a href=""https://redirect.github.com/pyca/cryptography/issues/9071"">#9071</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/9319"">#9319</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/bfa4d95f0f356f2d535efd5c775e0fb3efe90ef2""><code>bfa4d95</code></a> changelog for 41.0.3 (<a href=""https://redirect.github.com/pyca/cryptography/issues/9320"">#9320</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/0da7165aa73c0a4865b0a4d9e019db3c16eea55a""><code>0da7165</code></a> backport fix the memory leak in fixedpool (<a href=""https://redirect.github.com/pyca/cryptography/issues/9272"">#9272</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/9309"">#9309</a>)</li>; <li>See full diff in <a href=""https://github.com/pyca/cryptography/compare/41.0.2...41.0.3"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.gith",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13357:318,perform,performance,318,https://hail.is,https://github.com/hail-is/hail/pull/13357,9,"['load', 'perform']","['loading', 'performance']"
Performance,"Bumps [cryptography](https://github.com/pyca/cryptography) from 41.0.5 to 41.0.6.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pyca/cryptography/blob/main/CHANGELOG.rst"">cryptography's changelog</a>.</em></p>; <blockquote>; <p>41.0.6 - 2023-11-27</p>; <pre><code>; * Fixed a null-pointer-dereference and segfault that could occur when loading; certificates from a PKCS#7 bundle. Credit to **pkuzco** for reporting the; issue. **CVE-2023-49083**; <p>.. _v41-0-5:; </code></pre></p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/f09c261ca10a31fe41b1262306db7f8f1da0e48a""><code>f09c261</code></a> 41.0.6 release (<a href=""https://redirect.github.com/pyca/cryptography/issues/9927"">#9927</a>)</li>; <li>See full diff in <a href=""https://github.com/pyca/cryptography/compare/41.0.5...41.0.6"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=cryptography&package-manager=pip&previous-version=41.0.5&new-version=41.0.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14046:380,load,loading,380,https://hail.is,https://github.com/hail-is/hail/pull/14046,3,['load'],['loading']
Performance,"Bumps [cryptography](https://github.com/pyca/cryptography) from 42.0.2 to 42.0.4.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/pyca/cryptography/blob/main/CHANGELOG.rst"">cryptography's changelog</a>.</em></p>; <blockquote>; <p>42.0.4 - 2024-02-20</p>; <pre><code>; * Fixed a null-pointer-dereference and segfault that could occur when creating; a PKCS#12 bundle. Credit to **Alexander-Programming** for reporting the; issue. **CVE-2024-26130**; * Fixed ASN.1 encoding for PKCS7/SMIME signed messages. The fields ``SMIMECapabilities``; and ``SignatureAlgorithmIdentifier`` should now be correctly encoded according to the; definitions in :rfc:`2633` :rfc:`3370`.; <p>.. _v42-0-3:</p>; <p>42.0.3 - 2024-02-15; </code></pre></p>; <ul>; <li>Fixed an initialization issue that caused key loading failures for some; users.</li>; </ul>; <p>.. _v42-0-2:</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/fe18470f7d05f963e7267e34fdf985d81ea6ceea""><code>fe18470</code></a> Bump for 42.0.4 release (<a href=""https://redirect.github.com/pyca/cryptography/issues/10445"">#10445</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/aaa2dd06ed470695de818405a982d4c459869803""><code>aaa2dd0</code></a> Fix ASN.1 issues in PKCS#7 and S/MIME signing (<a href=""https://redirect.github.com/pyca/cryptography/issues/10373"">#10373</a>) (<a href=""https://redirect.github.com/pyca/cryptography/issues/10442"">#10442</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/7a4d012991061974da5d9cb7614de65eac94f49b""><code>7a4d012</code></a> Fixes <a href=""https://redirect.github.com/pyca/cryptography/issues/10422"">#10422</a> -- don't crash when a PKCS#12 key and cert don't match (<a href=""https://redirect.github.com/pyca/cryptography/issues/10423"">#10423</a>) ...</li>; <li><a href=""https://github.com/pyca/cryptography/commit/df314bb182bdfd661333969a94325e4680d785f6""><",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14332:828,load,loading,828,https://hail.is,https://github.com/hail-is/hail/pull/14332,3,['load'],['loading']
Performance,"Bumps [de.undercouch.download](https://github.com/michel-kraemer/gradle-download-task) from 5.3.0 to 5.3.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/michel-kraemer/gradle-download-task/releases"">de.undercouch.download's releases</a>.</em></p>; <blockquote>; <h2>5.3.1</h2>; <p>Bug fixes:</p>; <ul>; <li>Downgrade slf4j to fix warning on console about missing slf4j provider</li>; <li>Allow <code>download</code> and <code>verify</code> extensions to be created on demand in custom tasks, so these tasks can be made compatible with Gradle's configuration cache (see <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/284"">#284</a>). Thanks to <a href=""https://github.com/liblit""><code>@​liblit</code></a> for testing!</li>; </ul>; <p>Maintenance:</p>; <ul>; <li>Update dependencies</li>; <li>Improve documentation</li>; <li>Add integration tests for Gradle 6.9.3 and 7.6</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/a0374fc7c895ae53309ea351e989571204e0ea5f""><code>a0374fc</code></a> Bump up version number to 5.3.1</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/612f57a382b8640cc730dc5e75d1c809e3e772bd""><code>612f57a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/michel-kraemer/gradle-download-task/issues/291"">#291</a> from michel-kraemer/dependabot/npm_and_yarn/screencas...</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/53af1049f5514afe58e884d487d7c57dae47759d""><code>53af104</code></a> Bump http-cache-semantics from 4.1.0 to 4.1.1 in /screencast</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task/commit/398c14c05c6448b380ac35c6095598299c5e23c5""><code>398c14c</code></a> Update dependencies</li>; <li><a href=""https://github.com/michel-kraemer/gradle-download-task",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12707:607,cache,cache,607,https://hail.is,https://github.com/hail-is/hail/pull/12707,1,['cache'],['cache']
Performance,"Bumps [google-auth](https://github.com/googleapis/google-auth-library-python) from 1.27.0 to 2.6.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/googleapis/google-auth-library-python/releases"">google-auth's releases</a>.</em></p>; <blockquote>; <h2>v2.6.0</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/962"">#962</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/52c8ef90058120d7d04d3d201adc111664be526c"">52c8ef9</a>)</li>; </ul>; <h3>Bug Fixes</h3>; <ul>; <li>revert &quot;feat: add api key support (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/826"">#826</a>)&quot; (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/964"">#964</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/f9f23f4370f2a7a5b2c66ee56a5e700ef03b5b06"">f9f23f4</a>)</li>; </ul>; <h2>v2.5.0</h2>; <h3>Features</h3>; <ul>; <li>ADC can load an impersonated service account credentials. (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/956"">#956</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/a8eb4c8693055a3420cfe9c3420aae2bc8cd465a"">a8eb4c8</a>)</li>; </ul>; <h2>v2.4.1</h2>; <h3>Bug Fixes</h3>; <ul>; <li>urllib3 import (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/953"">#953</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/commit/c8b5cae3da5eb9d40067d38dac51a4a8c1e0763e"">c8b5cae</a>)</li>; </ul>; <h2>v2.4.0</h2>; <h3>Features</h3>; <ul>; <li>add 'py.typed' declaration (<a href=""https://github-redirect.dependabot.com/googleapis/google-auth-library-python/issues/919"">#919</a>) (<a href=""https://github.com/googleapis/google-auth-library-python/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11546:348,load,load,348,https://hail.is,https://github.com/hail-is/hail/pull/11546,1,['load'],['load']
Performance,"Bumps [importlib-metadata](https://github.com/python/importlib_metadata) from 3.10.1 to 4.12.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/python/importlib_metadata/blob/main/CHANGES.rst"">importlib-metadata's changelog</a>.</em></p>; <blockquote>; <h1>v4.12.0</h1>; <ul>; <li>py-93259: Now raise <code>ValueError</code> when <code>None</code> or an empty; string are passed to <code>Distribution.from_name</code> (and other; callers).</li>; </ul>; <h1>v4.11.4</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/379"">#379</a>: In <code>PathDistribution._name_from_stem</code>, avoid including; parts of the extension in the result.</li>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/381"">#381</a>: In <code>PathDistribution._normalized_name</code>, ensure names; loaded from the stem of the filename are also normalized, ensuring; duplicate entry points by packages varying only by non-normalized; name are hidden.</li>; </ul>; <h1>v4.11.3</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/372"">#372</a>: Removed cast of path items in FastPath, not needed.</li>; </ul>; <h1>v4.11.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/369"">#369</a>: Fixed bug where <code>EntryPoint.extras</code> was returning; match objects and not the extras strings.</li>; </ul>; <h1>v4.11.1</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/367"">#367</a>: In <code>Distribution.requires</code> for egg-info, if <code>requires.txt</code>; is empty, return an empty list.</li>; </ul>; <h1>v4.11.0</h1>; <ul>; <li>bpo-46246: Added <code>__slots__</code> to <code>EntryPoints</code>.</li>; </ul>; <h1>v4.10.2</h1>; <ul>; <li><a href=""https://github-redirect.dependabot.com/python/importlib_metadata/issues/365"">#365</a> and bpo-46546: Avoid leakin",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12000:895,load,loaded,895,https://hail.is,https://github.com/hail-is/hail/pull/12000,1,['load'],['loaded']
Performance,"Bumps [orjson](https://github.com/ijl/orjson) from 3.6.4 to 3.6.7.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.6.7</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd64 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number reporting in; <code>JSONDecodeError</code>.</li>; <li>Fix <code>orjson.OPT_STRICT_INTEGER</code> not raising an error on; values exceeding a 64-bit integer maximum.</li>; </ul>; <h2>3.6.5</h2>; <h3>Fixed</h3>; <ul>; <li>Fix build on macOS aarch64 CPython 3.10.</li>; <li>Fix build issue on 32-bit.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.6.7 - 2022-02-14</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of deserializing almost-empty documents.</li>; <li>Publish arm7l <code>manylinux_2_17</code> wheels to PyPI.</li>; <li>Publish amd4 <code>musllinux_1_1</code> wheels to PyPI.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix build requiring <code>python</code> on <code>PATH</code>.</li>; </ul>; <h2>3.6.6 - 2022-01-21</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing <code>datetime.datetime</code> using <code>tzinfo</code> that; are <code>zoneinfo.ZoneInfo</code>.</li>; </ul>; <h3>Fixed</h3>; <ul>; <li>Fix invalid indexing in line and column number",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11572:281,perform,performance,281,https://hail.is,https://github.com/hail-is/hail/pull/11572,2,['perform'],['performance']
Performance,"Bumps [orjson](https://github.com/ijl/orjson) from 3.9.10 to 3.10.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.10.0</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</li>; <li>sdist uses metadata 2.3 instead of 2.1.</li>; <li>Improve Windows PyPI builds.</li>; </ul>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.10.0 - 2024-03-27</h2>; <h3>Changed</h3>; <ul>; <li>Support serializing <code>numpy.float16</code> (<code>numpy.half</code>).</",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14427:545,load,loads,545,https://hail.is,https://github.com/hail-is/hail/pull/14427,1,['load'],['loads']
Performance,"Bumps [orjson](https://github.com/ijl/orjson) from 3.9.10 to 3.9.15.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/releases"">orjson's releases</a>.</em></p>; <blockquote>; <h2>3.9.15</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Build now depends on Rust 1.72 or later.</li>; </ul>; <h2>3.9.13</h2>; <h3>Fixed</h3>; <ul>; <li>Serialization <code>str</code> escape uses only 128-bit SIMD.</li>; <li>Fix compatibility with CPython 3.13 alpha 3.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Publish <code>musllinux_1_2</code> instead of <code>musllinux_1_1</code> wheels.</li>; <li>Serialization uses small integer optimization in CPython 3.12 or later.</li>; </ul>; <h2>3.9.12</h2>; <h3>Fixed</h3>; <ul>; <li>Minimal <code>musllinux_1_1</code> build due to sporadic CI failure.</li>; </ul>; <h3>Changed</h3>; <ul>; <li>Update benchmarks in README.</li>; </ul>; <h2>3.9.11</h2>; <h3>Changed</h3>; <ul>; <li>Improve performance of serializing. <code>str</code> is significantly faster. Documents; using <code>dict</code>, <code>list</code>, and <code>tuple</code> are somewhat faster.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/ijl/orjson/blob/master/CHANGELOG.md"">orjson's changelog</a>.</em></p>; <blockquote>; <h2>3.9.15 - 2024-02-23</h2>; <h3>Fixed</h3>; <ul>; <li>Implement recursion limit of 1024 on <code>orjson.loads()</code>.</li>; <li>Use byte-exact read on <code>str</code> formatting SIMD path to avoid crash.</li>; </ul>; <h2>3.9.14 - 2024-02-14</h2>; <h3>Fixed</h3>; <ul>; <li>Fix crash serializing <code>str</code> introduced in 3.9.11.</li>; </ul>; <h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14357:324,load,loads,324,https://hail.is,https://github.com/hail-is/hail/pull/14357,6,"['load', 'optimiz']","['loads', 'optimization']"
Performance,"Bumps [pandas](https://github.com/pandas-dev/pandas) from 1.3.0 to 1.4.1.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.4.1</h2>; <p>This is the first patch release in the 1.4.x series and includes some regression fixes and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.1/whatsnew/v1.4.1.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.0/whatsnew/v1.4.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <pre><code>conda install -c conda-forge pandas; </code></pre>; <p>Or via PyPI:</p>; <pre><code>python3 -m pip install --upgrade pandas; </code></pre>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.4.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.4.0. If all goes well, we'll release pandas 1.4.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4/whatsnew/v1.4.0.html"">whatsnew</a> for a list of all the changes. pandas 1.4.0 supports Python 3.8 and higher.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11539:1012,perform,performance,1012,https://hail.is,https://github.com/hail-is/hail/pull/11539,1,['perform'],['performance']
Performance,"Bumps [pandas](https://github.com/pandas-dev/pandas) from 1.3.5 to 1.5.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>; <blockquote>; <h2>Pandas 1.5.0</h2>; <p>This release includes some new features, bug fixes, and performance improvements. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html"">full whatsnew</a> for a list of all the changes. pandas 1.5.0 supports Python 3.8 and higher.</p>; <p>The release will be available on the defaults and conda-forge channels:</p>; <p><code>conda install -c conda-forge pandas</code></p>; <p>Or via PyPI:</p>; <p><code>python3 -m pip install --upgrade pandas</code></p>; <p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>; <h2>Pandas 1.5.0rc0</h2>; <p>We are pleased to announce a release candidate for pandas 1.5.0. If all goes well, we'll release pandas 1.5.0 in about two weeks.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.5/whatsnew/v1.5.0.html"">whatsnew</a> for a list of all the changes.</p>; <p>The release will be available on conda-forge and PyPI.</p>; <p>The release can be installed from PyPI</p>; <pre><code>python -m pip install --upgrade --pre pandas==1.5.0rc0; </code></pre>; <p>Or from conda-forge</p>; <pre><code>conda install -c conda-forge/label/pandas_rc pandas==1.5.0rc0; </code></pre>; <p>Please report any issues with the release candidate on the pandas issue tracker.</p>; <h2>Pandas 1.4.4</h2>; <p>This is a patch release in the 1.4.x series and includes some regression and bug fixes. We recommend that all users upgrade to this version.</p>; <p>See the <a href=""https://pandas.pydata.org/pandas-docs/version/1.4.4/whatsnew/v1.4.4.html"">full whatsnew</a> for a list of all the changes.</p>; <p>The release will ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12292:325,perform,performance,325,https://hail.is,https://github.com/hail-is/hail/pull/12292,1,['perform'],['performance']
Performance,"Bumps [protobuf](https://github.com/protocolbuffers/protobuf) from 3.19.6 to 4.21.12.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/protocolbuffers/protobuf/releases"">protobuf's releases</a>.</em></p>; <blockquote>; <h2>Protocol Buffers v3.20.3</h2>; <h1>Java</h1>; <ul>; <li>Refactoring java full runtime to reuse sub-message builders and prepare to; migrate parsing logic from parse constructor to builder.</li>; <li>Move proto wireformat parsing functionality from the private &quot;parsing; constructor&quot; to the Builder class.</li>; <li>Change the Lite runtime to prefer merging from the wireformat into mutable; messages rather than building up a new immutable object before merging. This; way results in fewer allocations and copy operations.</li>; <li>Make message-type extensions merge from wire-format instead of building up; instances and merging afterwards. This has much better performance.</li>; <li>Fix TextFormat parser to build up recurring (but supposedly not repeated); sub-messages directly from text rather than building a new sub-message and; merging the fully formed message into the existing field.</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-h4h5-3hr4-j3g2"">Security Advisory for Java users</a></li>; </ul>; <h2>Protocol Buffers v3.20.2</h2>; <h1>C++</h1>; <ul>; <li>Reduce memory consumption of MessageSet parsing</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-8gq9-2x98-w8hf"">Security Advisory for C++ and Python users</a></li>; </ul>; <h2>Protocol Buffers v3.20.1</h2>; <h1>PHP</h1>; <ul>; <li>Fix building packaged PHP extension (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9727"">#9727</a>)</li>; <li>Fixed composer.json to only advertise compatibility with PHP 7.0+. (<a href=""https://github-redirect.dependabot.com/protocolbuffers/protobuf/issues/9",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12563:942,perform,performance,942,https://hail.is,https://github.com/hail-is/hail/pull/12563,1,['perform'],['performance']
Performance,"Bumps [protobuf](https://github.com/protocolbuffers/protobuf) from 3.20.2 to 4.21.9.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/protocolbuffers/protobuf/releases"">protobuf's releases</a>.</em></p>; <blockquote>; <h2>Protocol Buffers v3.20.3</h2>; <h1>Java</h1>; <ul>; <li>Refactoring java full runtime to reuse sub-message builders and prepare to; migrate parsing logic from parse constructor to builder.</li>; <li>Move proto wireformat parsing functionality from the private &quot;parsing; constructor&quot; to the Builder class.</li>; <li>Change the Lite runtime to prefer merging from the wireformat into mutable; messages rather than building up a new immutable object before merging. This; way results in fewer allocations and copy operations.</li>; <li>Make message-type extensions merge from wire-format instead of building up; instances and merging afterwards. This has much better performance.</li>; <li>Fix TextFormat parser to build up recurring (but supposedly not repeated); sub-messages directly from text rather than building a new sub-message and; merging the fully formed message into the existing field.</li>; <li>This release addresses a <a href=""https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-h4h5-3hr4-j3g2"">Security Advisory for Java users</a></li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li>See full diff in <a href=""https://github.com/protocolbuffers/protobuf/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=protobuf&package-manager=pip&previous-version=3.20.2&new-version=4.21.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12518:941,perform,performance,941,https://hail.is,https://github.com/hail-is/hail/pull/12518,1,['perform'],['performance']
Performance,"Bumps [pycodestyle](https://github.com/PyCQA/pycodestyle) from 2.8.0 to 2.9.1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pycodestyle/blob/main/CHANGES.txt"">pycodestyle's changelog</a>.</em></p>; <blockquote>; <h2>2.9.1 (2022-08-03)</h2>; <p>Changes:</p>; <ul>; <li>E275: fix false positive for yield expressions.</li>; </ul>; <h2>2.9.0 (2022-07-30)</h2>; <p>Changes:</p>; <ul>; <li>E221, E222, E223, E224: add support for <code>:=</code> operator. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1032"">#1032</a>.</li>; <li>Drop python 2.7 / 3.5.</li>; <li>E262: consider non-breaking spaces (<code>\xa0</code>) as whitespace. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1035"">#1035</a>.</li>; <li>Improve performance of <code>_is_binary_operator</code>. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1052"">#1052</a>.</li>; <li>E275: requires whitespace around keywords. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1063"">#1063</a>.</li>; <li>Add support for python 3.11. PR <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1070"">#1070</a>.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/10a4427c75740717b43448339fcf71f11bc33d1a""><code>10a4427</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1092"">#1092</a> from PyCQA/2_9_1</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/c33e852a5938b823b04dd981260bd1664c643385""><code>c33e852</code></a> Release 2.9.1</li>; <li><a href=""https://github.com/PyCQA/pycodestyle/commit/c97e4f86bd60e449a64be6c0de5b5ec5bb28b8e9""><code>c97e4f8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/PyCQA/pycodestyle/issues/1091"">#1091</a> from asottile/E275-yield-expression</li>; <li>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12476:819,perform,performance,819,https://hail.is,https://github.com/hail-is/hail/pull/12476,1,['perform'],['performance']
Performance,"Bumps [pylint](https://github.com/PyCQA/pylint) from 2.13.4 to 2.13.5.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/PyCQA/pylint/blob/main/ChangeLog"">pylint's changelog</a>.</em></p>; <blockquote>; <h1>What's New in Pylint 2.13.5?</h1>; <p>Release date: 2022-04-06</p>; <ul>; <li>; <p>Fix false positive regression in 2.13.0 for <code>used-before-assignment</code> for; homonyms between variable assignments in try/except blocks and variables in; subscripts in comprehensions.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6069"">#6069</a>; Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6136"">#6136</a></p>; </li>; <li>; <p><code>lru-cache-decorating-method</code> has been renamed to <code>cache-max-size-none</code> and; will only be emitted when <code>maxsize</code> is <code>None</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6180"">#6180</a></p>; </li>; <li>; <p>Fix false positive for <code>unused-import</code> when disabling both <code>used-before-assignment</code> and <code>undefined-variable</code>.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/6089"">#6089</a></p>; </li>; <li>; <p>Narrow the scope of the <code>unnecessary-ellipsis</code> checker to:</p>; <ul>; <li>functions &amp; classes which contain both a docstring and an ellipsis.</li>; <li>A body which contains an ellipsis <code>nodes.Expr</code> node &amp; at least one other statement.</li>; </ul>; </li>; <li>; <p>Fix false positive for <code>used-before-assignment</code> for assignments taking place via; nonlocal declarations after an earlier type annotation.</p>; <p>Closes <a href=""https://github-redirect.dependabot.com/PyCQA/pylint/issues/5394"">#5394</a></p>; </li>; <li>; <p>Fix crash for <code>redefined-slots-in-subclass</code> when the type of the slot is not a const or a string.</p>; <p>Closes <a href=""https://github-redi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11739:744,cache,cache-decorating-method,744,https://hail.is,https://github.com/hail-is/hail/pull/11739,2,['cache'],"['cache-decorating-method', 'cache-max-size-none']"
Performance,"Bumps [scipy](https://github.com/scipy/scipy) from 1.9.3 to 1.10.0.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/scipy/scipy/releases"">scipy's releases</a>.</em></p>; <blockquote>; <h1>SciPy 1.10.0 Release Notes</h1>; <p>SciPy <code>1.10.0</code> is the culmination of <code>6</code> months of hard work. It contains; many new features, numerous bug-fixes, improved test coverage and better; documentation. There have been a number of deprecations and API changes; in this release, which are documented below. All users are encouraged to; upgrade to this release, as there are a large number of bug-fixes and; optimizations. Before upgrading, we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.10.x branch, and on adding new features on the main branch.</p>; <p>This release requires Python <code>3.8+</code> and NumPy <code>1.19.5</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>A new dedicated datasets submodule (<code>scipy.datasets</code>) has been added, and is; now preferred over usage of <code>scipy.misc</code> for dataset retrieval.</li>; <li>A new <code>scipy.interpolate.make_smoothing_spline</code> function was added. This; function constructs a smoothing cubic spline from noisy data, using the; generalized cross-validation (GCV) criterion to find the tradeoff between; smoothness and proximity to data points.</li>; <li><code>scipy.stats</code> has three new distributions, two new hypothesis tests, three; new sample statistics, a class for greater control over calculations; involving covariance matrices, and many other enhancements.</li>; </ul>; <h1>New features</h1>; <h1><code>scipy.datasets</code> introduction</h1>; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13227:659,optimiz,optimizations,659,https://hail.is,https://github.com/hail-is/hail/pull/13227,1,['optimiz'],['optimizations']
Performance,"Bumps [sortedcontainers](https://github.com/grantjenks/python-sortedcontainers) from 2.1.0 to 2.4.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/grantjenks/python-sortedcontainers/blob/master/HISTORY.rst"">sortedcontainers's changelog</a>.</em></p>; <blockquote>; <h2>2.4.0 (2021-05-16)</h2>; <p><strong>API Changes</strong></p>; <ul>; <li>Implement SortedDict methods: <strong>or</strong>, <strong>ror</strong>, and <strong>ior</strong> per PEP 584.</li>; </ul>; <h2>2.3.0 (2020-11-08)</h2>; <p><strong>Bugfixes</strong></p>; <ul>; <li>Make sort order stable when updating with large iterables.</li>; </ul>; <h2>2.2.2 (2020-06-07)</h2>; <p><strong>Miscellaneous</strong></p>; <ul>; <li>Add &quot;small slice&quot; optimization to <code>SortedList.__getitem__</code>.</li>; <li>Silence warning when testing <code>SortedList.iloc</code>.</li>; </ul>; <h2>2.2.1 (2020-06-06)</h2>; <p><strong>Miscellaneous</strong></p>; <ul>; <li>Fix a warning regarding <code>classifiers</code> in setup.py.</li>; </ul>; <h2>2.2.0 (2020-06-06)</h2>; <p><strong>Miscellaneous</strong></p>; <ul>; <li>Change SortedDict to avoid cycles for CPython reference counting.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/a1f52d6713dd2c2713a881d4f4d86ed68ff71cab""><code>a1f52d6</code></a> Bump version to 2.4.0</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/2678a78b6dacbe2352bff7876a26759d84971dac""><code>2678a78</code></a> Implement SortedDict methods: <strong>or</strong>, <strong>ror</strong>, and <strong>ior</strong> (<a href=""https://github-redirect.dependabot.com/grantjenks/python-sortedcontainers/issues/171"">#171</a>)</li>; <li><a href=""https://github.com/grantjenks/python-sortedcontainers/commit/9887989b21fc21fe572e0b4c30a3f3aa1eabbdca""><code>9887989</code></a> Bump version to 2.3.0</li>; <li><a href=""https://github.com/gra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11476:760,optimiz,optimization,760,https://hail.is,https://github.com/hail-is/hail/pull/11476,1,['optimiz'],['optimization']
Performance,"Bumps [svelte](https://github.com/sveltejs/svelte) from 3.38.2 to 3.49.0.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/sveltejs/svelte/blob/master/CHANGELOG.md"">svelte's changelog</a>.</em></p>; <blockquote>; <h2>3.49.0</h2>; <ul>; <li>Improve performance of string escaping during SSR (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/5701"">#5701</a>)</li>; <li>Add <code>ComponentType</code> and <code>ComponentProps</code> convenience types (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/6770"">#6770</a>)</li>; <li>Add support for CSS <code>@layer</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7504"">#7504</a>)</li>; <li>Export <code>CompileOptions</code> from <code>svelte/compiler</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7658"">#7658</a>)</li>; <li>Fix DOM-less components not being properly destroyed (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7488"">#7488</a>)</li>; <li>Fix <code>class:</code> directive updates with <code>&lt;svelte:element&gt;</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7521"">#7521</a>, <a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7571"">#7571</a>)</li>; <li>Harden attribute escaping during SSR (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/pull/7530"">#7530</a>)</li>; </ul>; <h2>3.48.0</h2>; <ul>; <li>Allow creating cancelable custom events with <code>createEventDispatcher</code> (<a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/4623"">#4623</a>)</li>; <li>Support <code>{@const}</code> tag in <code>{#if}</code> blocks <a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7241"">#7241</a></li>; <li>Return the context object in <code>setContext</code> <a href=""https://github-redirect.dependabot.com/sveltejs/svelte/issues/7427"">#7427</a></li>; <li>Allow comments ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12032:289,perform,performance,289,https://hail.is,https://github.com/hail-is/hail/pull/12032,3,['perform'],['performance']
Performance,"Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.4 to 1.26.5.; <details>; <summary>Release notes</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/releases"">urllib3's releases</a>.</em></p>; <blockquote>; <h2>1.26.5</h2>; <p>:warning: <strong>IMPORTANT: urllib3 v2.0 will drop support for Python 2</strong>: <a href=""https://urllib3.readthedocs.io/en/latest/v2-roadmap.html"">Read more in the v2.0 Roadmap</a></p>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting the authority component.</li>; </ul>; <p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=""https://github.com/sponsors/urllib3"">GitHub Sponsors</a></strong></p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>; <blockquote>; <h2>1.26.5 (2021-05-26)</h2>; <ul>; <li>Fixed deprecation warnings emitted in Python 3.10.</li>; <li>Updated vendored <code>six</code> library to 1.16.0.</li>; <li>Improved performance of URL parser when splitting; the authority component.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/urllib3/urllib3/commit/d1616473df94b94f0f5ad19d2a6608cfe93b7cdf""><code>d161647</code></a> Release 1.26.5</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2d4a3fee6de2fa45eb82169361918f759269b4ec""><code>2d4a3fe</code></a> Improve performance of sub-authority splitting in URL</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/2698537d52f8ff1f0bbb1d45cf018b118e91f637""><code>2698537</code></a> Update vendored six to 1.16.0</li>; <li><a href=""https://github.com/urllib3/urllib3/commit/07bed791e9c391d8bf12950f76537dc3c6f90550""><code>07bed79</code></a> Fix deprecation warnings for Py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10544:595,perform,performance,595,https://hail.is,https://github.com/hail-is/hail/pull/10544,1,['perform'],['performance']
Performance,"Bumps [wrapt](https://github.com/GrahamDumpleton/wrapt) from 1.13.3 to 1.14.1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/GrahamDumpleton/wrapt/blob/develop/docs/changes.rst"">wrapt's changelog</a>.</em></p>; <blockquote>; <h2>Version 1.14.1</h2>; <p><strong>Bugs Fixed</strong></p>; <ul>; <li>When the post import hooks mechanism was being used, and a Python package with; its own custom module importer was used, importing modules could fail if the; custom module importer didn't use the latest Python import hook finder/loader; APIs and instead used the deprecated API. This was actually occurring with the; <code>zipimporter</code> in Python itself, which was not updated to use the newer Python; APIs until Python 3.10.</li>; </ul>; <h2>Version 1.14.0</h2>; <p><strong>Bugs Fixed</strong></p>; <ul>; <li>; <p>Python 3.11 dropped <code>inspect.formatargspec()</code> which was used in creating; signature changing decorators. Now bundling a version of this function; which uses <code>Parameter</code> and <code>Signature</code> from <code>inspect</code> module when; available. The replacement function is exposed as <code>wrapt.formatargspec()</code>; if need it for your own code.</p>; </li>; <li>; <p>When using a decorator on a class, <code>isinstance()</code> checks wouldn't previously; work as expected and you had to manually use <code>Type.__wrapped__</code> to access; the real type when doing instance checks. The <code>__instancecheck__</code> hook is; now implemented such that you don't have to use <code>Type.__wrapped__</code> instead; of <code>Type</code> as last argument to <code>isinstance()</code>.</p>; </li>; <li>; <p>Eliminated deprecation warnings related to Python module import system, which; would have turned into broken code in Python 3.12. This was used by the post; import hook mechanism.</p>; </li>; </ul>; <p><strong>New Features</strong></p>; <ul>; <li>Binary wheels provided on PyPi for <code>aarch64</code> Linux s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12102:569,load,loader,569,https://hail.is,https://github.com/hail-is/hail/pull/12102,1,['load'],['loader']
Performance,"Bumps [zipp](https://github.com/jaraco/zipp) from 3.17.0 to 3.18.1.; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/jaraco/zipp/blob/main/NEWS.rst"">zipp's changelog</a>.</em></p>; <blockquote>; <h1>v3.18.1</h1>; <p>No significant changes.</p>; <h1>v3.18.0</h1>; <h2>Features</h2>; <ul>; <li>Bypass ZipFile.namelist in glob for better performance. (<a href=""https://redirect.github.com/jaraco/zipp/issues/106"">#106</a>)</li>; <li>Refactored glob functionality to support a more generalized solution with support for platform-specific path separators. (<a href=""https://redirect.github.com/jaraco/zipp/issues/108"">#108</a>)</li>; </ul>; <h2>Bugfixes</h2>; <ul>; <li>Add special accounting for pypy when computing the stack level for text encoding warnings. (<a href=""https://redirect.github.com/jaraco/zipp/issues/114"">#114</a>)</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/jaraco/zipp/commit/bfae83474a730e8cc9b8a71027fb859b46b3875c""><code>bfae834</code></a> Finalize</li>; <li><a href=""https://github.com/jaraco/zipp/commit/487066ec9757c3c82e96014d0b30906996c6280d""><code>487066e</code></a> Merge changelog into last release.</li>; <li><a href=""https://github.com/jaraco/zipp/commit/4584ee2dcfb10d5314ad319d9d5b140c90bc2951""><code>4584ee2</code></a> Move changelog entry, saved to the wrong location :(</li>; <li><a href=""https://github.com/jaraco/zipp/commit/3c06d30b91b37a118536d9d424e0a8b893e78a6e""><code>3c06d30</code></a> Finalize</li>; <li><a href=""https://github.com/jaraco/zipp/commit/48b72b8db6ae5f7712323aca6b340744db15f576""><code>48b72b8</code></a> Merge pull request <a href=""https://redirect.github.com/jaraco/zipp/issues/113"">#113</a> from jaraco/feature/glob-perf</li>; <li><a href=""https://github.com/jaraco/zipp/commit/171fa98236a1adfc316c3bc5cdc5eaa4b9548424""><code>171fa98</code></a> Add news fragment.</li>; <li><a href=""https://github.com/jaraco/zipp/commit/ac8ea7a5",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14473:376,perform,performance,376,https://hail.is,https://github.com/hail-is/hail/pull/14473,1,['perform'],['performance']
Performance,"By joining against the `batches` row in the insert `FOR UPDATE`, these queries try to grab an exclusive lock on the row in the `batches` table. This is a problem if the transaction already has a shared lock on that row, like it does in `mark_job_complete`. In that case any two concurrent MJCs from the same batch will deadlock.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14379:278,concurren,concurrent,278,https://hail.is,https://github.com/hail-is/hail/pull/14379,1,['concurren'],['concurrent']
Performance,"CC: @danking . I tested this change works by replicating Lindo's job download (~80 Gi) on an overloaded node with 8 simultaneous jobs trying to download data in parallel. Before this proposed change and after I fixed some other issues in #10522, 75% of his jobs would fail with this error (the ones I presume on the persistent SSDs rather than the local SSDs):. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; ```. Now, all of the downloads succeed after my change. I found this link to be very helpful figuring out what the issue was. ; https://bugs.python.org/issue33413",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10534:925,concurren,concurrent,925,https://hail.is,https://github.com/hail-is/hail/pull/10534,1,['concurren'],['concurrent']
Performance,"CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. Stacked on #12212 . This PR threads through region requests from the user and feeds that information into the scheduler. The architecture of a pool per machine type has not changed. We explicitly chose not to have a new pool per region x machine_type. Instead, the control loop looks at the front of the job queue and tries to predict which jobs are likely to be scheduled. From those jobs, we then find which regions the jobs can run in and create the number of corresponding instances. We use the fair share calculation to estimate how many jobs per user can be scheduled in 2.5 minutes assuming the scheduling loop runs once per second. We then grab this many jobs from the queue for each user and estimate the ""scheduling iteration"" at which each iteration of the scheduler each chunk of user jobs would be scheduled. We sort the overall set of jobs that we've chosen by the ""scheduling iteration"". We also include the regions as part of the sorting queries with None (any region) being sorted last. This is to compact the free cores across jobs so as to avoid fragmentation of instances created and for jobs with no region specifications to fill in the remaining cores in any region. For the hailtop.batch client, I added a new setting in `~/.config/hail` to set the default regions for all jobs in the ServiceBackend and a new method on `Job` that sets the list of regions to run in. Things to double check once everything is working is the sort orders on the scheduling queries are correct. . Once this PR goes in, then we can merge #11840 with some minor changes. There will also be a follow-up PR that gets rid of the CI-specific code in the scheduler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221:519,queue,queue,519,https://hail.is,https://github.com/hail-is/hail/pull/12221,2,['queue'],['queue']
Performance,"CHANGELOG: BatchPoolExecutor now raises an informative error message for a variety of ""system"" errors, such as missing container images. If the main container fails for reasons beyond BatchPoolExecutor's control, such; as a missing container image, we previously did not report these errors. In; fact, we encountered errors when trying to load the output file that cannot; exist if the main container errors. Smaller included changes:; - directly use the asynchronous, low-level client instead of the synchronous,; low-level client; - introduce an `async_cancel` now that we have access to the async client.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9543:339,load,load,339,https://hail.is,https://github.com/hail-is/hail/pull/9543,1,['load'],['load']
Performance,"CHANGELOG: Eliminate quadratic behavior in `BlockMatrix.to_matrix_table_row_major`. Users should expect significant reduction in run-time. There are two significant changes in this PR:; - Teach `LZ4InputBlockBuffer` how to skip bytes without decompressing every block, and; - Teach BlockMatrix to use a small cache of rows when converting from a BlockMatrix to a row-wise RDD. ### Blocked LZ4 Byte Skipping. We compress in blocks of 16 KiB. The blocks begin with an 32-bit integer indicating the decompressed length. When we're skipping large numbers of bytes we can request an `LZ4InputBlockBuffer` to skip decompression if the entire block will be skipped. ### BlockMatrix Blocks to Rows Caching; Currently, for every row in every block, BM opens a file, skips to the appropriate location, reads that one row, writes it into an RVB, and then closes the file. This has terrible cache and I/O performance. Instead, we allocate 32 MiB to cache the rows of each block. We divide the cache evenly across all rows. The new implementation requires the cache can at least fit one row of the block, with 32 MiB we're good up to ~4 million (total) columns. We'll need to reimplement this to also use a tree-aggregate long before we get to 4 million columns. ### Benchmark Results. This branch vs main (3149211fb79b):; ```; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; to_matrix_table_row_major 716.3% 251.300 1800.000; ----------------------; Harmonic mean: 716.3%; Geometric mean: 716.3%; Arithmetic mean: 716.3%; Median: 716.3%; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9328:309,cache,cache,309,https://hail.is,https://github.com/hail-is/hail/pull/9328,6,"['cache', 'perform']","['cache', 'performance']"
Performance,CHANGELOG: Fix hl.import_plink docs to properly report the type of `is_case` and `quant_pheno`. I verified the correct types by checking `LoadPlink.scala`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9495:138,Load,LoadPlink,138,https://hail.is,https://github.com/hail-is/hail/pull/9495,1,['Load'],['LoadPlink']
Performance,CHANGELOG: Fixed bug causing poor performance and memory leaks for Matrix.annotate_rows aggregations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12995:34,perform,performance,34,https://hail.is,https://github.com/hail-is/hail/pull/12995,1,['perform'],['performance']
Performance,"CHANGELOG: Implement the KING method for relationship inference as hl.methods.king. Just look at the last commit. The other commits are PRs that I hope will merge; on Tuesday. This PR implements `hl.methods.king` a new, relatively fast, method for; relationship inference on genotype data. I am eager for criticism of the ""Notes""; section in which I attempt to describe the KING method to a Hail user with only; a basic understanding of genotype matrices and Hail. I also include a benchmark which exercises MT->BM, matrix multiply, and; BM->MT. We have an opportunity for a substantial improvement in performance by; BM->replacing the BM interface by one which permits multiple entry fields. In; BM->particular, note that I have to convert from row-partitioning to; BM->block-partitioning four times!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9343:602,perform,performance,602,https://hail.is,https://github.com/hail-is/hail/pull/9343,1,['perform'],['performance']
Performance,"CHANGELOG: In Query-on-Batch, `naive_coalsce` no longer performs a full write/read of the dataset. It now operates identically to the Query-on-Spark implementation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13042:56,perform,performs,56,https://hail.is,https://github.com/hail-is/hail/pull/13042,1,['perform'],['performs']
Performance,"CHANGELOG: Introduce `hailctl fs sync` which robustly transfers one or more files between Amazon S3, Azure Blob Storage, and Google Cloud Storage. There are really two distinct conceptual changes remaining here. Given my waning time available, I am not going to split them into two pull requests. The changes are:. 1. `basename` always agrees with the [`basename` UNIX utility](https://en.wikipedia.org/wiki/Basename). In particular, the folder `/foo/bar/baz/`'s basename is *not* `''` it is `'baz'`. The only folders or objects whose basename is `''` are objects whose name literally ends in a slash, e.g. an *object* named `gs://foo/bar/baz/`. 2. `hailctl fs sync`, a robust copying tool with a user-friendly CLI. `hailctl fs sync` comprises two pieces: `plan.py` and `sync.py`. The latter, `sync.py` is simple: it delegates to our existing copy infrastructure. That copy infastructure has been lightly modified to support this use-case. The former, `plan.py`, is a concurrent file system `diff`. `plan.py` generates and `sync.py` consumes a ""plan folder"" containing these files:. 1. `matches` files whose names and sizes match. Two columns: source URL, destination URL. 2. `differs` files or folders whose names match but either differ in size or differ in type. Four columns: source URL, destination URL, source state, destination state. The states are either: `file`, `dif`, or a size. If either state is a size, both states are sizes. 3. `srconly` files only present in the source. One column: source URL. 4. `dstonly` files only present in the destination. One column: destination URL. 5. `plan` a proposed set of object-to-object copies. Two columns: source URL, destination URL. 6. `summary` a one-line file containing the total number of copies in plan and the total number of bytes which would be copied. As described in the CLI documentation, the intended use of these commands is:. ```; hailctl fs sync --make-plan plan1 --copy-to gs://gcs-bucket/a s3://s3-bucket/b; hailctl fs sync --use",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14248:968,concurren,concurrent,968,https://hail.is,https://github.com/hail-is/hail/pull/14248,1,['concurren'],['concurrent']
Performance,"CHANGELOG: MatrixTable.aggregate_cols no longer forces a distributed computation. This should be what you want in the majority of cases. In case you know the aggregation is very slow and should be parallelized, use mt.cols().aggregate instead. Most of the time, `aggregate_cols` will be much faster performing the aggregation locally. Currently, we generate a `TableAggregate` over a `TableParallelize` of the columns. We shouldn't try to optimize that to a local computation during compilation; `TableParallelize` should express the intent that the computation is expensive and really should be parallelized. This should be considered part of the semantics the compiler must preserve. This PR changes `aggregate_cols` to explicitly generate a local computation using `StreamAgg` (which was only exposed in Python relatively recently, which is why we haven't made this change sooner). Longer term, aggregating columns should probably get its own IR node, especially once we start partitioning along columns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13405:299,perform,performing,299,https://hail.is,https://github.com/hail-is/hail/pull/13405,2,"['optimiz', 'perform']","['optimize', 'performing']"
Performance,"CHANGELOG: On some pipelines, since at least 0.2.58 (commit 23813afd5b), Hail could use essentially unbounded amounts of memory. This change removes ""optimization"" rules that accidentally caused that. Closes #13606",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13619:150,optimiz,optimization,150,https://hail.is,https://github.com/hail-is/hail/pull/13619,1,['optimiz'],['optimization']
Performance,"CHANGELOG: Reduce latency on simple pipelines by as much as 50% by reducing decoding time. Force count essentially tests decoding because it forces decoding but then just increments a counter by one. Analysis of profile results indicates that the array inplace decoder was perhaps 50% of time, but exactly what part of decoding was unclear. I attempted many different things. I eventually settled on loop unrolling as the primary benefit. After team meeting, I applied @patrick-schultz 's advice to use bit twiddling to further improve the speed. ---. I assessed the latency using `time python3` on this file:. ```python; import hail as hl; hl.init(master='local[1]'); hl._set_flags(write_ir_files='1'); hl.read_matrix_table('/Users/dking/projects/hail-data/foo.mt')._force_count_rows(); ```. `foo.mt` is a subset of the `variant_data` from a VDS with ~80k samples, ~300k variants, stored in ~1.6GiB. 1. This PR: 34s, 33s; 2. no twiddling: 43s, 43s https://github.com/hail-is/hail/compare/main...danking:hail:unroll-64; 3. no twiddling & 8 element blocks: 37s, 38s https://github.com/hail-is/hail/compare/main...danking:hail:unroll-8; 4. `main` (`481cfc201b [query] fix backoff code (#13713)`): 68s, 69s. In YourKit, I observe that (1) reads 50-70MB/s with one core whereas (4) reads 15-35MB/s. I also assessed the 10-core latency and JIT effects:. - (1) starts at ~12s, warms to ~6s (+- 0.5s). Peak bandwidth 490MB/s.; - (4) starts at ~17s and warms up to ~11s (+- 2s). Peak bandwidth ~250MB/s. I suspect, with this PR, the multi-core speed is fast enough to saturate any of our file stores (including my laptop, which I think taps out just around ~500MB/s). Big thanks to everyone who contributed, particularly @patrick-schultz, whose suggestion to use bit-twiddling, squeezeed another 10% off the 8 element blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13776:18,latency,latency,18,https://hail.is,https://github.com/hail-is/hail/pull/13776,3,['latency'],['latency']
Performance,"CHANGELOG: Remove memory leak in `BlockMatrix.to_matrix_table_row_major` and `BlockMatrix.to_table_row_major`. Also, make the cache size used in both methods configurable. The `ref` variable was holding entire blocks in memory for no reason. It was a; vestiage of debugging. Moreover, the configurable cache permits users to fine tune; memory usage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9501:126,cache,cache,126,https://hail.is,https://github.com/hail-is/hail/pull/9501,3,"['cache', 'tune']","['cache', 'tune']"
Performance,CHANGELOG: Substantially improve the performance of `import_gtf`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8887:37,perform,performance,37,https://hail.is,https://github.com/hail-is/hail/pull/8887,1,['perform'],['performance']
Performance,"CHANGELOG: Use indexed VEP cache files for GRCh38 on both dataproc and QoB. Fixes #13989. In this PR, I did the following:; 1. Installed samtools into the Docker image to get rid of errors in the log output; 2. Added the `--merged` flag so that VEP will use the directory `homo_sapiens_merged` for the cache. Outstanding Issues:; 1. The FASTA files that are in `homo_sapiens/` were not present in the merged dataset. Do we keep both the `homo_sapiens` and `homo_sapiens_merged/` directories in our bucket or do we transfer the FASTA files to the merged directory?; 2. Once we decide the answer to (1), then I can fix this in dataproc. The easiest thing to do is to add the tar file with the `_merged` data to the dataproc vep folders and use the `--merged` flag. However, that will double the startup time for VEP on a worker node in dataproc. Before:; <img width=""617"" alt=""Screenshot 2023-12-05 at 12 42 16 PM"" src=""https://github.com/hail-is/hail/assets/1693348/bee7fff5-782c-4f19-aa88-26383ed386b7"">. After:; <img width=""619"" alt=""Screenshot 2023-12-05 at 12 46 30 PM"" src=""https://github.com/hail-is/hail/assets/1693348/3d731759-6c69-4f1c-9c73-92bfb05c239a"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071:27,cache,cache,27,https://hail.is,https://github.com/hail-is/hail/pull/14071,2,['cache'],['cache']
Performance,"CHANGELOG: `hl.Table.parallelize` is much more flexible and now successfully imports most Hail-compatible data. I really wanted to load the hail-is/hail pull requests into Hail. I did not want to specify; the types of all 271 fields. I souped up Hail's `impute_type`:. - If an empty array, set, dict or `None` appears at any nesting level, but a ""peer"" is non-empty and; non-missing, we accept the peer's type.; - We take the union of two struct types as long as they agree on their intersection.; - If we discover a dict that cannot be imputed as a Hail dict, we try to impute it as a struct. If you like this change, I'll add tests. Note: I had to change `HailType` to include `None`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10045:131,load,load,131,https://hail.is,https://github.com/hail-is/hail/pull/10045,1,['load'],['load']
Performance,"CHANGELOG: `hl.import_table` is up to twice as fast for small tables. The big change is optimizing for the single file, no filters case in which; we need not scan for the first extant row, that row *must* be in the first; partition, if it exists at all. Unfortunately there is no zero-RPC way to; determine the number of partitions in a table, so I must catch an error; about the lack of a zeroth partition. I also did some refactoring:. 1. Move some functions to a utility file and add lots of indents and newlines to make them readable.; 2. Use `hl.format` for constructing strings.; 3. Make `should_filter_line` into `should_remove_line` for clarity of name.; 4. Modify `should_remove_line` to use short-circuiting and/or instead of array folds.; 5. Modify `should_remove_line` to indicate (via returning None) when there are no filters enabled.; 6. Add types.; 7. Fix a bug where we assumed that `.collect()[0]` would be `None` if there were no values in the table. (It raises an error); 8. Deduplicate `hail.utils.deduplicate` (haha: I mean, there is already code for doing field dedupe)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11782:88,optimiz,optimizing,88,https://hail.is,https://github.com/hail-is/hail/pull/11782,1,['optimiz'],['optimizing']
Performance,"CHANGELOG: early implementation of regenie. I'd like to get this basic version in and iterate. It works on local. Has a few todos and fixmes; I've seen Cotton, others use these, and they'll be gone in fairly short order, saves a bit of time over making a formal issue, for something that is clearly wip. . This looks like a scary amount of lines, but almost all of the work is held in regenie-batch.py. Example files are included in contrib/regenie/regenie, which is a redacted copy of their repo, and which is sufficient, lighter-weight than the full. When you have a chance, I'd like to discuss increasing the max capacity of the SSD (striping partitions). I want this as a fallback mechanism, when not enough memory can be allocated (lowmem). It will likely perform terribly with persistent storage. I'm happy to contribute that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194:761,perform,perform,761,https://hail.is,https://github.com/hail-is/hail/pull/9194,1,['perform'],['perform']
Performance,"CHANGELOG: make hail's optimization rewriting filters to interval-filters smarter and more robust. Completely rewrites ExtractIntervalFilters. Instead of matching against very specific patterns, and failing completely for things that don't quite match (e.g. an input is let bound, or the fold implementing ""locus is contained in a set of intervals"" is written slightly differently), this uses a standard abstract interpretation framework, which is almost completely insensitive to the form of the IR, only depending on the semantics. It also correctly handles missing key fields, where the previous implementation often produced an unsound transformation of the IR. Also adds a much more thorough test suite than we had before. At the top level, the analysis takes a boolean typed IR `cond` in an environment where there is a reference to some `key`, and produces a set `intervals`, such that `cond` is equivalent to `cond & intervals.contains(key)` (in other words `cond` implies `intervals.contains(key)`, or `intervals` contains all rows where `cond` is true). This means for instance it is safe to replace `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond)`. Then in a second pass it rewrites `cond` to `cond2`, such that `cond & (intervals.contains(key))` is equivalent to `cond2 & intervals.contains(key)` (in other words `cond` implies `cond2`, and `cond2 & intervals.contains(key)` implies `cond`). This means it is safe to replace the `TableFilter(t, cond)` with `TableFilter(TableFilterIntervals(t, intervals), cond2)`. A common example is when `cond` can be completely captured by the interval filter, i.e. `cond` is equivant to `intervals.contains(key)`, in which case we can take `cond2 = True`, and the `TableFilter` can be optimized away. This all happens in the function; ```scala; def extractPartitionFilters(ctx: ExecuteContext, cond: IR, ref: Ref, key: IndexedSeq[String]): Option[(IR, IndexedSeq[Interval])] = {; if (key.isEmpty) None; else {; val e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13355:23,optimiz,optimization,23,https://hail.is,https://github.com/hail-is/hail/pull/13355,1,['optimiz'],['optimization']
Performance,"CI gets the logs for every failing job in the batch and sends it with the deploy_status response. This performs O(n_jobs) work for a single request, but this API is developers only and used only by the deploy test which has a small batch. We can revisit if this becomes a problem.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8337:103,perform,performs,103,https://hail.is,https://github.com/hail-is/hail/pull/8337,1,['perform'],['performs']
Performance,Cache `reference_entry_fields_to_keep` in the VDS combiner,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10963:0,Cache,Cache,0,https://hail.is,https://github.com/hail-is/hail/issues/10963,1,['Cache'],['Cache']
Performance,Cache blockmatrix type,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6775:0,Cache,Cache,0,https://hail.is,https://github.com/hail-is/hail/pull/6775,1,['Cache'],['Cache']
Performance,Cache from in notebook,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5016:0,Cache,Cache,0,https://hail.is,https://github.com/hail-is/hail/pull/5016,1,['Cache'],['Cache']
Performance,Cache regression results,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4557:0,Cache,Cache,0,https://hail.is,https://github.com/hail-is/hail/pull/4557,1,['Cache'],['Cache']
Performance,Can load 5000 cols in 5 seconds. Previously 250 cols took 3 minutes. fixes #4153,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4159:4,load,load,4,https://hail.is,https://github.com/hail-is/hail/pull/4159,1,['load'],['load']
Performance,Cannot load a PLINK file containing 20 million variants,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5564:7,load,load,7,https://hail.is,https://github.com/hail-is/hail/issues/5564,1,['load'],['load']
Performance,"Changes:; - Add WaitableSharedPool that allows you to submit a collection of jobs (which start running as soon as they are submitted) against a shared AsyncWorkerPool and then `wait()` at the end for all submitted jobs to complete.; - Use in scheduler threads. I also unscientifically tweaked a few of the settings:; - Use queue_size=100 in shared async worker pool in scheduler. This is 1x the parallelism, which seems like plenty. (Queue size is how much pending work to queue up before blocking.); - Drop work per iteration from 1000 to 300.; - Increase the number of database connections to 50. This is 1/2 the parallelism in the scheduler, the argument being half the work goes into servicing web requests (e.g. talking to workers) and the other half talking to the database. FYI @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7961:434,Queue,Queue,434,https://hail.is,https://github.com/hail-is/hail/pull/7961,2,"['Queue', 'queue']","['Queue', 'queue']"
Performance,"ChannelException; at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.writ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2129,concurren,concurrent,2129,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,"Check partitioning during joins, so we don't have issues trying to load one partition thousands of times",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3291:67,load,load,67,https://hail.is,https://github.com/hail-is/hail/issues/3291,1,['load'],['load']
Performance,"Closes #4527; Closes #4761. This is a workaround to prevent issues with MatrixUnionRows when the; entries arrays are in different places in the rvRowType in each of the; children. Furthermore, it prevents issues if the entries array is; pruned and then re-added later in rebuild, where it will often be; inserted, likely by MatrixMapRows, at the end of the rvRowType. This; rearrangement caused the type equality assertion in Optimize to fail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4891:426,Optimiz,Optimize,426,https://hail.is,https://github.com/hail-is/hail/pull/4891,1,['Optimiz'],['Optimize']
Performance,"Closes hail-is/hail-production-issues#9. Details [here](https://github.com/hail-is/hail-production-issues/issues/9#issuecomment-1049356188). This is my bad. One of the query service PRs allowed spec writing to happen in; parallel with DB insertion (which reduces latency a bit), but if the spec fails; to write or is cancelled, then the DB has a spec token that points at a; cloud object which does not necessarily exist. I think we do not need the spec token, but removing it does not seem likely; to improve performance much. We still need to hit the DB to get the start_job_id.; There was some discussion about the necessity of the token [here](https://github.com/hail-is/hail/pull/7949#discussion_r370406517).; I think that discussion came to the wrong conclusion. GCS is atomic and globally; consistent. Writing to an already present spec object is atomic. The only issue; I forsee is the possibility that the spec is different the second time. The spec; should have the same semantic content, but if the characters are different the; spec index could be very briefly wrong. We could fix this by storing both the; spec and the index in one GCS file. cc: @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11404:263,latency,latency,263,https://hail.is,https://github.com/hail-is/hail/pull/11404,2,"['latency', 'perform']","['latency', 'performance']"
Performance,Code cache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5426:5,cache,cache,5,https://hail.is,https://github.com/hail-is/hail/pull/5426,1,['cache'],['cache']
Performance,"Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB); Collecting jedi>=0.16; Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB); Collecting pygments; Using cached Pygments-2.8.1-py3-none-any.whl (983 kB); Collecting backcall; Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB); Collecting parso<0.9.0,>=0.8.0; Using cached parso-0.8.1-py2.py3-none-any.whl (93 kB); Collecting ptyprocess>=0.5; Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB); Collecting wcwidth; Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB); Collecting ipython-genutils; Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB); Collecting requests-oauthlib>=0.7.0; Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB); Collecting oauthlib>=3.0.0; Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB); Installing collected packages: six, pyasn1, urllib3, rsa, pyparsing, pyasn1-modules, protobuf, idna, chardet, certifi, cachetools, requests, pytz, packaging, oauthlib, multidict, googleapis-common-protos, google-auth, yarl, typing-extensions, requests-oauthlib, MarkupSafe, google-api-core, attrs, async-timeout, wrapt, wcwidth, tornado, PyYAML, python-dateutil, py4j, ptyprocess, pillow, parso, num",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:6851,cache,cached,6851,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"Collecting six>=1.5.2; Using cached six-1.15.0-py2.py3-none-any.whl (10 kB); Collecting packaging>=16.8; Using cached packaging-20.9-py2.py3-none-any.whl (40 kB); Collecting pillow>=4.0; Using cached Pillow-8.1.2-cp38-cp38-manylinux1_x86_64.whl (2.2 MB); Collecting python-dateutil>=2.1; Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB); Collecting PyYAML>=3.10; Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB); Collecting Jinja2>=2.7; Using cached Jinja2-2.11.3-py2.py3-none-any.whl (125 kB); Collecting wrapt<2,>=1.10; Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl; Collecting pyasn1-modules>=0.2.1; Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB); Collecting rsa<5,>=3.1.4; Using cached rsa-4.7.2-py3-none-any.whl (34 kB); Collecting cachetools<5.0,>=2.0.0; Using cached cachetools-4.2.1-py3-none-any.whl (12 kB); Collecting google-api-core<2.0.0dev,>=1.21.0; Using cached google_api_core-1.26.1-py2.py3-none-any.whl (92 kB); Collecting pytz; Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB); Collecting googleapis-common-protos<2.0dev,>=1.6.0; Using cached googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB); Collecting protobuf>=3.12.0; Using cached protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB); Collecting MarkupSafe>=0.23; Using cached MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB); Collecting pyparsing>=2.0.2; Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB); Collecting pyasn1<0.5.0,>=0.4.6; Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB); Collecting py4j==0.10.7; Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB); Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0; Using cached prompt_toolkit-3.0.17-py3-none-any.whl (367 kB); Collecting pickleshare; Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB); Collecting traitlets>=4.2; Using cached traitlets-5.0.5-py3-none-any.whl (100 kB); Collecting pexpect>4.3; Using cached pexpect-4.8.0-py2.py3-none-",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:5661,cache,cached,5661,https://hail.is,https://github.com/hail-is/hail/issues/10197,1,['cache'],['cached']
Performance,"Command:. hail-new read -i /user/lfran/exac_all.split.vds \; filtersamples --remove -c ""file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/CANCER/samples_to_keep.sample_list"" \; variantqc \; filtervariants --keep -c 'va.qc.MAC > 0' \; count \; filtersamples --keep -c 'false' \; write -o /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds. Error:. Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357). Hail log attached. [hail.log.txt](https://github.com/broadinstitute/hail/files/225215/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/309:1495,load,loadClass,1495,https://hail.is,https://github.com/hail-is/hail/issues/309,3,['load'],['loadClass']
Performance,"Consider a NxM matrix, X, where N << M. The product X ⨯ Xᵀ has size NxN. If the block size is chosen on the order of N, say, N/2, then the number of partitions in the product is four. The number of partitions in X is much much larger. As a result, we miss out on some parallelism because four nodes must perform a tremendous number of multiplications. When N << M, we may consider using tree aggregations until N and M are the same order of magnitude. In particular, we can break the sum into chunks and use a tree aggregation. Consider:. ```; +---------+ +----+ +----+; | A | | B | = | C |; +---------+ | | +----+; | |; +----+; ```; We can slice A column-wise into three chunks and B column-wise into three chunks:. ```; +---------+ +----+ +----+; | : : | | .. | 1 = | C |; +---------+ | | 2 +----+; 1 2 3 | ˙˙ | 3; +----+; ```. multiply the chunks and then sum them:. ```; +----+ +----+ +----+ +----+ +----+ +----+ +----+; | A1 | | B1 | + | A2 | | B2 | + | A3 | | B3 | = | C |; +----+ +----+ +----+ +----+ +----+ +----+ +----+; ```. This would be a tree aggregate with tree of size three. I think we should trigger this whenever N < M/10, and we should generate a 10-ary tree of summations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1975:304,perform,perform,304,https://hail.is,https://github.com/hail-is/hail/issues/1975,1,['perform'],['perform']
Performance,"Consider this pipeline, which measures the speed of single-core decoding.; ```; import hail as hl; hl.init(master='local[1]'); vds = hl.vds.read_vds(...); vds.variant_data._force_count_rows(); ```. On a 2021 MacBook Pro, [YourKit reports](https://github.com/hail-is/hail/pull/13787#issuecomment-1756358633) ~60MiB/s of bandwidth and 100% CPU use. Substantial amounts of time are reported in Zstd and I/O, #13840 endeavors to address those issues. Even with these issues addressed, we anticipate decoding to use a substantial portion of CPU time. In particular, our decoders perform a fair number of branches to handle missingness (consider that an array is stored as: length, bitset indicating which values are non-missing, the non-missing values). #13787 demonstrated a 50% reduction in run-time ([see benchmarks from before this change](https://github.com/hail-is/hail/pull/13776)) primarily due to replacing iteration (which branches O(N) times) with bitset-driven iteration over the missing bits (which branches O(N_PRESENT) times). Unfortunately, using the ideas in #13787 to improve struct decoding is tricky because struct fields are heterogenous. We could generate 16 different decoders and branch on 4 bits of missingness, but that is fairly large code. However, if we are decoding large arrays of structs (such as a whole partition or an entries array), we could ""transpose"" the data and store a struct of arrays, with one array per-field. This representation has several benefits:. 1. We may use the bitset-driven iteration from #13787.; 2. For fixed-width contiguous element types, we can use `memcpy` for rapid decoding.; 3. O(1) skipping off the unused fields of O(N) structs. ; 5. General purpose compression should perform better due to locality of data types.; 6. We have the opportunity to apply data-type aware compression on each array. This change does require a novel set of PTypes to map the Array(Struct) operations onto the physical Struct(Array) representation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13841:574,perform,perform,574,https://hail.is,https://github.com/hail-is/hail/issues/13841,2,['perform'],['perform']
Performance,"Consider this snippet; ```; broken_ht = hl.import_table('../data/bikes.csv'); # Look at the first 3 rows; broken_ht.show(3); ```; [`bikes.csv`](https://github.com/jvns/pandas-cookbook/blob/master/data/bikes.csv), but I removed the diacritic characters (AFAICT), the first line is now this:; ```; Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles). ```. This is the output:; ```; 2019-01-30 16:19:59 Hail: INFO: Reading table with no type imputation; Loading column 'Date;Berri 1;Brebeuf (donnees non disponibles);Cote-Sainte-Catherine;Maisonneuve 1;Maisonneuve 2;du Parc;Pierre-Dupuy;Rachel1;St-Urbain (donnees non disponibles)' as type 'str' (type not specified). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-8-5e24c8175e7d> in <module>; 1 broken_ht = hl.import_table('../data/bikes.csv'); 2 # Look at the first 3 rows; ----> 3 broken_ht.show(3). </Users/dking/anaconda2/envs/foofoo/lib/python3.7/site-packages/decorator.py:decorator-gen-848> in show(self, n, width, truncate, types, handler). ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561 ; 562 return wrapper. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in show(self, n, width, truncate, types, handler); 1330 Handler function for data string.; 1331 """"""; -> 1332 handler(self._show(n, width, truncate, types)); 1333 ; 1334 def index(self, *exprs):. ~/anaconda2/envs/foofoo/lib/python3.7/site-packages/hail/table.py in _show(self, n, width, truncate, types); 1225 n_fields = len(fields); 1226 ; -> 1227 types = [trunc(str(t.row[f].dtype)) for f in ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5222:561,Load,Loading,561,https://hail.is,https://github.com/hail-is/hail/issues/5222,1,['Load'],['Loading']
Performance,"Consider, for example, this deploy: https://ci.hail.is/batches/7956812. `test-dataproc-37` succeeded but `test-dataproc-38` failed (it timed out b/c the master failed to come online). You can see the error logs for the cluster here: https://cloudlogging.app.goo.gl/t1ux8oqy11Ba2dih7. It states a certain file either did not exist or we did not have permission to access it. [`test_dataproc-37`](https://batch.hail.is/batches/7956812/jobs/193) and [`test_dataproc-38`](https://batch.hail.is/batches/7956812/jobs/194) started around the same time and both uploaded four files into:. gs://hail-30-day/hailctl/dataproc/ci_test_dataproc/0.2.121-7343e9c368dc/. And then set it to public read/write. The public read/write means that permissions are not the issue. Instead, the issue is that there must be some sort of race condition in GCS which means that if you ""patch"" (aka overwrite) an existing file, it is possible that a concurrent reader will see the file as not existing. Unfortunately, I cannot confirm this with audit logs of the writes and read because [public objects do not generate audit logs](https://cloud.google.com/logging/docs/audit#data-access).; > Publicly available resources that have the Identity and Access Management policies [allAuthenticatedUsers](https://cloud.google.com/iam/docs/overview#allauthenticatedusers) or [allUsers](https://cloud.google.com/iam/docs/overview#allusers) don't generate audit logs. Resources that can be accessed without logging into a Google Cloud, Google Workspace, Cloud Identity, or Drive Enterprise account don't generate audit logs. This helps protect end-user identities and information.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13573:811,race condition,race condition,811,https://hail.is,https://github.com/hail-is/hail/pull/13573,2,"['concurren', 'race condition']","['concurrent', 'race condition']"
Performance,"Context (which however conflates a few different issues): https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Error.20__C1188collect_distributed_array/near/276162426. Running the `prepare_pext` step of the https://github.com/broadinstitute/gnomad-browser pipeline using Hail 0.2.105 results in the following error:. ```; 2022-12-01 05:56:34.825 Hail: INFO: Loading <StructExpression of type struct{ensg: str, symbol: str, max_pexts: str, Spleen: str, Brain_FrontalCortex_BA9_: str, SmallIntestine_TerminalIleum: str, Artery_Coronary: str, Skin_SunExposed_Lowerleg_: str, Brain_Hippocampus: str, Esophagus_Muscularis: str, Brain_Nucleusaccumbens_basalganglia_: str, Artery_Tibial: str, Brain_Hypothalamus: str, Adipose_Visceral_Omentum_: str, Brain_CerebellarHemisphere: str, Nerve_Tibial: str, Breast_MammaryTissue: str, Liver: str, Skin_NotSunExposed_Suprapubic_: str, AdrenalGland: str, Pancreas: str, Lung: str, Pituitary: str, Muscle_Skeletal: str, Colon_Transverse: str, Artery_Aorta: str, Heart_AtrialAppendage: str, Adipose_Subcutaneous: str, Esophagus_Mucosa: str, Heart_LeftVentricle: str, Brain_Cerebellum: str, Brain_Cortex: str, Thyroid: str, Stomach: str, WholeBlood: str, Brain_Anteriorcingulatecortex_BA24_: str, Brain_Putamen_basalganglia_: str, Brain_Caudate_basalganglia_: str, Colon_Sigmoid: str, Esophagus_GastroesophagealJunction: str, Brain_Amygdala: str, mean_proportion: str}> fields. Counts by type:; str: 42; Traceback (most recent call last): (0 + 1) / 1]; File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/genes.py"", line 327, in <module>; run_pipeline(pipeline); File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/pyfiles_zcrmfxsz.zip/data_pipeline/pipeline.py"", line 200, in run_pipeline; File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/pyfiles_zcrmfxsz.zip/data_pipeline/pipeline.py"", line 167, in run; File ""/tmp/22ce8d09e1014663bbe5d8b6f080b286/pyfiles_zcrmfxsz.zip/data_pipeline/pipeline.py"", line 133, in run; File ""<decorator-gen-1066>"", line 2, in write",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12532:385,Load,Loading,385,https://hail.is,https://github.com/hail-is/hail/issues/12532,1,['Load'],['Loading']
Performance,"Context.java:343); 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911); 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); 	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); 	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); 	at java.lang.Thread.run(Thread.java:748); </details>. <details>; <summary>Working hail.log</summary>. ```; 2018-10-09 15:04:33 Hail: INFO: SparkUI: http://10.32.119.167:4040; 2018-10-09 15:04:33 Hail: INFO: Running Hail version devel-17a988f2a628; 2018-10-09 15:04:33 SharedState: INFO: loading hive config file: file:/Users/michafla/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml; 2018-10-09 15:04:33 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/').; 2018-10-09 15:04:33 SharedState: INFO: Warehouse path is 'file:/Users/michafla/projects/R/pkg/hailr/inst/unitTests/spark-warehouse/'.; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@16ba3696{/SQL,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2780d0b8{/SQL/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7cea1161{/SQL/execution,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@696b1f0{/SQL/execution/json,null,AVAILABLE,@Spark}; 2018-10-09 15:04:33 ContextHan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513:13597,load,loading,13597,https://hail.is,https://github.com/hail-is/hail/issues/4513,1,['load'],['loading']
Performance,"Context:; For LDPrune, we need to compute a correlation matrix for the variants in the dataset, and then get an entries table from that matrix. Since computing the correlation matrix is slow, we need some infrastructure to help us compute only the rows in the entries table that we need to perform the filtering for LDPrune.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3185:290,perform,perform,290,https://hail.is,https://github.com/hail-is/hail/pull/3185,1,['perform'],['perform']
Performance,Convert BGEN asserts to ifs for performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3771:32,perform,performance,32,https://hail.is,https://github.com/hail-is/hail/pull/3771,1,['perform'],['performance']
Performance,Correct import_bgen docs based on the optimization I made last week (empty `entry_fields` takes a fast path that doesn't parse the genotypes),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3531:38,optimiz,optimization,38,https://hail.is,https://github.com/hail-is/hail/pull/3531,1,['optimiz'],['optimization']
Performance,"Could there be some functionality to make it easier to run GWAS on a large set of phenotypes at once? For instance, in metabolomics data sets there can be around 10,000 phenotypes (many of which are highly correlated or chemically related) and you'd like to see GWAS results from all of these. Rather than submitting 10,000 separate jobs, could there be a way to optimize this analysis by considering them all together? I'm sure this would be useful in other fields besides metabolomics.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/620:363,optimiz,optimize,363,https://hail.is,https://github.com/hail-is/hail/issues/620,1,['optimiz'],['optimize']
Performance,"Creating network namespaces can often take hundreds of milliseconds (and sometimes seconds with `iptables` contention), so Batch takes this off the job hot path by pre-allocating namespaces. All job namespaces are configured identically and there is a fixed number of ""slots"" on any batch worker (`CORES * 4`), so pre-allocation and asynchronous recycling of namespaces is fairly straight-forward so long as we never attempt to run more containers on a worker than the number of slots (which the scheduling system should prohibit). However, since we started running long-lived JVM containers (#11397), the number of containers running on a given worker can easily be *greater* than `N_SLOTS`. On a 16-core machine, we create 30 JVMs that sit idle waiting for JVMJobs all the while occupying a precious network namespace. I thought for the longest time that #13402 was a race condition so was trying to trigger it through a barrage of quick jobs. Turns out all it took was running >34 long-running jobs on a single 16-core worker. In a dev deploy of `main`, running a batch with 35 quarter-core `sleep 150` jobs fails with a single job timing out waiting for a network. On this branch, I am able to run the same 35 job batch as well as a batch with 64 quarter-core jobs. Unfortunately, we don't have a great way to test ""run all these jobs at once on the same worker"". Resolves #13402",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13678:870,race condition,race condition,870,https://hail.is,https://github.com/hail-is/hail/pull/13678,1,['race condition'],['race condition']
Performance,"Currently there is a race condition in which we can shut down the filestore before; the aiohttp app stops accepting connections. If that happens, jobs will get; partially scheduled on the worker, but there will be no working file store so the; jobs cannot complete successfully. The partially scheduled jobs in turn leave the; worker in a bad, non-idle state.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10949:21,race condition,race condition,21,https://hail.is,https://github.com/hail-is/hail/pull/10949,1,['race condition'],['race condition']
Performance,"Currently, tasks to schedule new instances are put on the event loop inside the `Pool` and `JobPrivateInstanceManager` constructors. `Pool.create` and `JobPrivateInstanceManager.create` first instantiate an object of their respective type and then load existing instances from the database into the in-memory instance collection. This could potentially cause the create instances loop to trigger while we're drawing ""existing"" instances, which causes the assertion error in https://github.com/hail-is/hail-tasks/issues/24 when the create instances loop and load instances query race to add the instance to the in-memory data structure. This change moves the task creation from the constructor to the `create` method, so we don't start creating instances until all existing instances are accounted for. I think I would have liked to simply pass the constructor a list of instances, but we can't create an `Instance` without an `InstanceCollection`. Resolves hail-is/hail-tasks#24. I also threw in a bit of cleanup, i.e. removing some variable assignments that didn't seem very helpful and resolving a lint issue where we used `items` where we could just use `values`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11766:248,load,load,248,https://hail.is,https://github.com/hail-is/hail/pull/11766,2,['load'],['load']
Performance,"Currently, the MJS and MJC requests from the worker to the driver for a given job can race, as they are run as independent asyncio tasks. This results in unnecessary database load and deadlocks between the MJS and MJC SQL procedures. Rather than address the procedures directly, we enforce that we will never run MJS and MJC concurrently. The system is resilient to never receiving an MJS (as MJC will add any attempt data if not present), so we can make the following changes to the worker:; - Serialize the submission of MJS and MJC requests by having the MJC task wait on the MJS future; - Give up retrying MJS once the job has completed because we will instead just send an MJC. This could potentially reduce the database load for very short jobs. I ran a load test of 10k `true` jobs and `sleep 5` jobs a few times against my namespace and saw 0 deadlocks 🎉",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11824:175,load,load,175,https://hail.is,https://github.com/hail-is/hail/pull/11824,4,"['concurren', 'load']","['concurrently', 'load']"
Performance,Custom parser for variant (key) in LoadVCF.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2419:35,Load,LoadVCF,35,https://hail.is,https://github.com/hail-is/hail/pull/2419,1,['Load'],['LoadVCF']
Performance,D$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13004,Load,LoadMatrix,13004,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,DD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:297); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3480:13642,concurren,concurrent,13642,https://hail.is,https://github.com/hail-is/hail/issues/3480,2,['concurren'],['concurrent']
Performance,DD.scala:1095); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2125); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). is.hail.utils.HailException: Error parsing row fields in row 0:; expected 5 fields but only 5 found.; File: foo; Line:; 7	75216143	75216143	C/T	+; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:28); 	at is.hail.io.LoadMatrixParser.parseLine(LoadMatrix.scala:33); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:383); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6$$anonfun$apply$7.apply(LoadMatrix.scala:377); 	at is.hail.utils.WithContext.wrap(Context.scala:41); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:377); 	at is.hail.io.LoadMatrix$$anonfun$16$$anonfun$apply$6.apply(LoadMatrix.scala:375); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:575); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:573); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5718:13067,Load,LoadMatrix,13067,https://hail.is,https://github.com/hail-is/hail/issues/5718,1,['Load'],['LoadMatrix']
Performance,"D`, possibly with some non-empty key. This is consistent with the rule that the `rvd` must always have a stronger/longer key than the `TableType`.; * **small tweaks** - Now I start working through the `TableIR` nodes, rewriting them to remove explicit uses of `UnpartitionedRVD`. The general plan is to sandwich the rvd logic between `toOrderedRVD` and `toOldStyleRVD`. The first takes an `UnpartitionedRVD` to an `OrderedRVD` with empty key (and leaves `OrderedRVD`s alone), and the second takes an `OrderedRVD` to an `UnpartitionedRVD` if its key was empty, and leaves it alone otherwise. Once they're all rewritten this way, I redefine `toOldStyleRVD` to always return `OrderedRVD`, and `UnpartitionedRVD` is no longer used.; * **remove `TableUnkey`** - With `UnpartitionedRVD` going away, `TableUnkey` is no longer necessary, it's equivalent to keying by an empty key.; * **small tweaks** - these next two rewrite more `TableIR` nodes; * **Merge master** - the big one; * **tweak MatrixColsTable** - 1) Optimize `coerce` by checking if the requested key is empty, avoiding a scan in that case. 2) Optimize `sortedColsValue` by checking if the column key is empty, avoiding the sort in that case. 3) Simplify `colsRVD`, removing the case on the type of the `RVD`, just calling `coerce` and letting the previous optimizations avoid unnecessary work.; * **`distinctByKey` fix** - While looking over `TableIR` implementations, I noticed a bug in `distinctByKey`: you need to be sure no key is split across multiple partitions. To be sure the empty key edge case still works, I added a test to check that `strictify` on an empty-key partitioner will always collapse everything to one partition.; * **Flipped switch** - redifines `toOldStyleRVD` to just return the `OrderedRVD` unchanged, and asserts that `TableValue.rvd` is always an `OrderedRVD`.; * **rest of the `TableIR` tweaks** - added a factory method `OrderedRVD.unkeyed` to replace `UnpartitionedRVD.apply`.; * the rest are simple tidying up",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4319:1927,Optimiz,Optimize,1927,https://hail.is,https://github.com/hail-is/hail/pull/4319,2,"['Optimiz', 'optimiz']","['Optimize', 'optimizations']"
Performance,Data formats that use Characters (including old VDSes) will now be loaded as `TString`s. Resolves #1710,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1745:67,load,loaded,67,https://hail.is,https://github.com/hail-is/hail/pull/1745,1,['load'],['loaded']
Performance,"Default persist (used by ServiceBackend) currently does nothing. As we've discussed, there's more work to flesh out a cache/persist strategy in the new setting.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5158:118,cache,cache,118,https://hail.is,https://github.com/hail-is/hail/pull/5158,1,['cache'],['cache']
Performance,"Defines a TStream/PStream type stub. I've omitted some number of things that other types need to define, as the purpose of the stream type is going to be to ensure that we're never fully instantiating collections where we shouldn't be, e.g. all the rows in a table partition. To that end, I've omitted definitions for ordering since I don't forsee a need for ordering on the entire stream (as opposed to on the element, or a subset thereof), as well as generators for annotations, etc. It basically otherwise mimics the PArray/TArray definitions, but I've made it extend Type/PType directly since most of the extra methods on containers seem irrelevant to streams, having mostly to do with e.g. length and loading specific elements. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5610:706,load,loading,706,https://hail.is,https://github.com/hail-is/hail/pull/5610,1,['load'],['loading']
Performance,"Deploying grafana in our GKE cluster gives us instant and easy access to the stackdriver backend with the same querying capabilities of our current front-end, but without the clutter and insanely slow load times. See [here](https://internal.hail.is/dgoldste/grafana/d/TVkleyLMk/detailed-service-resource-utilization?orgId=1) for some example dashboards I set up to look at resources across our services (credentials are the default admin/admin). This alleviates the immediate pain of using the console (for metrics only, not logging), but my longer aim is that getting more regular use out of our metrics can reveal deeper pain points of our monitoring stack and if/where we need to eat up more responsibility from google monitoring. This is a StatefulSet, so configuration through the UI will persist and is done manually. If we find that our dashboards are stable and boilerplate enough, I'd like to move to a code-based dashboard configuration. Sadly, `check-yaml` does not appreciate our jinja templating in yaml, so I've removed it for now. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10013:201,load,load,201,https://hail.is,https://github.com/hail-is/hail/pull/10013,1,['load'],['load']
Performance,Discovered because I accidentally had an old build directory in $HAIL/hail/python that was sneakily being rsync'd into the build/deploy directory and then silently picked up by bdist_wheel as a build cache.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12008:200,cache,cache,200,https://hail.is,https://github.com/hail-is/hail/pull/12008,1,['cache'],['cache']
Performance,Docker requires the `-f` tag to purge an image from its cache if multiple tags reference the same image ID. Checked to make sure that this couldn't accidentally disturb the worker container and added an assert because we should never even try to remove the worker image.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10681:56,cache,cache,56,https://hail.is,https://github.com/hail-is/hail/pull/10681,1,['cache'],['cache']
Performance,Don't construct intermediate string.; Print full exception on metadata load failure.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/899:71,load,load,71,https://hail.is,https://github.com/hail-is/hail/pull/899,1,['load'],['load']
Performance,"Don't presist in repartition/coalesce.; Fixed Int overflow bug in OrderedRDD.coalesce.; Convert to GenotypeStream in cache, persist.; Fixed filteralleles annotation bug.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/915:117,cache,cache,117,https://hail.is,https://github.com/hail-is/hail/pull/915,1,['cache'],['cache']
Performance,"Don't split variable-length encoded ints across compression blocks. Save a comparison in the inner loop. As usual, has seemingly negligible effect on performance. I'm sure it would be significant on the C side. I don't encode as signed yet. It makes the termination condition more complicated and I want to think on it a bit more.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2430:150,perform,performance,150,https://hail.is,https://github.com/hail-is/hail/pull/2430,1,['perform'],['performance']
Performance,Dramatically improves performance of PC Relate. Some prior discussion at #2280 .,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2365:22,perform,performance,22,https://hail.is,https://github.com/hail-is/hail/pull/2365,1,['perform'],['performance']
Performance,"END is missing, add it, _add_end is a no-op if END is already present; + reference_data = VariantDataset._add_end(reference_data); +; vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; ```. There was nothing in the IR that stood out when I examined it, but I will admit that I'm not the best at digging into it. ### Version. https://github.com/chrisvittal/hail/tree/vds/repro-example. ### Relevant log output. ```shell; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 140a68008/00000001: not in 140a58008/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadByte(Memory.java:130); E 	at is.hail.annotations.Region$.loadByte(Region.scala:28); E 	at is.hail.annotations.Region$.loadBit(Region.scala:86); E 	at __C23148collect_distributed_array_matrix_native_writer.__m23333split_ToArray(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region478_486(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region16_503(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply_region14_529(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at __C23148collect_distributed_array_matrix_native_writer.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$10(BackendUtils.scala:90); E 	at is.hail.utils.package$.using(package.scala:673); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:166); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$9(BackendUtils.scala:89); E 	at is.hail.backend.local.LocalBackend.$anonfun$parallelizeAndComputeWithIndex$4(LocalBackend.scala:150); E 	at is.hail.uti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14705:2704,load,loadBit,2704,https://hail.is,https://github.com/hail-is/hail/issues/14705,1,['load'],['loadBit']
Performance,"Environment, Markup, FileSystemLoader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1620,Concurren,ConcurrentRestrictions,1620,https://hail.is,https://github.com/hail-is/hail/issues/11705,1,['Concurren'],['ConcurrentRestrictions']
Performance,"Environment:; - Spark 3.2.0; - Scala 2.12.15. Running: ; ```; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.2.0; ```; I get the error:; ```BUILD SUCCESSFUL in 2m 5s; 3 actionable tasks: 3 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; make: *** No rule to make target 'check-pip-lockfiles', needed by 'install-on-cluster'. Stop.; ```. Issue is fixed for me by renaming `install-on-cluster: $(WHEEL) check-pip-lockfiles` -> `install-on-cluster: $(WHEEL) check-pip-lockfile` on line 344 of hail/Makefile. Many thanks,; Barney",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12568:697,cache,cache,697,https://hail.is,https://github.com/hail-is/hail/issues/12568,1,['cache'],['cache']
Performance,"Error message is this:. ```; org.apache.spark.SparkException: Task not serializable. Caused by: java.io.NotSerializableException: htsjdk.samtools.reference.FastaSequenceIndex; Serialization stack:; 	- object not serializable (class: htsjdk.samtools.reference.FastaSequenceIndex, value: htsjdk.samtools.reference.FastaSequenceIndex@e7b265e); 	- writeObject data (class: java.util.HashMap); 	- object (class is.hail.io.reference.FastaReader$$anon$1, {}); 	- field (class: is.hail.io.reference.FastaReader, name: cache, type: class java.util.LinkedHashMap); 	- object (class is.hail.io.reference.FastaReader, is.hail.io.reference.FastaReader@5a0e0886); 	- field (class: is.hail.variant.GenomeReference, name: fastaReader, type: class is.hail.io.reference.FastaReader); 	- object (class is.hail.variant.GenomeReference, test); 	- field (class: is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, name: gr$13, type: class is.hail.variant.GRBase); 	- object (class is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, <function2>). plus many more lines; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2881:510,cache,cache,510,https://hail.is,https://github.com/hail-is/hail/pull/2881,1,['cache'],['cache']
Performance,"Error messages from GCR and AR are different, and sometimes it looks like this from GCR:. ```; ""unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12708:151,perform,perform,151,https://hail.is,https://github.com/hail-is/hail/pull/12708,1,['perform'],['perform']
Performance,"Error summary: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. ```; In [4]: tsvs = hl.hadoop_ls(""gs://my-bucket/*.tsv*""); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-4-3a6cee08b392> in <module>; ----> 1 tsvs = hl.hadoop_ls(""gs://my-bucket/*.tsv*""). /Library/Python/3.7/site-packages/hail/utils/hadoop_utils.py in hadoop_ls(path); 212 :obj:`list` [:obj:`dict`]; 213 """"""; --> 214 return Env.fs().ls(path); 215; 216. /Library/Python/3.7/site-packages/hail/fs/hadoop_fs.py in ls(self, path); 40; 41 def ls(self, path: str) -> List[Dict]:; ---> 42 return json.loads(self._utils_package_object.ls(self._jfs, path)); 43; 44 def mkdir(self, path: str) -> None:. /Library/Python/3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1255 answer = self.gateway_client.send_command(command); 1256 return_value = get_return_value(; -> 1257 answer, self.gateway_client, self.target_id, self.name); 1258; 1259 for temp_arg in temp_args:. /Library/Python/3.7/site-packages/hail/backend/spark_backend.py in deco(*args, **kwargs); 49 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 50 'Hail version: %s\n'; ---> 51 'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None; 52 except pyspark.sql.utils.CapturedException as e:; 53 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz. Java stack trace:; java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: CCDG::133.tsv.bgz; 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.concurrentGlobInternal(GoogleHadoopFileSystemBase.java:1284); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1261); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607:671,load,loads,671,https://hail.is,https://github.com/hail-is/hail/issues/9607,1,['load'],['loads']
Performance,Example stack trace:; ```; 2022-02-08 18:09:30 root: ERROR: IllegalArgumentException: requirement failed; From java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.rvd.RVDPartitioner.<init>(RVDPartitioner.scala:52); 	at is.hail.rvd.RVDPartitioner.extendKeySamePartitions(RVDPartitioner.scala:141); 	at is.hail.expr.ir.LoweredTableReader$$anon$2.coerce(TableIR.scala:382); 	at is.hail.expr.ir.GenericTableValue.toTableStage(GenericTableValue.scala:162); 	at is.hail.io.vcf.MatrixVCFReader.lower(LoadVCF.scala:1790); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:581); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1304); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$2(LowerTableIR.scala:561); 	at is.hail.expr.ir.lowering.LowerTableIR$.applyTable(LowerTableIR.scala:1035); 	at is.hail.expr.ir.lowering.LowerTableIR$.lower$1(LowerTableIR.scala:394); 	at is.hail.expr.ir.lowering.LowerTableIR$.apply(LowerTableIR.scala:547); 	at is.hail.expr.ir.lowering.LowerToCDA$.lower(LowerToCDA.scala:69); 	at is.hail.expr.ir.lowering.LowerToCDA$.apply(LowerToCDA.scala:18); 	at is.hail.expr.ir.lowering.LowerToDistributedArrayPass.transform(LoweringPass.scala:77). ```. called from here:; https://github.com/hail-is/hail/blob/d2f87d81dd1af43617740309e354d4bac8c672e0/hail/src/main/scala/is/hail/expr/ir/TableIR.scala#L382,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11335:554,Load,LoadVCF,554,https://hail.is,https://github.com/hail-is/hail/issues/11335,1,['Load'],['LoadVCF']
Performance,Executor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:11301,Load,LoadVCF,11301,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,"Failed to annotate a large vcf with vep. Command:; hail-new-vep read -i /user/aganna/CANCER.vds \; vep --config /psych/genetics_data/working/cseed/vep.properties \; write -o /user/aganna/CANCER.vep.vds. Error:; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/broadinstitute/hail/methods/VCFReport$; at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:125); at org.broadinstitute.hail.driver.Main$.main(Main.scala:276); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.broadinstitute.hail.methods.VCFReport$; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 12 more. [hail.log.txt](https://github.com/broadinstitute/hail/files/222874/hail.log.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/303:1357,load,loadClass,1357,https://hail.is,https://github.com/hail-is/hail/issues/303,3,['load'],['loadClass']
Performance,"FatalError: HailException: optimization changed type!; before: Matrix{global:+Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh38),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{CIEND:Array[Int32],CIPOS:Array[Int32],CS:String,END:Int32,IMPRECISE:Boolean,MC:Array[String],MEINFO:Array[String],MEND:Int32,MLEN:Int32,MSTART:Int32,SVLEN:Array[Int32],SVTYPE:String,TSD:String,AC:Array[Int32],AF:Array[Float64],NS:Int32,AN:Int32,EAS_AF:Array[Float64],EUR_AF:Array[Float64],AFR_AF:Array[Float64],AMR_AF:Array[Float64],SAS_AF:Array[Float64],DP:Int32,AA:String,VT:Array[String],EX_TARGET:Boolean,MULTI_ALLELIC:Boolean,STRAND_FLIP:Boolean,REF_SWITCH:Boolean,DEPRECATED_RSID:Array[String],RSID_REMOVED:Array[String],GRCH37_38_REF_STRING_MATCH:Boolean,NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH:Boolean,GRCH37_POS:Int32,GRCH37_REF:String,ALLELE_TRANSFORM:Boolean,REF_NEW_ALLELE:Boolean,CHROM_CHANGE_BETWEEN_ASSEMBLIES:Array[String]},a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh38),old_alleles:Array[String]},entry:Struct{GT:Call}}; after: Matrix{global:+Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh38),alleles:Array[String],rsid:String,qual:Float64,filters:Set[String],info:Struct{CIEND:Array[Int32],CIPOS:Array[Int32],CS:String,END:Int32,IMPRECISE:Boolean,MC:Array[String],MEINFO:Array[String],MEND:Int32,MLEN:Int32,MSTART:Int32,SVLEN:Array[Int32],SVTYPE:String,TSD:String,AC:Array[Int32],AF:Array[Float64],NS:Int32,AN:Int32,EAS_AF:Array[Float64],EUR_AF:Array[Float64],AFR_AF:Array[Float64],AMR_AF:Array[Float64],SAS_AF:Array[Float64],DP:Int32,AA:String,VT:Array[String],EX_TARGET:Boolean,MULTI_ALLELIC:Boolean,STRAND_FLIP:Boolean,REF_SWITCH:Boolean,DEPRECATED_RSID:Array[String],RSID_REMOVED:Array[String],GRCH37_38_REF_STRING_MATCH:Boolean,NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH:Boolean,GRCH37_POS:Int32,GRCH37_REF:String,ALLELE_TRANSFORM:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4524:59,optimiz,optimization,59,https://hail.is,https://github.com/hail-is/hail/issues/4524,1,['optimiz'],['optimization']
Performance,"Feedback welcome, not for merging. Goal with ScoreCovariance was to get a working interface rather than optimize speed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2219:104,optimiz,optimize,104,https://hail.is,https://github.com/hail-is/hail/pull/2219,1,['optimiz'],['optimize']
Performance,Felt a little silly that I can't run `hailctl curl default batch /` and see some HTML output. Perhaps more importantly this would allow a more sophisticated client-side UI to call `/api` methods if it wants to perform an action that isn't a page reload. This would also allow us to proxy the backend in local development environments (though I had written this prior to our recent discussion. Stacking on the CSRF PR #13604,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13616:210,perform,perform,210,https://hail.is,https://github.com/hail-is/hail/pull/13616,1,['perform'],['perform']
Performance,"FileNotFoundError as exc:; raise FatalError('Hail internal error. Please contact the Hail team and provide the following information.\n\n' + yamlx.dump({; 'service_backend_debug_info': self.debug_info(),; 'batch_debug_info': await self._batch.debug_info(); })) from exc; ; async with driver_output as outfile:; success = await read_bool(outfile); if success:; return await read_bytes(outfile); ; short_message = await read_str(outfile); expanded_message = await read_str(outfile); error_id = await read_int(outfile); ; reconstructed_error = fatal_error_from_java_error_triplet(short_message, expanded_message, error_id); if ir is None:; raise reconstructed_error; > raise reconstructed_error.maybe_user_error(ir); E hail.utils.java.FatalError: RuntimeException: Stream is already closed.; E ; E Java stack trace:; E java.util.concurrent.ExecutionException: java.lang.RuntimeException: Stream is already closed.; E 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); E 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); E 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:150); E 	at is.hail.backend.BackendUtils.collectDArray(BackendUtils.scala:44); E 	at __C256669Compiled.__m256730split_CollectDistributedArray(Emit.scala); E 	at __C256669Compiled.__m256689split_Let(Emit.scala); E 	at __C256669Compiled.apply(Emit.scala); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$_apply$7(CompileAndEvaluate.scala:74); E 	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:74); E 	at is.hail.expr.ir.CompileAndEvaluate$.$anonfun$apply$1(CompileAndEvaluate.scala:19); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:19); E 	at is.hail.expr.ir.lowering.LowerDistributedSort$.distribu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12976:13874,concurren,concurrent,13874,https://hail.is,https://github.com/hail-is/hail/issues/12976,1,['concurren'],['concurrent']
Performance,"First crack at supporting multi phenotype logistic regression. No matrix optimizations, as is implemented in multi phenotype linear regression, but I attempt to follow a similar approach as far as far as API and single call of mapPartitions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5072:73,optimiz,optimizations,73,https://hail.is,https://github.com/hail-is/hail/pull/5072,1,['optimiz'],['optimizations']
Performance,First step in fixing: https://github.com/hail-is/hail/issues/5358. @chrisvittal FYI this should improve the performance of multiple aggregations across samples in MatrixTable.anntoate_rows.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5598:108,perform,performance,108,https://hail.is,https://github.com/hail-is/hail/pull/5598,1,['perform'],['performance']
Performance,"First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. I ran with -DD, the file is there in gs://, something is going wrong in the container. It happens with and without -m. I tried to upgrade google/cloud-sdk, but ran into a problem: after updating the instance base image, the worker container can no longer get credentials from the metadata server and therefore gets permission denied when trying to copy out the logs. Upon reflection, in our setup, containers being able to access the metadata server seems very insecure! So we should (1) make sure containers we run can't access the metadata service, (2) run the instance as no service account, or an account with no privileges. Then we need to figure out how to get the credentials to to the worker to copy out logs. I also added a retry (3x) to the setup/cleanup scripts. I think ultimately using the client libraries directly instead of gsutil might ultimately be the way to go (and it makes it easier for us to see what errors we're getting and which we want to retry). Changes:; - retry in setup/cleanup; - fix ""make deploy"" in batch2 (build worker image); - I fixed up the worker Google image builder logic. There was a race condition with the step command. I broke it into two manual steps. The instance steps itself in the first step. The user should verify the instance is stopped and then run the second step. This can be automated later.; - Fixed bug in mark_jobs_complete updating ready_cores. It counted all children, not just children that are going to transition to ready.; - fixed bug in delete tables script: batch => batches",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7445:1326,race condition,race condition,1326,https://hail.is,https://github.com/hail-is/hail/pull/7445,1,['race condition'],['race condition']
Performance,Fix IR size printed in log statement after optimize,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5441:43,optimiz,optimize,43,https://hail.is,https://github.com/hail-is/hail/pull/5441,1,['optimiz'],['optimize']
Performance,Fix LoadVCF exception message,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2402:4,Load,LoadVCF,4,https://hail.is,https://github.com/hail-is/hail/pull/2402,1,['Load'],['LoadVCF']
Performance,Fix performance of Table.head,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5156:4,perform,performance,4,https://hail.is,https://github.com/hail-is/hail/pull/5156,1,['perform'],['performance']
Performance,Fix table/matrixtable construction performance,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4952:35,perform,performance,35,https://hail.is,https://github.com/hail-is/hail/pull/4952,1,['perform'],['performance']
Performance,Fix this issue:. ```; XMLHttpRequest cannot load https://github.com/hail-is/hail/blob/master/www/navbar.html. Origin http://discuss.hail.is is not allowed by Access-Control-Allow-Origin.; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1024:44,load,load,44,https://hail.is,https://github.com/hail-is/hail/issues/1024,1,['load'],['load']
Performance,Fix wasSplit for BGEN loader [0.1],MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2239:22,load,loader,22,https://hail.is,https://github.com/hail-is/hail/pull/2239,1,['load'],['loader']
Performance,Fixed reuse of code in loadElement in SJavaArray,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10901:23,load,loadElement,23,https://hail.is,https://github.com/hail-is/hail/pull/10901,1,['load'],['loadElement']
Performance,"Fixes #13346. Another user was confused by this: https://github.com/hail-is/hail/issues/14102. Unfortunately, the world appears to have embraced missing values in VCF array fields even though the single element case is ambiguous. In #13346, I proposed a scheme by which we can disambiguate many of the cases, but implementing it ran into challenges because LoadVCF.scala does not expose whether or not an INFO field was a literal ""."" or elided entirely from that line. Anyway, this error message actually points users to the fix. I also changed some method names such that every method is ArrayType and never TypeArray.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14105:357,Load,LoadVCF,357,https://hail.is,https://github.com/hail-is/hail/pull/14105,1,['Load'],['LoadVCF']
Performance,"Fixes #4251. @jbloom22 Is there anything else I should add? Maybe something about the relative performance of each approach? I also thought about using two separate linreg aggregator annotations, but that didn't seem better than the group_by approach.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4458:95,perform,performance,95,https://hail.is,https://github.com/hail-is/hail/pull/4458,1,['perform'],['performance']
Performance,"Fixes #5095 and tips Google that we don't want it to index /devel /stable (also 301 redirects are cached, should be faster).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5344:98,cache,cached,98,https://hail.is,https://github.com/hail-is/hail/pull/5344,1,['cache'],['cached']
Performance,"Fixes #5449. We don't have machinery for testing performance behavior of something; like show() right now, so I can't test it easily. But I did verify by; hand.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5468:49,perform,performance,49,https://hail.is,https://github.com/hail-is/hail/pull/5468,1,['perform'],['performance']
Performance,Fixes #7063. I forgot to change the cached methods in Emit to account for return type when I changed it to allow a single function to specify multiple return types.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7066:36,cache,cached,36,https://hail.is,https://github.com/hail-is/hail/pull/7066,1,['cache'],['cached']
Performance,Fixes O(N) performance of mt.entries().show(),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7867:11,perform,performance,11,https://hail.is,https://github.com/hail-is/hail/pull/7867,1,['perform'],['performance']
Performance,"Fixes connection timeout after 8 hours. . When we transition to aiomysql, will port well to a pooled connection version (`async with self.pool.acquire() as conn:`. Even now however, the time it takes to acquire a connection is not the bottleneck during login. cc @danking assigned you as well in case you're on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5815:235,bottleneck,bottleneck,235,https://hail.is,https://github.com/hail-is/hail/pull/5815,1,['bottleneck'],['bottleneck']
Performance,"For #604: I changed the max-width to 80em from 45em. If this is not wide enough, then we should probably remove the max-width property. For #605: It was extremely difficult to replicate the issue, but I believe it's because the mathjax and jquery operations are running asynchronously and the mathjax finishes before the jquery code has finished populating the DOM. I added a ""defer"" attribute to the mathjax script loading, so the script downloads in the background, but doesn't get executed until the DOM has been populated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/630:416,load,loading,416,https://hail.is,https://github.com/hail-is/hail/pull/630,1,['load'],['loading']
Performance,"For example,; ```; mt = hl.split_multi_hts(mt, permit_shuffle=True); mt = mt.annotate_cols(foo=hl.agg.sum(...)); ```; performs the aggregation in one giant partition. This is a known issue, and we are working on it. A workaround is to write the result of `split_multi` to disk before using it (which is a good idea regardless). The cause appears to the interaction of several things:; * `split_multi` computes two separate pieces, then merges them together with a `TableUnion`; * The aggregation doesn't care how the mt is keyed, and we propagate that upstream; * Because of that, the `TableUnion` ends up unioning two unkeyed tables; * `TableUnion`, as currently implemented, can only produce a result with a strict partitioner. In the case of no key fields, that means one partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13407:118,perform,performs,118,https://hail.is,https://github.com/hail-is/hail/issues/13407,1,['perform'],['performs']
Performance,"For some reason we delete secrets when re-creating them. This creates a race condition which Batch; appears to encounter whenever it is under heavy load during a deploy. @cseed, do you recall why you chose to delete and then use apply in ci/create_database.py?. I am using the command sequence suggested here: https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10217:72,race condition,race condition,72,https://hail.is,https://github.com/hail-is/hail/pull/10217,2,"['load', 'race condition']","['load', 'race condition']"
Performance,"Forgot that we were importing navbar.css. We could go back to a separate navbar.css, but I think that is less optimal because I don't think you can include the html directly in xslt, and importing it using jquery can cause the navbar to flash immediately after page load.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8489:266,load,load,266,https://hail.is,https://github.com/hail-is/hail/pull/8489,1,['load'],['load']
Performance,"Found the problem where EXISTS in a correlated subquery should be rewritten as IN; https://dev.mysql.com/doc/refman/5.7/en/optimizing-subqueries.html. ```; -> FROM jobs; -> INNER JOIN batches ON jobs.batch_id = batches.id; -> LEFT JOIN aggregated_job_resources; -> ON jobs.batch_id = aggregated_job_resources.batch_id AND; -> jobs.job_id = aggregated_job_resources.job_id; -> LEFT JOIN resources; -> ON aggregated_job_resources.resource = resources.resource; -> INNER JOIN job_attributes; -> ON jobs.batch_id = job_attributes.batch_id AND; -> jobs.job_id = job_attributes.job_id AND; -> job_attributes.`key` = 'name'; -> WHERE (jobs.batch_id = 14327) AND; -> (jobs.batch_id, jobs.job_id) IN (SELECT batch_id, job_id FROM job_attributes WHERE `key` = 'pheno' AND `value` = '50'); -> GROUP BY jobs.batch_id, jobs.job_id; -> ORDER BY jobs.batch_id, jobs.job_id ASC; -> LIMIT 50;; +----+-------------+--------------------------+------------+--------+--------------------------------------------------+--------------------------+---------+-----------------------------------------+------+----------+---------------------------------+; | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |; +----+-------------+--------------------------+------------+--------+--------------------------------------------------+--------------------------+---------+-----------------------------------------+------+----------+---------------------------------+; | 1 | SIMPLE | batches | NULL | const | PRIMARY | PRIMARY | 8 | const | 1 | 100.00 | Using temporary; Using filesort |; | 1 | SIMPLE | job_attributes | NULL | ref | PRIMARY,job_attributes_key_value | job_attributes_key_value | 1081 | const,const,const | 3057 | 100.00 | Using where |; | 1 | SIMPLE | jobs | NULL | eq_ref | PRIMARY,jobs_batch_id_state_always_run_cancelled | PRIMARY | 12 | const,batch.job_attributes.job_id | 1 | 100.00 | NULL |; | 1 | SIMPLE | job_attributes | NULL | eq_ref | PRIMARY,jo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9870:123,optimiz,optimizing-subqueries,123,https://hail.is,https://github.com/hail-is/hail/pull/9870,1,['optimiz'],['optimizing-subqueries']
Performance,"Francesco found that after filtering to the purcell 5k (6k sites after a `count`), the loadings file produced by PCA only contained 400 variants.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/806:87,load,loadings,87,https://hail.is,https://github.com/hail-is/hail/issues/806,1,['load'],['loadings']
Performance,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3447:62,load,loaded,62,https://hail.is,https://github.com/hail-is/hail/issues/3447,1,['load'],['loaded']
Performance,"From Cotton:. I think the request/s and request latency metrics in Grafana are not actually the metrics for the Kubernetes service as we'd hoped. In particular, the reqs/s makes no sense. This post:. https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-4-the-kubernetes-api-server-72f1e1210770. indicates we should have an apiserver_request_count and apiserver_request_latencies_bucket which sound like what we want. They give ""the Prometheus configuration for getting metrics from the Kubernetes API server, even in environments where the masters are hosted for you"" (and I think our master is hosted for us in GKE). In particular, we have no analogous apiserver scrape config with ""role: endpoints"" in our setup. Here is the apiserver code with all the metrics they collect: https://github.com/kubernetes/apiserver/blob/master/pkg/endpoints/metrics/metrics.go",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6495:48,latency,latency,48,https://hail.is,https://github.com/hail-is/hail/issues/6495,1,['latency'],['latency']
Performance,From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1646); 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:1986); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1973); 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:1986); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:2306,Load,LoadPlink,2306,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,"Full error message:. ```[Stage 2:> (0 + 0) / 16]2018-02-28 23:41:58 Hail: INFO: interval filter loaded 2453 of 103675 partitions; 2018-02-28 23:42:08 Hail: WARN: deprecation: 'drop_cols' will be removed before 0.2 release; Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 415, in check_all; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 53, in check; hail.typecheck.check.TypecheckFailure. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 220, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 94, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 187, in main; vds, sample_table = generate_qc_annotations(vds, medians=not args.skip_medians); File ""<decorator-gen-616>"", line 2, in __getitem__; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 478, in _typecheck; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 425, in check_all; TypeError: __getitem__: parameter 'item': expected (str or tuple[(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression]),(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression])]), found int: '0'```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3038:96,load,loaded,96,https://hail.is,https://github.com/hail-is/hail/issues/3038,1,['load'],['loaded']
Performance,Generates a lot of bytecode and called in performance-sensitive places. Helps with profiling a bit.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10337:42,perform,performance-sensitive,42,https://hail.is,https://github.com/hail-is/hail/pull/10337,1,['perform'],['performance-sensitive']
Performance,"Getting this over the past few days when doing, well, basically any query. Log: [hail.log.txt](https://github.com/hail-is/hail/files/755839/hail.log.txt). ```; Caused by: java.lang.ClassNotFoundException: is.hail.sparkextras.ReorderedPartitionsRDDPartition; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1360:348,load,loadClass,348,https://hail.is,https://github.com/hail-is/hail/issues/1360,2,['load'],['loadClass']
Performance,"Getting this with current master on the cloud:. ```; Use of uninitialized value in hash element at /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/Bio/EnsEMBL/Variation/Utils/VEP.pm line 4255, <VARS> line 1.; [Stage 18:=> (273 + 410) / 13592]Traceback (most recent call last):; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 75, in <module>; main(args, pops); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/subset.py"", line 51, in main; 'va.rf').write(args.output + "".autosomes.vds"", overwrite=True); File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/utils.py"", line 452, in post_process_vds; vds = vds.vep(config=vep_config, csq=True, root='va.info.CSQ', force=True); File ""<decorator-gen-110>"", line 2, in vep; File ""/tmp/7ff73b01-6ea1-4254-a49d-01e9075ab5b0/pyhail-attr.zip/hail/java.py"", line 93, in handle_py4j; hail.java.FatalError: NoSuchElementException: None.get; [Stage 18:=> (277 + 409) / 13592]java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@2a632cbb rejected from java.util.concurrent.ThreadPoolExecutor@974d518[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2913]; ```. Lmk if you need more log.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1518:948,concurren,concurrent,948,https://hail.is,https://github.com/hail-is/hail/issues/1518,4,"['concurren', 'queue']","['concurrent', 'queued']"
Performance,"Given our rate limit increases and turning on additional service tests, 5 concurrent PR batches is too much for the 4-core database to handle. This is a mitigation while we figure out the right way to maintain that load.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11762:74,concurren,concurrent,74,https://hail.is,https://github.com/hail-is/hail/pull/11762,2,"['concurren', 'load']","['concurrent', 'load']"
Performance,"God it feels good to make a pull request again. The following operation copies every file in gs://danking/test/ into a folder named s3://hail-test-bucket/test/. python3 -m hailtop.aiotools.copy null '[{""from"": ""gs://danking/test"", ""to"": ""s3://hail-test-bucket/test""}]'. The `null` can be replaced with a quoted GCP project id if one of the sources is a Requester Pays bucket. For example:. python3 -m hailtop.aiotools.copy '""broad-ctsa""' '[{""from"": ""gs://hail-datasets-us/"", ""to"": ""s3://hail-datasets-us-east-1""}]'. As you can see, the syntax is rough, but expressive. Explicitly listing all the files to be transferred should not impair the throughput. python3 -m hailtop.aiotools.copy null '[; {""from"": ""gs://danking/test/data1"", ""to"": ""s3://hail-test-bucket/test/data1""},; {""from"": ""gs://danking/test/data2"", ""to"": ""s3://hail-test-bucket/test/data2""},; {""from"": ""gs://danking/test/sub-directory1"", ""to"": ""s3://hail-test-bucket/test/sub-directory1""},; ...; ]'. Better syntax is forthcoming: https://github.com/hail-is/hail/pull/9913",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10778:642,throughput,throughput,642,https://hail.is,https://github.com/hail-is/hail/pull/10778,1,['throughput'],['throughput']
Performance,"Google Cloud Storage documentation and [best practices] for object names; recommends avoiding sequential names like 'part-0nnnn'. We already use; UUIDs for randomness to avoid two tasks writing to the exact same; object, but by using the UUID as a prefix rather than a suffix we; (to a degree) uniformly distribute part file names over a range,; (hopefully) improving throughput. [best practices]: https://cloud.google.com/storage/docs/best-practices#naming",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10836:368,throughput,throughput,368,https://hail.is,https://github.com/hail-is/hail/pull/10836,1,['throughput'],['throughput']
Performance,"Hail Version: 0.2.108. Running the line w/ Zstd compressed UKB bgen files:; `ht = hl.experimental.pc_project(; mt.GT,; loadings_ht.loadings,; loadings_ht.pca_af,; ); `; I get the error at the end of spark execution: ; `Exception in thread ""map-output-dispatcher-0"" Exception in thread ""map-output-dispatcher-1"" java.lang.UnsatisfiedLinkError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.recommendedCOutSize()J; `; (full error attached); [error.txt](https://github.com/hail-is/hail/files/10458606/error.txt). Any ideas?; Many Thanks,; Barney",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12608:131,load,loadings,131,https://hail.is,https://github.com/hail-is/hail/issues/12608,1,['load'],['loadings']
Performance,Hail cannot load several plink files in parallel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3975:12,load,load,12,https://hail.is,https://github.com/hail-is/hail/issues/3975,1,['load'],['load']
Performance,"Hail failed to load VCF with haploid calls, such as those on chr Y or chrX nonPar male regions. According to VCF spec v4.2, those haploid calls should be represented as 0 or 1 instead of 0/0, or 1/1.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1010:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/issues/1010,1,['load'],['load']
Performance,Hail failed to load haploid calls on chrX nonPar male or chrY regions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1010:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/issues/1010,1,['load'],['load']
Performance,Hail optimizer changes the type but does not show a difference in printable type,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527:5,optimiz,optimizer,5,https://hail.is,https://github.com/hail-is/hail/issues/4527,1,['optimiz'],['optimizer']
Performance,"Hail seems to always use the first reference genome that is used within an operation. Using this example file:; ```; contig_38	pos_38	contig_37	pos_37; chr1	10000	1	10000; ```. ---. The following code; ```; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```; Fails with ""HailException: Invalid locus 'chr1:10000' found. Contig 'chr1' is not in the reference genome 'GRCh37'."". ---. Reversing the order of those annotations changes the error message.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds.show(); ```; Fails with ""HailException: Invalid locus '1:10000' found. Contig '1' is not in the reference genome 'GRCh38'."". ---. And it works with a `cache` in between the annotations.; ```python; ds = hl.import_table(""test.tsv"", types={""pos_38"": hl.tint, ""pos_37"": hl.tint}); ds = ds.annotate(locus_37=hl.locus(ds.contig_37, ds.pos_37, reference_genome=""GRCh37"")); ds = ds.cache(); ds = ds.annotate(locus_38=hl.locus(ds.contig_38, ds.pos_38, reference_genome=""GRCh38"")); ds.show(); ```. outputs; ```; +-----------+--------+-----------+--------+---------------+---------------+; | contig_38 | pos_38 | contig_37 | pos_37 | locus_37 | locus_38 |; +-----------+--------+-----------+--------+---------------+---------------+; | str | int32 | str | int32 | locus<GRCh37> | locus<GRCh38> |; +-----------+--------+-----------+--------+---------------+---------------+; | ""chr1"" | 10000 | ""1"" | 10000 | 1:10000 | chr1:10000 |; +-----------+--------+-----------+--------+---------------+---------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7063:1096,cache,cache,1096,https://hail.is,https://github.com/hail-is/hail/issues/7063,2,['cache'],['cache']
Performance,Hail's Matrix Multiply Should Perform Better When Massively Reducing Size,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1975:30,Perform,Perform,30,https://hail.is,https://github.com/hail-is/hail/issues/1975,1,['Perform'],['Perform']
Performance,"Hail's optimizer should be smart enough to push `TableFilter` into a `TableExplode`. Consider these two equivalent pipelines on *tiny* data, a ten-by-ten matrix. ```; import hail as hl; mt = hl.balding_nichols_model(3, 10, 10); t = mt.entries(); t.filter(t.GT.is_hom_ref()).export('foo.tsv'); ```; ```; foo.tsv; merge time: 45.459ms; ```. ```; import hail as hl; mt = hl.balding_nichols_model(3,10,10); mt.filter_entries(mt.GT.is_hom_ref()).entries().export('foo2.tsv'); ```; ```; foo2.tsv; merge time: 23.856ms; ```. This will likely also require improving Hail's filter movement. I observed a `TableFilter` getting stuck behind a `TableMapGlobals`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6905:7,optimiz,optimizer,7,https://hail.is,https://github.com/hail-is/hail/issues/6905,1,['optimiz'],['optimizer']
Performance,"HailException and LowererUnsupportedOperation get returned as 400 with the error message,; other exceptions as 500 with stack trace. Also, some docker fixes. @jigold, I think this might explain why deploying from your computer is so slow. Docker includes the file permissions in the metadata when checking the image cache. The CI uses umask 022 (group not writable). I changed up my computer, and noticed everything was being rebuilt from scratch (requiring me to push massive images). I added some checks in docker/Makefile that the expectations for the image. I couldn't find a way to fix this globally. If the checks trigger, I think the solution is to set your umask to 022 going forward and chmod -R g-w your Hail source tree.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8636:316,cache,cache,316,https://hail.is,https://github.com/hail-is/hail/pull/8636,1,['cache'],['cache']
Performance,HailException: optimization changed type! after running split_multi(),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4827:15,optimiz,optimization,15,https://hail.is,https://github.com/hail-is/hail/issues/4827,1,['optimiz'],['optimization']
Performance,"Hello, ; I am getting this error when I try to save a mt after annotate_cols(). ```; Hail version: 0.2.34-914bd8a10ca2; Error summary: IllegalArgumentException: null; ```. Here is my code:; ```; phenotypes = hl.import_table('pheno.csv', impute=True, delimiter=','). phenotypes=phenotypes.key_by('WES'); mt = mt.annotate_cols(phenotype=phenotypes[mt.s]); mt.write('out.mt', overwrite = True); ```; It seems if I don't save, I don't see any problem in downstream performance, but I want to save this `mt`, otherwise my downstream work would be more computation-heavy. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8908:461,perform,performance,461,https://hail.is,https://github.com/hail-is/hail/issues/8908,1,['perform'],['performance']
Performance,"Here is Nirvana PR, as approved by the folks over at Illumina. They have some tweaks and changes they want to make in the works to improve performance and documentation, but this is functional and ready for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2127:139,perform,performance,139,https://hail.is,https://github.com/hail-is/hail/pull/2127,1,['perform'],['performance']
Performance,"Hey Hail,; I've been trying to get Hail working in a HPC environment. I was hoping to get multiple users to work on hail at the same time using the same shared filesystem. My design was to use a central code and library repository where there is a $CODE_HOME/hail/ and a $CODE_HOME/miniconda/ python installation, which all users PATHs are pointing to. This worked fine for both interactive and spark-submit uses with a single user, but today when I was testing with multiple users the HailContext would fail to form intermittently on a call to hc = HailContext() with either one of two errors. Note, each user today was ssh'ed into a different node and we were all using different jupyter notebooks simultaneously. There were five of us, and everytime we would all try to start HailContext at least one of us would fail out with these errors. Most of the time all five of us would fail out. Also note that concurrent calls to python only would be fine, with from hail import * working fine. Any help at all would be wonderful, as we would really like to work collaboratively on the cluster at the same time and all be referencing the same hail and python installations so we can keep our code synchronized. The first error that we would get would be. ---------; OSError Traceback (most recent call last); <ipython-input-11-2841f1963bb0> in <module>(); ----> 1 hc_rav = HailContext(). /scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.pyc in __init__(self, sc, appName, master, local, log, quiet, append, parquet_compression, min_block_size, branching_factor, tmp_dir); 45; 46 from pyspark import SparkContext; ---> 47 SparkContext._ensure_initialized(); 48; 49 self._gateway = SparkContext._gateway. /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf); 254 with SparkContext._lock:; 255 if not SparkContext._gateway:; --> 256 SparkContext._gateway = gateway or launch_gateway(conf); 257 SparkContext._jvm =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:907,concurren,concurrent,907,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['concurren'],['concurrent']
Performance,"Hi folks,. In evaluating Hail to see whether it fits my use case (a variant frequency database) I ran into an issue with importing VCF files from GIAB. It turns out that these use type `String` for the `PS` `##FORMAT` entry. Subsequently, Hail fails to import these with the error:; ```; is.hail.utils.HailException: HG001.vcf.gz:column 492: invalid character 'P' in integer literal; ```; This is because of the default behaviour of `htsjdk` to ""repair"" these according to the VCF ""standard"". `htsjdk` exposes `codec.disableOnTheFlyModifications` to toggle this behaviour which can be called from somewhere around https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1143. Ideally I would like to expose this toggle also at the `import_vcf` method of Hail.; I'll create a PR to do so accordingly ASAP. Comments/questions?. Thanks!. Regards,. Mark",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6012:693,Load,LoadVCF,693,https://hail.is,https://github.com/hail-is/hail/issues/6012,1,['Load'],['LoadVCF']
Performance,"Hi! . I know this is out of the blue, but we would like the ability to fetch resource_usage data from an endpoint programmatically to evaluate our job performance. I thought it might be worth suggesting this upstream to see if it's something you'd like too :). This PR:; 1. Use the internal method to fetch the dataframes for a job; 2. Transform the data frame to dictionary with `orient='split'`. And FWIW, here's how to convert it back into a dataframe:. ```python; import pandas as pd. response = {} # response from Hail Batch; dataframes = {; key: pd.DataFrame(data=values['data'], columns=values['columns']); for key, values in response.items(); }; ```. I tested this in on a dev deploy and it worked pretty well, but happy to add testing if you can direct me to a place to add it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14328:151,perform,performance,151,https://hail.is,https://github.com/hail-is/hail/pull/14328,1,['perform'],['performance']
Performance,"Hi, ; I would like to use hail to perform clumping and threshold (C+T). After I search the Hail Docs, I just get how to perform pruning. ; In my opinion, I select the significant SNPs using `filter_rows` function, then select the independent SNPs using `ld_prune`. Is it correct? Thanks!. Best,; Sheng",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11748:34,perform,perform,34,https://hail.is,https://github.com/hail-is/hail/issues/11748,2,['perform'],['perform']
Performance,"Hi,. Here are our latest updates to the johnc branch originally created by John Compitello. We have improved the overall annotation performance by increasing the default block_size to 500K. Please let us know if you have any questions. Best,; Shuli & the Nirvana team",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2300:132,perform,performance,132,https://hail.is,https://github.com/hail-is/hail/pull/2300,1,['perform'],['performance']
Performance,"Hi,; While loading a plink binary file generated by plink2, I receive the following error in my hail.log: . hail: info: running: importplink --bfile plinktest_chr21 --delimiter ' '; hail: info: Found 152249 samples in fam file.; hail: info: Found 982854 variants in bim file.; ^M[Stage 0:> (0 + 0) / 279]^M[Stage 0:> (0 + 31) / 279]hail: importplink: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 0.0 failed 4 times, most recent failure: Lost task 18.3 in stage 0.0 (TID 60, 10.93.109.80): java.io.EOFException: Cannot seek to a negative offset; at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:399); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:325); at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:62); at org.broadinstitute.hail.io.HadoopFSDataBinaryReader.seek(HadoopFSDataBinaryReader.scala:17); at org.broadinstitute.hail.io.plink.PlinkBlockReader.seekToFirstBlockInSplit(PlinkBlockReader.scala:34); at org.broadinstitute.hail.io.plink.PlinkBlockReader.<init>(PlinkBlockReader.scala:23); at org.broadinstitute.hail.io.plink.PlinkInputFormat.getRecordReader(PlinkInputFormat.scala:11); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:237); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/715:11,load,loading,11,https://hail.is,https://github.com/hail-is/hail/issues/715,1,['load'],['loading']
Performance,"High level take-aways:. - Hail docs now have syntax highlighting (we just needed to import pygments.css).; - Search works again.; - There are now only two root HTML templates: `site/templates/base.html` and; `web_common/web_common/templates/layout.html`. I cannot unify these further; because our services and our main websites actually differ significantly.; - The search/nav bar is now present in the HTML, no JS nonsense to; asynchronously load it into place after HTML rendering.; - Site now has a `make watch` which watches for changes and automatically; re-renders the HTML.; - Site now has a few make rules that facilitate experimenting with how the docs; are displayed within the context of the current development version of site's; CSS & HTML.; - XSLT is now only used by the C++ tests. Smaller things:. - Removed bootstrap dependencies. Did we ever actually use these?; - Removed ""clipboard.js"" dependency. Also not clear from where this came.; - Removed use of the `subtitle` tag, which isn't actually an HTML tag?. Future work:. - Simplify our CSS. It's not possible to logically reason about our CSS. And it; interacts in bad ways with the latent RTD themes. I want a unified Hail visual; theme.; - Clean up the search-related JavaScript in nav-bottom.html and; search.html. These both seem too complicated to just make search work. ---. The thrust of this PR is to restructure Hail's website and documentation to; entirely rely on Jinja2 templates. Previously, we used a mix of Jinja2, XSLT,; and in-browser JavaScript DOM manipulation to piece together a web page. Now, all of Hail's non-service website derives from; `site/templates/base.html`. It is a Jinja2 template with four blocks: title,; meta_description, head, and content. It ensures that:; - Hail's CSS is loaded,; - the Hail icon is set,; - the fonts are loaded,; - the source code highlighter is loaded (prism.js, only used outside the docs); - the nav bar is present and configured.; The nav bar is somewhat complicated. ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9597:443,load,load,443,https://hail.is,https://github.com/hail-is/hail/pull/9597,1,['load'],['load']
Performance,"I added a `StreamLen` IR node. The justification for this were optimization issues in `Simplify` situations like. ```; case ArrayLen(ToArray(StreamMap(s, _, _))) => ArrayLen(ToArray(s)); ```. The above is not a valid optimization in all cases because if the elements of `s` are not realizable, the optimization will have created an invalid IR that will fail at emit time since elements of an array must be realizable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8783:63,optimiz,optimization,63,https://hail.is,https://github.com/hail-is/hail/pull/8783,3,['optimiz'],['optimization']
Performance,"I added a new Grafana panel without alerts that hopefully will let us catch problems if jobs aren't getting scheduled in a timely manner. I think to have an alert, we'd want to measure what the average wait time of a job in the queue is which would require more infrastructure (keeping track of last state change). We can consider adding that now -- not sure how much work it would be.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12212:228,queue,queue,228,https://hail.is,https://github.com/hail-is/hail/pull/12212,1,['queue'],['queue']
Performance,"I added this when I was hitting class too large exceptions in a previous PR, to see if it would help. I didn't end up using it there, but it still seems like a good optimization. @tpoterba I haven't thought about this very hard, would be interested in your thoughts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11004:165,optimiz,optimization,165,https://hail.is,https://github.com/hail-is/hail/pull/11004,1,['optimiz'],['optimization']
Performance,"I agree with this comment: https://stackoverflow.com/a/17329465/431282. > Lazy val is *not* free (or even cheap). Use it only if you absolutely need laziness for correctness, not for optimization. I think we should avoid lazy val. This was borne out when profiling `RegionValue` stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2316:183,optimiz,optimization,183,https://hail.is,https://github.com/hail-is/hail/pull/2316,1,['optimiz'],['optimization']
Performance,I also added a small optimization that doesn't copy the column annotations into the row unless we're aggregating. I did this for both `filter_rows` and `annotate_rows`. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3565:21,optimiz,optimization,21,https://hail.is,https://github.com/hail-is/hail/pull/3565,1,['optimiz'],['optimization']
Performance,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1807:261,load,loadClass,261,https://hail.is,https://github.com/hail-is/hail/issues/1807,2,['load'],['loadClass']
Performance,"I am proposing a new iterator abstraction that I think should be preferred to Scala `Iterator` throughout most of the codebase, especially for iterators of region values. This is a low-level change, which could affect all code involving iterators, so I welcome feedback from everybody. The new abstractions are what I called `FlipbookIterator` and `StagingIterator` (I'm open to name suggestions). My goal was to simplify and raise the level of abstraction of most of the iterator manipulating code in the codebase—which can be subtle and bug-prone, and difficult to read—while paying as minimal as possible a performance overhead for the abstraction. This was surprisingly subtle to find the right abstractions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3016:610,perform,performance,610,https://hail.is,https://github.com/hail-is/hail/pull/3016,1,['perform'],['performance']
Performance,"I am running AWS EMR 6.2.0, including; * Hadoop 3.2.1; * Python 3.7.10; * Java 1.8.0; * Spark 3.0.1; * Scala 2.12.10. On that cluster, I am building HAIL version 0.2.74-0c3a74d12093 using the command. ```; sudo make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.10 SPARK_VERSION=3.0.1; ```. Compilation succeeds. I am able to load Hail into my Zeppelin notebook pyspark kernel; I am able to load and describe hail table stored in AWS S3. ```; %pyspark; # Import and launch Hail; import hail as hl; hl.init(sc); # Load files; ht_s = hl.read_table('s3://npm-hail/SG10K_Health_r5.3.1/n9770/SG10K_Health_r5.3.1.n9770.samples.ht'); # Desctibe; ht_s.describe(); ```. I am not able to show data. ```; %pyspark; ht_s.show(); ```. the `show()` call run infinitely into zeppelin and SparkUI note a started job, but no task launched. FYI hail version 0.2.60 on same EMR stack works without issue",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10832:341,load,load,341,https://hail.is,https://github.com/hail-is/hail/issues/10832,3,"['Load', 'load']","['Load', 'load']"
Performance,"I am using Hail 0.2.54. However, I also tested with the latest build.gradle file. I run the following make install command:; `make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.2`. However, I got this error message which did not appear before. ` > Could not resolve org.scalanlp:breeze-natives_2.11:+.; Required by:; project :; > Failed to list versions for org.scalanlp:breeze-natives_2.11.; > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error; * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.; * Get more help at https://help.gradle.org. BUILD FAILED in 29s; make: *** [build/libs/hail-all-spark.jar] Error 1`. It seems that is caused by https://repo.hortonworks.com/content/repositories/releases/ server is done.; I am wondering whether there is any maven substitute can be used temporarily to compile hail.jar?. Thanks in advance for your help.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9419:430,load,load,430,https://hail.is,https://github.com/hail-is/hail/issues/9419,1,['load'],['load']
Performance,"I assessed three query options, but keep in mind that the database is currently at nearly 100% CPU so these timings are high variance. I'm honestly pretty surprised that the first one isn't the fastest. It seems to me like the obvious SQL query and yet it does not perform as well as those using lateral joins. Option 2 and Option 3 both use lateral joins. Option 2 uses a LEFT JOIN followed by a `WHERE ... IS NOT NULL` which is definitionally an `INNER JOIN`. For that reason, I also explored Option 3, directly using `INNER JOIN`, which appears to be very slightly faster and lower variance. | Option | Timing (mean + stddev, in seconds) |; |--------|------------------------------------|; | 1 | 9.92 +- 2.02 |; | 2 | 5.35 +- 1.87 |; | 3 | 5.11 +- 1.15 |. ## Option 1; ```; SELECT DISTINCT; group_resources.batch_id,; group_resources.update_id,; group_resources.job_group_id; FROM job_group_inst_coll_cancellable_resources AS group_resources; INNER JOIN job_group_self_and_ancestors AS descendant; ON descendant.batch_id = group_resources.batch_id; AND descendant.job_group_id = group_resources.job_group_id; INNER JOIN job_groups_cancelled AS cancelled; ON descendant.batch_id = cancelled.id; AND descendant.ancestor_id = cancelled.job_group_id; ORDER BY group_resources.batch_id desc, group_resources.update_id desc, group_resources.job_group_id desc; LIMIT 1000;; ```; ```; 1000 rows in set (11.81 sec); 1000 rows in set (8.91 sec); 1000 rows in set (11.86 sec); 1000 rows in set (7.08 sec); ```; ## Option 2; ```; SELECT DISTINCT; group_resources.batch_id,; group_resources.update_id,; group_resources.job_group_id; FROM job_group_inst_coll_cancellable_resources AS group_resources; LEFT JOIN LATERAL (; SELECT; 1 AS cancelled; FROM job_group_self_and_ancestors AS descendant; INNER JOIN job_groups_cancelled AS cancelled; ON descendant.batch_id = cancelled.id; AND descendant.ancestor_id = cancelled.job_group_id; WHERE descendant.batch_id = group_resources.batch_id; AND descendant.job_group_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14366:265,perform,perform,265,https://hail.is,https://github.com/hail-is/hail/pull/14366,1,['perform'],['perform']
Performance,"I believe I addressed all the comments from #2431. The one thing I couldn't quite figure out is that you suggested testing using a randomly generated vsm and then calling make_table().export() (which in Scala, best as I can figure, is makeKT().export(f)). This does some things that makes re-importing using LoadMatrix non-identical:; - the exported header includes a header entry for the row ID column also, which the data we were looking at before didn't (n entries in the first line, n + 1 entries in subsequent lines), which seems like a reasonable thing to want to deal with---I added a flag in LoadMatrix called hasRowKeyLabel which drops the first item of the header line if that's the case, although it's not exposed to HailContext/the Python interface.; - the exported sample IDs somehow get "".g""s tacked on the ends (presumably because they came from the ""g"" struct), so the sample IDs would never match.; I ended up writing my own export function for the test (since I assume we don't really want to be exporting to this format IRL)---please let me know if I should handle that differently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2440:308,Load,LoadMatrix,308,https://hail.is,https://github.com/hail-is/hail/pull/2440,2,['Load'],['LoadMatrix']
Performance,"I believe we are encountering this known Kryo limitation: https://github.com/EsotericSoftware/kryo/issues/497, https://github.com/EsotericSoftware/kryo/issues/382 (also see related GATK issue: https://github.com/broadinstitute/gatk/issues/1524). Danfeng saw the referenced stack trace when trying to broadcast the variants for Plink (see: [LoadPlink.scala:202](https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/plink/LoadPlink.scala#L202). She was running a import_plink, count. The details in EsotericSoftware/kryo#382 indicate that a bad interaction between the data and a hash function can cause this integer map to exceed its size limitations at a load factor of 5%. Even a 20x increase in footprint puts us at 400 million. Each element of that array has 6 entries, so we're at 1.2 billion. That definitely feels like the danger zone. Maybe there's more variants than Danfeng expects, maybe there's more overhead than we've accounted for. The GATK folks have been chasing down the fix. Kryo [released 4.0.0](https://github.com/EsotericSoftware/kryo/issues/431) which should fix this issue. Spark [upgraded to Kryo 4.0.0](https://github.com/apache/spark/pull/22179) on September 8th of 2018. (resolving [Spark-20389](https://issues.apache.org/jira/browse/SPARK-20389)). This change made it to 2.4.0, but it was not back ported to other versions of Spark. GATK [references a temporary fix via JVM options](; https://github.com/broadinstitute/gatk/issues/1524#issuecomment-189368808), which apparently forces the JVM to use an alternative hash function with better behavior in this specific case:; ```; spark.executor.extraJavaOptions -XX:hashCode=0; spark.driver.extraJavaOptions -XX:hashCode=0; ```; A [generally interesting blog post on Java's hashCode](https://srvaroa.github.io/jvm/java/openjdk/biased-locking/2017/01/30/hashCode.html), which I haven't fully read, claims that the JVM previously defaulted to a PRNG draw for an object's hash code. In JDK 8 it uses some ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5564:340,Load,LoadPlink,340,https://hail.is,https://github.com/hail-is/hail/issues/5564,3,"['Load', 'load']","['LoadPlink', 'load']"
Performance,I can break this up further if you want. Big changes:. - change batch.py to support multi-line commands (use `{\n...\n}`); - change batch.py and job.py to support per-job environment variables (and add tests to test_batch.py); - add `partition` to hail top mirroring the implementation in Scala; - implement BatchPoolExecutor which attempts to faithfully implement the interface of concurrent.futures.Executor,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9035:382,concurren,concurrent,382,https://hail.is,https://github.com/hail-is/hail/pull/9035,1,['concurren'],['concurrent']
Performance,"I changed the Spark `persist`s to `writeRead` which writes and then reads. I tried to maintain this invariant: a block matrix partition always reads a linear number of partitions in the number of referenced block matrices. In particular, the result of *any* matmul must `writeRead`. I removed the boxing of Doubles to check for NaN. I avoided a bunch of allocation when performing matmul by using a fused multiply and add operation (`dgemm`). I sped up conversation to BlockMatrix somewhat by introducing an iterator that caches the firstelementoffset. I substantially improved `BlockMatrix.checkpoint` by using the fast lz4 codec. *new*: I also added some tasteful cache'ing to PCRelate which substantially reduced the time spent reading data from disk. cc: @johnc1231 @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7962:370,perform,performing,370,https://hail.is,https://github.com/hail-is/hail/pull/7962,3,"['cache', 'perform']","['cache', 'caches', 'performing']"
Performance,I changed this from 2=>1 in April of last year unintentionally while debugging; (it's easy to get interleaved prints/logs with 2 concurrent worker threads). https://github.com/hail-is/hail/pull/8535/files#diff-bf51d09b286fddaa730b426824ccb12dac8b9032e0c88bde81882f3cb1423df8R14,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10710:129,concurren,concurrent,129,https://hail.is,https://github.com/hail-is/hail/pull/10710,1,['concurren'],['concurrent']
Performance,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3394:495,Load,LoadVCF,495,https://hail.is,https://github.com/hail-is/hail/pull/3394,1,['Load'],['LoadVCF']
Performance,"I cover the two primary methods: PC-Relate and KING. # PC-Relate. Suppose:. 1. We have $X_1$, an $M$ -by- $N$ matrix of genotypes, with $M$ variants and $N$ samples. Suppose we have a new dataset $X_2$ which strictly adds $l$ new variants and $k$ new samples to $X_1$. 2. We have a truncated-SVD of $U S V^T = X_1$ [1]. 3. We believe the ancestry space represented by the truncated-SVD still accurately represents the ancestry space of $X_2$. 4. We have already calculated the PC-Relate kinship matrix $\phi_1$ of $X_1$. We would like to calculate $\phi_2$ the kinship matrix of $X_2$ while only performing $O(k^2 M + kNM)$ work. ---. [1] PC-Relate, as presented in Conomos, et al., uses the PC scores and linear regression to define the ancestry space as follows.; 1. Calculate the first $k$ (different from $k$ above) PC scores of $X$ (which are defined in terms of the $k$-truncated SVD: $S V^T$).; 2. For each variant $s$, find the best linear fit using ordinary least squares for the equation $x_s = \alpha_s + \beta_s S V^T$. $\alpha$ is a scalar intercept term. $\beta_s$ is a $N$-vector. $x_s$ is the vector of genotypes for variant $s$ (the $s$-th row of $X$); 3. Defined the individual specific allele frequency for sample $i$ at variant $s$: $\mu_{is} = \widehat{\alpha_s} + \widehat{\beta_s} S V^T$. At one point, Patrick noticed that this rigamarole is unnecessary. The $k$-truncated SVD is the best rank $k$ approximation of $X$. I think our conclusion was that defining $\mu$ in terms of the $k$-truncated SVD is equivalent: $\mu = U S V^T$. # KING. Suppose again (1) and that we already have the $\phi_1$ KING's kinship estimator on $X_1$. We would like to calculate $\phi_2$, the KING kinship estimator matrix of $X_2$ while only performing $O(k^2 M + kNM)$ work. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13808:596,perform,performing,596,https://hail.is,https://github.com/hail-is/hail/issues/13808,2,['perform'],['performing']
Performance,"I detect no performance difference on blanczos running the benchmark; locally. This pattern appears a lot in the NDArrayEmitter, though,; so we should fix it everywhere and see what happens!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9708:12,perform,performance,12,https://hail.is,https://github.com/hail-is/hail/pull/9708,1,['perform'],['performance']
Performance,"I dev deployed all *_image steps on a single worker running `main` and saw many fail with corrupted filesystems. I imagine this is because multiple jobs were extracting the same filesystem into the same place. The previous change to using a r/w lock for pulling and deleting images is correct, but we must lock on the image id when *extracting* the actual filesystem. With this change everything passed in my dev. The `BATCH_WORKER_IMAGE_ID` fix from before didn't actually work because of not properly escaping the `{` in the f-string. I also moved the `docker rmi` step to be first in the image cleanup process because I imagine if docker refuses to remove an image we shouldn't remove it from our own cache either.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10675:704,cache,cache,704,https://hail.is,https://github.com/hail-is/hail/pull/10675,1,['cache'],['cache']
Performance,"I didn't bust js yet because there are some external libraries (MathJax) that cary their own version strings and I don't want to break them. In testing seems safe, although I admit the regex isn't the most specific. In the worst case I believe we would append an unnecessary version string, which shouldn't break anything (just will cache the browser to reload the css instead of using cache)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6997:333,cache,cache,333,https://hail.is,https://github.com/hail-is/hail/pull/6997,2,['cache'],['cache']
Performance,"I do not know why. ```; # k logs -l app=batch | head; INFO	| 2018-10-26 17:04:45,840 	| server.py 	| <module>:44 | REFRESH_INTERVAL_IN_SECONDS 300; INFO	| 2018-10-26 17:04:45,844 	| server.py 	| <module>:53 | instance_id = 168f090933ba4db4ac6ba8d0add8460d; INFO	| 2018-10-26 17:04:45,849 	| server.py 	| run_forever:416 | run_forever: run target kube_event_loop; INFO	| 2018-10-26 17:04:45,850 	| server.py 	| run_forever:416 | run_forever: run target polling_event_loop; INFO	| 2018-10-26 17:04:45,850 	| server.py 	| run_forever:416 | run_forever: run target flask_event_loop; * Serving Flask app ""batch"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; * Debug mode: off; # k logs -l app=hail-ci | head; INFO	| 2018-10-26 16:47:18,826 	| environment.py 	| <module>:51 | BATCH_SERVER_URL http://batch.default; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:52 | SELF_HOSTNAME http://hail-ci; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:53 | REFRESH_INTERVAL_IN_SECONDS 60; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:54 | WATCHED_TARGETS [('hail-is/hail:master', True), ('hail-is/hail:0.1', True), ('hail-is/hail:bgen-changes', False), ('Nealelab/cloudtools:master', True)]; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:55 | INSTANCE_ID = ef1bb52a88dd49fb893869bf49063980; INFO	| 2018-10-26 16:47:18,827 	| environment.py 	| <module>:56 | CONTEXT = hail-ci-0-1; * Serving Flask app ""ci"" (lazy loading); * Environment: production; WARNING: Do not use the development server in a production environment.; Use a production WSGI server instead.; ```. This obviously causes issues because CI is still waiting for batch jobs to finish.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4653:613,load,loading,613,https://hail.is,https://github.com/hail-is/hail/issues/4653,2,['load'],['loading']
Performance,I don't think it is used anymore. Builds are failing because it is returning 500. > Unable to load Maven meta-data from https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml.; > Could not get resource 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'.; > Could not GET 'https://repo.hortonworks.com/content/repositories/releases/org/scalanlp/breeze-natives_2.11/maven-metadata.xml'. Received status code 500 from server: Server Error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9418:94,load,load,94,https://hail.is,https://github.com/hail-is/hail/pull/9418,1,['load'],['load']
Performance,"I ended up restructuring the summarizing to make the formatting easier. I also ended up putting the summary stuff on the expression; the vague goal is to allow a call to `table.summarize()` to cache all the summaries of the (nested) row fields, so that subsequent calls to e.g. `table.locus.summarize()` should be free.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7280:193,cache,cache,193,https://hail.is,https://github.com/hail-is/hail/pull/7280,1,['cache'],['cache']
Performance,"I finally figured out how to get the authorization bearer token for the Grafana robot into Grafana automatically. The problem I'm running into right now is when we load a datasource from a configuration file, we can not edit any of the settings in the UI. We'd want to make sure all the prometheus settings we want are inside the new config file. I also don't want to accidentally overwrite any of the existing configuration.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772:164,load,load,164,https://hail.is,https://github.com/hail-is/hail/pull/10772,1,['load'],['load']
Performance,"I found this while on a PR based on:. ```; * 8e61ad87c - (3 days ago) [batch] fix scheduler -- schedule job timeout 1sec (#8022) - jigold (hi/master, master); ```. The error was:; ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 281, in run; await docker_call_retry(self.container.start); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 87, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 188, in start; data=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'OCI runtime start failed: container process is already dead: unknown'); ```. Unfortunately the batch worker had already died by this point. ```; {; ""batch_id"": 1,; ""job_id"": 19,; ""name"": ""18"",; ""state"": ""Error"",; ""exit_code"": null,; ""duration"": 10408,; ""msec_mcpu"": 1040800,; ""cost"": ""$0.0000"",; ""status"": {; ""worker"": ""batch-worker-dking-16py5"",; ""batch_id"": 1,; ""job_id"": 19,; ""attempt_id"": ""5cs0mg"",; ""user"": ""dking"",; ""state"": ""error"",; ""format_version"": 2,; ""container_statuses"": {; ""main"": {; ""name"": ""main"",; ""state"": ""error"",; ""timing"": {; ""pulling"": {; ""start_time"": 1580760856472,; ""finish_time"": 1580760856486,; ""duration"": 14; },; ""creating"": {; ""start_time"": 1580760856486,; ""finish_time"": 1580760856629,; ""duration"": 143; },; ""runtime"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; },; ""starting"": {; ""start_time"": 1580760856630,; ""finish_time"": 1580760867038,; ""duration"": 10408; }; },; ""error"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 281, in run\n await docker_call_retry(self.container.start)\n File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 87, in docker_call_retry\n return await f(*args, **kwargs)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8029:673,load,loads,673,https://hail.is,https://github.com/hail-is/hail/issues/8029,1,['load'],['loads']
Performance,"I had to recreate from #7593 because I force-pushed after are rebase. cc: @cseed . Unfortunately, you're the only one around to review John. There's so many issues with this at current, but I think it would be better to get something in so I can actually start making forward progress towards something better. At a very practical level, I want various docker images to be cached instead of constantly rebuilding them as I try to develop this further.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7760:373,cache,cached,373,https://hail.is,https://github.com/hail-is/hail/pull/7760,1,['cache'],['cached']
Performance,"I introduced a performance bug when I made the BgenRecord eagerly decode the genotypes. The genotypes are not needed for fast keys. This change throws out Hadoop in favor of a Spark-based approach. I also did an instrumented profiling of `hl.import_bgen(...)._force_count_rows()` and found that almost all our time is spent in region value builder state manipulation. I included a small fix that makes `ArrayStack` actually use field references instead of method calls (this shaved about 1/10 off of force count rows time) (cc: @cseed). The real solution is to use a staged region value builder. I will follow this PR with a SRVB pull request. Finally, I will also hook it up to `MatrixRead` so that the dead fields pruner can prune bgen fields too (cc: @tpoterba)!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3893:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/pull/3893,1,['perform'],['performance']
Performance,"I left the changes to Query and Batch in separate commits for ease of review. I put these in the same PR because we don't really have standalone testing for JVM Jobs outside of Query-on-Batch so the FASTA use-case serves as a test here that cloudfuse is working properly for JVM Jobs. Would be great if Jackie you could review the batch commit and Tim could review the query commit. ## Hail Query; - Added support for the `FROM_FASTA_FILE` rpc and the service backend now passes sequence file information from RGs in every rpc; - Refactored the liftover handling in service_backend to not redundantly store liftover maps and just take them from the ReferenceGenome objects like I did for sequence files. This means that add/remove liftover/sequence functions on the Backend are just intended to sync up the backend with python, which is a no-op for the service backend.; - Don't localize the index file on fromFASTAFile/addSequence before creating the index object. `FastaSequenceIndex` just loads the whole file on construction so might as well stream it in from whatever storage it's in.; - FASTA caching is left alone because those files will be mounted and unmounted from the jvm container over the life of the job. JVM doesn't have to worry about disk usage because that's handled by Batch XFS quotas, so long as the service backend requests enough storage to fit the FASTA file. Batch will make sure that a given bucket (and therefore a given FASTA file) is mounted once per-user on a batch worker. ## Hail Batch; - Added support for read-only cloudfuse mounts for JVM jobs; - These mounts are shared between jobs on the same machine from the same user; - I did not change DockerJobs, but they could be very easily adapted to use this new mount-sharing code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736:992,load,loads,992,https://hail.is,https://github.com/hail-is/hail/pull/12736,1,['load'],['loads']
Performance,"I listed everything I'm aware of, though it's possible that some internal changes may have made performance impacts I don't know about.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8292:96,perform,performance,96,https://hail.is,https://github.com/hail-is/hail/pull/8292,1,['perform'],['performance']
Performance,"I mostly want this for debugging lowering on `LocalBackend`, but I added a SparkBackend implementation as well (goes through breeze and java literals, not going to be super performant).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10398:173,perform,performant,173,https://hail.is,https://github.com/hail-is/hail/pull/10398,1,['perform'],['performant']
Performance,"I need this functionality for compacting the billing tables. I'm not sure how we get this privilege to `batch`, `batch-admin` etc., but this should be fine for me to at least measure any performance gains.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13165:187,perform,performance,187,https://hail.is,https://github.com/hail-is/hail/pull/13165,1,['perform'],['performance']
Performance,I need to be able to load tokens for multiple users for #9553 and would like to pipe it through the auth utilities in a reasonable way so I can use them. Let me know if this looks ok or if there's a better way to do it?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9585:21,load,load,21,https://hail.is,https://github.com/hail-is/hail/pull/9585,1,['load'],['load']
Performance,"I need to write the CI integration, but this is my proposed distributed buffer service. It's not resilient to failure at all. `python3 -m dbuf 8`. Will create an 8-process dbuf. You can start follower cores on another machine with `python3 -m dbuf 8 --leader http://LEADER:LEADER_PORT`. For a sense of the performance, the following has 10 co-routines each sending 10k messages of 40k bytes each using a buffer size of 5MB (so each co-routine holds about 5MB in memory before flushing):; ```; # python3 scale_test.py 10 5 40000 10000; create; write aggregate-throughput: 0.333 GiB/s; read aggregate-throughput: 0.213 GiB/s; ```; This is on my laptop over loopback with `python3 -m dbuf 4`. Note that we send 4 GB (10 * 10k * 40k bytes). Each core will buffer 512 MiB, so each server core will flush to disk twice (the scale test explicitly equally distributes the load, so each server core gets 1 GB). I've got a Scala client as well which I'll add in another PR. ---. Update: same as above benchmark but I had to reduce the maximum data sent in one HTTP request to 1MiB:. ```; write aggregate-throughput: 0.194 GiB/s; read aggregate-throughput: 0.135 GiB/s; ```. I can no longer run the test on my laptop due to all the changes I made to dbuf to make it run in k8s. I don't know how much of the reduced throughput is due to the HTTP window size. I'll increase all the NGINX max request sizes at some point and retest.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7523:306,perform,performance,306,https://hail.is,https://github.com/hail-is/hail/pull/7523,14,"['load', 'perform', 'throughput']","['load', 'performance', 'throughput']"
Performance,"I picked the name since Cronus is the father of Zeus. Perhaps Saturn is more appropriate. Open to suggestions here. The UX flow:. 1. User loads up `https://hail.is/cronus` and sees a form with a button.; 2. Pressing the button starts a pod running Jupyter for the user that no one else has access to; 3. refreshing the page or going to `https://hail.is/cronus` again redirects to the Jupiter instance; 4. to get a fresh Jupyter instance, the user can clear their cookies. The components:. - a flask app (`cronus/cronus.py`) which launches pods and handles authentication (via cookies); - an nginx reverse proxy which uses `auth_request` to check the permissions with the flask app; - a pod running `Jupyter notebook` with hail `pip` installed. TODO:. - [x] add make targets to generate the `cronus-job` image (the jupyter notebook image); - [ ] maybe simplify the directives used in nginx? I kept throwing shit at it until it worked; - [ ] figure out how to teach flask url_for to use a root other than `/`. I don't know what HTTP proxy headers to set to inform it that it lives at a subdirectory of `hail.is`; - [ ] get rid of the button? creating a new pod needs to be a `POST` so that the web browser doesn't access twice or eagerly access it, etc. maybe I can use javascript on the root page to make the post request and redirect the page.; - [ ] testing? I could add some basic things, but the most time consuming and annoying thing was getting the reverse proxy settings right and testing that requires an nginx instance. @cseed I randomly assigned, should I be picking from you and Tim? What's the plan for review on these new things?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576:138,load,loads,138,https://hail.is,https://github.com/hail-is/hail/pull/4576,1,['load'],['loads']
Performance,"I plan to move this to `hl.dnd.DNDArray`. I had to make a couple changes to RVD and Table to make this work. They all; revolve around convincing Hail not to elide *critical* `key_by`s. The critical insight is that 1:1 partitioners (partitioners where each range; bound interval contains exactly one key) are special: permuting their keys is; free. I can take advantage of this by combining two changes:; 1. `RVD.enforceKey` is aware of these partitioners and avoids scans in that case; 2. Defeat the optimizer, which is unaware of these partitioners and misoptimizes; to operations that require shuffles. The first change is easy. I added `RVDPartitioner.keysIfOneToOne` which looks; for these kinds of partitioners in the special case of keys consisting of 32-; and 64-bit integers. The second change eluded me for a long time. Finally, I discovered; `isSorted=true` and realized the optimizer refuses to modify such; `TableKeyBy`s. I exposed this field in Python as: `Table._key_by_assert_sorted`. With this infrastructure in place, I was able to implement read, write, and; matrix-multiply for DNDArray!. In addition, to the arguable hacks above, a couple pain points remain:; 1. I do not know how to rename keys in Python without triggering shuffles. If I; write `key_by(x=t.y, y=t.x)`, Hail implements this as; `TableKeyBy(TableMapRows(TableKeyBy(Array(), ...)`. The inner key by throws; the keys away so that they can be modified with TableMapRows. Unfortunately,; this completely defeats my attempts to avoid shuffles. I avoid this issue by; not using fixed names for the x and y block coordinates (their names are; stored in `x_field` and `y_field`).; 2. Hail lacks `ndarray_sum`. Instead, I convert from ndarray to array so that I; can use `array_sum`. Unfortunately, this operation seems to completely; dominate all of my time. It takes about 10x as much time as the matrix; multiplies take. I do not understand this. I should be reading the entries in; column-major order. Performance; ----",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864:500,optimiz,optimizer,500,https://hail.is,https://github.com/hail-is/hail/pull/8864,2,['optimiz'],['optimizer']
Performance,"I played with a few options. I liked this one the best. Downside to `Value[T] extends Code[T]`:; - A bunch of code (using Array) assumes Code[T] is monomorphic. Either way I fixed those here (by switching to polymorphic IndexedSeq[T]); - Can't use the analogous setup for PValue since the hierarchy is more complicated. This is why I picked this solution. Downside to this solution: ; - Scala won't apply stacked implicits, so need to add additional implicits for e.g. Value[Int] to CodeInt. I do that here. In the end, `Value[T]` is a thing that can produce multiple `Code[T]`, which can then only be emitted once. I used `Value.get: Code[T]` over `load()` and renamed a few field accessors get => getField. If we like how this goes I can rip out `load()`. I fixed some know multiple emission of Code[T] in ETypes buildEncoder. I will slowly convert over the necessary stuff to `Value[T]` in later PRs. `Code.markEmitted` (not called) can be used to find Code[T] that are emitted multiple times.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8229:650,load,load,650,https://hail.is,https://github.com/hail-is/hail/pull/8229,2,['load'],['load']
Performance,"I pulled out a flag to cache in java, but accidentally got rid of the thing it was actually doing. This should be fixed now; with a smaller test mt I'm seeing the number of allocated regions be consistent between combOps:. ```; ...; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; 2019-08-06 17:21:17 Hail: INFO: Region count for combOp; regions: 27; blocks: 28; free: 25; used: 2; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6824:23,cache,cache,23,https://hail.is,https://github.com/hail-is/hail/pull/6824,1,['cache'],['cache']
Performance,"I realize this looks like a lot of code changes, but it's mostly copying and pasting two SQL procedures and changing one line in each. This adds 4 bits of metadata to requests that then can be queried as extra metadata:; - batch_id; - job_id; - batch_operation; - job_queue_time. Should be self-explanatory except job_queue time is the time in which the job is first set to ready to when it was scheduled on the worker (exact moment is when the job config is made to send to the worker). Example logging query. Note that the search on ""batch_id"" is not optimized so you definitely want to add some kind of time limit that's short on the window to search. I can add my Python script that scrapes these logs and makes a Plotly figure in a separate PR once this goes in. ```; (; resource.labels.container_name=""batch""; resource.labels.namespace_name=""{namespace}""; ) OR (; resource.labels.container_name=""batch-driver""; resource.labels.namespace_name=""{namespace}""; ) OR (; resource.type=""gce_instance""; logName:""worker.log""; labels.""compute.googleapis.com/resource_name"":""{namespace}""; ); jsonPayload.batch_id=""{batch_id}""; timestamp >= ""{start_timestamp}"" {end_timestamp}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13219:553,optimiz,optimized,553,https://hail.is,https://github.com/hail-is/hail/pull/13219,1,['optimiz'],['optimized']
Performance,"I staged `import_matrix_table` and achieved substantial performance improvements. A few changes were necessary:; - `FunctionBuilder` now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists. The main change is in `ImportMatrix.scala` which is both staged and based on scanning the string rather than using `String.split`. The approach is essentially a simplified, staged version of `import_vcf`. I benchmarked the change with:; ```; In [2]: %%time ; ...: import hail as hl ; ...: m = hl.import_matrix_table('/tmp/foo.tsv.gz', ; ...: row_fields={'f0': hl.tstr}, ; ...: no_header=True, ; ...: sep=' ', ; ...: min_partitions=16) ; ...: m = m.key_rows_by(locus=hl.parse_locus(m.f0)) ; ...: m._force_count_rows() ; ```. `/tmp/foo.tsv.gz` is a gzipped (not blocked) 1GB of 1000 rows each containing one row column and 500k sample columns. The entries are the integers from 0 to 499,999. The first column is the first run (when the JIT is warmed) and the second column is the mean of two subsequent runs. All times in seconds. Everything is necessarily executed on one core. | version | cold | warm |; | --- | --- | --- |; | this PR | 48 | 39.35 |; | this PR with one monolithic method | 235 | 73 |; | master (5fe6737263b4) | 91s | 83.5 |. I was disappointed with the performance of the monolithic method, so I dug in with `-XX:+PrintCompilation` and found that the JIT was having trouble doing on-stack replacement of the entry parsing loop. There was a cryptic message about the stack not being empty during an OSR compilation. I take this result as evidence that, in the JVM, small, fine-grained methods are critical for reliable performance. The new code, after JIT warming, is reading at 250 MB/s (1GB / 40 seconds) which is a half to a third of the performance of `cat`. It's more than twice as fast as the old code. Asi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987:56,perform,performance,56,https://hail.is,https://github.com/hail-is/hail/pull/6987,1,['perform'],['performance']
Performance,"I think the database insert didn't perform as well with > 1000 jobs per insert in the front_end create_jobs, but I can't figure out where I got that. Feel free to reject this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7604:35,perform,perform,35,https://hail.is,https://github.com/hail-is/hail/pull/7604,1,['perform'],['perform']
Performance,"I think this is a race condition with another process trying to pull the same image after the current process has pulled the image. That would mostly be solved by a per user Docker cache, but I think this solution is still needed as you could have a race condition on the cache timeout boundaries. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 287, in run; name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 91, in docker_call_retry; return await f(*args, **kwargs); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 48, in create; url, method=""POST"", data=config, params=kwargs; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 223, in _query_json; path, method, params=params, data=data, headers=headers, timeout=timeout; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(404, 'No such image: gcr.io/hail-vdc/ci-utils:e9pnvtf1078g'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193:18,race condition,race condition,18,https://hail.is,https://github.com/hail-is/hail/pull/8193,5,"['cache', 'load', 'race condition']","['cache', 'loads', 'race condition']"
Performance,"I think this is what was wrong with the `git_make_bash_image` taking a minute each time. Since every image without a `publishAs` uses `ci-intermediate`, the `ci-intermediate:cache-PR-X` tag is left pointing to whichever anonymous image built last in the PR run. This is certainly never `git_make_bash_image`, so every time it gets rebuilt, the cache-from that it is using points to an an image whose layers do not include a layer that is `RUN apt-get update && apt-get install -y git make bash`. If this PR runs twice, hopefully we'll see the first step go super quick.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12285:174,cache,cache-PR-X,174,https://hail.is,https://github.com/hail-is/hail/pull/12285,2,['cache'],"['cache-PR-X', 'cache-from']"
Performance,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862:682,load,load,682,https://hail.is,https://github.com/hail-is/hail/issues/3862,1,['load'],['load']
Performance,"I tried benchmarking this change and didn't see much of a difference in timings in my contrived high throughput example. However, I do think this index is better because I believe MySQL does the order by first and then filters records. @danking Can you take a look at this and make sure the index is actually an improvement. The speed of the query is linearly related to the number of records in the limit. So I think if we need to get the query speed back to under 10ms then we revert back to pulling a smaller number of records rather than 1000. I think 300 is fine and gets us to 10ms. I just didn't want to pull 10 jobs and then none of them are schedulable but the 100th one is. We can revisit this if the scheduler becomes the bottleneck after your changes to the gateway.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12350:101,throughput,throughput,101,https://hail.is,https://github.com/hail-is/hail/pull/12350,2,"['bottleneck', 'throughput']","['bottleneck', 'throughput']"
Performance,"I tried to run Hail with Spark 2.4.4 built for Scala 2.12, and it did not work. It does work with Spark 2.4.4 built for Scala 2.11. Here's the error I got with Scala 2.12:; > Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; > : java.lang.NoSuchMethodError: scala/Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; (loaded from file:/home/hammer/codebox/spark-2.4.4-bin-without-hadoop-scala-2.12/jars/scala-library-2.12.8.jar by sun.misc.Launcher$AppClassLoader@ac1080fa) called from class is.hail.HailContext$ (loaded from file:/home/hammer/anaconda3/lib/python3.7/site-packages/hail/hail-all-spark.jar by sun.misc.Launcher$AppClassLoader@ac1080fa).; > 	at is.hail.HailContext$.majorMinor$1(HailContext.scala:71); > 	at is.hail.HailContext$.checkSparkCompatibility(HailContext.scala:73); > 	at is.hail.HailContext$.createSparkConf(HailContext.scala:84); > 	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:134); > 	at is.hail.HailContext$.apply(HailContext.scala:270); > 	at is.hail.HailContext.apply(HailContext.scala); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); > 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); > 	at py4j.Gateway.invoke(Gateway.java:282); > 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); > 	at py4j.commands.CallCommand.execute(CallCommand.java:79); > 	at py4j.GatewayConnection.run(GatewayConnection.java:238); > 	at java.lang.Thread.run(Thread.java:819); >",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009:368,load,loaded,368,https://hail.is,https://github.com/hail-is/hail/issues/8009,2,['load'],['loaded']
Performance,"I use a Mac and try to install hail.; I use Mojave; I installed pyenv to modify my python versions.; I installed Python 3.7.9 since you recommend to use Python 3.7 as the latest version.; I then did a pip install hail, and it fails with pyspark:. Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1.tar.gz (215.7 MB); ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-pip-egg-info-vlaj8k6d; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-install-0g3aqft5/pyspark/; Complete output (47 lines):; Could not import pypandoc - required to package PySpark; WARNING: The wheel package is not available.; ERROR: Command errored out with exit status 1:; command: /Users/spascal/.pyenv/versions/3.7.9/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""'; __file__='""'""'/private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-ggmq8ipk; cwd: /private/var/folders/br/f16ml9tx5z32fhsdd1nqpymm0000gn/T/pip-wheel-hsj5k2xb/pypandoc/; Complete output (8 lines):; no pandoc found, building platform unspecific wheel.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742:285,cache,cached,285,https://hail.is,https://github.com/hail-is/hail/issues/9742,1,['cache'],['cached']
Performance,"I want a cluster pod resource waste metric. Consider one pod. if its request is 100mCPU and its actual load is 10mCPU we're ""wasting"" 90mCPU. I want to know the distribution of wasted pod CPU. I want to know the top 10 wasteful pods.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6509:103,load,load,103,https://hail.is,https://github.com/hail-is/hail/issues/6509,1,['load'],['load']
Performance,"I want this for interface purposes, but it's really not usable due to performance. It takes 2 minutes to collect sample.vcf. I think that py4j is the main culprit, but the history stuff also slows it down a ton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2209:70,perform,performance,70,https://hail.is,https://github.com/hail-is/hail/pull/2209,1,['perform'],['performance']
Performance,"I want to annotate a field like this:; ```Gene_Conseq_MAF=(va.annot.gene + ""\n"" + va.annot.most_severe_csq + ""\nMAF:"" + str(va.lmmreg.maf))```; (so a string with \n) such that when I load in R, the top loci will be highlighted as so. However, the exported .gz file actually has new lines at each “\n""; is there a way to avoid this?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1132:183,load,load,183,https://hail.is,https://github.com/hail-is/hail/issues/1132,1,['load'],['load']
Performance,"I want to track performance of logistic regression. Currently, on my laptop these two benchmarks clock in at 11 seconds for Breeze and 62 seconds for ndarrays. Now that I have a metric I'll try and optimize.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10492:16,perform,performance,16,https://hail.is,https://github.com/hail-is/hail/pull/10492,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,I want to use this in optimization.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7139:22,optimiz,optimization,22,https://hail.is,https://github.com/hail-is/hail/pull/7139,1,['optimiz'],['optimization']
Performance,I was asked to agree to some license after clearing my gradle cache. I do not think anyone needs this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10099:62,cache,cache,62,https://hail.is,https://github.com/hail-is/hail/pull/10099,1,['cache'],['cache']
Performance,"I was seeing crashes on workers in a local install because the Hail jar was loaded by a Spark class loader rather than the system class loader, so the toString in:. > String name = ClassLoader.getSystemResource(""include"").toString();. was failing with a null pointer exception. Fixed this two ways: don't unpack includes on the worker (compilation should only happen on the master) and use the same class loader that loaded the NativeCode object. Also some reformatting and style changes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4554:76,load,loaded,76,https://hail.is,https://github.com/hail-is/hail/pull/4554,5,['load'],"['loaded', 'loader']"
Performance,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2070:16,load,load,16,https://hail.is,https://github.com/hail-is/hail/issues/2070,1,['load'],['load']
Performance,"I'd like to give the user the ability to authenticate to our services from within a batch job. The specific use case I need it for is for the query service to be able to cache things with the memory service, but it seems like it could be more broadly applicable. I'm unsure whether this is the correct way to do it. This is currently not exposed in the python user interface (only in BatchClient), but I can pipe the option through in this PR if we want to.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437:170,cache,cache,170,https://hail.is,https://github.com/hail-is/hail/pull/9437,1,['cache'],['cache']
Performance,"I'd like to move master to Spark 2 and Scala 2.11. These changes get us as close as possible. They include:. - remove SparkExport, use reflection to get path of partition when loading from parquet; - remove SparkManager; - upgrade to Kudu 1.1.0 (Spark 2 support). The distance between this and Spark 2 is very small, see https://github.com/hail-is/hail/commit/95a588cfa72391d4303bf6891fd017ec211989db. When the master moves to Spark 2, we can maintain a spark1 branch until the on-prem machines get upgraded. Ideally, the spark1 branch could get rebased automatically as part of the CI, although I'm not quite sure how we'd handle conflicts. Alternatively, we could maintain a spark2 -> spark1 diff in the repo that gets applied as part of testing. Fixes https://github.com/hail-is/hail/issues/1117",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1124:176,load,loading,176,https://hail.is,https://github.com/hail-is/hail/pull/1124,1,['load'],['loading']
Performance,"I'd like your initial feedback before I start testing this on Azure. A substantially earlier version seemed to work fine on GCP with dev deploy. The major conceptual change I made is a `resource` now contains a `prefix` and a `version`. The `resource_name` is just `{prefix}/{version}`. The prefixes for GCP are the same as they were before and don't vary by region. However, the new prefixes for Azure are region specific. The version is `1` for all current resources. . I added a `latest_resource_versions` table that has the prefix mapped to the latest version. This is used to generate the current resource names. There is a new CloudResourceManager that is in charge of managing the spot billing pricing cache and updating the prices in the cache and the database from the cloud provider's API. Since I couldn't easily rename resources to products everywhere in the database due to anonymous foreign key constraints, I had to rename the existing `CloudResourceManager` to `CloudDriverAPI`. Feel free to suggest a better name. The GCPResourceManager is a skeleton right now, but we'll have to flesh it out in the new year when GCP moves to spot billing with varying prices. For the `AzureResourceManager`, I use a new pricing client to grab the latest vm and disk prices. I support all possible disk prices, but for now, I limited the VM query to just get the machine types we support right now. In the future, we could get all VM prices, but the query is around 40 seconds for that compared to 2 seconds now. I was worried if we had such a slow query that blocked driver startup, that would be bad and this is fine for now. There are two classes I added: a `Resource` and a `Price`. The Price is only implemented for Azure and is used to store cost results from the pricing API. The resource has a couple of different mixin classes with an abstract method to generate the quantified resource depending on the type (ex: ComputeResourceMixin). Then there's `AzureDiskResource`, `AzureVMResource`, e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11092:709,cache,cache,709,https://hail.is,https://github.com/hail-is/hail/pull/11092,2,['cache'],['cache']
Performance,"I'd love feedback, especially on:; * How/whether to test these things; * How to organize a growing collection of hash families, with different speed/power tradeoffs, and different key and hash word-lengths. (These will be used in inner-most loops, so performance matters, and I don't have a good sense of what Scala abstractions hurt performance.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2288:251,perform,performance,251,https://hail.is,https://github.com/hail-is/hail/pull/2288,2,['perform'],['performance']
Performance,"I'm keeping my LD extension branch separate until we add a proper sparse block matrix implementation, but I pulled out these functions on GridPartitioner since (i) they're some of the logic we'll need to make sparse block matrix useful and (ii) Meredith just built a step to compute variant windows in LDPrune, which can then be combined with this logic as part of her optimization strategy to not compute unneeded blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3094:369,optimiz,optimization,369,https://hail.is,https://github.com/hail-is/hail/pull/3094,1,['optimiz'],['optimization']
Performance,"I'm not 100% sure I know the implications of this change with regards to race conditions. I think it's okay? Also, why do we not include the timings if the job is deleted in `__exit__`? This is the current code:. ```python3; class ContainerStepManager:; def __init__(self, timing: Dict[str, float], is_deleted: Callable[[], bool], ignore_job_deletion: bool = False):; self.timing: Dict[str, float] = timing; self.is_deleted = is_deleted; self.ignore_job_deletion = ignore_job_deletion. def __enter__(self):; if self.is_deleted() and not self.ignore_job_deletion:; raise JobDeletedError(); self.timing['start_time'] = time_msecs(). def __exit__(self, exc_type, exc, tb):; if self.is_deleted() and not self.ignore_job_deletion:; return; finish_time = time_msecs(); self.timing['finish_time'] = finish_time; self.timing['duration'] = finish_time - self.timing['start_time']; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11082:73,race condition,race conditions,73,https://hail.is,https://github.com/hail-is/hail/pull/11082,1,['race condition'],['race conditions']
Performance,I'm opening this PR so I can get feedback more quickly by being at the front of the queue.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12263:84,queue,queue,84,https://hail.is,https://github.com/hail-is/hail/pull/12263,1,['queue'],['queue']
Performance,"I'm still not sure why writing to block matrix after a deep filter performs so badly, but in the meantime I want to document the fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4359:67,perform,performs,67,https://hail.is,https://github.com/hail-is/hail/pull/4359,1,['perform'],['performs']
Performance,"I'm trying grm for the first time, and I ran:. hail-new read -i /user/satterst/DBS_v2.4/temp.vds \; filtervariants --keep -c /user/satterst/purcell5k_nodups.interval_list \; count \; grm -f rel -o /user/satterst/DBS_v2.4/temp_rel_grm.tsv. This is 6247 exomes and 5284 variants. . Log file is here: /humgen/atgu1/fs03/satterst/hail.grm.log. I tried this once and let it go for over 40 minutes, and it stayed stuck at Stage 4: (0 + 25) / 25. I accidentally overwrote that log, so I did it again just now, and I didn't let it go for as long, but I observed the same behavior. . When I look at the job's task status page, I see the error I copied in the issue title. The details say:; org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 6, required: 8; Serialization trace:; data$mcD$sp (breeze.linalg.DenseMatrix$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.; at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:263); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:240); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). I'm curious if I'm doing something wrong or if grm is behaving badly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/321:1097,concurren,concurrent,1097,https://hail.is,https://github.com/hail-is/hail/issues/321,2,['concurren'],['concurrent']
Performance,"I'm trying to stop having us call `Region.loadBit` everywhere in `EBaseStruct.decode`. First step of that is not calling `setFieldMissing` and `setFieldPresent` everywhere. PRing for tests right now on first round of doing this, there are still some calls that need to be removed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10994:42,load,loadBit,42,https://hail.is,https://github.com/hail-is/hail/pull/10994,1,['load'],['loadBit']
Performance,"I've been supporting Hana as much as I can, but she needs someone who can be more dedicated and responsive than me. She uses a k8s cluster. She has a SEQR frontend deployment. She also has a Hail deployment (statefulset maybe?). The Hail pod has an SSD mounted read-only. That SSD has all the SEQR data in Hail Table form. There are many tables with annotations (variant metadata, like ""probability this variant is damaging"" or ""likely causes this to happen to the protein""). There are also ""per-family"" tables which contain all the sequences within a single family. Many queries are directly against a particular family. Those tables are small and quick to read. There's also one giant table containing all the sequences from all the families. That table is large and expensive to read. A lot of our engineering work has been around making sure queries against that table are fast. Tim, at one point, had enough of her system locally that he could experiment with running queries on his laptop against his SSD. He hacked on the queries themselves and on Hail itself until the bandwidth was fast enough that the queries should complete fast enough on the full dataset. Fast enough varies but generally a couple tens of seconds is OK. The work here is to pair with Hana to diagnose performance issues and make changes until the queries are acceptably fast. The first thing I would do is update her to the latest Hail (with the array decoder improvement as well as the memory overhead stuff on which Daniel is working). Then, with Hana's help, test the timing of some queries. If the queries are still too slow, your options are:; 1. Check the log files and the IR. Are there unnecessary shuffles? Is the code really large? Can we do less work maybe?; 2. Have Hana help you replicate her setup locally. You just need a slice of the data and enough of SEQR to run a query. Now hook up a profiler. What's slow? Can we do something about that?. ### Version. 0.2.124. ### Relevant log output. _No response_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882:1646,perform,performance,1646,https://hail.is,https://github.com/hail-is/hail/issues/13882,1,['perform'],['performance']
Performance,"I've left all the instances of `region.loadX` untouched (and left the methods on the region object, but this should let us avoid piping through region objects when we just need to read something. (Broken out from #6580)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6644:39,load,loadX,39,https://hail.is,https://github.com/hail-is/hail/pull/6644,1,['load'],['loadX']
Performance,IR-ify MT.dropRows. Optimize Filter(True()) and Filter(False()),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3908:20,Optimiz,Optimize,20,https://hail.is,https://github.com/hail-is/hail/pull/3908,1,['Optimiz'],['Optimize']
Performance,"If a many-partitions heavily-filtered matrix table is converted to a block matrix with `write_from_entry_expr`, parallelism is lost and kills performance. In the extreme case, imagine a MT with 4096 partitions, each with 1M rows, which are filtered to 1 row. There will be 1-way parallelism in the write. . We need to checkpoint the intermediate matrix if the loss of parallelism is above some threshold.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6995:142,perform,performance,142,https://hail.is,https://github.com/hail-is/hail/issues/6995,1,['perform'],['performance']
Performance,"If cols() sorts on the column key, then this optimization doesn't work for expressions with scans since they'd scan in a different order. (I also pulled out the `ContainsAgg` check for the next one, since MatrixFilterCols can't handle aggregations anyways.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4295:45,optimiz,optimization,45,https://hail.is,https://github.com/hail-is/hail/pull/4295,1,['optimiz'],['optimization']
Performance,"If there is no work to do, the scheduler threads should wait. This is likely causing the database load. run_if_changed isn't waiting if there is no work to do, so all three threads are spinning as fast as possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7955:98,load,load,98,https://hail.is,https://github.com/hail-is/hail/pull/7955,1,['load'],['load']
Performance,"Images are stored in a global cache, so we have to be careful that a private image pulled by one user isn't executed by another user. Therefore, pull each time for private images. We could speed this up by having a per-user private image cache.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7456:30,cache,cache,30,https://hail.is,https://github.com/hail-is/hail/pull/7456,2,['cache'],['cache']
Performance,"Implement an arbitrary inner product operation. This is the critical piece of; infrastructure that underlies all relatedness inference algorithms. The typical; inner product on real number matrices is defined as:. ```; L_ij : matrix of shape a by b; M_kl : matrix of shape b by c; N_il : matrix of shape a by c, the inner product of L and M. N_il = Sum_k (L_ik * M_kl); ```. This PR allows the user to define what `*` means and what `Sum` means. For; example, the KING paper defines an estimator for relatedness of homogeneous; populations called KING-homo. KING-homo's numerator is given by; `score_difference` below. ```python3; mt = hl.balding_nichols_model(2, 5, 5); mt = mt.select_entries(genotype_score=hl.float(mt.GT.n_alt_alleles())); da = hl.experimental.dnd.array(mt, 'genotype_score', block_size=3); score_difference = da.T.inner_product(; da,; lambda l, r: sqr(l - r),; lambda l, r: l + r,; hl.float(0),; hl.agg.sum; ); ```. The rest of KING-homo is just manipulation of row fields. Eventually we need to implement `ndarray_inner_product(self, other, product, sum)`; which does a cache-friendly pass over the data but applies arbitrary user; product and sum operations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9080:1092,cache,cache-friendly,1092,https://hail.is,https://github.com/hail-is/hail/pull/9080,1,['cache'],['cache-friendly']
Performance,"Implemented image untagging for image cleanup steps (like is done in GCR) for Azure. Since old layers still should be used for caching, this just removes the tag used for an image in a test build. We can then do something like [here](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-auto-purge#run-in-an-on-demand-task) where you can purge untagged layers that are older than some number of weeks where we believe they're no longer relevant to the layer cache. I also switched out the `registry-push-credentials` that CI uses to build images from the ACR admin login to CI's service principal and eliminated the admin login from the ACR terraform resource. I dev deployed CI and manually verified after a deploy that a tag that was cleaned up no longer showed up in acr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11100:482,cache,cache,482,https://hail.is,https://github.com/hail-is/hail/pull/11100,1,['cache'],['cache']
Performance,Importing from `batch_configuration` means that for this cache to be used you must define all environment variables that batch depends on. I severed this connection and fixed a use of the k8s cache in bootstrap.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11162:57,cache,cache,57,https://hail.is,https://github.com/hail-is/hail/pull/11162,2,['cache'],['cache']
Performance,Impose sorted partitioning on RDDs for huge performance gains,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/555:44,perform,performance,44,https://hail.is,https://github.com/hail-is/hail/pull/555,1,['perform'],['performance']
Performance,Improve performance of BaldingNicholsModel,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2314:8,perform,performance,8,https://hail.is,https://github.com/hail-is/hail/issues/2314,1,['perform'],['performance']
Performance,Improve performance of VCF Header Check,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2138:8,perform,performance,8,https://hail.is,https://github.com/hail-is/hail/pull/2138,1,['perform'],['performance']
Performance,Improve performance usage of StringTableReader by implementing StringTablePartitionReader instead of using rvbs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10753:8,perform,performance,8,https://hail.is,https://github.com/hail-is/hail/pull/10753,1,['perform'],['performance']
Performance,"Improved the optimizer so the IR generated for:. ```; mt = hl.import_vcf('sample.vcf'); mt.info.CCC.show(); ```. is reasonable. Run optimizer in compile, so we optimize (transformed) agg and seq ops. Simplify runs:; - propagate Begins up if possible; - inline single-use Lets; - (Apply annotate ...) => InsertFields if possible; - Turn MakeStruct of a bunch of FieldRefs into InsertFields. The optimizer now turns this:. ```; (TableMapGlobals Struct{} ""{}""; (MatrixRowsTable; (MatrixMapRows None None; (MatrixRead None False False ...); (Let __uid_1; (MakeStruct; (<expr>; (GetField CCC; (GetField info; (Ref ... va))))); (ApplyIR annotate; (MakeStruct; (locus; (GetField locus; (Ref ... va))); (alleles; (GetField alleles; (Ref ... va)))); (MakeStruct; (<expr>; (GetField `<expr>`; (Ref Struct{`<expr>`:Int32} __uid_1)))))))); (MakeStruct)); ```. I shit you not, that's literally what's generated by:. ```; import hail as hl; mt = hl.import_vcf('sample.vcf'); mt.info.CCC.show(); ```. into:. ```; (TableMapGlobals Struct{} ""{}""; (MatrixRowsTable; (MatrixMapRows None None; (MatrixRead ... False False ...); (InsertFields; (SelectFields (locus alleles); (Ref ... va)); (<expr>; (GetField CCC; (GetField info; (Ref ... va))))))); (MakeStruct)); ```. The only thing that's missing is to push the MatrixRowsTable into the MatrixMapRows. @tpoterba, I thought you had code for this? What happened?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3966:13,optimiz,optimizer,13,https://hail.is,https://github.com/hail-is/hail/pull/3966,4,['optimiz'],"['optimize', 'optimizer']"
Performance,Improves performance of GVCF import significantly:; ```; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; import_gvcf_force_count 81.2% 68.737 55.833; import_and_transform_gvcf 79.9% 75.692 60.464; ----------------------; Harmonic mean: 80.5%; Geometric mean: 80.6%; Arithmetic mean: 80.6%; Median: 80.6%; ```. Stacked on #8382,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8383:9,perform,performance,9,https://hail.is,https://github.com/hail-is/hail/pull/8383,1,['perform'],['performance']
Performance,"In QoB, the Query Driver and all Query Worker jobs reside in the same batch. Currently, the Query Driver submits worker jobs and then waits for them to finish before collecting their results from GCS. The way it waits is by polling the status of the batch and waiting for the number of completed jobs to reach `n_total_jobs - 1` (to account for itself). This is both awkward and prevents a couple potentially valuable pieces of functionality: you cannot run multiple concurrent Query Drivers, and you cannot take advantage of Batch's `cancel_after_n_failures` functionality because you do not want to cancel the Query Driver itself. The introduction of job groups addresses both these problems, as the Query Driver can create a job group for the stage of worker jobs and then await its completion. This PR makes a job group for the query driver and then the query driver creates a nested job group per stage of workers that it creates. Utilizing `cancel_after_n_failures` to fail a stage early is future work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14432:467,concurren,concurrent,467,https://hail.is,https://github.com/hail-is/hail/pull/14432,1,['concurren'],['concurrent']
Performance,"In R, when you load a data table, it auto-detects whether each column is a character vs. numeric type. It would be super if this could be implemented in Hail. I'm guessing it would take the form of ""if none of the fields in the column contain special characters or letters, then it's numeric, else it's character,"" (but maybe it's not so straight forward, not so sure...). . Anyways, when you have over 30 annotations that are numeric, it's a bit of a pain to have to go through writing all the -t flag options in Hail, so if it could be auto-detected, that would be super! . In the case where 'dummy' variables are used (like 1-5 for Batch), then the user should be able to say that that's a string or a ""factor"" as it is in R (or a character/string, which is essentially the same), for the purposes of analysis in linear regression.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/463:15,load,load,15,https://hail.is,https://github.com/hail-is/hail/issues/463,1,['load'],['load']
Performance,"In readBlock, membership was checked in a LinkedHashMap, then if it was; found, the value was retrieved from the cache. This lead to a data race; where a value could be evicted between the check and retrival. The; solution is to grab the block value out of the map, and if it is not; null, return it, otherwise, grab the block string, put it in the map,; and return it. Co-authored-by: Tim Poterba <tpoterba@broadinstitute.org>",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9427:113,cache,cache,113,https://hail.is,https://github.com/hail-is/hail/pull/9427,1,['cache'],['cache']
Performance,"In the Sphinx theme which the Hail docs use, the search page does not show anything unless a query has been provided and a search performed. https://github.com/readthedocs/sphinx_rtd_theme/blob/master/sphinx_rtd_theme/search.html. Thus, the Search link on the [home page of the Hail 0.2 docs](https://hail.is/docs/0.2/index.html) leads to a [blank page](https://hail.is/docs/0.2/search.html). ![image](https://user-images.githubusercontent.com/1156625/74118640-44333c80-4b8a-11ea-9147-7a0d188d44a0.png). To avoid confusion, this change removes the link to the search page from the home page.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8065:130,perform,performed,130,https://hail.is,https://github.com/hail-is/hail/pull/8065,1,['perform'],['performed']
Performance,"In the spirit of shrinking the `Backend` functionality to a core set of functions, move code cache access via execute context.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14691:93,cache,cache,93,https://hail.is,https://github.com/hail-is/hail/pull/14691,1,['cache'],['cache']
Performance,"In this PR, I rewrite `linear_regression_rows_nd` to use `_map_partitions` instead of `_group_within_partitions`. By doing this, I've eliminated the need to do a `key_by` at the end of `linear_regression_rows_nd`. I also think this makes the code clearer. . This PR also makes a few seemingly random changes that are actually bug fixes:. 1. When emitting `Apply` nodes, we were grabbing the `Code[Region]` from the first argument to the `MethodBuilder`. However, the assumption that the first argument will always be a `Region` seems to no longer be true. As such, we just construct a `CodeParam` from the `StagedRegion` we have available. . 2. In the NDArrayEmitter, I want to make sure I call the local `emit` method that passes off to `emitWithRegion`, for the same reason as 1: (Can't trust first argument to be a `Region`). 3. In `EmitStream`, I need to use `memoizeField` instead of `memoize`, because regular `memoize` saves to a `LocalRef`, and that will get reset to 0 when `next` is called on a stream. Lesson: don't trust locals for things that must live between elements of a stream. I feel like you have a better idea of how the Stream stuff gets emitted than I do Patrick. I'm curious if what I wrote in `process_block` could be written in a way that would lead to better code getting emitted, as I still need to figure out how to squeeze more performance out of this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9469:1358,perform,performance,1358,https://hail.is,https://github.com/hail-is/hail/pull/9469,1,['perform'],['performance']
Performance,Incorporates grafana which we did not have at the time of previous revision and removes the nginx config that is no longer used. We now use Envoy as our load balancer. I'll make a separate dev doc explaining the gateways.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14400:153,load,load,153,https://hail.is,https://github.com/hail-is/hail/pull/14400,1,['load'],['load']
Performance,Increase verbosity of Compile optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5749:30,optimiz,optimization,30,https://hail.is,https://github.com/hail-is/hail/pull/5749,1,['optimiz'],['optimization']
Performance,"Installing hail from the whl is going to break this cache every time so we end up installing the gcloud sdk on every build, which takes 40 seconds. It'd be nice to also move the extra pip packages further up the image but I think they're ordered this way so that pip gives us hail-compatible versions. I think the best fix would be to track these packages in a requirements.txt that is pinned and constrained on the hail package's requirements.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12301:52,cache,cache,52,https://hail.is,https://github.com/hail-is/hail/pull/12301,1,['cache'],['cache']
Performance,"Intern types rather than recording singletons.; Remove canonical field stuff from LoadVCF (unused), move to LoadGDB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2426:82,Load,LoadVCF,82,https://hail.is,https://github.com/hail-is/hail/pull/2426,2,['Load'],"['LoadGDB', 'LoadVCF']"
Performance,It appears (see [1] and [2]) that compiling AVX2 instructions (which hail uses to calculate IBD quickly) on a Mac using some versions of MacPorts GCC doesn't work. The Hail team recommends compiling with Clang when on Mac OS X. We _do not recommend_ removing AVX2 compatibility (either by adding `-mno-avx` or removing `-march=native`) because the AVX2 instructions are vital to IBD performance. [1] http://stackoverflow.com/questions/10327939/error-no-such-instruction-while-assembling-project-on-mac-os-x; [2] https://github.com/Theano/Theano/issues/1980,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1341:383,perform,performance,383,https://hail.is,https://github.com/hail-is/hail/issues/1341,1,['perform'],['performance']
Performance,"It definitely looks like ""ZONE_RESOURCE_POOL_EXHAUSTED"" is the cause of these GPU test failures. In this case it looks like it took ~4 minutes to successfully get a VM (after two exhaustion errors) & schedule the job. By then, our uniform 6 minute timeout per test left us with just two minutes. It looks like the job actually did succeed in the worker (seems to have taken ~2 minutes, seems long, does testing for CUDA do some kind of initialization work?). Looks like backing that off to 10 minutes might be just enough to eventually get us a GPU. Might be worth pulling that into its own build.yaml test job so that it does not block the queue of other tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13739:641,queue,queue,641,https://hail.is,https://github.com/hail-is/hail/pull/13739,1,['queue'],['queue']
Performance,It doesn't make sense to be able to delete or cancel an individual job since they must be part of a batch now. I also deleted `list_jobs` since a job must be a part of a batch. I left in `job.wait()` because I felt the tests in `test_dag` were important and shouldn't be deleted and the wait functionality is needed for there not to be race conditions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6023:336,race condition,race conditions,336,https://hail.is,https://github.com/hail-is/hail/pull/6023,1,['race condition'],['race conditions']
Performance,"It exists on LoadVCF in Scala, but isn't exposed in Python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3341:13,Load,LoadVCF,13,https://hail.is,https://github.com/hail-is/hail/issues/3341,1,['Load'],['LoadVCF']
Performance,It looks like this change did it: https://github.com/hail-is/hail/pull/4936. The styling isn't loading and I'm seeing the following errors in the console:. Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.js/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; bootstrap.min.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; style.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; navbar.css/:1 Failed to load resource: net::ERR_NAME_NOT_RESOLVED; (index):13 Uncaught ReferenceError: $ is not defined; at (index):13,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4948:95,load,loading,95,https://hail.is,https://github.com/hail-is/hail/issues/4948,6,['load'],"['load', 'loading']"
Performance,"It seems this is probably a bit slower than the current code on GCP (but there is variation so I'm not completely sure, and it still seems fast). It does give a functional S3 fileystem, tho. I will post timings below. Timings were done with my `test-copy` timing framework. - Add size_hint to create_part, used by the S3 backend and passed by copy. - Add a weighted semaphore (that can acquire n instead of just 1) and use it to limit the data in flight. It isn't completely clear how to do this. I could do, say, use a semaphore with value 10 * PART_SIZE and acquire the size of the object (which will be up to PART_SIZE). That might be a good idea, but instead I used 10 * BUFFER_SIZE and acquire the minimum of the BUFFER_SIZE and object size. This specifically limits the total intermediate buffer size. 10 was a mostly random choice, so you might try benchmarking to see if it makes a difference. - I made the copy part size destination filesystem specific. This is because the S3 multi-part upload API calls the partition contents be loaded into memory and 128MiB is too much for parallel uploads. The S3 default is 8MiB. - I create a new async writeable paired with a syncronous byte collector. It is used for the S3 multi-part upload call, which requires an the body to be a bytes/bytearray. - I tried to use readinto/write instead of read/write in SourceCopier.{_copy_file, _copy_part}, but in S3, the get_object API call returns a StreamingBody:. https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html. that doesn't support readinto(). - In SourceCopier._copy_part, it might be worth benchmarking reading the entire part into memory and then writing it out like we're forced to do on the AWS backend. To do this, we'd be forced to turn PART_SIZE down to ~8MiB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752:1040,load,loaded,1040,https://hail.is,https://github.com/hail-is/hail/pull/10752,1,['load'],['loaded']
Performance,"It takes one minute to build the docs *even if nothing has changed since the; last build*. There are a few things that lengthen the feedback cycle:. - We defeat Sphinx's input cache by deleting and re-copying over all the source; files.; - We defeat Sphinx's output cache by `mv`ing the output to a new location.; - We check that Hail is installed (at a cost of two seconds) *every* time we; build the docs. This isn't necessary, Sphinx prints a reasonable message; (""cannot import ..."") if Hail is not installed.; - We create a wheel file every time we build the docs at a cost of several; seconds.; - We recreate the tutorials tar even if it has not changed. Instead, I propose this PR:. - Do not copy the source files.; - Copy the output to the new location.; - Do not check hail is installed.; - Do not even install Hail.; - Use Make to check if the tutorial tar need be recreated. Regarding not installing Hail: even install-editable takes two seconds. It is; the developer's responsibility to ensure the right version of Hail is; installed. When you check out a branch just run `make install-editable`; once. Then edit the docs to your heart's desire, never re-install Hail. With this PR it takes ~3.5 seconds to rebuild the docs if nothing has; changed. We do work proportional to the number of changed files, not; proportional to all files. Sphinx itself takes 2-3 seconds, so we can't do much; better than this. Dice came up Patrick, but I imagine @tpoterba has thoughts.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9348:176,cache,cache,176,https://hail.is,https://github.com/hail-is/hail/pull/9348,2,['cache'],['cache']
Performance,"It was wrong for me to check `hasNext` before checking `childIterationCount != blockSize`. If you look at https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/io/vcf/LoadVCF.scala#L1306, you can see that `LoadVCF`'s `hasNext` loads values into memory. By calling `hasNext` and then not actually processing that value, I was loading a value into memory and then invalidating the pointer before it processed in the next group of rows. . Thanks to @tpoterba for helping me figure this out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8743:185,Load,LoadVCF,185,https://hail.is,https://github.com/hail-is/hail/pull/8743,4,"['Load', 'load']","['LoadVCF', 'loading', 'loads']"
Performance,It's just a string so `json.loads` fails on it. Not sure why I did that anyway. This has been broken on CI for a bit now. CI still manages fine because it checks everything on an interval but the callback helps it respond immediately to when batches finish for a PR test or deploy.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12631:28,load,loads,28,https://hail.is,https://github.com/hail-is/hail/pull/12631,1,['load'],['loads']
Performance,It's wildly unsafe. It's better to scope the unsafety in; `IEmitCode.handle` for `loadField`. Also add `PNDArrayValue.shapes` to handle the previous use case of; `PBaseStructValue.apply`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9289:82,load,loadField,82,https://hail.is,https://github.com/hail-is/hail/pull/9289,1,['load'],['loadField']
Performance,Iterate on optimization,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5862:11,optimiz,optimization,11,https://hail.is,https://github.com/hail-is/hail/pull/5862,1,['optimiz'],['optimization']
Performance,Iterator$$anon$11.next(Iterator.scala:496); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:415); 	at is.hail.rvd.RVD.$anonfun$head$2(RVD.scala:526); 	at is.hail.rvd.RVD.$anonfun$head$2$adapted(RVD.scala:526); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$2(ContextRDD.scala:366); 	at is.hail.sparkextras.ContextRDD.sparkManagedContext(ContextRDD.scala:164); 	at is.hail.sparkextras.ContextRDD.$anonfun$runJob$1(ContextRDD.scala:365); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:131); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2254); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2203); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2202); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2202); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682:7711,concurren,concurrent,7711,https://hail.is,https://github.com/hail-is/hail/issues/10682,1,['concurren'],['concurrent']
Performance,Iterator.to(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358); 	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431); 	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345); 	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1431); 	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021); 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2276); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:136); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at java.base/java.lang.Thread.run(Thread.java:829). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2673); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2609); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2608); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2608); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSet,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936:4771,concurren,concurrent,4771,https://hail.is,https://github.com/hail-is/hail/issues/12936,1,['concurren'],['concurrent']
Performance,"Jobs with large logs (>2GiB-ish) can break workers because the current worker code attempts to load the whole log as `bytes` before uploading it to blob storage. This loading into `bytes` also plagues the batch front end when loading logs from blob storage to present to the user.; ; This updates the worker and front end to always stream through logs, never load them into memory. Additionally, in order to make page loads in the UI reasonable, we limit the length of the log that is shown in the UI, with some advice to download the file if it's too large to render on the page. Fixes #13329",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14076:95,load,load,95,https://hail.is,https://github.com/hail-is/hail/pull/14076,5,['load'],"['load', 'loading', 'loads']"
Performance,"John, to gain some familiarity with your codebase, a proposed change. Makes the interface 1 method smaller, and reduces a bit of complexity in element value loading. Also fixes a potential source of errors long term: element loading should depend on the representation (as this controls memory layout), and not the elementType passed to the PNDArray constructor. This came up as I was writing down the invariants for PNDArray for the PTypes design doc. Feel free to push back on this if you have plans for getElementAddress (although if that's the case we should get rid of loadElementToIRIntermediate)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8177:157,load,loading,157,https://hail.is,https://github.com/hail-is/hail/pull/8177,3,['load'],"['loadElementToIRIntermediate', 'loading']"
Performance,"Just a heads up, any image that relies on pip, may want to pin to version 18.1 (if you want to use --no-cache-dir, as was done in notebook). https://github.com/pypa/pip/issues/6197",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5238:104,cache,cache-dir,104,https://hail.is,https://github.com/hail-is/hail/issues/5238,1,['cache'],['cache-dir']
Performance,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3017:598,Load,LoadMatrix,598,https://hail.is,https://github.com/hail-is/hail/issues/3017,2,['Load'],['LoadMatrix']
Performance,"Konrad is seeing bookend problems related to table joins with different data ranges. For example, trying to left join a table with only chr22 and a table with all chromosomes loads all chr1-21 partitions with the first partition of the left. This is a problem for any type of join, but we can easily optimize for left/right by removing partitions from the opposite table that cannot possibly overlap any partition of the result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3685:175,load,loads,175,https://hail.is,https://github.com/hail-is/hail/issues/3685,2,"['load', 'optimiz']","['loads', 'optimize']"
Performance,"L. This will only impact users who build; <code>cryptography</code> from source (i.e., not from a <code>wheel</code>), and specify their; own version of OpenSSL. For those users, the <code>CFLAGS</code>, <code>LDFLAGS</code>,; <code>INCLUDE</code>, <code>LIB</code>, and <code>CRYPTOGRAPHY_SUPPRESS_LINK_FLAGS</code> environment; variables will no longer be respected. Instead, users will need to; configure their builds <code>as documented here</code>_.</li>; <li>Added support for; :ref:<code>disabling the legacy provider in OpenSSL 3.0.x&lt;legacy-provider&gt;</code>.</li>; <li>Added support for disabling RSA key validation checks when loading RSA; keys via; :func:<code>~cryptography.hazmat.primitives.serialization.load_pem_private_key</code>,; :func:<code>~cryptography.hazmat.primitives.serialization.load_der_private_key</code>,; and; :meth:<code>~cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateNumbers.private_key</code>.; This speeds up key loading but is :term:<code>unsafe</code> if you are loading potentially; attacker supplied keys.</li>; <li>Significantly improved performance for; :class:<code>~cryptography.hazmat.primitives.ciphers.aead.ChaCha20Poly1305</code></li>; </ul>; <!-- raw HTML omitted -->; </blockquote>; <p>... (truncated)</p>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/pyca/cryptography/commit/d6951dca25de45abd52da51b608055371fbcde4e""><code>d6951dc</code></a> changelog + security fix backport (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8231"">#8231</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/138da90c8450446b19619e3faa77b9da54c34be3""><code>138da90</code></a> workaround scapy bug in downstream tests (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8218"">#8218</a>) (<a href=""https://github-redirect.dependabot.com/pyca/cryptography/issues/8228"">#8228</a>)</li>; <li><a href=""https://github.com/pyca/cryptography/commit/69527bc7",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12668:2987,load,loading,2987,https://hail.is,https://github.com/hail-is/hail/pull/12668,8,['load'],['loading']
Performance,LD prune optimization & question,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3697:9,optimiz,optimization,9,https://hail.is,https://github.com/hail-is/hail/pull/3697,1,['optimiz'],['optimization']
Performance,LD prune performance is unacceptable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4506:9,perform,performance,9,https://hail.is,https://github.com/hail-is/hail/issues/4506,1,['perform'],['performance']
Performance,"Large trees of union_cols have to concatenate entries arrays pairwise, creating a lot of junk in memory. A `MatrixMultiWayUnionCols` IR node should be straightforward to lower to `TableMultiWayZipJoin`, such that concatenating the entries arrays is completely deforested. We could then optimize nested `MatrixUnionCols` to a single `MatrixMultiWayUnionCols`, and/or expose a `multi_way_union_cols` method in python. https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/how.20does.20union_cols.20work/near/165089884",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6066:286,optimiz,optimize,286,https://hail.is,https://github.com/hail-is/hail/issues/6066,1,['optimiz'],['optimize']
Performance,"Let's build it from scratch, but better, faster, ... Philosophy: Minimal magic, minimal reliance on outside work, don't use it unless we understand it. Goal: <16ms interactions, including <16ms page transitions. Should feel identical to a desktop app in terms of performance, but maintain state like a website (i.e `get` variables). TODO:; - [ ] Profile/logout should be responsive: no user icon / dropdown until narrow view; - [x] Default to redirect rather than popup; - [x] Clicking on login should clear state if auth failed; - [ ] Write test for token verification on backend; - [ ] Add profile page; - [ ] Finish auth/redirect notebook logic in gateway; - [ ] Add notebook state endpoints in gateway; - [ ] Add notebook state view in frontend; - [ ] Break this up into ~10 commits, targeting <= 200 LOC each (with first commit being checking in package-lock.json); - [ ] Deal with cross-origin tracking issues in Safari. This may require using the ""custom domains"" feature of auth0, paid. Workaround could be to poll/websocket request to api server to refresh tokens. . To run:; ```sh; cd packages/web-client; docker build . -t blah; docker run --env-file=env-example -p 3000:3000 blah npm run start; ```; then navigate to `http://localhost:3000`. \# lines: Most come from the package.json.lock files. These maintain versioning information.; * [It is recommended to check in .lock files]( https://stackoverflow.com/questions/44206782/do-i-commit-the-package-lock-json-file-created-by-npm-5); * They're huge, sorry.; # Documentation; ### JS; https://javascript.info. We use the subset termed [ES2018](https://flaviocopes.com/es2018/). Compatibility across all browsers is ensured by [transpilation using BabelJS, to some lower JS target](https://babeljs.io/docs/en/). Polyfills should not be used, except when impossible to support a browser (this is configurable). I mostly don't care about anything that isn't an evergreen browser, so I think we should support: Edge, Safari, Chrome, Firefox. A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162:263,perform,performance,263,https://hail.is,https://github.com/hail-is/hail/pull/5162,1,['perform'],['performance']
Performance,Linear Regression ND Performance Fix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9666:21,Perform,Performance,21,https://hail.is,https://github.com/hail-is/hail/pull/9666,1,['Perform'],['Performance']
Performance,"Load VCF PP as Hail PL. Load GT and GQ normally, but for the purpose of input validation, interpret them with respect to PP rather than PL.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/349:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/issues/349,2,['Load'],['Load']
Performance,Load VCFs faster,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4744:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/pull/4744,1,['Load'],['Load']
Performance,Load less data in simple cases,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3626:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/pull/3626,1,['Load'],['Load']
Performance,Load partially missing calls as missing in import_vcf.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2404:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/pull/2404,1,['Load'],['Load']
Performance,Load vcf file error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6747:0,Load,Load,0,https://hail.is,https://github.com/hail-is/hail/issues/6747,1,['Load'],['Load']
Performance,LoadMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2246:0,Load,LoadMatrix,0,https://hail.is,https://github.com/hail-is/hail/pull/2246,2,['Load'],['LoadMatrix']
Performance,LoadVCF fails with non-standard bases,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:0,Load,LoadVCF,0,https://hail.is,https://github.com/hail-is/hail/issues/3015,1,['Load'],['LoadVCF']
Performance,LoadVCF should error on call fields that are not Number=1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3008:0,Load,LoadVCF,0,https://hail.is,https://github.com/hail-is/hail/issues/3008,1,['Load'],['LoadVCF']
Performance,"Loader; E ImportError: cannot import name 'Markup' from 'jinja2' (/home/circleci/conda/envs/lib/python3.7/site-packages/jinja2/__init__.py); [error] java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; [error] 	at scala.Predef$.require(Predef.scala:281); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14(build.sbt:288); [error] 	at $1fb87e3247134917ca70$.$anonfun$pythonSettings$14$adapted(build.sbt:278); [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49); [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62); [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67); [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:280); [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19); [error] 	at sbt.Execute.work(Execute.scala:289); [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:280); [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); [error] 	at java.lang.Thread.run(Thread.java:748); [error] (hail / hailtest) java.lang.IllegalArgumentException: requirement failed: Python tests in Hail environment failed; ```. To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: ; https://discuss.hail.is/. Please include the full Hail version and as much detail as possible. -----------------------------------------------------------------------------",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705:1674,Concurren,ConcurrentRestrictions,1674,https://hail.is,https://github.com/hail-is/hail/issues/11705,6,"['Concurren', 'concurren']","['ConcurrentRestrictions', 'concurrent']"
Performance,Loadmatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2431:0,Load,Loadmatrix,0,https://hail.is,https://github.com/hail-is/hail/pull/2431,1,['Load'],['Loadmatrix']
Performance,Logging improvements: log context calling Optimize and IR size,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5428:42,Optimiz,Optimize,42,https://hail.is,https://github.com/hail-is/hail/pull/5428,1,['Optimiz'],['Optimize']
Performance,"Looking at the IR generated by table.flatten, this snippet:; ```; >>> import hail as hl; >>> t = hl.utils.range_table(10); >>> t2 = t.annotate(**{f'f{i}': i for i in range(5)}); >>> t2.flatten().collect(); ```; generates the following IR:; ```; (GetField rows; (TableCollect; (TableMapRows; (TableOrderBy (Aidx); (TableMapRows; (TableRange 10 8); (InsertFields; (SelectFields (idx); (Ref row)); None; (f0; (I32 0)); (f1; (I32 1)); (f2; (I32 2)); (f3; (I32 3)); (f4; (I32 4))))); (Let __uid_3; (Ref row); (InsertFields; (SelectFields (); (SelectFields (idx f0 f1 f2 f3 f4); (Ref row))); None; (idx; (GetField idx; (Ref __uid_3))); (f0; (GetField f0; (Ref __uid_3))); (f1; (GetField f1; (Ref __uid_3))); (f2; (GetField f2; (Ref __uid_3))); (f3; (GetField f3; (Ref __uid_3))); (f4; (GetField f4; (Ref __uid_3)))))))); ```; If we look at the last `TableMapRows` IR, the entire thing `(Let __uid_3 …)` is entirely a no-op, but we're still compiling and generating code for the (post-optimization) IR:; ```; (InsertFields; (SelectFields (); (Ref row)); None; (idx; (GetField idx; (Ref row))); (f0; (GetField f0; (Ref row))); (f1; (GetField f1; (Ref row))); (f2; (GetField f2; (Ref row))); (f3; (GetField f3; (Ref row))); (f4; (GetField f4; (Ref row)))); ```. (cc @tpoterba I added a second `ForwardLets` in `Optimize` before the `Simplify`, although I'm not sure that's actually the correct place to put it; in this case, I think it may eventually come out in the wash given how many passes we make through any given pipeline, but I've noticed that currently our python tends to generate IR of the form:; ```; (TableMapRows; (Let __uid_n; (Ref row); <mapped value, sometimes using (Ref __uid_n) and sometimes (Ref row)>; ```; and that redundant binding at the top level means that the first Simplify pass misses quite a few optimizations! I'm not super attached to leaving it there, but I do think we might want to consider forwarding Lets on any IRs from python before optimization.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7719:978,optimiz,optimization,978,https://hail.is,https://github.com/hail-is/hail/pull/7719,4,"['Optimiz', 'optimiz']","['Optimize', 'optimization', 'optimizations']"
Performance,"M blocks / 2.1M chunks), regions.size = 19, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-24 01:58:51.515 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	| ; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionFailureScenario.toStorageException(JsonResumableSessionFailureScen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721:4660,concurren,concurrent,4660,https://hail.is,https://github.com/hail-is/hail/issues/13721,1,['concurren'],['concurrent']
Performance,"MContentLoaded in ~60-120ms (excluding network latency). ## Why not static/HTML web?; In practice: there is no such thing. Even document-centric sites often need dynamic templates, and will therefore use PHP, Python, NodeJS, Go, Rust, etc. These only work on a server, and only serve interpolated, static documents. Any interactive elements require Javascript. As soon as you need Javascript, the choice becomes Vanilla JS, JQuery, or something more structured. Vanilla JS requires a lot of boilerplate (verbose event binding, DOM modification, needs polyfills since browser incompatibilities). JQuery makes this easier, but is 1) very slow, 2) provides no structure. Vanilla JS and JQuery tend to devolve to soup of global state-modifying code, with a lot of time spent on figuring out how to update values in DOM elements. . React/Next make DOM modification declarative, and very very easy. They provide a great deal of structure (especially with Next handling tooling), and thanks to the virtual dom / reconciliation process, performs, in many cases, much faster than directly modifying the DOM (HTML) (i.e plain JS). React also handles necessities like properly escaping all inputs, for XSS attack prevention. All of this in a bundle size that isn't significantly bigger than JQuery, without all of those benefits (and React is rapidly shrinking). It's possible to avoid Javascript. One can simulate interactivity by issuing a server GET request for a new page, i.e click on a link with a GET variable ?someVar=val and get a new page. This is slow (full round trip cost), and puts much more load on the server (since it not only needs to make the db call, but interpret PHP/Python to render the view). . There is a good reason why JS and monolithic single page applications became popular, with all of the initial-load (bundle size) downsides: client-side rendering allows perceived performance on the order of native mobile or desktop applications. Achieving interactive UI's without JS or Web A",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931:1533,perform,performs,1533,https://hail.is,https://github.com/hail-is/hail/pull/4931,1,['perform'],['performs']
Performance,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2093:134,optimiz,optimize,134,https://hail.is,https://github.com/hail-is/hail/pull/2093,2,['optimiz'],['optimize']
Performance,"Main change: add `var mark: Int` to `BaseIR`.; On profiling the benchmark `matrix_multi_write_nothing`, I noticed a significant amount of time was spent ; - iterating through zipped arrays in requiredness ; - Adding and removing elements from `HashSet`s.; In fact, half the time spent in requiredness was removing ir nodes from the `HashSet` set used as the queue! With this change, requiredness runs like a stabbed rat!. Explanation of `mark`:; This field acts as a flag that analyses can set. For example:; - `HasSharing` can use the field to see if it has visited a node before.; - `Requiredness` uses this field to tell if a node is currently enqueued. The `nextFlag` method in `IrMetadata` allows for analyses to get a fresh value they can set the `mark` field. ; This removes the need to traverse the IR after analyses to re-zero every `mark` field.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13991:358,queue,queue,358,https://hail.is,https://github.com/hail-is/hail/pull/13991,1,['queue'],['queue']
Performance,Make Optimize less noisy when coming from Compile.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3999:5,Optimiz,Optimize,5,https://hail.is,https://github.com/hail-is/hail/pull/3999,1,['Optimiz'],['Optimize']
Performance,Make the parser faster; add support for cached IR nodes,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4195:40,cache,cached,40,https://hail.is,https://github.com/hail-is/hail/pull/4195,1,['cache'],['cached']
Performance,"Make the same resiliency changes I made to site recent to the other stateless services. Schedule them on 3 nodes, tolerate pre-emptibles, and autoscale 3-10 replicas. Preemptibles might be too aggressive, we should watch uptime. We probably want at least once instance running on non-preemptibles. We can do that explicitly by duplicating the pod spec, but I don't see a way to do it with tolerations and/or (anti-)affinities. We can also do this with notebook2 since it is stateless, but I'll leave that for another PR. I changed the Makefile structure, basically, don't support local docker build anymore and always pull from the repo and use --cache-from. I will modify the rest of the projects analogously in a separate PR. Switch infrastructure modules (gateway, router-resolver) to new jinja2 templating, instead of old @foo@ sed-based templating.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6201:647,cache,cache-from,647,https://hail.is,https://github.com/hail-is/hail/pull/6201,1,['cache'],['cache-from']
Performance,"Masahiro got this error message: . ```; File ""/tmp/59d4e99c253d424a9211eec0bdb4cd37/write_hardcall_mt.py"", line 20, in <module>; hl.export_bgen(mt, f'gs://ukbb-hail/ukb31063.dosage.hard_call.gwas_samples.chr{chrom}', gp=mt.GP, varid=mt.rsid); File ""</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1226>"", line 2, in export_bgen; File ""/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/opt/conda/default/lib/python3.6/site-packages/hail/methods/impex.py"", line 235, in export_bgen; Env.hail().utils.ExportType.getExportType(parallel)))); File ""/opt/conda/default/lib/python3.6/site-packages/hail/backend/backend.py"", line 109, in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); File ""/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/opt/conda/default/lib/python3.6/site-packages/hail/utils/java.py"", line 225, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$$anonfun$apply$1.apply(CompileAndEvaluate.scala:16); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:14); at is.hail.backend.Backend$$anonfun$execute$1.apply(Backend.scala:56)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8161:774,load,loads,774,https://hail.is,https://github.com/hail-is/hail/issues/8161,1,['load'],['loads']
Performance,"Maximal independent set has had a bug/misfeature since https://github.com/hail-is/hail/pull/2975. That PR added an `hl.int64(...)` coercion around the tie_breaker function. This allowed users to pass tie_breakers that returned floating point numbers, but it *changed the meaning*. The sign of values with magnitude greater than or equal to one was preserved. All values in (-1, 1) were converted to 0, thus treating them as indistinguishable for the purposes of the MIS. This PR fixes this long standing bug and adds a simple test for that case. Supporting arbitrary numeric types is actually quite simple! The conversion from any Hail numeric type to float64 is sign-preserving (AFAIK), which is the only property we need to preserve the user's intended ordering. This change also introduces two mild, obvious performance improvements:; - Use one region for the entire MIS calculation, clearing for each invocation of tie_breaker (MIS is single-threaded); - Read the tie_breaker value using simple Region and type methods rather than allocating a new SafeRow each time the tie_breaker is invoked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7729:811,perform,performance,811,https://hail.is,https://github.com/hail-is/hail/pull/7729,1,['perform'],['performance']
Performance,"Maybe some Google API cannot handle 3 batch-drivers under full load?. Also, this docker inspect thing is just pervasive and extraordinarily annoying.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13155:63,load,load,63,https://hail.is,https://github.com/hail-is/hail/pull/13155,1,['load'],['load']
Performance,"Memory's local storage is a simple cache, so it is safe for k8s to; evict it from the node if the node is underutilized. https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10864:35,cache,cache,35,https://hail.is,https://github.com/hail-is/hail/pull/10864,1,['cache'],['cache']
Performance,Merge after #2147 Priority queue.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2148:27,queue,queue,27,https://hail.is,https://github.com/hail-is/hail/pull/2148,1,['queue'],['queue']
Performance,Metadata loading fix.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/899:9,load,loading,9,https://hail.is,https://github.com/hail-is/hail/pull/899,1,['load'],['loading']
Performance,"MiB); 2022-05-14 12:09:09 SparkContext: INFO: Created broadcast 0 from broadcast at SparkBackend.scala:311; 2022-05-14 12:09:11 root: INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 30: Thread-4; 2022-05-14 12:09:11 root: ERROR: HailException: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; From is.hail.utils.HailException: /data/public/prs/ex_antonk.bim:1013423: Invalid locus '11:135009883' found. Position '135009883' is not within the range [1-135006516] for reference genome 'GRCh37'.; offending line: 11	.	0	135009883	CT	C; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:30); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:28); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.utils.Context.wrapException(Context.scala:21); 	at is.hail.utils.WithContext.foreach(Context.scala:51); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2(LoadPlink.scala:37); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$2$adapted(LoadPlink.scala:36); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1(LoadPlink.scala:36); 	at is.hail.io.plink.LoadPlink$.$anonfun$parseBim$1$adapted(LoadPlink.scala:35); 	at is.hail.io.fs.FS.$anonfun$readLines$1(FS.scala:222); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.io.fs.FS.readLines(FS.scala:213); 	at is.hail.io.fs.FS.readLines$(FS.scala:211); 	at is.hail.io.fs.HadoopFS.readLines(HadoopFS.scala:72); 	at is.hail.io.plink.LoadPlink$.parseBim(LoadPlink.scala:35); 	at is.hail.io.plink.MatrixPLINKReader$.fromJValue(LoadPlink.scala:179); 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:88); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1720); 	at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11836:1853,Load,LoadPlink,1853,https://hail.is,https://github.com/hail-is/hail/issues/11836,1,['Load'],['LoadPlink']
Performance,Minor expr optimizations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2477:11,optimiz,optimizations,11,https://hail.is,https://github.com/hail-is/hail/pull/2477,1,['optimiz'],['optimizations']
Performance,"Mostly code reorg. Also:. moved rewriters to ir objects; call Optimize before intepreting; removed Filter{Rows, Cols} rules (non-IR), those should get folded back into the MT methods like other AST-based rules; re-enabled Fitler{Rows, Cols}IR fusion rules since logical and/or is back",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3330:62,Optimiz,Optimize,62,https://hail.is,https://github.com/hail-is/hail/pull/3330,1,['Optimiz'],['Optimize']
Performance,"Mostly infrastructure. Added NewAST base class for Matrix and KeyTable ASTs with a primitive term rewriting engine. This should eventually be a base for AST, too (because we'll want to rewrite value expressions, too). I broke VariantMetadata into two parts: VSMMetadata (static types/metdata for VSM) and VSMLocalValue (part of MatrixValue that is computed/stored on master and broadcast). Added MatrixRead, FilterSamples and FilterVariants matrix AST nodes. Simple optimizer that pushes filters into read and some minor optimizations of filters.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778:466,optimiz,optimizer,466,https://hail.is,https://github.com/hail-is/hail/pull/1778,2,['optimiz'],"['optimizations', 'optimizer']"
Performance,"Moved key table type to Row, and misc performance improvements",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1583:38,perform,performance,38,https://hail.is,https://github.com/hail-is/hail/pull/1583,1,['perform'],['performance']
Performance,"Moves multi-pod deployments over to using Headless Services, which enables client-side load-balancing to the underlying pods. See #12095 for more context. The reason I put this in its own PR is that Kubernetes won't let me apply the `clusterIP: None` changes to existing `Services`, and I must delete the `Service` resources first. I can manually delete and apply new headless services in a way that is compatible with what is currently on main and with just a few seconds of downtime, but I should do this manually just before this PR merges.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094:87,load,load-balancing,87,https://hail.is,https://github.com/hail-is/hail/pull/12094,1,['load'],['load-balancing']
Performance,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2136:731,Load,LoadVCF,731,https://hail.is,https://github.com/hail-is/hail/issues/2136,6,['Load'],['LoadVCF']
Performance,"Multiple users in HPC, Possible Concurrency/Threading problem",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1525:32,Concurren,Concurrency,32,https://hail.is,https://github.com/hail-is/hail/issues/1525,1,['Concurren'],['Concurrency']
Performance,"NB, this is a stacked PR. To see just these changes see [this commit](https://github.com/hail-is/hail/pull/12883/commits/ae51e0a9af12e4c89a44e7ce3235f3f665ff4830). ---. [VPC Flow Logs](https://cloud.google.com/vpc/docs/flow-logs):. > VPC Flow Logs records a sample of network flows sent from and received by VM instances, including; > instances used as Google Kubernetes Engine nodes. These logs can be used for network monitoring,; > forensics, real-time security analysis, and expense optimization. I found the collection process the most elucidating part of the documentation. My summary of that; process follows:. 1. Packets are sampled on the network interface of a VM. Google claims an average sampling rate of; 1/30. This rate reduces if the VM is under load. This rate is immutable to us. 2. Within an ""aggregation interval"", packets are aggregated into ""records"" which are keyed (my term); by source & destination. There are currently six choices for aggregation interval: 5s, 30s, 1m,; 5m, 10m, and 15m. 3. Records are sampled. The sampling rate is a user configured floating point number (precision; unclear) between 0 and 1. 4. Metadata is optionally added to the records. The metadata captures information about the source; and destination VM such as project id, VM name, zone, region, GKE pod, GKE service, and geographic; information of external parties. The user may elect to receive all metadata, no metadata, or a; specific set of metadata fields. 5. The records are written to Google Cloud Logging. The pricing of VPC Flow Logs is described at the [network pricing page](https://cloud.google.com/vpc/network-pricing#network-telemetry). Notice that, if logs are only sent to Cloud Logging (not to BigQuery, Pub/Sub, or Cloud Storage):. > If you store your logs in Cloud Logging, logs generation charges are waived, and only Logging charges apply. I believe in this phrase ""logs generation charges"" refers to *VPC Flow logs* generation charges. The Google Cloud Logging [pricing page]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12883:487,optimiz,optimization,487,https://hail.is,https://github.com/hail-is/hail/pull/12883,2,"['load', 'optimiz']","['load', 'optimization']"
Performance,"NG: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181114-1827-0.2-721af83bc30a.log; Exception in thread ""dispatcher-event-loop-8"" Exception in thread ""refresh progress"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.util.zip.ZipCoder.getBytes(ZipCoder.java:80); at java.util.zip.ZipFile.getEntry(ZipFile.java:310); at java.util.jar.JarFile.getEntry(JarFile.java:240); at java.util.jar.JarFile.getJarEntry(JarFile.java:223); at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042); at sun.misc.URLClassPath.getResource(URLClassPath.java:239); at java.net.URLClassLoader$1.run(URLClassLoader.java:365); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:198); at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3.apply(HeartbeatReceiver.scala:196); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732); at org.apache.spark.HeartbeatReceiver.org$apache$spark$HeartbeatReceiver$$expireDeadHosts(HeartbeatReceiver.scala:196); at org.apache.spark.HeartbeatRe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780:1909,load,loadClass,1909,https://hail.is,https://github.com/hail-is/hail/issues/4780,1,['load'],['loadClass']
Performance,New load and extracts for hail datasets,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6803:4,load,load,4,https://hail.is,https://github.com/hail-is/hail/pull/6803,1,['load'],['load']
Performance,No detectable difference in performance:. ```; $ hail-bench compare /tmp/before.json /tmp/after2.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; matrix_table_decode_and_count 101.5% 4.216 4.278; ----------------------; Geometric mean: 101.5%; Simple mean: 101.5%; Median: 101.5%; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7143:28,perform,performance,28,https://hail.is,https://github.com/hail-is/hail/pull/7143,1,['perform'],['performance']
Performance,No error checking on LoadVCF if user-provided header file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2437:21,Load,LoadVCF,21,https://hail.is,https://github.com/hail-is/hail/pull/2437,1,['Load'],['LoadVCF']
Performance,"Not all of the Hail Tables and MatrixTables that are publicly available on the gnomAD [downloads](https://gnomad.broadinstitute.org/downloads) page are currently available in the Datasets API/Annotation DB. . This PR makes the following changes to the datasets available via the Hail Datasets API/Annotation DB:. - Add `gnomad_genome_sites` Table, versions: 3.1.1, 3.1.2; - Add `gnomad_hgdp_1kg_subset_dense` MatrixTable, version: 3.1.2; - Rename `gnomad_hgdp_1kg_callset` MatrixTable to `gnomad_hgdp_1kg_subset_dense`, version: 3.1; - Add `gnomad_hgdp_1kg_subset_sparse` MatrixTable, version: 3.1.2; - Add `gnomad_hgdp_1kg_subset_sample_metadata` Table, version: 3.1.2; - Add `gnomad_hgdp_1kg_subset_variant_annotations` Table, version: 3.1.2; - Add `gnomad_variant_co-occurrence` Table, version: 2.1.1; - Add `gnomad_pca_variant_loadings` Table, versions: 2.1, 3.1. Other general changes:. - Add/update the schema `.rst` files, for the datasets listed above, for the [docs](https://hail.is/docs/0.2/datasets/schemas.html); - Set the example dataset loaded in `hl.experimental.load_dataset` to be the most recent `gnomad_hgdp_1kg_subset_dense` MatrixTable (version 3.1.2)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11608:1051,load,loaded,1051,https://hail.is,https://github.com/hail-is/hail/pull/11608,1,['load'],['loaded']
Performance,"Note this PR replaces the previous [Feature/sas token merge](https://github.com/hail-is/hail/pull/12877) because the original PR branch got jacked up beyond repair. All the comments on the earlier PR are responded to there and addressed in the code for this one. This PR is to enable `hail-az/https` Azure file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; - Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; - Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new azure-mgmt-storage package requirement.; - Updated `AzureAsyncFS` to use `(account, container, credential)` tuple as internal `BlobServiceClient` cache key; - Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token; - Update `RouterFS.ls` function and associated listfiles function to allow for trailing query strings during path traversal; - Update `AsyncFS.open_from` function to handle query-string urls in zero-length case; - Change to existing behavior: `LocalAsyncFSURL.__str__` no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; - Updated `InputResource` to not include the SAS token as part of the destination file name; - Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions to respect the new model, where it is no longer safe to extend URLs by just appending new segments with `+ ""/""` because there may be a query string, and added `'sas/azure-https'` test case to the fixture. Running tests for the SAS case requires some new test variables to allow the test code to generate SAS toke",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140:909,cache,cache,909,https://hail.is,https://github.com/hail-is/hail/pull/13140,1,['cache'],['cache']
Performance,"Now that test databases are hosted on their own servers instead of the single cloud-hosted MySQL, we can ramp up the parallelism both in our tests and in the number of PRs that we run at once. I recall that even before we had this DB bottleneck we still restricted the number of PRs running at once for cost reasons, but if that's not the case we could remove that restriction entirely.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12974:234,bottleneck,bottleneck,234,https://hail.is,https://github.com/hail-is/hail/pull/12974,1,['bottleneck'],['bottleneck']
Performance,O: BatchConfig parsed.; 2024-11-05 02:43:37.209 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2024-11-05 02:43:37.783 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]; 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]; 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]; 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) [jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]; 	at java.lang.Thread.run(Thread.java:829) [?:?]; Caused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String length (20013488) exceeds the maximum length (20000000); 	at com.fasterxml.jackson.core.StreamReadConstraints.validateStringLength(StreamReadConstraints.java:324) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.ReadConstrainedTextBuffer.validateStringLength(ReadConstrainedTextBuffer.java:27) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.util.TextBuffer.finishCurrentSegment(TextBuffer.java:939) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishString2(UTF8StreamJsonParser.java:2584) ~[jackson-core-2.15.2.jar:2.15.2]; 	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._finishAndReturnString(UTF8StreamJsonPars,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749:2985,concurren,concurrent,2985,https://hail.is,https://github.com/hail-is/hail/issues/14749,1,['concurren'],['concurrent']
Performance,"OK, so. This continues to be a mammoth PR despite a day's worth of pruning. I think it would be good to start getting some eyes on it. Over all, I feel a bit weird about it. The intention is for this to be a functional but not scalable or reliable shuffler. It will allow Hail Query to exist, albeit in a limited way (keys cannot exceed shuffler memory). However, in parallel to getting this PR merged, I'm designing the real shuffler: a horizontally scalable sorting system. So. We have to live with this code for a few months, so let's make sure we feel good about it, but also know that this is all going away in a few months. 🤷‍♀ . # High Level Overview; - implement the shuffler as a single machine, multi-threaded service which buffers keys until the write phase of a shuffle is done, then sorts the keys, then serves them to clients.; - implement non-spark shuffling as: write records to `dbuf` and write pairs of (data key, dbuf key) to shuffler, then read back re-partitioned keys and fetch records from dbuf.; - I use SBT because the Akka examples use it, it's not obvious how to do this SBT assembly merging thing in Gradle; - I'm really not using Akka properly. There's all this DataSource stuff that I don't understand. I'll probably have to get this right to get good performance, but it doesn't seem critical now and the Akka docs are incredibly hard to understand.; - I turn the optimizer off in the tests because it often optimizes away shuffles into local sorts. There are some FIXMEs throughout the code that I would appreciate thoughts on.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8205:227,scalab,scalable,227,https://hail.is,https://github.com/hail-is/hail/pull/8205,6,"['multi-thread', 'optimiz', 'perform', 'scalab']","['multi-threaded', 'optimizer', 'optimizes', 'performance', 'scalable']"
Performance,"OK, this one is slightly subtle. A physical value at runtime actually has two forms:. 1. It is a bunch a bunch of bytes in memory at a particular address (e.g. a struct field, an array element, or a freestanding value in memory). 2. It is a value made up of JVM primitive values (or, more abstractly, Code[T]'s) that can be operated on immediately. Note, one option for (2) is just the address (1). This is what we do for structs (but note, not for arrays). Therefore, one thing we need is an operation that constructs a PValue from a physical type and an address to go from (1) => (2). I call this `PType.load`. It will be used in, for example, loadElement or loadField. See the use in loadElement below in PCanonicalIndexableValue. We also need something that goes from (2) => (1). There are two cases, whether the memory has been allocated already, or not, and I call them `PValue.store` and `PValue.allocateAndStore`. PType.load should be abstract and the implementation should be pushed to the leaves. I will do that once the full set of PValues are filled in. load/store will eventually allow us to eliminate all the IRIntermediate business. There was some complaint about my `PValue.apply` switching on PType. Some of the calls to it will go away in favor of load. I think of load as a kind of PValue constructor that takes a single argument pointing to memory. There will be other constructors depending on the PType. Those will eliminate the other calls to PValue.apply. Hopefully this discussion clears things up. FYI @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8247:606,load,load,606,https://hail.is,https://github.com/hail-is/hail/pull/8247,8,['load'],"['load', 'loadElement', 'loadField']"
Performance,"On Ubuntu 20.10, with Python 3.8.6 and hail 0.2.64 installed from pip, I get: ​I get `TypeError: an integer is required (got type bytes)` immediately upon importing hail. A full transcript is below. (pyve is an alias to create a python virtual env and activate it). ---. ```; snafu$ pyve; + python3.8 -m venv venv/3.8; + source venv/3.8/bin/activate; + pip install -U setuptools pip; Collecting setuptools; Using cached setuptools-54.1.2-py3-none-any.whl (785 kB); Collecting pip; Using cached pip-21.0.1-py3-none-any.whl (1.5 MB); Installing collected packages: setuptools, pip; Attempting uninstall: setuptools; Found existing installation: setuptools 44.0.0; Uninstalling setuptools-44.0.0:; Successfully uninstalled setuptools-44.0.0; Attempting uninstall: pip; Found existing installation: pip 20.1.1; Uninstalling pip-20.1.1:; Successfully uninstalled pip-20.1.1; Successfully installed pip-21.0.1 setuptools-54.1.2; (3.8) ✔ ~/sandbox/hail [master|𝚫8?2]; snafu$ pip install hail ipython; Collecting hail; Using cached hail-0.2.64-py3-none-any.whl (97.5 MB); Collecting ipython; Using cached ipython-7.21.0-py3-none-any.whl (784 kB); Collecting pandas<1.1.5,>=1.1.0; Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB); Collecting python-json-logger==0.1.11; Using cached python_json_logger-0.1.11-py2.py3-none-any.whl; Collecting gcsfs==0.7.2; Using cached gcsfs-0.7.2-py2.py3-none-any.whl (22 kB); Collecting requests==2.22.0; Using cached requests-2.22.0-py2.py3-none-any.whl (57 kB); Collecting tabulate==0.8.3; Using cached tabulate-0.8.3-py3-none-any.whl; Collecting nest-asyncio; Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB); Collecting parsimonious<0.9; Using cached parsimonious-0.8.1-py3-none-any.whl; Collecting pyspark<2.4.2,>=2.4; Using cached pyspark-2.4.1-py2.py3-none-any.whl; Collecting tqdm==4.42.1; Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB); Collecting bokeh<2.0,>1.3; Using cached bokeh-1.4.0-py3-none-any.whl; Collecting Deprecated<1.3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197:413,cache,cached,413,https://hail.is,https://github.com/hail-is/hail/issues/10197,3,['cache'],['cached']
Performance,"On a test of 1-2 partitions with 5000 samples, this takes the second stage of a densify from 2 minutes down to 1.4 minutes (only loading GT).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6967:129,load,loading,129,https://hail.is,https://github.com/hail-is/hail/pull/6967,1,['load'],['loading']
Performance,"One more time, with feeling! (was: #10072). - [x] (@tpoterba) a1f3b2a5c9 add fails_service_backend; - [ ] (@tpoterba, @cseed) dc0bee7ce1 [hail] introduce and use mktemp and mktempd; - [ ] (@tpoterba) 4b663be367 [hail] make is.hail.expr.ir.functions threadsafe; - [ ] (@tpoterba) d3c1f0987c [hail] fix use of row requiredness in lowerDistributedSort; - [ ] (@catoverdrive) aab6ba98be [query-service] handle void-typed IRs in query-service; - [ ] (@catoverdrive) a1619cff36 [query-service] make user cache thread-safe; - [ ] (@tpoterba) c315fcb0b1 [query-service] bugfix: preserve globals through a shuffle; - [ ] (@catoverdrive) 912c21f709 [shuffler] log ShuffleCodecSpec anytime it is created; - [x] (@daniel-goldstein) c2495837e7 [scala-lsm] bugfix: least key may equal greatest key; - [x] (@daniel-goldstein) 5fb3db703e [services] discovered new transient error; - [x] (@daniel-goldstein) 9cd0999938 [shuffler] more assertions in ShuffleClient; - [x] (@daniel-goldstein) a71a3c9b8c [shuffler] bugfix: shuffler needs a HailContext to decode loci; - [x] (@daniel-goldstein) 41b06aeaa8 [query-service] move hail.jar earlier in Dockerfile; - [x] (@daniel-goldstein) 8df4029698 [query-service] permit pod scaling and remove cpu limit; - [ ] (@catoverdrive) 0354e1f557 [query-service] simplify socket handling; - [x] (@jigold) 6690a4decc [batch] teach JVMJob where to find the hail configuration files; - [x] (@daniel-goldstein) ae2e3d2996 [query-service] switch to services team approved logging; - [ ] (@tpoterba) b18f86e647 [query-service] query workers need a hail context; - [ ] (@daniel-goldstein, @catoverdrive) 6d5d0b68af [query-service] use a UNIX Domain Socket for Py-Scala communication; - [ ] (@daniel-goldstein, @catoverdrive) 0d42df8b08 [query-service] run tests against query service; - [x] (@jigold) f9d361e686 [query-service] aiohttp.ClientSession must be created in async code; - [ ] (@cseed) c35f2e10e3 [query-service][hail][build.yaml] address miscellaneous comments from cotton; - [x]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10100:498,cache,cache,498,https://hail.is,https://github.com/hail-is/hail/pull/10100,1,['cache'],['cache']
Performance,Optimize IR generated in ArrayAgg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5765:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/5765,1,['Optimiz'],['Optimize']
Performance,Optimize MatrixMapCols with aggregation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4174:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/4174,1,['Optimiz'],['Optimize']
Performance,Optimize MatrixValue.entriesRVD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3950:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/3950,1,['Optimiz'],['Optimize']
Performance,Optimize PCA to LinReg transition,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/60:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/issues/60,1,['Optimiz'],['Optimize']
Performance,Optimize TSV export using local compression,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1707:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/issues/1707,1,['Optimiz'],['Optimize']
Performance,Optimize TableKeyBy(TableFilterIntervals(TableKeyBy)),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6147:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/issues/6147,1,['Optimiz'],['Optimize']
Performance,Optimize import annotations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/824,1,['Optimiz'],['Optimize']
Performance,"Optimize lowered TableHead to not scan a partition past the requested number of rows. Currently TableHead performs a loop, calculating the lengths of the first 4 partitions, then the first 16, etc. If we fix it to not count a partition multiple times, instead starting each loop at the first uncounted partition, we can further optimize to stop scanning a partition after the number of rows still needed (requested number minus sum of partitions counted so far). Note that there is no similar optimization to TableTail. We must compute the full length of each partition, to compute how many rows to drop.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9637:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/9637,4,"['Optimiz', 'optimiz', 'perform']","['Optimize', 'optimization', 'optimize', 'performs']"
Performance,Optimize sampleqc and various.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/93:0,Optimiz,Optimize,0,https://hail.is,https://github.com/hail-is/hail/pull/93,1,['Optimiz'],['Optimize']
Performance,Optimized BGEN import.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1551:0,Optimiz,Optimized,0,https://hail.is,https://github.com/hail-is/hail/pull/1551,1,['Optimiz'],['Optimized']
Performance,Optimized coalesce for VSMs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/681:0,Optimiz,Optimized,0,https://hail.is,https://github.com/hail-is/hail/pull/681,2,['Optimiz'],['Optimized']
Performance,Optimized sampleqc for (fixed) VSM structure. Added; downsamplevariants. Make sure all file IO goes through hadoop IO; interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/93:0,Optimiz,Optimized,0,https://hail.is,https://github.com/hail-is/hail/pull/93,1,['Optimiz'],['Optimized']
Performance,OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:5473,concurren,concurrent,5473,https://hail.is,https://github.com/hail-is/hail/issues/4055,1,['concurren'],['concurrent']
Performance,OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015:7337,concurren,concurrent,7337,https://hail.is,https://github.com/hail-is/hail/issues/3015,7,"['Load', 'concurren']","['LoadVCF', 'concurrent']"
Performance,"Our secret cache fails on ~1 in 10000 jobs. I observed this while running some; large scale tests which will soon become standard PR tests. In anticipation of this,; I fixed the k8s_cache. In particular, note how *everyone* who wins the lock tries to; remove it from the dictionary; however, only *one* task can do that successfully. The new code avoids locks entirely. It is a bit longer because I eagerly remove; out of date keys when I see them and use a future to notify all waiter simultaneouly. I also updated memory to use this cache for user credentials.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11040:11,cache,cache,11,https://hail.is,https://github.com/hail-is/hail/pull/11040,2,['cache'],['cache']
Performance,PCA loadings don't drop scores/eigenvalues,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5554:4,load,loadings,4,https://hail.is,https://github.com/hail-is/hail/issues/5554,1,['load'],['loadings']
Performance,"PIs for configuring Grafana settings.</li>; <li>api-change:<code>rbin</code>: This release adds support for Rule Lock for Recycle Bin, which allows you to lock retention rules so that they can no longer be modified or deleted.</li>; </ul>; <h1>1.29.15</h1>; <ul>; <li>bugfix:Endpoints: Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.29.14</h1>; <ul>; <li>api-change:<code>route53</code>: Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/boto/botocore/commit/f0dd67f9b7cc2791f301f3fd135f0c97d9c66bae""><code>f0dd67f</code></a> Merge branch 'release-1.29.16'</li>; <li><a href=""https://github.com/boto/botocore/commit/22c3cb362c0ef00c6de404140f06a14d0e195f39""><code>22c3cb3</code></a> Bumping version to 1.29.16</li>; <li><a href=""https://github.com/boto/botocore/commit/4aa5f864b62b6193ed0729a4ac71c010877fe377""><code>4aa5f86</code></a> Update to latest models</li>; <li><a href=""https://github.com/boto/botocore/commit/a7e153d9c822fae5c55d30ef476bdf4f55a4d027""><code>a7e153d</code></a> Merge branch 'release-1.29.15'</li>; <li><a href=""https://github.com/boto/botocore/commit/a942b57854dd35a37766d7973c3fb980a2de4068""><code>a942b57</code></a> Merge branch 'release-1.29.15' into develop</li>; <li><a href=""http",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12503:1465,latency,latency,1465,https://hail.is,https://github.com/hail-is/hail/pull/12503,1,['latency'],['latency']
Performance,"PRing for test suite, but it's mostly working. . Todo:. - [x] Optimization for already sorted tables; - [x] ~~Handle sort by descending~~ (deferred to subsequent PR); - [x] Handle tables with no partitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11050:62,Optimiz,Optimization,62,https://hail.is,https://github.com/hail-is/hail/pull/11050,1,['Optimiz'],['Optimization']
Performance,PTypes 21: Remove TContainer.loadElement (with Code),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4628:29,load,loadElement,29,https://hail.is,https://github.com/hail-is/hail/pull/4628,1,['load'],['loadElement']
Performance,PTypes 23: Convert more references to TContainer.loadElement,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4631:49,load,loadElement,49,https://hail.is,https://github.com/hail-is/hail/pull/4631,1,['load'],['loadElement']
Performance,Part 1 of chipping away at config.mk. This puts the two make targets for building the vm image in GCP into a single script. It loads variables that used to come from config.mk from kubernetes. Added a convenience function to offer a confirmation prompt before running the script. Here's an example:. ```; (hailenv) dgoldste@wmce3-cb7 hail % $HAIL/batch/gcp-create-worker-image.sh; Building image with properties:; Version: 12; Project: hail-vdc; Zone: us-central1-a; Are you sure? [y/N] n; (hailenv) dgoldste@wmce3-cb7 hail %; ```. Tested by running with a high image version number (3010 to be precise),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11327:127,load,loads,127,https://hail.is,https://github.com/hail-is/hail/pull/11327,1,['load'],['loads']
Performance,Performance boost for sites-only VDS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1824:0,Perform,Performance,0,https://hail.is,https://github.com/hail-is/hail/pull/1824,1,['Perform'],['Performance']
Performance,Performance optimizations for shuffling small rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4457:0,Perform,Performance,0,https://hail.is,https://github.com/hail-is/hail/pull/4457,2,"['Perform', 'optimiz']","['Performance', 'optimizations']"
Performance,Performance regression in import_vcf - sorting coercion now happens on every evaluation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9064:0,Perform,Performance,0,https://hail.is,https://github.com/hail-is/hail/issues/9064,1,['Perform'],['Performance']
Performance,Persist and cache return a VDS so we can persist again,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1380:12,cache,cache,12,https://hail.is,https://github.com/hail-is/hail/pull/1380,1,['cache'],['cache']
Performance,"Picking up where #13776 left off. CHANGELOG: improved speed of reading hail format datasets from disk. This PR speeds up decoding arrays in two main ways:; * instead of calling `arrayType.isElementDefined(array, i)` on every single array element, which expands to; ```scala; val b = aoff + lengthHeaderBytes + (i >> 3); !((Memory.loadByte(b) & (1 << (i & 7).toInt)) != 0); ```; process elements in groups of 64, and load the corresponding long of missing bits once; * once we have a whole long of missing bits, we can be smarter than branching on each bit. After flipping to get `presentBits`, we use the following psuedocode to extract the positions of the set bits, with time proportional to the number of set bits:; ```; while (presentBits != 0) {; val idx = java.lang.Long.numberOfTrailingZeroes(presentBits); // do something with idx; presentBits = presentBits & (presentBits - 1) // unsets the rightmost set bit; }; ```. To avoid needing to handle the last block of 64 elements differently, this PR changes the layout of `PCanonicalArray` to ensure the missing bits are always padded out to a multiple of 64 bits. They were already padded to a multiple of 32, and I don't expect this change to have much of an effect. But if needed, blocking by 32 elements instead had very similar performance in my benchmarks. I also experimented with unrolling loops. In the non-missing case, this is easy. In the missing case, I tried using `if (presentBits.bitCount >= 8)` to guard an unrolled inner loop. In both cases, unrolling was if anything slower. Dan observed benefit from unrolling, but that was combined with the first optimization above (not loading a bit from memory every element), which I beleive was the real source of improvement.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13787:330,load,loadByte,330,https://hail.is,https://github.com/hail-is/hail/pull/13787,5,"['load', 'optimiz', 'perform']","['load', 'loadByte', 'loading', 'optimization', 'performance']"
Performance,Please don't approve yet. Some initial experiments are showing that the performance here is pretty terrible - probably related to more interpreter usage.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5091:72,perform,performance,72,https://hail.is,https://github.com/hail-is/hail/pull/5091,1,['perform'],['performance']
Performance,"Plus:; - Boolean ldc (load constant) instructions need an int, not a boolean. JVM seems OK with it, but the asm bytecode verifier rejects it.; - In Apply codegen, the zip in function lookup was potentially truncating the arguments, selecting an incorrect function. Fix, and simplify the definition of `methods`.; - Fix/simplify asm error reporting from asm in lir Emit. The old code was crashing inside asm. I used the new code to debug some bytecode issues, it works well.; - compute max locals/stack, needed by the asm verifier (CheckClass). @konradjk This fixes your class not found issue.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8537:22,load,load,22,https://hail.is,https://github.com/hail-is/hail/pull/8537,1,['load'],['load']
Performance,PoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:81,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3467:4462,Load,LoadVCF,4462,https://hail.is,https://github.com/hail-is/hail/issues/3467,1,['Load'],['LoadVCF']
Performance,Pretty horrible performance. Most of the time is spent in the IR evaluation to annotate entries:. ```; (Let __iruid_65; (InsertFields; (Ref row); None; (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap __iruid_66; (ArrayRange; (I32 0); (I32 10); (I32 1)); (Literal Struct{} <literal value>)))); (InsertFields; (Ref __iruid_65); None; (`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (ArrayMap __iruid_67; (ArrayRange; (I32 0); (ArrayLen; (GetField __cols; (Ref global))); (I32 1)); (InsertFields; (ArrayRef; (GetField `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`; (Ref __iruid_65)); (Ref __iruid_67)); None; (x; (ApplyBinaryPrimOp Add; (GetField col_idx; (ArrayRef; (GetField __cols; (Ref global)); (Ref __iruid_67))); (GetField row_idx; (Ref __iruid_65))))))))))); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6541:16,perform,performance,16,https://hail.is,https://github.com/hail-is/hail/pull/6541,1,['perform'],['performance']
Performance,Priority queue,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2147:9,queue,queue,9,https://hail.is,https://github.com/hail-is/hail/pull/2147,1,['queue'],['queue']
Performance,Problem loading plink file,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/715:8,load,loading,8,https://hail.is,https://github.com/hail-is/hail/issues/715,1,['load'],['loading']
Performance,PutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760:8364,concurren,concurrent,8364,https://hail.is,https://github.com/hail-is/hail/issues/3760,1,['concurren'],['concurrent']
Performance,"Python CLI tools like `hailctl` suffer from slow startup times, which infuriate me. This is in part because the first thing that happens is python has to recursively load all imported packages, since imports are traditionally done at the top-level. Very conveniently, setting the `PYTHONPROFILEIMPORTTIME` environment variable will cause python to emit a profile to stderr, which you can visualize with tools like [tuna](https://github.com/nschloe/tuna). So running. ```; PYTHONPROFILEIMPORTTIME=1 hailctl dev config show 2> profile.log; tuna profile.log; ```. gave me this. <img width=""1576"" alt=""Screen Shot 2022-01-28 at 2 58 28 PM"" src=""https://user-images.githubusercontent.com/24440116/151614364-d57a4478-1516-4397-ac72-4f2b9c6c081b.png"">. showing that importing `aiohttp` is responsible for half the time it takes me to run `hailctl dev config show`, which is literally just printing a local file!! There's no reason this shouldn't be instantaneous, but reducing it to ~300ms, which this change did, is fine enough for me for now. Generally people don't care about import time because most applications are long-lived and what does a few seconds matter, so `pylint` by default wants us to put imports at the top level. I would say this is a valid exception.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11293:166,load,load,166,https://hail.is,https://github.com/hail-is/hail/pull/11293,1,['load'],['load']
Performance,"Quality of life improvement, wrapper around saving an ndarray and loading through numpy. Most changes are in the tests which previously could only extract elements from ndarrays to assert the transformation worked.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6020:66,load,loading,66,https://hail.is,https://github.com/hail-is/hail/pull/6020,1,['load'],['loading']
Performance,"RCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz --plugin LoF,loftee_path:/opt/vep/Plugins/,gerp_bigwig:/opt/vep/.vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/opt/vep/.vep/human_ancestor.fa.gz,conservation_file:/opt/vep/.vep/loftee.sql --dir_plugins /opt/vep/Plugins/ -o STDOUT' failed with non-zero exit status 2; VEP Error output:; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /opt/vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /opt/vep/Plugins/splice_site_scan.pl line 241.; DBI connect('dbname=/opt/vep/.vep/loftee.sql','',...) failed: unable to open database file at /opt/vep/Plugins/LoF.pm line 126. -------------------- EXCEPTION --------------------; MSG: ERROR: No cache found for homo_sapiens, version 95. STACK Bio::EnsEMBL::VEP::CacheDir::dir /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:328; STACK Bio::EnsEMBL::VEP::CacheDir::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:227; STACK Bio::EnsEMBL::VEP::CacheDir::new /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/CacheDir.pm:111; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all_from_cache /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:115; STACK Bio::EnsEMBL::VEP::AnnotationSourceAdaptor::get_all /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/AnnotationSourceAdaptor.pm:91; STACK Bio::EnsEMBL::VEP::BaseRunner::get_all_AnnotationSources /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/BaseRunner.pm:175; STACK Bio::EnsEMBL::VEP::Runner::init /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:123; STACK Bio::EnsEMBL::VEP::Runner::run /opt/vep/src/ensembl-vep/modules/Bio/EnsEMBL/VEP/Runner.pm:194; STACK ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513:2200,cache,cache,2200,https://hail.is,https://github.com/hail-is/hail/issues/14513,4,['cache'],['cache']
Performance,RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413:3867,concurren,concurrent,3867,https://hail.is,https://github.com/hail-is/hail/issues/3413,1,['concurren'],['concurrent']
Performance,RVD$$anonfun$orderedJoin$1.apply(KeyedOrderedRVD.scala:56); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$26.apply(ContextRDD.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$czipPartitions$1$$anonfun$apply$26.apply(ContextRDD.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$22$$anonfun$apply$23.apply(ContextRDD.scala:310); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$21$$anon$3.hasNext(OrderedRVD.scala:1015); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:357); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:471); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-eb5d13fe97fc; Error summary: HailException: OrderedRVD error! Unexpected PK in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Invalid PK: [quam]; Full key: [quam],MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055:12898,concurren,concurrent,12898,https://hail.is,https://github.com/hail-is/hail/issues/4055,2,['concurren'],['concurrent']
Performance,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3063:21727,concurren,concurrent,21727,https://hail.is,https://github.com/hail-is/hail/issues/3063,2,['concurren'],['concurrent']
Performance,Read(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) at io.netty.channel.AbstractChannelHandlerContext.innelHandlerContext.java:348) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) at lerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(Abstrac0) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) at io.netty.channel.AbstractChannelRead(AbstractChannelHandlerContext.java:348) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) at ocessSelectedKey(NioEventLoop.java:645) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) at ava:459) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) at io.netty.util.concurrent. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106:10942,concurren,concurrent,10942,https://hail.is,https://github.com/hail-is/hail/issues/8106,1,['concurren'],['concurrent']
Performance,"Reading function/contexts from GCS on query workers can contribute a significant portion of the runtime for small jobs. For a simple query like `hl.utils.range_table(10).collect()`, the jobs in the batch can range in time from 5-9 seconds depending on GCS latency. This builds on #9484 to add write-through capability to `memory` and a `ServiceCacheableFS` in Scala. The cacheable FS reads/writes through `memory` and falls back to GCS, so in the good path the ServiceBackend writes the compiled function and contexts to `memory`, workers read inputs and write outputs exclusively from/to memory, from which the ServiceBackend reads the results. From small benchmarks in dev, this cuts down read times on the workers by ~30-40% compared to the worst case GCS latencies and roughly matches the current implementation in the best case. Writing the outputs is comparable to writing through an already warmed up GCS connection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10315:256,latency,latency,256,https://hail.is,https://github.com/hail-is/hail/pull/10315,2,"['cache', 'latency']","['cacheable', 'latency']"
Performance,"Reduces necessary template for a new page to:. ```xslt; <?xml version=""1.0"" encoding=""ISO-8859-15""?>; <xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">. <xsl:import href=""template.xslt""/>. <xsl:template name=""page-title"">Foo Bar</xsl:template>; <xsl:template name=""meta-description"">; <meta name=""description"" content=""Hail Foo Bar Baz""/>; </xsl:template>. </xsl:stylesheet>. ```. also moves scripts around to reduce blocking html loading. https://developers.google.com/speed/docs/insights/BlockingJS. cc @mkveerapen",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8455:461,load,loading,461,https://hail.is,https://github.com/hail-is/hail/pull/8455,1,['load'],['loading']
Performance,Refactor LoadVCF to use MatrixRead,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3840:9,Load,LoadVCF,9,https://hail.is,https://github.com/hail-is/hail/pull/3840,1,['Load'],['LoadVCF']
Performance,Refactored table reader coercion and caching mechanism. ### What changed?. - Removed `shouldCacheQueryInfo` method from `Backend` class; - Introduced `CoercerCache` in `ExecuteContext`; - Refactored `LoweredTableReader.makeCoercer` to return a function instead of a class; - Removed local caching in `GenericTableValue` and `LoweredTableReader`; - Added `NoCaching` utility . ### Why make this change?. This change aims to optimize table reader coercion by:; - Centralizing caching logic in `ExecuteContext`; - Allowing more flexible caching strategies across different backend implementations,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14696:423,optimiz,optimize,423,https://hail.is,https://github.com/hail-is/hail/pull/14696,1,['optimiz'],['optimize']
Performance,"Reference</code> and <code>NativeLibrary</code> - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1440"">#1440</a>: Support for LoongArch64 - <a href=""https://github.com/Panxuefeng-loongson""><code>@​Panxuefeng-loongson</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1444"">#1444</a>: Update embedded libffi to 1f14b3fa92d4442a60233e9596ddec428a985e3c and rebuild native libraries - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; </ul>; <h2>Bug Fixes</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1438"">#1438</a>: Handle arrays in structures with differing size - <a href=""https://github.com/matthiasblaesing""><code>@​matthiasblaesing</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/issues/1442"">#1442</a>: Handle race condition in <code>c.s.j.p.win32.PdhUtil#PdhEnumObjectItems</code> - <a href=""https://github.com/dbwiddis""><code>@​dbwiddis</code></a>.</li>; </ul>; <h2>Important Changes</h2>; <ul>; <li><code>Memory#dispose</code>, <code>CallbackReference#dispose</code> and <code>NativeLibrary#dispose</code>; were called by the <code>Object#finalize</code> override. These calls were replaced by; the use of a cleaner. It is not guaranteed anymore, that <code>dispose</code> is called; on subclasses on finalization.</li>; </ul>; <h1>Release 5.11.0</h1>; <h2>Features</h2>; <ul>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1398"">#1398</a>: Increase <code>c.s.j.p.win32.Sspi#MAX_TOKEN_SIZE</code> on Windows 8/Server 2012 and later - <a href=""https://github.com/dbwiddis""><code>@​dbwiddis</code></a>.</li>; <li><a href=""https://github-redirect.dependabot.com/java-native-access/jna/pull/1403"">#1403</a>: Rebuild AIX binaries with libffi 3.4.2 (other archite",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12438:2124,race condition,race condition,2124,https://hail.is,https://github.com/hail-is/hail/pull/12438,1,['race condition'],['race condition']
Performance,"Releases version 0.2.56. Stacked on #9373, since I'm mainly releasing for performance improvements in #9363, #9373, and #9374.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9386:74,perform,performance,74,https://hail.is,https://github.com/hail-is/hail/pull/9386,1,['perform'],['performance']
Performance,Remove Optimize method signature for BlockMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5834:7,Optimiz,Optimize,7,https://hail.is,https://github.com/hail-is/hail/pull/5834,1,['Optimiz'],['Optimize']
Performance,"Remove the `Begin` node, as its behavior can now be represented by the `Let` node. Besides removing redundant nodes, this will also make the new ssa-style text representation simpler. The `Begin` node emmitter performed method splitting, emitting groups of 16 children in seperate methods. This preserves that behavior by doing a similar optimization in the `Let` emitter. This is a significant change in how we split generated code into methods, so we should watch out for how this affects things.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14068:210,perform,performed,210,https://hail.is,https://github.com/hail-is/hail/pull/14068,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,Rename EmitValue.get to EmitValue.load,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9732:34,load,load,34,https://hail.is,https://github.com/hail-is/hail/pull/9732,1,['load'],['load']
Performance,"Renamed and moved `datasets/annotation_db.json` config file to `hail/experimental/datasets.json` and modified urls in `dataset[path]` to use a region parameter to load datasets from bucket in the appropriate region. Modified `load_datasets()` function to no longer use the `config_file` parameter, and to require user to specify `region` parameter. The checked-in `hail/experimental/datasets.json` file will now be used as the config file.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9411:163,load,load,163,https://hail.is,https://github.com/hail-is/hail/pull/9411,1,['load'],['load']
Performance,Reopening. Needed by https://github.com/hail-is/hail/pull/2097. Will address performance once Row is gone and we're full unsafe.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2098:77,perform,performance,77,https://hail.is,https://github.com/hail-is/hail/pull/2098,1,['perform'],['performance']
Performance,"Replace uses of `new SFooCode(...).memoize(cb)` with one of; * `new SFooValue(...)`; * for pointer types, `pType.loadCheapSCode(cb, addr)`. With `memoize` no longer used, all `SCode` methods are unused. I delete all the class bodies, but leave the concrete `SCode` classes, because `SSettable.store(cb, SValue)` is still implemented as `SSettable.store(cb, sv.get)`. Fixing that will be the next PR. Delete `loadCheapSCodeField`, replace uses with `cb.memoizeField(loadCheapSCode(...))`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11240:113,load,loadCheapSCode,113,https://hail.is,https://github.com/hail-is/hail/pull/11240,3,['load'],"['loadCheapSCode', 'loadCheapSCodeField']"
Performance,"Replaces #8533. I add two build steps: `test_dataproc` and `deploy`. Both of the new steps are; scoped for `dev` and `deploy`. However, we intend to only run these steps when; the pip version changes (i.e. when we ""release""). These steps only perform work; when hail-is/hail lacks a tag for the pip version described in; `hail/Makefile`. Otherwise, they `exit 0` with an informative note. The `test_dataproc` step, unfortunately, builds hail. The hailctl artifacts are; placed in `gs://hail-common/hailctl/dataproc/ci_test_dataproc/...`. Otherwise; test_dataproc operates identically to `make test-dataproc`. The `deploy` step uses `wheel-container.tar` rather than building; Hail (again). I migrated the `deploy` and `test-dataproc` code out of the; `Makefile` and into bash scripts. I did not migrate the artifact upload out of the; `Makefile`. The `dev` scope is only intended for debugging production issues or; prospectively testing dataproc on a suspicious change set. ---. The PR test results are uninformative as to the correctness of this change; because these steps are not scoped `test`. I tested [test_dataproc in a dev; deploy](https://ci.hail.is/batches/32357). I have not tested `deploy.sh`. I take; responsibility for executing the next deploy. ---. If CI deploy is broken but CI can still run dev-deploys, then a developer may; deploy hail with `hailctl`:. ```; hailctl dev deploy hail-is/hail:master --steps deploy; ```. One may also deploy from a laptop. You need curl >=7.55.0 (that version; implemented reading headers from a file). Create $HOME/.pypirc and put this; there:. ```; [pypi]; username: hailteam; password: GET_THIS_FROM_THE_USUAL_PLACE; ```. get a github access token with repo; privileges (https://github.com/settings/tokens), create; $HOME/.github-oauth-header, and put this there:. ```; Authorization: token YOUR_ACCESS_TOKEN_HERE; ```. Now, deploy from your laptop:. ```; make deploy GITHUB_OAUTH_HEADER_FILE=$HOME/.github-oauth-header DEPLOY_REMOTE=THE_REMOTE_FO",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8550:243,perform,perform,243,https://hail.is,https://github.com/hail-is/hail/pull/8550,1,['perform'],['perform']
Performance,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2803:519,load,loadInt,519,https://hail.is,https://github.com/hail-is/hail/issues/2803,2,['load'],"['loadInt', 'loadLength']"
Performance,"Required for adequate performance in forthcoming lowered MatrixIRs involving the cols. There are a few changes here. 1) LiftLiterals is renamed to LiftNonCompilable. This pass functions to lift non-compilable IRs into MapGlobals nodes.; 2) Introduced EvaluateNonCompilable pass. This evaluates all non-compilable nodes, and replaces them either with primitives (I32, Str, NA), or with references to a struct of compound values.; 3) TableMapGlobals uses EvaluateNonCompilable and the compiler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5125:22,perform,performance,22,https://hail.is,https://github.com/hail-is/hail/pull/5125,1,['perform'],['performance']
Performance,Resolves a performance problem with _to_table that caused a re-scan,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3105:11,perform,performance,11,https://hail.is,https://github.com/hail-is/hail/pull/3105,1,['perform'],['performance']
Performance,"Revert ""[auth] Cache user sessions in memory""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12148:15,Cache,Cache,15,https://hail.is,https://github.com/hail-is/hail/pull/12148,1,['Cache'],['Cache']
Performance,"Reverts hail-is/hail#14374. #14373, #14377 and #14379 appear to have restored Batch's throughput.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14387:86,throughput,throughput,86,https://hail.is,https://github.com/hail-is/hail/pull/14387,1,['throughput'],['throughput']
Performance,Reverts hail-is/hail#9738. Concurrently creating certs doesn't work when OpenSSL assigns consecutive serial numbers.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10141:27,Concurren,Concurrently,27,https://hail.is,https://github.com/hail-is/hail/pull/10141,1,['Concurren'],['Concurrently']
Performance,"Reverts hail-is/hail#9874. This is invalid, doesn't handle the fact that negative ones can be present in the shape. Unless we handle that in python/IR, we can't do this optimization.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9877:169,optimiz,optimization,169,https://hail.is,https://github.com/hail-is/hail/pull/9877,1,['optimiz'],['optimization']
Performance,"Right now it's not entirely clear what to do to ensure that a build works on the dataflow cluster, as well as to generate a set of benchmarks that capture a set of performance statistics well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/156:164,perform,performance,164,https://hail.is,https://github.com/hail-is/hail/issues/156,1,['perform'],['performance']
Performance,"Right now the Grafana is exposed as a k8s service speaking http with only the grafana auth. This puts an nginx sidecar in front of Grafana to bring TLS all the way through the the Grafana pod and perform dev authentication. This required adding an api endpoint to auth that can verify a connection based on the session and not an Authorization header. Other services like `router-resolver` have gotten away with not having this since they construct the Authorization header in python before hitting the `userinfo` endpoint, but this seems like a straightforward addition that will make it easier for internal authentication such as this case. This does another deviant thing which is using a `runImage` step to template the nginx config instead of templating inside the container at container start time (like router and site currently do). It is a little janky, because there are essentially two jinja passes, one in CI to render the shell script for the job, and then the jinja line in the job itself to render the nginx config. But this looks to be the most straightforward way I could figure out without adding another `build.py` Step type and even in that case it would have to be some sort of no-op job.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10139:196,perform,perform,196,https://hail.is,https://github.com/hail-is/hail/pull/10139,1,['perform'],['perform']
Performance,"Right now, we perform a full scan in `to_dense_mt`, we have information to do less work and densify in a single pass. - [ ] Expose partitioning in python; - [ ] For each partition in the variants table, use `ref_block_max_length` to determine the full reference interval necessary to densify that partition; - [ ] Use `map_partitions` of the variants and `query_table` on the reference to get two streams with all information necessary to densify.; - [ ] Join the streams and use the current algorithm/scan to do the work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14499:14,perform,perform,14,https://hail.is,https://github.com/hail-is/hail/issues/14499,1,['perform'],['perform']
Performance,"Rule Lock for Recycle Bin, which allows you to lock retention rules so that they can no longer be modified or deleted.</li>; </ul>; <h1>1.26.15</h1>; <ul>; <li>bugfix:Endpoints: [<code>botocore</code>] Resolve endpoint with default partition when no region is set</li>; <li>bugfix:s3: [<code>botocore</code>] fixes missing x-amz-content-sha256 header for s3 object lambda</li>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] Adding support for Amazon AppFlow to transfer the data to Amazon Redshift databases through Amazon Redshift Data API service. This feature will support the Redshift destination connector on both public and private accessible Amazon Redshift Clusters and Amazon Redshift Serverless.</li>; <li>api-change:<code>kinesisanalyticsv2</code>: [<code>botocore</code>] Support for Apache Flink 1.15 in Kinesis Data Analytics.</li>; </ul>; <h1>1.26.14</h1>; <ul>; <li>api-change:<code>route53</code>: [<code>botocore</code>] Amazon Route 53 now supports the Asia Pacific (Hyderabad) Region (ap-south-2) for latency records, geoproximity records, and private DNS for Amazon VPCs in that region.</li>; </ul>; <h1>1.26.13</h1>; <ul>; <li>api-change:<code>appflow</code>: [<code>botocore</code>] AppFlow provides a new API called UpdateConnectorRegistration to update a custom connector that customers have previously registered. With this API, customers no longer need to unregister and then register a connector to make an update.</li>; <li>api-change:<code>auditmanager</code>: [<code>botocore</code>] This release introduces a new feature for Audit Manager: Evidence finder. You can now use evidence finder to quickly query your evidence, and add the matching evidence results to an assessment report.</li>; <li>api-change:<code>chime-sdk-voice</code>: [<code>botocore</code>] Amazon Chime Voice Connector, Voice Connector Group and PSTN Audio Service APIs are now available in the Amazon Chime SDK Voice namespace. See <a href=""https://docs.aws.amazon.com/chime-sdk/late",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12502:1620,latency,latency,1620,https://hail.is,https://github.com/hail-is/hail/pull/12502,2,['latency'],['latency']
Performance,"S$: INFO: close: gs://neale-bge/foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index; 2023-09-22 19:11:12.656 : INFO: TaskReport: stage=0, partition=0, attempt=0, peakBytes=656384, peakBytesReadable=641.00 KiB, chunks requested=4, cache hits=2; 2023-09-22 19:11:12.656 : INFO: RegionPool: FREE: 641.0K allocated (257.0K blocks / 384.0K chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-22 19:11:12.656 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/neale-bge/o?name=foo.ht/index/part-0-c7ba7549-bf68-42db-a8ef-0f1b13721c79.idx/index&uploadType=resumable; {; ""error"": {; ""code"": 403,; ""message"": ""dking-ae4q6@hail-vdc.iam.gserviceaccount.com does not have storage.objects.create access to the Google Cloud Storage object. Permissio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13697:9750,concurren,concurrent,9750,https://hail.is,https://github.com/hail-is/hail/issues/13697,1,['concurren'],['concurrent']
Performance,"Saves memory on the master but has an unfortunate side effect of; drastically multiplying the number of times a tabix file is read, as it; is read once per partition per vcf rather than once per vcf. This change places tabix reading in a more critical path of the gVCF; merger, I would appreciate a more detailed performance audit of that; code, in addition to looking over this change. From my measurements it looks like we pay a 20-30 second cost per partition of 100 gVCFs. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5596:313,perform,performance,313,https://hail.is,https://github.com/hail-is/hail/pull/5596,1,['perform'],['performance']
Performance,"SciPy 1.11.1 Release Notes</h1>; <p>SciPy <code>1.11.1</code> is a bug-fix release with no new features; compared to <code>1.11.0</code>. In particular, a licensing issue; discovered after the release of <code>1.11.0</code> has been addressed.</p>; <h1>Authors</h1>; <ul>; <li>Name (commits)</li>; <li>h-vetinari (1)</li>; <li>Robert Kern (1)</li>; <li>Ilhan Polat (4)</li>; <li>Tyler Reddy (8)</li>; </ul>; <p>A total of 4 people contributed to this release.; People with a &quot;+&quot; by their names contributed a patch for the first time.; This list of names is automatically generated, and may not be fully complete.</p>; <h1>SciPy 1.11.0 Release Notes</h1>; <p>SciPy <code>1.11.0</code> is the culmination of 6 months of hard work. It contains; many new features, numerous bug-fixes, improved test coverage and better; documentation. There have been a number of deprecations and API changes; in this release, which are documented below. All users are encouraged to; upgrade to this release, as there are a large number of bug-fixes and; optimizations. Before upgrading, we recommend that users check that; their own code does not use deprecated SciPy functionality (to do so,; run your code with <code>python -Wd</code> and check for <code>DeprecationWarning</code> s).; Our development attention will now shift to bug-fix releases on the; 1.11.x branch, and on adding new features on the main branch.</p>; <p>This release requires Python <code>3.9+</code> and NumPy <code>1.21.6</code> or greater.</p>; <p>For running on PyPy, PyPy3 <code>6.0+</code> is required.</p>; <h1>Highlights of this release</h1>; <ul>; <li>Several <code>scipy.sparse</code> array API improvements, including <code>sparse.sparray</code>, a new; public base class distinct from the older <code>sparse.spmatrix</code> class,; proper 64-bit index support, and numerous deprecations paving the way to a; modern sparse array experience.</li>; <li><code>scipy.stats</code> added tools for survival analysis, multiple hypothe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13228:1278,optimiz,optimizations,1278,https://hail.is,https://github.com/hail-is/hail/pull/13228,1,['optimiz'],['optimizations']
Performance,"Scorecard: Use Sanic, cache fully-shaped GitHub response",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242:22,cache,cache,22,https://hail.is,https://github.com/hail-is/hail/pull/5242,1,['cache'],['cache']
Performance,"Script:; ```python3; #!/usr/bin/env python3; import hail as hl; hl.init(log='/dev/null'); mt = hl.import_vcf('src/test/resources/sample.vcf'); mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); ```. Output:; ```; 2019-06-24 19:12:05 WARN Utils:66 - Your hostname, wp086-661 resolves to a loopback address: 127.0.1.1; using 10.1.8.50 instead (on interface wlp2s0); 2019-06-24 19:12:05 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2019-06-24 19:12:06 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Running on Apache Spark version 2.4.0; SparkUI available at http://wp086-661.broadinstitute.org:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-e95038bbed35; LOGGING: writing to /dev/null; Traceback (most recent call last):; File ""/tmp/x"", line 4, in <module>; mt.filter_rows(mt.locus < hl.Locus('1', 1)).show(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-1000>"", line 2, in show; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2569, in show; actual_n_cols = self.count_cols(); File ""</home/BROAD.MIT.EDU/cvittal/.cache/hail-env/lib/python3.6/site-packages/decorator.py:decorator-gen-994>"", line 2, in count_cols; File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/python/hail/matrixtable.py"", line 2404, in count_cols; return Env.backend().execute(ir); File ""/home/BROAD.MIT.EDU/cvittal/src/hail-alt/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458:517,load,load,517,https://hail.is,https://github.com/hail-is/hail/issues/6458,1,['load'],['load']
Performance,See here for the problem I'm solving with this PR: https://dev.hail.is/t/ndarray-matmul-performance-improvements/176. cc @danking,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8037:88,perform,performance-improvements,88,https://hail.is,https://github.com/hail-is/hail/pull/8037,1,['perform'],['performance-improvements']
Performance,"See the FAQ Style Guide. **Annotations**; - [ ] Do I need to define the types when using `annotatesamples table`?; - [ ] How does Hail annotate variants overlapping different intervals in an interval list?; - [ ] How do I input phenotype information into Hail?; - [ ] Is there a way to see all annotations present in the dataset?. **Expression Language**; - [ ] Can I use regular expressions in the Hail expression language?; - [ ] how can i filter samples based on whether or not they have a particular variant?. **Data Representation**; - [ ] How are insertion and deletion variants coded in the VDS?; - [ ] How are the boundaries for Pseudo-autosomal variants determined?. **Exporting Data**; - [ ] How can I export all global annotations to a file?; - [ ] How do I export my data so there are separate VCFs per chromosome?; - [ ] How do I export my annotations as a JSON file?; - [ ] How do I export updated call statistics (AC, AF) to the info field of the VCF?. **Developer Tools**; - [ ] Is there a style guide I should use for IntelliJ?. **Importing Data**; - [ ] How do I import data from a VCF file?; - [ ] How do I import annotations in JSON format?; - [ ] Is the UCSC file 0 or 1 based?. **Methods**; - [ ] Does Hail handle sex chromosomes differently in variantqc and sampleqc?; - [ ] How do I parse the variant annotations from VEP to find the worst functional consequence?; - [ ] How do I find all variants where the functional change on the canonical transcript results in a missense mutation?; - [ ] Is rHetHom calculated over indels+SNPs or just SNPs?; - [ ] Are sampleqc and variantqc calculated only on PASS variants?. **Optimize Pipeline**; - [ ] When should I write my data to a VDS file?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/812:1641,Optimiz,Optimize,1641,https://hail.is,https://github.com/hail-is/hail/issues/812,1,['Optimiz'],['Optimize']
Performance,"SelectGlobals(TensorIR, body: IR). In the body of TensorContract and TensorMap2, four refs are free: `l`, `r`, `i`, and `j`. In the body of TensorMap, three refs are free: `e`, `i`, `j`. In the body of TensorContract, all four refs are aggregables. In the TensorMap and TensorMap2, they are scalar values. No aggregations are allowed in the body of TensorSelectGlobals. It is just `SparkContext.broadcast`. ## From Python. C[[ u @ v ]] := TensorContract(; C[[ u ]],; C[[ v ]],; 1,; 0,; hl.agg.sum(l * r)). C[[ u + v ]] := TensorMap2(; C[[ u ]],; C[[ v ]],; l + r). C[[ u + 1 ]] := TensorMap(; C[[ u ]],; e + I32(1)). C[[ u + hl.ndarray(...) ]] := TensorMap(; TensorSelectGlobals(; C[[ u ]],; uuid1,; C[[ hl.ndarray(...) ]]); e + NDArrayIndex(GetField(""globals"", uuid1), i, j)). ## Transformations. This representation admits elegant transformations:. TensorMap2(TensorMap(u, x), v, body); <=>; TensorMap2(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). TensorMap(TensorMap(u, x), y); <=>; TensorMap(u, Let(uuid1, x, y[Ref(uuid1)/e])). TensorContraction(TensorMap(u, x), v, body); <=>; TensorContraction(u, v, Let(uuid1, x[l/e], body[Ref(uuid1)/l])). the above rule needs care wrt aggregations, namely the let must be pushed under the aggregation, but no further. All these rules need to be careful because we don't want to lose the ability to send something through BLAS. Perhaps these rules should be left entirely to the ""pipeline""-level (see: Arcturus' recent work) optimizer (after translation to tables of small tensors is complete, at which point BLAS operations are explicit). ## Compilation. At first, we pattern match the items that can be represented via BlockMatrix, err'ing on unrepresentable expressions. C2[[ TensorMap(u, ApplyUnaryPrimOp(Plus(), Ref(""e""), F64(n))) ]]; =; u.scalarAdd(n). C2[[ TensorMap2(u, v, ApplyBinaryPrimOp(Plus(), Ref(""l""), Ref(""r""))) ]]; =; u.add(v). C2[[ TensorContraction(u, v, (ApplyAggOp Sum () None ((ApplyBinaryPrimOp `*` (Ref l) (Ref r))))) ]]; =; u.dot(v)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5195:2316,optimiz,optimizer,2316,https://hail.is,https://github.com/hail-is/hail/issues/5195,1,['optimiz'],['optimizer']
Performance,Service tests have high latency to the JVM. Let's not separately call `hl.eval` so many times when we don't need to.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10448:24,latency,latency,24,https://hail.is,https://github.com/hail-is/hail/pull/10448,1,['latency'],['latency']
Performance,"Should be able to load file from export_blah types='/path/to/types' directly into TextTableConfig (or new hc.import_table) without having to explicitly load as string, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1743:18,load,load,18,https://hail.is,https://github.com/hail-is/hail/issues/1743,2,['load'],['load']
Performance,"Shuffles have wide dependencies, and it appears that Spark does not enforce that the Nth partition of a shuffled RDD is computed on the same node as the Nth partition of its parent. To keep RDDs `Ordered`, we sometimes need to shuffle, even though the typical key doesn't move at all. It would likely provide a sizable performance gain to enforce that partitions stay on the same node after the shuffle: this way, network traffic (often the rate-limiting step) is kept to a minimum. To do this, we need to optionally override the `getPreferredLocations` function of the ShuffledRDD created in `OrderedRDD.apply` to provide the preferred locations of the parent RDD. This flag should be used in `splitmulti`. **NB:** it's possible this won't actually help much, since if there are 3 preferred hosts for the parent, we may only have a 1/3 chance of landing on the same one.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/723:319,perform,performance,319,https://hail.is,https://github.com/hail-is/hail/issues/723,1,['perform'],['performance']
Performance,"Slowly getting rid of the regions being threading through non-allocating functions, part i + 1. This changes the method signature of CodeOrdering method signatures from f(region1, v1, region2, v2) to f(v1, v2). Some other function signatures (e.g. PInterval.loadStart) were also updated as necessary. No functionality has been changed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6700:258,load,loadStart,258,https://hail.is,https://github.com/hail-is/hail/pull/6700,1,['load'],['loadStart']
Performance,"Some context:. We have 3 kinds of IRs currently: ; - Value IRs (MakeStruct, I32, ApplyComparisonOp, etc); - TableIRs (TableRange, TableFilter, etc); - MatrixIRs (MatrixMapRows, MatrixExplodeCols, etc). One of the passes of the optimizer is to reformulate MatrixIRs in terms of TableIRs, and we're in the middle of this push to write lowerers for each MatrixIR node (which means writing an algorithm in LowerMatrixTable.scala, and removing the node's 'execute' method). Here I implement lowering MatrixAnnotateRowsTable as either TableIntervalJoin or TableLeftJoinRightDistinct (a future PR should split MatrixAnnotateRowsTable into 2 nodes like Table has, probably). The one bit of extra complexity in Python comes from implementing foreign-key joins explicitly (this was previously handled by the IR node itself).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075:227,optimiz,optimizer,227,https://hail.is,https://github.com/hail-is/hail/pull/5075,1,['optimiz'],['optimizer']
Performance,Some links:. http://findbugs.sourceforge.net/; https://docs.gradle.org/current/userguide/findbugs_plugin.html; https://github.com/sksamuel/scalac-scapegoat-plugin; https://stackoverflow.com/questions/22617713/whats-the-current-state-of-static-analysis-tools-for-scala; https://stackoverflow.com/questions/1598882/are-there-any-tools-for-performing-static-analysis-of-scala-code,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/81:337,perform,performing-static-analysis-of-scala-code,337,https://hail.is,https://github.com/hail-is/hail/issues/81,1,['perform'],['performing-static-analysis-of-scala-code']
Performance,"Some useful reading:; - [Phantom References in Java Reference Objects](http://www.kdgregory.com/index.php?page=java.refobj#PhantomReferences); - [java.lang.ref](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/package-summary.html#reachability); - [java.lang.ref.PhantomReference](https://docs.oracle.com/javase/8/docs/api/java/lang/ref/PhantomReference.html). The structure is now this:; - `RegionPool` becomes `RegionPoolNativeMemoryOwner`; - a new `RegionPool` class is simply a (unique) reference to a `RegionPoolNativeMemoryOwner`; - `RegionPoolNativeMemoryFreer` is a `PhantomReference` to `RegionPool`; - There is just one instance of all three classes in memory at any given time.; - A single thread is blocking on the reference queue waiting for something to free. How it works:; - The reachability of `RegionPool` defines the reachability the native memory.; - When the `RegionPool` becomes unreachable, the JVM places the corresponding `RegionPoolNativeMemoryFreer` on its reference queue.; - The `cleaner` thread removes the phantom reference from the queue; - At this point, the `RegionPool` is effectively gone, there are no references to it anywhere. Because of that fact, the `RegionPoolNativeMemoryFreer` can free all the native memory by calling `RegionPoolNativeMemoryOwner.free`.; - The `cleaner` thread removes the `RegionPoolNativeMemoryFreer` from the `refs` set which allows the `RegionPoolNativeMemoryFreer` itself to be GC'ed. We do leak the cleaner thread. It will live for the entirety of the JVM process, which seems fine since once you start Hail, you probably want to keep using Hail. I can't find a reference for this, but I'm fairly certain you need to prevent the PhantomReference itself from being GC'ed. That's why I have that set of refs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8203:744,queue,queue,744,https://hail.is,https://github.com/hail-is/hail/pull/8203,3,['queue'],['queue']
Performance,"Source); 2019-07-14 20:55:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(1, bw2-sw-dp3j.c.seqr-project.internal, 43693, None); 2019-07-14 20:55:04 BlockManagerMaster: INFO: Removed 1 successfully in removeExecutor; 2019-07-14 20:55:04 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 1 at RPC address 10.128.0.126:36052, but got no response. Marking as slave lost.; java.io.IOException: Failed to send RPC RPC 7115985797891097797 to /10.128.0.126:36044: java.nio.channels.ClosedChannelException; at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:357); at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:334); at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507); at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481); at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420); at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:122); at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:987); at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:869); at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1316); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738); at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730); at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:38); at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1081); at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1128); at io.netty.channel.AbstractChannelHandlerCo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635:2217,concurren,concurrent,2217,https://hail.is,https://github.com/hail-is/hail/issues/6635,1,['concurren'],['concurrent']
Performance,Special case UKB format in BGEN loader.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2491:32,load,loader,32,https://hail.is,https://github.com/hail-is/hail/pull/2491,1,['load'],['loader']
Performance,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2074:613,optimiz,optimization,613,https://hail.is,https://github.com/hail-is/hail/pull/2074,3,"['Load', 'optimiz']","['LoadVCF', 'optimization', 'optimizations']"
Performance,"Spinning up a VM takes around two minutes. Downloading fresh container images; takes additional time, maybe a whole minute. The cost of timing out is high: an otherwise passing PR test run may fail; demanding a bump and delaying merging of said PR by fifteen to twenty minutes. The cost of waiting two more minutes is that a resource deadlock may last; two extra minutes. We address deadlocks by scaling up and limiting concurrent; PR tests to four.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6878:420,concurren,concurrent,420,https://hail.is,https://github.com/hail-is/hail/pull/6878,1,['concurren'],['concurrent']
Performance,"Stacked on #12006 . This PR actually uses the new tables within the client application. The billing page should now load quickly. Note that the old aggregated billing tables are still in the database and being populated. The key thing to note is I switched how we are computing the cost. First, I aggregate the usage by resource before multiplying by the resource rate and summing. Therefore, the old and new numbers should be close, but not identical in the UI. The reviewer should double check that there are no references to `aggregated_*_resources$` tables (the ones that do not have the date interval) within the application code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11997:116,load,load,116,https://hail.is,https://github.com/hail-is/hail/pull/11997,1,['load'],['load']
Performance,Stacked on #12761. - This PR gets rid of the triggers needed for the migration as well as no longer needed columns; - There's some comments about the attempt resources `deduped_resource_id` column in the migration script. The goal is to swap the resource_id with the deduped_resource_id. This might be more complicated than just keeping both columns. I'm not sure. I also want to check the performance and blocking of the rename and generating the foreign key constraints.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12762:390,perform,performance,390,https://hail.is,https://github.com/hail-is/hail/pull/12762,1,['perform'],['performance']
Performance,"Stacked on: https://github.com/hail-is/hail/pull/7031. Changes:; - primary change was to add `Tokens.namespace_token_or_error` which prints a friendly error of the user doesn't have the necessary authentication; - added `hailctl auth list`, and made `hailctl dev config` with no options print out the current configuration; - implemented @danking's suggestion: change some natural entrypoints (BatchClient, get_userinfo, etc.) to take optional `deploy_config` argument and load the default config if not given",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7035:473,load,load,473,https://hail.is,https://github.com/hail-is/hail/pull/7035,1,['load'],['load']
Performance,Stacked on: https://github.com/hail-is/hail/pull/8179. This adds ModuleBuilder. A module is a collection of classes whose bytecode should be loaded together (like a function and some associated dependent functions). Dependent functions now add themselves to the module.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8186:141,load,loaded,141,https://hail.is,https://github.com/hail-is/hail/pull/8186,1,['load'],['loaded']
Performance,"Stacks on #5526. Once that commit goes in, the only changes will be to the /wait route, which will now issue a Kubernetes watcher, notebook-state.html to support JS state updates (and some minor changes to the organization of the notebook reporting UI). Relevant commit: ; 4f4e2b6f875e33da5787f665de1400f0f00a3623. This checks whether the route may be reached both for jobs that are Ready (at page refresh), and those that have just transitioned into ready state (as alerted by the websocket route). Next update will generalize the UI to N notebooks, to handle the case that internet-synchrony issues cause 2 non-deleted notebooks to be generated. This still needs a bit of work; when reach call fails due to 502, it continues issuing 502's, requiring a refresh. First attempts to cache bust, client side and nginx-side failed. cc @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5540:781,cache,cache,781,https://hail.is,https://github.com/hail-is/hail/pull/5540,1,['cache'],['cache']
Performance,Still to do:; - What should the index file look like for 0 keys? I think it should be 0 bytes and the reader needs to look at the nKeys in the metadata first.; - Incorporate into LoadBGEN,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4049:179,Load,LoadBGEN,179,https://hail.is,https://github.com/hail-is/hail/pull/4049,1,['Load'],['LoadBGEN']
Performance,"Struct's loadField has been changed to load the address directly in the; case of the field being a collection type. PCanonicalIntervalValue was; using loadStart and loadEnd rather than startOffset and endOffset which; caused addresses to be loaded twice in the case of intervals of; collections, leading to segfaults. This code is currently unused, but will be used in EmitCodeOrdering #8725",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9032:9,load,loadField,9,https://hail.is,https://github.com/hail-is/hail/pull/9032,5,['load'],"['load', 'loadEnd', 'loadField', 'loadStart', 'loaded']"
Performance,"Summary of changes:; - At the end of schedule, log total time and number of jobs scheduled.; - Only log database timing if total query took >20ms.; - Make sure context_manager is cleaned up in gear.Transaction.; - Limit workers to max 250 requests/s incoming to batch driver. I used an nginx limit to do this, but it is per pod, so I turned off autoscaling and increased CPU to roughly what I saw when 100K cores was hammering against a dead driver.; - Increase the worker exponential backoff from 30s to 2m. The main thing I was trying to address was the driver getting overloaded when trying to restart with a large standing cluster. It isn't totally clear why the cluster failed in the first place. I made a few other changes to mitigate the issue before adding the nginx limit, so I'm not 100% sure which combination of changes fixed the problem:. - I put a 60s timeout on the scheduler loop. This probably isn't necessary, although the scheduler does get bogged down if many of the instances it tries to schedule on are not responding. - I put a 10s timeout on mark_job_complete. - I put a maximum of 150 active mark_job_complete requests being processed, and returned service unavailable when the max was hit. I don't think this problem is completely solved. I think we want to keep the driver in the ~80% CPU load regime where everything is being processed quickly. I think we want to back off workers if, for example, mark_job_complete is taking more than 95%ile in the not overloaded case. I'm not sure who should do this, although it could be the batch-driver if internal-gateway is doing front-line throttling. Exiting in the overload case should be very cheap. We might want to prioritize mark_job_complete over the scheduler in that case, too. @danking I'd love to get some metrics for the scheduling loop: schedules/s, jobs/s, and time once this goes in. Should I switch to logging json to make that easier?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149:1316,load,load,1316,https://hail.is,https://github.com/hail-is/hail/pull/8149,1,['load'],['load']
Performance,"Summary of changes:; - Overhaul tmpdir handling. Remove most of the old code. Added local_tmpdir to `init`. tmpdir is the networked tmpdir. local_tmpdir is the tmpdir used for local files on both the driver and the executors. Added tmpdir and localTmpdir to ExecuteContext. ExecuteContext removes tmp files on close. Tmp file base is now required, try to give good base names. Tmp file names are now generated by being sufficiently random.; - Removed fs from HailContext. This involved threading ctx and fs through lots of code (most of the changes).; - Added ExecuteContext to EmitModuleBuilder and friends. This is necessary because EmitMethodBuilder gives generated code access to backend, fs, etc. which are carried by the ctx.; - Some IR (mostly readers, but also VEP, which needs to load the VEP configuration to determine its type) have overall parameters that control their behavior (e.g. the VCF reader path) but have to do IO to determine other state (like the matrix type, determined from the VCF header). This complicates pretty printing, serialization, and equality. I clarified this. In particular, I seperate the parameters (see, for example, MatrixVCFReaderParameters) which are specified on creation and used for serialization and equality from other derived state. IR no longer close over ctx or fs and they don't need to do IO after their intiial construction.; - MatrixSpec has subspecs for the marginal tables, and TableSpec has the global and rows RVD. These are now loaded on construction, so lowering no longer neesd to do IO.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8581:789,load,load,789,https://hail.is,https://github.com/hail-is/hail/pull/8581,2,['load'],"['load', 'loaded']"
Performance,"Summary of changes:; - add is.hail.lir, a low-level IR for emitting JVM bytecode; - lir handles local variable initialization. It uses dataflow analysis to compute which variables must be initialized. It will no longer be necessary to initialize locals to satisfy the JVM bytecode verifier.; - Modify Code[T] to use lir instead of asm directly. Code[T] can only be used once and this is now checked.; - Remove joinpoint and ParameterPack. This primarily involved making EmitStream use Labels instead of joinpoint, and specializing routines that required ParameterPack to work over EmitCode instead.; - Because Code[T] can only be used once, push Value[T], PValue and EmitValue throughout the code base. For example, the Emit environment is now an Emit[EmitValue]. This was mostly a lot of tedious changes: remove `.load()` in places, add calls to `memoize`, and change `Code[T] => Value[T]` in various places.; - EmitMetholdBuilder has newEmit{Local, Field} for creating places to store EmitCodes. I think there are two main issues to clean up before this goes in, or soon after:. This code doesn't try to optimize short-circuit boolean operations (||, &&, etc.) like the old code did, tho it seems the old code wasn't always working. Either way, this should get fixed. It is relatively easy to handle in `Code[T]`. I will fix this before the final version. I left jointpoint.Ctrl and have implicit conversions that freely convert between `Code[Unit]` and `Code[Ctrl]`. This is a bit tedious, but I guess `Code[T]` should support `Code[Nothing]` for type checking user code, although it will still treat it like a `Code[Unit]`. I will fix this later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8312:815,load,load,815,https://hail.is,https://github.com/hail-is/hail/pull/8312,2,"['load', 'optimiz']","['load', 'optimize']"
Performance,"Summary of changes:; - rip out method wrapping from Emit level; - Emit now uses locals everywhere instead of fields; - improved SimplifyControl; - Changed CodeRegion to call Memory directly, instead of calling Region methods. This saves a bytecode on native memory accesses.; - add lir.SplitMethod to break up methods. For large methods, this breaks the body of each basic block into one (or more) external functions and spills locals to fields. Splitting is controlled by SplitMethod.TargetMethodSize, currently set to 2000. PR'ing for testing. I have a few more improvements and then I will performance test. Here are the method sizes after splitting for the large `MakeStruct` example:. ```; is/hail/codegen/generated/C8; <init> 4; apply 235; apply 19; setPartitionIndex 11; addPartitionRegion 5; __wrapped16 30; __wrapped17 2003; __wrapped18 2008; __wrapped19 2006; __wrapped20 2008; __wrapped21 2006; __wrapped22 2008; __wrapped23 2006; __wrapped24 2008; __wrapped25 2006; ... you get the picture, remaining 100 methods elided ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8333:593,perform,performance,593,https://hail.is,https://github.com/hail-is/hail/pull/8333,1,['perform'],['performance']
Performance,"Summary: hailctl batch list --help; ```; usage: hailctl batch list [-h] [--query QUERY] [--limit LIMIT] [--all] [--before BEFORE] [--full] [--no-header] [-o O]. List batches. optional arguments:; -h, --help show this help message and exit; --query QUERY, -q QUERY; see docs at https://batch.hail.is/batches; --limit LIMIT, -l LIMIT; number of batches to return (default 50); --all, -a list all batches (overrides --limit); --before BEFORE start listing before supplied id; --full when output is tabular, print more information; --no-header do not print a table header; -o O specify output format (json, yaml, csv, tsv, or any tabulate format); ```. Details:; * Default listing to a limit of 50 records, once batch statuses are; cached from `list_batches`, this should result in 1 http request for the; default behavior of this tool.; * Teach --limit option to cap the number of records returned; * Teach --all to override --limit; * Teach --before to pass a last_batch_id query parameter to list_batches; * Teach --full to print all status information; * Teach --no-header to enable not printing a header for tabular output; * Teach -o {format} to change the output format the following are supported:; - json: always full json output, like hitting the list enpoint manually; - yaml: like json, but yaml!; - csv/tsv: simple comma/tab separated output for machine processing; - any python-tabulate output format, listed here:; https://github.com/astanin/python-tabulate#table-format. The only default that has been changed is the listing limit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9557:728,cache,cached,728,https://hail.is,https://github.com/hail-is/hail/pull/9557,1,['cache'],['cached']
Performance,"Support Zstdandard compression for hail input and output block buffers. Zstd is notable for having both very fast compression speed and adequate decompression speed, such that we expect to be network limited for decompression. Further tests may show that Zstd is more performant than LZ4, leading to a proper switch from one format to the other.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981:268,perform,performant,268,https://hail.is,https://github.com/hail-is/hail/pull/12981,1,['perform'],['performant']
Performance,"Surfaced because sometimes k8s secrets 404 for CI pipelines and we got FK constraint failures because there is no batch 0. No danger of bad data being written, just noise and unnecessary database load. Thank you foreign key checks!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14373:196,load,load,196,https://hail.is,https://github.com/hail-is/hail/pull/14373,1,['load'],['load']
Performance,"Switch apiserver from flask to aiohttp. Mostly boilerplate, except calling into the JVM is blocking. Therefore, I execute JVM calls via a concurrent ThreadPoolExecutor with (a somewhat randomly selected) 16 threads. py4j is thread safe and executes each request on the server (Java) side in a separate thread: https://github.com/bartdag/py4j/blob/master/py4j-python/src/py4j/java_gateway.py#L898",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624:138,concurren,concurrent,138,https://hail.is,https://github.com/hail-is/hail/pull/5624,1,['concurren'],['concurrent']
Performance,"TEBOOK-1041707) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Information Exposure <br/>[SNYK-PYTHON-NOTEBOOK-2441824](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2441824) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.png ""medium severity"") | **449/1000** <br/> **Why?** Has a fix available, CVSS 4.7 | Access Restriction Bypass <br/>[SNYK-PYTHON-NOTEBOOK-2928995](https://snyk.io/vuln/SNYK-PYTHON-NOTEBOOK-2928995) | `notebook:` <br> `5.7.16 -> 6.4.12` <br> | No | No Known Exploit ; ![low severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/l.png ""low severity"") | **399/1000** <br/> **Why?** Has a fix available, CVSS 3.7 | Race Condition <br/>[SNYK-PYTHON-PROMPTTOOLKIT-6141120](https://snyk.io/vuln/SNYK-PYTHON-PROMPTTOOLKIT-6141120) | `prompt-toolkit:` <br> `1.0.18 -> 3.0.13` <br> | No | No Known Exploit ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **696/1000** <br/> **Why?** Proof of Concept exploit, Has a fix available, CVSS 7.5 | Regular Expression Denial of Service (ReDoS) <br/>[SNYK-PYTHON-PYGMENTS-1086606](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1086606) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | Proof of Concept ; ![high severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/h.png ""high severity"") | **589/1000** <br/> **Why?** Has a fix available, CVSS 7.5 | Denial of Service (DoS) <br/>[SNYK-PYTHON-PYGMENTS-1088505](https://snyk.io/vuln/SNYK-PYTHON-PYGMENTS-1088505) | `pygments:` <br> `2.5.2 -> 2.15.0` <br> | No | No Known Exploit ; ![medium severity](https://res.cloudinary.com/snyk/image/upload/w_20,h_20/v1561977819/icon/m.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14205:5451,Race Condition,Race Condition,5451,https://hail.is,https://github.com/hail-is/hail/pull/14205,1,['Race Condition'],['Race Condition']
Performance,"TODO. Goal is all non-stretch items done by late tomorrow night/early Friday morning. Friday - Sunday testing, Cotton takes a closer look on Monday. - [x] No SQL; store user / svc / token labels (all things that need to be validated before redirect); - [x] Websockets; - [x] Service, pod definitions, makefile updates => notebook-v2 service name; - [x] Deploy notebook service, Deploy web service ( say web service name, mapping to web.hail.is ); - [x] Direct modification of gateway: check site service for breaks after each change to prevent user ; - [x] Test in cluster; - [x] Make sure Notebook v1 still works; - [ ] Stretch, and only in v3 so Feb 5 entropy minimized: asynchttp + uvloop; - [ ] Stretch ?: route by pod ip instead of svc name: DNS propagation latency significantly longer than pod instantiation time, which sucks for users, both because notebook instances will look broken when they're not, and because if we mask that the apparent latency to first useful operation is multiples of that needed. new: ; Cotton is right, mysql is adding too much complexity for the minimal use case, esp. with gevent conflicting with PyMySQL, necessitating per route handler connection. old:; Not ready to be merged, would like to improve SQL connection handling. 6a4599df5dfe0affdb5e367dd9cdc70cca59fd17 onward dependent on this. MySQL use is unoptimized because PyMySQL doesn't play well with gevent in the following way: initial impression from reading was that monkey.patch_all() before creation of global connection should result in connection spawned for each new request, or to at least private to a greenlet. Doesn't appear to be the case, plenty of connection errors. So establishing connection within each request, which is slow. . Python C library also out, because it does not play well with Python threading/greenlet/monkey patch implementations. MySQL Connector is an option, provides thread pools, but is also slowest option, by up to 10x, for small requests, like our are likely to be",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215:763,latency,latency,763,https://hail.is,https://github.com/hail-is/hail/pull/5215,2,['latency'],['latency']
Performance,"TableIRSuite extensively uses the function; ```; def collect(tir: TableIR): TableCollect = TableCollect(TableKeyBy(tir, FastIndexedSeq())); ```; to compare the result of a `TableIR` with the expected collection. But `TableCollect` makes no promises what order the results will be in, and in particular the optimizer is allowed to remove that `TableKeyBy`. This PR redefines that function to use the collect aggregator, which does promise the order rows are collected. It also fixes a small type error in the `TableJoin` lowering case that was uncovered.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9054:306,optimiz,optimizer,306,https://hail.is,https://github.com/hail-is/hail/pull/9054,1,['optimiz'],['optimizer']
Performance,TableImport should refuse to load non-splittable .gz,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/973:29,load,load,29,https://hail.is,https://github.com/hail-is/hail/issues/973,1,['load'],['load']
Performance,Tested in a dev deploy'd load test that # of add_attempt_resources queries == # of jobs instead of double as you can currently observe in default.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12461:25,load,load,25,https://hail.is,https://github.com/hail-is/hail/pull/12461,1,['load'],['load']
Performance,"TextTableReader.read takes (optional) nPartitions.; importannotations: new option -n/--npartitions.; Added hintPartition to OrderedRDD coerce and friends.; When loading variant annotations, load with as many partitions as the RDD we're going to join with, and hint with its partitioner.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824:161,load,loading,161,https://hail.is,https://github.com/hail-is/hail/pull/824,2,['load'],"['load', 'loading']"
Performance,"The Encoder/Decoder methods weren't handling non-struct values correctly (this is mostly fine from an Encoder/Decoder perspective, since they don't handle non-struct/array values). I fixed this in #6727 because I wanted to encode arbitrary values, and changed EmitPackDecoder/EmitPackEncoder to handle arbitrary values so that I could test this. I also pulled out some more peripheral changes from that PR, mostly defining some (currently unused, untested) methods like `Code.orEmpty` and a version of `newMethod` that lets you give the method a real name for ease of debugging, as well as some more changes to move more load/store methods off of region instances and onto the Region object.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6759:621,load,load,621,https://hail.is,https://github.com/hail-is/hail/pull/6759,1,['load'],['load']
Performance,"The `BlockingInputBuffer` allocates a somewhat large array of; bytes each time it is allocated. As such, it is important to avoid; allocating a `BlockingInputBuffer` for each row if each row is; significantly smaller than the buffer size. This change removes problematic methods from `RegionValue`, `RVD`,; and `CodecSpec` that have poor performance. In every case, a small; code change enables one allocation per-partition. This required the; implementation of `RestartableByteArrayInputStream` which is a thread-; unsafe version of `ByteArrayInputStream` that, crucially, can; be restarted with a new `Array[Byte]`. ---. I rebased this off of my shuffler branch. With this change on the shuffler branch (which otherwise didn't change Spark shuffles), I saw these benchmark results:; ```; # hailctl dev benchmark compare more-allocs.json fewer-allocs.json; Name Ratio Time 1 Time 2; ---- ----- ------ ------; shuffle__key_rows_by_mt 105.2% 25.528 26.860; shuffle__key_rows_by_4096_byte_rows 102.7% 1.052 1.081; shuffle__key_rows_by_65k_byte_rows 102.7% 19.311 19.832; shuffle__order_by_10m_int 47.0% 93.554 44.011; ----------------------; Geometric mean: 85.0%; Simple mean: 89.4%; Median: 102.7%; ```. The first benchmark is dominated by LZ4 calls in Kryo. The second and third benchmarks are dominated by the construction of the MT. I suspect this is due to unnecessary data copying (when Hail constructs an array of structs it creates the structs out of line and copies them into place).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7108:338,perform,performance,338,https://hail.is,https://github.com/hail-is/hail/pull/7108,1,['perform'],['performance']
Performance,The `Consume` was getting optimized away in (StreamLen(StreamMap ...)),MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9612:26,optimiz,optimized,26,https://hail.is,https://github.com/hail-is/hail/pull/9612,1,['optimiz'],['optimized']
Performance,"The `UserData` dict is loaded from the `auth` database and passed around to different services, but mostly to look at the current user's username or some other metadata. Because it's in so many places I'm a little worried about it getting logged, which we can't do because it contains the user's `session_id`. But `userdata['session_id']` is used in so few places that I removed `session_id` from the dict and retrieve it explicitly where it's needed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13618:23,load,loaded,23,https://hail.is,https://github.com/hail-is/hail/pull/13618,1,['load'],['loaded']
Performance,"The capacity on the cache is pretty arbitrary, but given that bunches are going to get churned through very quickly and then never used again, it seemed nice to have the assertion that every layer of the cache is always small and shouldn't be an issue to search through in a blocking manner. I tested this with a dev-deployed load-test and observed the number of `get_token_start_id` queries drop from O(jobs) to ~4 per second at max throughput. No difference in profiling, this is just an attempt to reduce the number of queries we're hitting the database with.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12023:20,cache,cache,20,https://hail.is,https://github.com/hail-is/hail/pull/12023,4,"['cache', 'load', 'throughput']","['cache', 'load-test', 'throughput']"
Performance,"The changelog for [6.0.0](https://github.com/johnrengelman/shadow/releases/tag/6.0.0) claims performance improvements. In practice,; I save maybe a few second on the `shadowJar` step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10263:93,perform,performance,93,https://hail.is,https://github.com/hail-is/hail/pull/10263,1,['perform'],['performance']
Performance,"The code as written doesn't seem to allow this to happen. Did someone else bind to that port? It looks like it can happen if an unhandled exception occurs during docker stop or delete, in which case we free the port even though the container might still have the port open. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 354, in run; start_container, self.container); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 94, in wrapper; return await asyncio.wait_for(f(*args, **kwargs), timeout); File ""/usr/local/lib/python3.6/asyncio/tasks.py"", line 358, in wait_for; return fut.result(); File ""/usr/local/lib/python3.6/site-packages/batch/worker.py"", line 142, in start_container; return await container.start(); File ""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py"", line 170, in start; data=kwargs,; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 291, in __aenter__; resp = await self._coro; File ""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py"", line 206, in _do_query; raise DockerError(response.status, json.loads(what.decode(""utf8""))); aiodocker.exceptions.DockerError: DockerError(500, 'driver failed programming external connectivity on endpoint batch-20376-job-59-main (8a971634c54c03a1e7df1b4255814137c92e10d310b3d47a1fe6cb7432222ed0): Error starting userland proxy: listen tcp 0.0.0.0:46572: bind: address already in use'); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8411:1146,load,loads,1146,https://hail.is,https://github.com/hail-is/hail/issues/8411,1,['load'],['loads']
Performance,"The current `_variants_per_file` interface isn't usable by mortals. We should have something like `hl.import_bgen('/path/to/bgen', ..., variants=ht.key)` where `ht` is a table with key locus and alleles. This can use the new index file format to get the set of variants to load. Depends on: https://github.com/hail-is/hail/issues/4018",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4019:273,load,load,273,https://hail.is,https://github.com/hail-is/hail/issues/4019,1,['load'],['load']
Performance,"The current implementation of `Container.delete`,; ```python; async def delete(self):; log.info(f'deleting {self}'); self.deleted_event.set(); await self.delete_container(); ```. provokes a race between the `run` task and the `delete` task. The former sees the `deleted_event`, raises and jumps to `delete_container`, so both tasks might be trying to delete the container at the same time. This races here,; ```python; if self.container_is_running():; try:; log.info(f'{self} container is still running, killing crun process'); self.process.terminate(); self.process = None; await check_exec_output('crun', 'kill', '--all', self.container_name, 'SIGTERM'); except asyncio.CancelledError:; raise; except Exception:; log.exception('while deleting container', exc_info=True). ```; where we might queue two `crun kill` calls, the second of which fails because it cannot find any such container. Calling `delete_container` from within the `delete` method is a remnant from an older implementation of deletion, before we used `deleted_event` to explicitly signal to the `run` task that it's time to wrap things up. This is no longer necessary. The simplified way to think about deletion now is:. - Calling `Container.delete` just sets an `asyncio.Event` that the container has been deleted.; - Anything in the `run` task of the container that can be interrupted by a deletion waits on that event and short circuits the run process if it is set.; - The `run` task is the only task that calls `delete_container`, and always calls it when it is done.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10714:793,queue,queue,793,https://hail.is,https://github.com/hail-is/hail/pull/10714,1,['queue'],['queue']
Performance,"The current implementation of piping into the worker's process and calling `readline` fails on log lines that are greater than 64KiB. This directs the container's stdout/stderr to a log file in their XFS project, so a job cannot blow up the worker with excessive logging. It then just calls `read` on the entire log file when requested, so there still is some issue of loading the log into memory, but no more so than currently exists. The added test currently fails in default.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10683:369,load,loading,369,https://hail.is,https://github.com/hail-is/hail/pull/10683,1,['load'],['loading']
Performance,"The current main version of the Query Service uses a fresh class loader for every query. This means each driver job and worker job starts with uncompiled classes for any class in Hail. This change uses a shared class loader for all jobs with the same SHA. This enables use of previously JIT'ed Hail classes. This noticeably improves no-op performance from ~8 seconds to ~3 seconds. Most of that remaining 3 seconds is due to Query-on-Batch and Batch, not Query. Currently, Hail generates classes using a counter. When a driver or worker re-uses an old class loader, it would mistakenly re-use classes generated by a previous Hail Query-on-Batch job because they share the same name. This PR avoids that entirely by using a fresh class loader per job for *generated* classes. This PR parameterizes the entire Hail Query system by a class loader. This class loader is passed in from the initiator of the driver or worker job. We could, eventually, re-use class loaders:; - across jobs for a single batch; - across jobs for a single user; - across jobs for a single billing project; - across all jobs. I think the first three are somewhat uncontroversial but we need to fix the class naming problem. The fourth introduces a new security risk. I think we have a lot of performance to squeeze out of QoB before we need to take that step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11212:65,load,loader,65,https://hail.is,https://github.com/hail-is/hail/pull/11212,9,"['load', 'perform']","['loader', 'loaders', 'performance']"
Performance,The docker file describes a sufficient environment to build and test hail 0.1. The Makefile wraps up Docker image production. The `hail-docs-trampoline.sh` delays the `git rev-parse` until the docs are actually built which allows `gradle downloadDependencies` to run without the `.git` folder present which allows me to cache some of the gradle dependencies once rather than per-build. `hail-ci-build-image` contains the name of a docker image in which to build and test hail 0.1. `hail-ci-build.sh` describes how to build and test hail 0.1 and populates the `artifacts` directory with the results and an index file.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4123:320,cache,cache,320,https://hail.is,https://github.com/hail-is/hail/pull/4123,1,['cache'],['cache']
Performance,"The errors look like this:; ```; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1074"", ""message"": ""update job (278, 6858, 'main') with pod batch-278-job-6858-5879db""}; {""levelname"": ""INFO"", ""asctime"": ""2019-07-02 13:36:45,504"", ""filename"": ""batch.py"", ""funcNameAndLine"": ""update_job_with_pod:1087"", ""message"": ""job (278, 6858, 'main') mark complete""}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 65, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6545:695,concurren,concurrent,695,https://hail.is,https://github.com/hail-is/hail/issues/6545,1,['concurren'],['concurrent']
Performance,"The fix for Notebook is on line 242. Copying from my Zulip post:. > I made a mistake when I implemented TLS.; >; > In the following code snippet we use ssl_client_session which should probably be; > called in_cluster_ssl_client_session. It's supposed to be used to communicate; > with other services in the cluster. That needs to be changed back to; > aiohttp.ClientSession which loads the normal system certificates (including the; > VeriSign root certs that signed the public certs that gateway uses, different; > from the internal certs that our services use).; >; > In particular, note that the error says ""unable to get local issuer; > certificate."" That means that the local trust store lacks a certificate that; > trusts the remote server's certificate. In Dania's case, the default python on; > OS X lacks all certificates, so every remote server is untrusted. In notebook's; > case, ssl_client_session creates an SSL/TLS session that only trusts Hail; > internal services (in particular, it does not trust the certificates that; > gateway uses for incoming public traffic). The error also says that the server; > in question is workshop.hail.is which is a public domain (note the hail.is), so; > that traffic is going through the public gateway with its public certificates.; >; > ```; > # don't have dev credentials to connect through internal.hail.is; > ready_url = deploy_config.external_url(; > service,; > f'/instance/{notebook[""notebook_token""]}/?token={notebook[""jupyter_token""]}'); > try:; > async with ssl_client_session(; > timeout=aiohttp.ClientTimeout(total=1),; > headers=headers,; > cookies=cookies) as session:; > async with session.get(ready_url) as resp:; > ```. I also changed the names and functionality of the functions in tls. Now; `in_cluster_ssl_context` will error if there is no ssl configuration found; instead of silently (and confusingly) using an SSLContext suited for public; communication (and wrong for in-cluster communication). I added `get_context_specific_",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9120:380,load,loads,380,https://hail.is,https://github.com/hail-is/hail/pull/9120,1,['load'],['loads']
Performance,"The fix is somewhat subtle and relies on PruneDeadFields, which is called (a) by the optimizer and (b) by the compiler, so this is safe. The important piece is that the process of upcasting strips requiredness. This isn't a great design, but I'm comfortable with it for now since I hope physical types will solve this in The Right Way™ before 0.2 release anyway. fixes #4134",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4265:85,optimiz,optimizer,85,https://hail.is,https://github.com/hail-is/hail/pull/4265,1,['optimiz'],['optimizer']
Performance,"The fourth in a series of PCRelate Improvements. Sprinkling `cache` on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 36 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""phi"").count(); end = timer(); print(end-start); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2252:61,cache,cache,61,https://hail.is,https://github.com/hail-is/hail/pull/2252,1,['cache'],['cache']
Performance,"The high-level problem: . The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM includes assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding of worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. One test (which was added since the service tests were removed) had to be marked as failing. Some; Hail operations rely on writing to the local file system. Implementing that properly in the Query; Worker will take some thought. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10314:213,load,load,213,https://hail.is,https://github.com/hail-is/hail/pull/10314,4,"['cache', 'load', 'multi-thread']","['cache', 'load', 'multi-threaded']"
Performance,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. A fresh class loader for; each Hail version allows the classes to co-exist in the same JVM. We cache jars on the local; filesystem. ---. `javac` compiles Java files to JVM Bytecode. JVM Bytecode is normally stored in `class` files. A JAR; file is, essentially, a TAR file of a directory of class files. `java` needs to find the `class` file that defines any Class. A `ClassLoader` defines:. 1. (`findClass`) How to *find* the definition of a Class known to the current `ClassLoader`. 2. (`findResource`) How to *find* an arbitrary file known to the current `ClassLoader`. 3. (`loadClass` and `getResource`) The order in which to find a class in a set of; `ClassLoader`s (e.g. if two `ClassLoader`s know about the same Class, which one should load the; class?). Every `ClassLoader` has a `parent` `ClassLoader`. The default implementation of `loadClass` and; `getResource` prefers loading classes from its parent ClassLoader before anything else. We invert; the loading order to allow multiple definitions of the same Class in the same JVM. In particular,; each instance of `LoadSelfFirstURLClassLoader` prefers to use its own definition of a Class. Each; `LoadSelfFirstURLClassLoader` instance knows about one version of the Hail JAR. The remaining subtle issue is how to load resources. For example, `HailBuildInfo` needs to load the; build info resource file. To do so, you need an instance of a `ClassLoader` that can find the; file you want. Often times, you use `this.getClass().getClassLoader()`, which is the class loader; used to load the current class. Hail does not do this. I believe we do not do this because of issues; with how TestNG loads classes. :sigh: As a result, I also modify the worker Thread's; ContextClassLoader for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10279:212,load,load,212,https://hail.is,https://github.com/hail-is/hail/pull/10279,5,"['cache', 'load']","['cache', 'load', 'loadClass', 'loader']"
Performance,"The high-level problem:. The Hail Query Service is redeployed with each commit to `main`. Each deployment has a new JAR file; whose ABI is backwards-incompatible. The high-level solution:. Hail Batch Workers can load the JAR for a given Hail version on-demand. Although not a long-term; solution, we currently start a fresh JVM for each job. As a result, we can simply start the JVM with; the correct JAR on its classpath. We cache jars on the local filesystem. I had to abandon the old approach for two reasons:. 1. Multiple JVMs race to download the JAR. In the new approach, the python worker process uses a; lock to ensure at most one coroutine is downloading a given version of a JAR at the same time. 2. The JVM assumes that a child ClassLoader does not redefine a class from the parent; ClassLoader. That's why ClassLoaders always prefer to load a class from the parent ClassLoader's; classes. When we decide to re-use JVMs or use a single multi-threaded JVM, we'll need to ensure the top-level; ClassLoader *does not have Hail on its classpath*. I looked briefly at this approach and found it; more work than the current approach. ---. My apologies for eliminating JVMProcess in this PR. It's an unrelated change which facilitated my; understanding worker.py. I essentially inlined JVMProcess into JVMJob and eliminated any duplicative; code. ---. After making this change I restored the tests. Some tests had bitrotted. In the process of fixing; those tests, I found a few other bugs. Fixing these lower-level bugs unlocked a number of new; tests. A couple tests (which were added since the service tests were removed) had to be marked as; failing. Here are the bugs I fixed:. 1. Correct the error message raised when tests are run in a non-main thread (we look for this; message and start an event loop for Hail's async code because asyncio refuses to start an event; loop in a non-main thread). 2. Use a `SafeRow` to copy the globals data out of a Region and into durable, GC'ed objects. 3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390:212,load,load,212,https://hail.is,https://github.com/hail-is/hail/pull/10390,4,"['cache', 'load', 'multi-thread']","['cache', 'load', 'multi-threaded']"
Performance,"The main change is to the communication protocol between the client and; the driver and between the driver and the worker. In main, both the driver and the client send messages back to the client; and driver (respectively) by writing to a file in cloud storage. In both; cases, the file (in main) has one of these two structures:. 0x00 # is_success (False); UTF-8 encoded string # the stack trace. 0x01 # is_success (True); UTF-8 encoded string # JSON message to send back to the client or driver. In this PR, the success case does not change. The failure case becomes:. 0x00 # is_sucess (False); UTF-8 encoded string # short message; UTF-8 encoded string # expanded message; 4-byte signed integer # error id. The Python client (in `service_backend.py`) and the Driver (in; `ServiceBackendSocketAPI2`) changes to read this and raise the right error if; an error id is present. I also uncovered three unrelated problems that are fixed in this PR:; 1. PlinkVariant needs to be serializable because it is broadcasted.; 2. We open an input stream in LoadPlink which ought to be closed, but there is no mechanism to do so in the ServiceBackend. I just ignore it for now. cc: @tpoterba, I'm not sure what the right answer is here.; 3. Two uses of the broadcasted file system that should use the ExecuteContext's file system.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11624:1046,Load,LoadPlink,1046,https://hail.is,https://github.com/hail-is/hail/pull/11624,1,['Load'],['LoadPlink']
Performance,"The master node should not need to store every partition's aggregator in memory. It can join them as soon as they arrive, blocking further IO until it is prepared to receive more aggregators. Is this possible in the Spark model? Are the iterators of an RDD lazily reading from the network or do they load all the data into memory before calling `RDD.compute`?. This is relevant to running `hl.sample_qc` on 100k whole genomes, split across 10,000 partitions (1 billion aggregators).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4526:300,load,load,300,https://hail.is,https://github.com/hail-is/hail/issues/4526,1,['load'],['load']
Performance,The match on method identity caused mismatches between the variable; stored in loaded across splits.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9947:79,load,loaded,79,https://hail.is,https://github.com/hail-is/hail/pull/9947,1,['load'],['loaded']
Performance,"The need to supply the region when reading objects via a pointer irked me. It turns out the only reason we do this is to determine whether or not we need to deep copy objects when copying to a region that may or may not be the same as the old region. It seemed to me perfectly reasonable to ask a region if it allocated an object rather than carry around an extra reference so I drafted this change, dependent on such a change not impacting performance too dramatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13881:441,perform,performance,441,https://hail.is,https://github.com/hail-is/hail/pull/13881,1,['perform'],['performance']
Performance,"The new generic lines coerce code could produce a partitioner with unsafe values. Those unsafe values ended up in the Compile cache, which become invalid when owning region was cleared. This fixes the memory errors I was seeing when running with the local backend. It is possible it will fix (some?) of the errors you were investigating, @johnc1231.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8987:126,cache,cache,126,https://hail.is,https://github.com/hail-is/hail/pull/8987,1,['cache'],['cache']
Performance,The old query service would cache user information. The new query service just; constructs a new FS for every query (its not too expensive). None of this code; is used currently.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11651:28,cache,cache,28,https://hail.is,https://github.com/hail-is/hail/pull/11651,1,['cache'],['cache']
Performance,"The original problem we were seeing was this error message:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. I'm not sure why we didn't get a better error message from the ThreadPoolExecutor, but this fix definitely solves the OOM issue. We were assuming `close` in Python means the entire write has completed, but that's not true. `close` just means the file handle in Python has been closed, but the data is still stored in the kernel's write buffer until it gets a chance to have written the data. When the pd-ssds were slow (either network bandwidth or ext3/4 is slow) and we're trying to write a lot of data, this meant we were building up lots of data in the write buffer for previously ""completed"" tasks and the semaphore was not doing its job to limit the number of ""active"" tasks to 50. There may still be another error here that we're not retrying, but I wasn't able to replicate the bug after making this fix with 5 replicas of downloading 80 Gi x 8 jobs per node.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10591:722,concurren,concurrent,722,https://hail.is,https://github.com/hail-is/hail/pull/10591,1,['concurren'],['concurrent']
Performance,"The real issue here is that `mt2`'s `af` field is not from the same object as `mt`, but the error message is really misleading, it moves your focus to the `mt.GT.n_alt_alleles()` which is actually fine. ```; In [13]: import hail as hl ; ...: mt = hl.balding_nichols_model(2, 5, 5) ; ...: mt2 = hl.balding_nichols_model(2, 5, 5) ; ...: mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af) ; Initializing Hail with default parameters...; 2020-07-28 10:40:36 WARN Utils:66 - Your hostname, wm06b-953 resolves to a loopback address: 127.0.0.1; using 192.168.0.54 instead (on interface en0); 2020-07-28 10:40:36 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address; 2020-07-28 10:40:37 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 2020-07-28 10:40:37 WARN Hail:37 - This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.; Compatibility is not guaranteed.; Running on Apache Spark version 2.4.1; SparkUI available at http://192.168.0.54:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.49-c6975678edc4; LOGGING: writing to /Users/dking/projects/hail/hail/hail-20200728-1040-0.2.49-c6975678edc4.log; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; 2020-07-28 10:40:39 Hail: INFO: balding_nichols_model: generating genotypes for 2 populations, 5 samples, and 5 variants...; Traceback (most recent call last):; File ""<ipython-input-13-f638f6c0399a>"", line 4, in <module>; mt = mt.annotate_entries(x = mt.GT.n_alt_alleles() * mt2.af); File ""/Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 1988, in __mul__; return self._bin_op_numeric(""*"", other); File ""/Users/dking/projects/hail/hail/py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9163:747,load,load,747,https://hail.is,https://github.com/hail-is/hail/issues/9163,1,['load'],['load']
Performance,"The separation makes that metric more accurate. Moreover, I check the queue before rescheduling. This saves a database call, a kubernetes call, and set churn (we remove then add back the job).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6799:70,queue,queue,70,https://hail.is,https://github.com/hail-is/hail/pull/6799,1,['queue'],['queue']
Performance,"The serialization changed briefly, still load VDSes written with that verison.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/729:41,load,load,41,https://hail.is,https://github.com/hail-is/hail/pull/729,1,['load'],['load']
Performance,The tests relying on Batch are getting slower because it takes a long time to download and build Docker images and we're putting more load on Batch. This will increase parallelism and reduce test failures due to timeouts.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9441:134,load,load,134,https://hail.is,https://github.com/hail-is/hail/pull/9441,1,['load'],['load']
Performance,"The third in a series of PCRelate Improvements. Depends on #2249 . Sprinkling `cache` on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 40 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2253:79,cache,cache,79,https://hail.is,https://github.com/hail-is/hail/pull/2253,1,['cache'],['cache']
Performance,"The third in a series of PCRelate Improvements. Depends on #2249. Sprinkling cache on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 80 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```. Ready for a final look after #2270 lands. Creating a PR so that @konradjk and others can take it for a spin if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2280:77,cache,cache,77,https://hail.is,https://github.com/hail-is/hail/pull/2280,1,['cache'],['cache']
Performance,The wheel container prevents this image from ever being cached.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8623:56,cache,cached,56,https://hail.is,https://github.com/hail-is/hail/pull/8623,1,['cache'],['cached']
Performance,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3861:755,perform,performance,755,https://hail.is,https://github.com/hail-is/hail/issues/3861,2,"['load', 'perform']","['loaders', 'performance']"
Performance,"There are two problems with this:; - it is a massive de-optimization if we forward an expensive computation into a loop (e.g. arraymap); - in the above case, it is an error in if the body of the let is non-deterministic. To do this right we need to build a control flow graph.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4497:56,optimiz,optimization,56,https://hail.is,https://github.com/hail-is/hail/pull/4497,1,['optimiz'],['optimization']
Performance,"There is no hail operation that removes keys and guarantees no change to order (see https://github.com/hail-is/hail/issues/6929). In `BaseExpression._to_table`, we use the following expression which works in practice but is not guaranteed to work.; ```python; entries = ds.key_cols_by().entries(); entries = entries.order_by(*ds.row_key); return ..., entries.select(name); ```. In particular:; - `order_by(*ds.row_key)` does not guarantee that entries from the same row are appear in the order of the corresponding columns; - `order_by(*ds.row_key)` does not guarantee that entries from a pair of rows with equivalent keys appear in the order of the corresponding rows. There exists an operation that guarantees this:; ```python; ridx = Env.get_uid(); cidx = Env.get_uid(); entries = ds.key_cols_by().add_row_index(ridx).add_col_index(cidx).entries(); entries = entries.order_by(ridx, cidx); return ..., entries.select(name); ```; But this requires a scan to produce the row index and then a scan to verify the table rows are in the correct order. We consider this unacceptable performance. Instead, we rely on the above expression and add the following tests which we hope will alert us if we accidentally change the behavior:. ```; rmt33 = hl.utils.range_matrix_table(3, 3, n_partitions=2). mt = rmt33.choose_cols([1, 2, 0]); assert mt.col.collect() == [hl.Struct(col_idx=x) for x in [1, 2, 0]]. mt = rmt33.key_rows_by(rowkey=-mt.row_idx); assert mt.row.collect() == [hl.Struct(row_idx=x) for x in [2, 1, 0]]. mt = rmt33.annotate_entries(; x=(rmt33.row_idx + 1) * (rmt33.col_idx + 1)); mt = mt.key_rows_by(rowkey=-mt.row_idx); mt = mt.choose_cols([2, 1, 0]); assert mt.x.collect() == [9, 6, 3, 6, 4, 2, 3, 2, 1]. t = hl.utils.range_table(3); t = t.key_by(-t.idx); assert t.idx.collect() == [2, 1, 0]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6930:1078,perform,performance,1078,https://hail.is,https://github.com/hail-is/hail/issues/6930,1,['perform'],['performance']
Performance,There was a race condition where `crun run` could have been cancelled before the container was actually created. This change fixes this problem by checking whether the container exists before trying to delete it.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10855:12,race condition,race condition,12,https://hail.is,https://github.com/hail-is/hail/pull/10855,1,['race condition'],['race condition']
Performance,These are the exact same error:; ```; In [4]: import asyncio; ...: import concurrent; ...: asyncio.TimeoutError == concurrent.futures.TimeoutError; Out[4]: True; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10946:74,concurren,concurrent,74,https://hail.is,https://github.com/hail-is/hail/pull/10946,2,['concurren'],['concurrent']
Performance,"These queries are responsible for getting the paginated list of jobs in the batch UI. I believe this missing order by went unnoticed because the queries were naturally getting returned in order based on the jobs primary key, but we've recently seen a couple page loads of jumbled or missing jobs. Seems like something around the query stats changed and the database adjusted its plan slightly. Regardless, we want this order by explicit.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14585:263,load,loads,263,https://hail.is,https://github.com/hail-is/hail/pull/14585,1,['load'],['loads']
Performance,These tests were timing out consistently now. I think 3 minutes is too strict given what we know about the performance of our current system.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13320:107,perform,performance,107,https://hail.is,https://github.com/hail-is/hail/pull/13320,1,['perform'],['performance']
Performance,"These two tests take a very long time in the service and do not benefit from the; massive horizontal scalability of Hail Query on Hail Batch. These tests also; somewhat frequently cause OOMs in the service backend, so the point is perhaps; moot. Nonetheless, I think this is an overall positive change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10943:101,scalab,scalability,101,https://hail.is,https://github.com/hail-is/hail/pull/10943,1,['scalab'],['scalability']
Performance,"This PR Python-izes all but the LocalLDPrune step of `ld_prune` while adding modular functionality for window indices that also simplifies computing big banded correlation (where the band is in terms of position, centimorgans, or any other non-decreasing value per contig). I later plan to simplify computing correlation, but basically:; ```; ld_matrix = ... BlockMatrix stuff starting from mt ...; starts, stops = hl.locus_windows(mt.rows(), radius=1e6); banded_ld_matrix = ld_matrix.sparsify_row_intervals(starts, stops); ```; Or if centimorgan coordinates are a row field (via annotation or import from plink format):; ```; ht = mt.rows(); starts, stops = hl.locus_windows(ht, radius=1e6, value_expr=ht.centimorgans); ```. Changes:; - added `array_windows` to methods/misc, with docs and test.; - added `locus_windows` to methods/genetics, with docs and test.; - reworked `ld_prune` to use `locus_windows` and `sparsify_row_intervals`, moved squaring op from expression language to block matrix `r2_bm`, moved arg checks from scala to python.; - then deleted `UpperIndexBounds`, `UpperIndexBoundsSuite`, `BlockMatrix.filteredEntriesTable` and tests.; - improved `test_ld_prune` and modified `ldprune.vcf` to make it much smaller and span three chromosomes instead of one for better testing; - allowed `sparsify_row_intervals` to accept ndarrays so user need not convert output of `locus_windows` (which should naturally be an ndarray) with `[int(s) for s in starts]`. If the arg checking or py4j communication become a bottleneck, we can add passage through file similar to what we do for arrays of doubles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3715:1522,bottleneck,bottleneck,1522,https://hail.is,https://github.com/hail-is/hail/pull/3715,1,['bottleneck'],['bottleneck']
Performance,"This PR adds `utils/StackSafe.scala`, which contains generic tools for writing stack safe code. To illustrate its use, I've converted `NormalizeNames` and `RewriteBottomUp` to be stack safe. This approach optimizes for the minimal possible change to existing code to make it stack safe. I originally expected this to have mediocre performance, and designed this to have optimization opportunities--requiring more substantial rewrites--where we found it was necessary. In a follow up PR, I converted the IR parser to be stack safe. In benchmarking that, I'm not able to see any performance penalty (if anything, the stack safe version looks slightly faster, which is probably just noise). So it's possible this will perform well enough as is, but we can keep an eye on it as we convert more passes. The basic idea is to rewrite functions that can be called recursively (directly or indirectly through a path of mutually recursive functions) from `f: (...) => A` to `f: (...) => StackFrame[A]`. Where the former evaluates, executing all recursive calls, and then returns the `A` result, the later returns a description of the work to be done before making any recursive calls. The method `StackFrame[A].run(): A` executes that description in a non-recursive loop. `StackFrame` is a monad, implementing `map` and `flatMap`, which allows the `for` syntactic sugar to be used. When a method makes several recursive calls, this can be significantly more readable. The public api is small. There are the free functions; ```scala; def done[A](result: A): StackFrame[A]; def call[A](body: => StackFrame[A]): StackFrame[A]; ```; and the methods; ```scala; abstract class StackFrame[A] {; def flatMap[B](f: A => StackFrame[B]): StackFrame[B]; def map[B](f: A => B): StackFrame[B] = flatMap(a => Done(f(a))); def run(): A; }; ```; `done` is basically the return statement. `call` is very important: it wraps a recursive call in a thunk, so that returning a `StackFrame` doesn't require descending all the way to t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9320:205,optimiz,optimizes,205,https://hail.is,https://github.com/hail-is/hail/pull/9320,5,"['optimiz', 'perform']","['optimization', 'optimizes', 'perform', 'performance']"
Performance,"This PR adds an optimization where a two dimensional matrix multiply of matrices containing float32 or float64s will be performed by SGEMM or DGEMM respectively. In the future, we should use this during tensor multiplies as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7807:16,optimiz,optimization,16,https://hail.is,https://github.com/hail-is/hail/pull/7807,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"This PR adds stream nodes `StreamMultiMerge` and `StreamZipJoin`, which will be used to implement `TableUnion` and `TableMultiWayZipJoin`. The two nodes were so similar, both in implementation and in interface, that I thought bundling them into one PR would actually make it *easier* to review, but I can split them up if you disagree. The implementations of both nodes use tournament trees, a very simple data structure ideal for this problem. Think of it as a priority queue specialized to holding exactly `k` elements, so when you pop the top element, you must immediately replace it with a new value. A tournament tree is just what it sounds like. It is a complete binary tree with `k` leaves. Conceptually, the leaves hold the current `k` elements (the heads of each of the `k` input streams), while each internal node records the result of the comparison between the ""winners"" of the two subtrees, where in this case the winner is the least element. Thus we can find the smallest of all `k` elements by looking at the root node. If we remove the smallest element, and replace it with the next value from that stream, we change the value in the corresponding leaf node, then we just need to replay the comparisons at all internal nodes on the path to the root. Note that to replay a comparison, we only need to know what element *lost* at this node previously. It must have lost to the previous overall winner, the element we just replaced. Using that observation, we only need to store the `k` current values in the `k` leaves, and in each of the `k-1` internal nodes we store the index of the element which lost the comparison at that node. That just leaves the overall winner, which we can store in a separate variable. Note that each element besides the overall winner loses exactly one ""match"", so the internal nodes store a permutation of the indices 0 to (k-1), minus the overall winner. This is a so-called ""loser tree"". In the implementation, I store the `k` leaves in a `Array[Long]`, w",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9033:471,queue,queue,471,https://hail.is,https://github.com/hail-is/hail/pull/9033,1,['queue'],['queue']
Performance,This PR adds support for optionally loading a dosage values as the call value for genotype loci. This is to support VCF files (such as the ones we use at 23andMe) that have dosages (`DS`) instead of genotype call information (typically `GT`).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5077:36,load,loading,36,https://hail.is,https://github.com/hail-is/hail/pull/5077,1,['load'],['loading']
Performance,"This PR adds the Azure AsyncFS. I added a new secret with the Azure credentials for a new service account, adopted the container `hail-test-****` as the test container, and made the container have a 1 day retention policy (we'll want to check and make sure this actually works). The performance seemed decent. We'll probably want to benchmark this formally at some point, but I think it's good enough for now to have something that works.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10795:283,perform,performance,283,https://hail.is,https://github.com/hail-is/hail/pull/10795,1,['perform'],['performance']
Performance,"This PR attempts to simplify the use of TLS and HTTP(S) in Hail. The big changes; are in `hail/python/hailtop`. In particular I removed several functions with; confusingly overlapping functionality in `tls.py`. Instead, we now have three; functions:. - `internal_server_ssl_context`; - `internal_client_ssl_context`; - `external_client_ssl_context`. The client context is configured to seek certificates from its peers. Both; internal contexts load the Hail certificate chain specified in the Hail SSL; Config. The external client context does not load the hail certificate chain. I intend all Hail's HTTP(S) requests to use `httpx.py` (so named to not conflict; with modules named `http`). Again, I have simplified the landscape. We now have; two functions:. - `httpx.client_session`: The constructor for all asynchronous, HTTPS client; sessions.; - `httpx.blocking_client_session`: The constructor for all synchronous, HTTPS; client sessions. Both sessions have the exact same configuration parameters. The API is exactly; the same except the blocking client session replaces asynchronous methods with; synchronous ones. Both sessions accept the `aiohttp.ClientSession` constructor parameters. They; support one new parameter and modify the behavior of one old parameter.; - `retry_transient`: when set to `True` this parameter will retry all transient; errors in all requests made by this session. This defaults to `True`.; - `raise_for_status`: this parameter now defaults to `True` and includes the; response body text in the error message. Both; Both parameters may be overridden on a per-request basis. - `httpx.ResponseManager` and `httpx.ClientSession` work together to enable; `retry_transient` and `raise_for_status`. Aiohttp has this unusual structure where; all the request methods are synchronous but they return an object that is both; awaitable and an async context manager. I mirror their structure exactly. The; `httpx.ResponseManager` is both awaitable and an async context manager.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9554:444,load,load,444,https://hail.is,https://github.com/hail-is/hail/pull/9554,2,['load'],['load']
Performance,"This PR implements the core IBS operations in terms of vectorized C code. In particular, we use the `libsimdpp` library to take advantage of whatever the widest available register is (many modern CPUs have AVX2 256 bit integer registers; Knights Landing will introduce AVX512 512-bit integer registers). The performance improvement is massive. We can compute the full IBD matrix on 2,535 samples and ~37 million variants in just under 17 minutes. We believe the complexity of this code is `O(nSamples^2 * nVariants)`. Assuming the scaling works out well, we should be able to compute 100,000 Variants and 40,000 samples in the same time. There were a couple issues I had to workaround, but hopefully we can re-use those workarounds:. - compiling native code from gradle; - packaging native code for `test`, `installDist`, and the JARs; - building native code specialized to certain architectures. Still left to do:. - [x] break the C tests into a separate file and call from gradle `test`; - [x] maybe use a library ([libsimdpp?](https://github.com/p12tic/libsimdpp)) to do the SIMD so we're agnostic to the underlying architecture (right now if you don't have AVX, we fall all the way back to 64-bit registers, rather than 128-bit SSE registers) ; - [x] some minor clean up of the IBSFFI class. Future Work:; - implement IBSExpectations in C as well; - expand this work to KING (or other structure correcting IBD calculations)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092:308,perform,performance,308,https://hail.is,https://github.com/hail-is/hail/pull/1092,1,['perform'],['performance']
Performance,"This PR improves and consolidates several that were pending. It does a few things:. - all values in the file formats are stored natively (encoded), not using json,; - in particular, the globals and cols are not stored in the json, and not loaded when the read is performed by the user (but when it is executed by the backend).; - MT parts (globals, rows, cols, entries) are all stored in separate directories and each is itself a valid Table file; - extensions for MT and Table are no longer enforced; - the metadata now stores an ""RVDSpec"" which is a recipe for reading an RVD. This gives us the flexibility to define new RVD types (e.g. HashedRVD) or modify the existing classes (by forking and adding additional parameters, etc.); - the rows and entries are stored separately and zipped together on read. If dropSamples = true, don't even load the entries.; - created UnpartitionedRVD, cleaned up the RVD interface. fixes: #2810",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2858:239,load,loaded,239,https://hail.is,https://github.com/hail-is/hail/pull/2858,3,"['load', 'perform']","['load', 'loaded', 'performed']"
Performance,"This PR is the first iteration of an AsyncFS-based copy interface. It adds RouterAsyncFS.copy. The goal of these changes is to establish the interface and behavior. I expect several follow-on PRs:. - Revise the original copy interface proposal and add to dev-docs.; - ~~Parallelizes the transfers concurrently with async and across multiple threads.~~; - ~~After parallizing, copy will involve a lot of paralellism. Throwing an exception on the first error will be very non-deterministic. Instead, copy will return a report that collects all the errors that were encountered in the course of copying, and summarizes how many files/bytes were copied.~~; - Use multi-process parallelism; - Avoid overwriting the destination if it exists and has a matching checksum (or size).; - ~~Introduce multi-part transfers~~; - add a post-pass for Google Storage to detect file-and-directory errors.; - Adds support for S3.; - Add `hailctl cp ...` (PR); - Use copy in Batch. After this goes in, these can mostly be developed in parallel. A few principles guided the implementation of copy: perform the minimal number of system calls or API requests per copy, and only do error checking when it doesn't involve additional FS operations. For example, it is too expensive to exhaustively check if we're creating a path that is a file and a directory in Google Storage. I considered doing additional and exhaustive checking for the actual copy arguments. For example, currently, `cp -T /path/to/file /path/to/dir` will not generate an error on Google Storage. In the end, I decided to go with the current behavior and I will add an option to do a postpass to check for file-and-dir paths. To achieve this, for each transfer, I simultaneously stat the destination (if needd) to determine if it is a file, directory or doesn't exist. For each source, I simultaneously try to copy it as a file and a directory. When copying each source, we don't need to know the type of the destination until after we've stat'ed the sour",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822:297,concurren,concurrently,297,https://hail.is,https://github.com/hail-is/hail/pull/9822,1,['concurren'],['concurrently']
Performance,"This PR is to enable `hail-az;` file references to contain SAS tokens to enable bearer-auth style file access to Azure storage. Basic summary of the changes:; 	- Update `AzureAsyncFS` url parsing function to look for and separate out a SAS-token-like query string. Note: made fairly specific to SAS tokens - generic detection of query string syntax interferes with glob support and '?' characters in file names; 	- Added `generate_sas_token` convenience function to `AzureAsyncFS`. Adds new `azure-mgmt-storage` package requirement.; 	- Updated `AzureAsyncFS` to use `(account, credential)` tuple as internal `BlobServiceClient` cache key; 	- Updated `AzureAsyncFSURL` and `AzureFileListEntry` to track the token separately from the name, and extend the base classes to allow returning url with or without a token ; 	- Update `RouterFS.ls` function and associated `listfiles` function to allow for trailing query strings during path traversal ; 	- Change to existing behavior: `LocalAsyncFSURL.__str__`no longer returns 'file:' prefix. Done to make `str()` output be appropriate for input to `fs` functions across all subclasses; 	- Updated `inter_cloud/test_fs.py` to generically use query-string-friendly file path building functions; - Updated InputResource to not include the SAS token as part of the destination file name . `test_fs.py` has been updated to respect the new model, where it is no longer safe to extend URLs by just appending new segments with + ""/"" because there may be a query string. But actually running those tests for the SAS case will require some new test variables to allow the test code to generate SAS tokens (`build.yaml/test_hail_python_fs`): ; ```; export HAIL_TEST_AZURE_ACCOUNT=hailtest; export HAIL_TEST_AZURE_CONTAINER=hail-test-4nxei; # Required for SAS testing on Azure; export HAIL_TEST_AZURE_RESGRP=hailms02; export HAIL_TEST_AZURE_SUBID=12ab51c6-da79-4a99-8dec-3d2decc97343; ```; So the SAS case is disabled for now (`test_fs.py`):; ```; @pytest.fixture(param",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12877:629,cache,cache,629,https://hail.is,https://github.com/hail-is/hail/pull/12877,1,['cache'],['cache']
Performance,"This PR reimplements the Wald and LRT test types of hail's logistic regression rows in terms of ndarrays. It is not yet in a state where it should replace hail's current Breeze based logistic regression since:. 1. I still have to implement the `'firth'` and `'score'` test types (there's some extra work to implement Firth specifically that's going to mean adding a new ndarray method); 2. It is slower than Hail's Breeze based logistic regression. I'd still like to get it in its current state, as it's still a feature add to the local and service backends, and I intend to follow up by adding a benchmark for this to track performance changes. . This PR also fixes two bugs I found along the way:. 1. In Python, `TailLoop._compute_type` was not recursively calling `_compute_type` on its params, which was a mistake. ; 2. TailLoop creates bindings, but didn't have a `PruneDeadFields` rule. I added one based on the `Let` rule we already had.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10469:625,perform,performance,625,https://hail.is,https://github.com/hail-is/hail/pull/10469,1,['perform'],['performance']
Performance,"This PR removes the `as_array` parameter on `pca` and `hwe_normalized_pca`. The scores and loadings tables are constructed to always have a field of array type, mirroring the eigenvalues array. This simplifies the interface and makes pipeline (and PC indexing) behavior more predictable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3280:91,load,loadings,91,https://hail.is,https://github.com/hail-is/hail/pull/3280,1,['load'],['loadings']
Performance,"This PR separates the ability to broadcast values in generated code on the backend from the collectDArray function. In order to support this, I did three things:. - allowed the function builder to build and store Encoder/Decoder objects, which wasn't strictly necessary but makes generating encoders/decoders for staged code much easier (I'll probably run some performance comparisons on this, shortly, but hopefully this doesn't introduce unacceptable amounts of overhead.); - added broadcast support on BackendUtils to be able to broadcast serialized Hail values with a given decoder; - rewrote the CollectDistributedArray codegen to exercise the broadcasting on BackendUtils instead of manually serializing/broadcasting/deserializing the globals in generated code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8040:361,perform,performance,361,https://hail.is,https://github.com/hail-is/hail/pull/8040,1,['perform'],['performance']
Performance,This PR tries compacting the v2 tables instead of the intended v3 tables to test CI performance improvements on tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13156:84,perform,performance,84,https://hail.is,https://github.com/hail-is/hail/pull/13156,1,['perform'],['performance']
Performance,"This PR tries to address the error we saw last Friday on Azure where there was a stuck worker that could not pull ubuntu:20.04 from Dockerhub. The error message in the worker logs was. ```; DockerError(500, 'Head \""https://haildev.azurecr.io/v2/ubuntu/manifests/20.04\"": denied: retrieving permissions failed'); ```. I looked at the system logs and the actual error message was this:. ```; Mar 03 16:56:12 batch-worker-default-standard-nj0qy dockerd[4066]: time=""2023-03-03T16:56:12.112691249Z"" level=info msg=""Attempting next endpoint for pull after error: Head \""https://haildev.azurecr.io/v2/ubuntu/manifests/20.04\"": denied: retrieving permissions failed""; ```. Higher up in the logs was:. ```; Mar 03 16:54:50 batch-worker-default-standard-nj0qy dockerd[4066]: time=""2023-03-03T16:54:50.520878176Z"" level=debug msg=""Fetching manifest from remote"" digest=""sha256:9fa30fcef427e5e88c76bc41ad37b7cc573e1d79cecb23035e413c4be6e476ab"" error=""<nil>"" remote=""docker.io/library/ubuntu:20.04""; Mar 03 16:54:50 batch-worker-default-standard-nj0qy dockerd[4066]: time=""2023-03-03T16:54:50.762789745Z"" level=debug msg=""Fetching manifest from remote"" digest=""sha256:9fa30fcef427e5e88c76bc41ad37b7cc573e1d79cecb23035e413c4be6e476ab"" error=""ref moby/1/index-sha256:9fa30fcef427e5e88c76bc41ad37b7cc573e1d79cecb23035e413c4be6e476ab locked: unavailable"" remote=""docker.io/library/ubuntu:20.04""; ```. My working hypothesis is described in detail here that the image cache with locks got corrupted with the simultaneous pulls: https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/Azure.20CI.20appears.20hung/near/339452619. To mitigate this, when we get the error ""denied: retrieving permissions failed"", we try and delete the image and then try pulling again once more before erroring gracefully. At least for now, this errors the user's job, but that's better than the status quo where the job is stuck.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12758:1450,cache,cache,1450,https://hail.is,https://github.com/hail-is/hail/pull/12758,1,['cache'],['cache']
Performance,"This PR will not be merged as-is, but split along the 3 commits contained within: ; - Add `Coalesce` IR node; - Expose pruning on FilterIntervals relational functions. These should be promoted to full IR nodes, especially after this PR.; - Add `ExtractIntervalFilters` optimizer pass. I also have yet to add tests for the last commit. What does this PR do?. ```python; In [2]: mt = hl.read_matrix_table('data/1kg.rep.mt'). In [3]: mt.filter_rows(mt.locus.contig == '16').count(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[3]: (384, 284). In [4]: mt.filter_rows(mt.locus.contig == '16').count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[4]: 384. In [5]: mt.filter_rows((mt.locus.contig == '16') | (mt.locus.contig == '19')).count(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[5]: (730, 284). In [6]: mt.filter_rows(hl.literal({'16', '19'}).contains(mt.locus.contig)).count_rows(); Hail: INFO: interval filter loaded 10 of 128 partitions; Out[6]: 730. In [7]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000)).count_rows(); Hail: INFO: interval filter loaded 2 of 128 partitions; Out[7]: 82. In [8]: mt.filter_rows((mt.locus.contig == '16') & (mt.locus.position > 10_000_000) & (mt.locus.position < 12_000_000)).count_rows(); Hail: INFO: interval filter loaded 5 of 128 partitions; Out[8]: 384. In [9]: mt.filter_rows(mt.locus == hl.parse_locus('1:3761547')).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[9]: (1, 284). In [10]: mt.filter_rows(hl.parse_locus_interval('16:20000000-30000000').contains(mt.locus)).count(); Hail: INFO: interval filter loaded 1 of 128 partitions; Out[10]: (35, 284); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5979:269,optimiz,optimizer,269,https://hail.is,https://github.com/hail-is/hail/pull/5979,9,"['load', 'optimiz']","['loaded', 'optimizer']"
Performance,"This PR:. - Introduces mapping over NDArrays in JVM emitter. ; - Introduces the `NDArrayEmitter` class, mimicking the cxx analogue. This class is used as the basis of our `NDArray` deforesting efforts (see `emitLoops`); - Fixes a bug in `PStruct`'s load field function. ; - Introduces useful helper functions on `PNDArray`: `numElements` and `makeDefaultStrides`. Ready for review",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6983:249,load,load,249,https://hail.is,https://github.com/hail-is/hail/pull/6983,1,['load'],['load']
Performance,"This PR:. - Pushes code builders through PNDArray interface instead of method builders; - Starts switching away from `PNDArray.data.load` to using methods like `dataPArrayPointer` and `dataFirstElementPointer`. All `dataPArrayPointer` calls will go away when ndarrays are no longer backed by `PArray`. ; - Speeds up repeated calls to `loadElement` on `SNDArrayPointerSettable`, which speeds up linear regression nd benchmark ~12%. Now we are approximately 60% slower than breeze linear regression.'; - Removes `CodePTuple`, since all instances of its use are removed now and it predated the current `PCode` system",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9883:132,load,load,132,https://hail.is,https://github.com/hail-is/hail/pull/9883,2,['load'],"['load', 'loadElement']"
Performance,"This adds a `label` column to pools, which can be used to select a subset of pools to consider when scheduling a job. The label can be specified for each job by setting the `_pool_label` attribute, e.g. `job._pool_label = 'seqr'` will consider all pools that have the `seqr` label. Note: this incurs a DB migration. `batch/sql/add-seqr-pools.sql` is an example for adding a copy of the default preemptible pools, with an additional `seqr` label applied. CPG limits the number of instances in those dedicated pools to prevent long running seqr loading pipeline jobs from starving other batches for resources. #assign services",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879:543,load,loading,543,https://hail.is,https://github.com/hail-is/hail/pull/11879,1,['load'],['loading']
Performance,This adds a control to CI where an operator can adjust the rate limit on a particular service in a particular namespace. CI stores and propagates that information when it generates the Envoy configs for `gateway` and `internal-gateway`. This enables operators to shield an overwhelmed batch driver by decreasing its rate limit without needing to push a code change. Increasing the rate limit can increase throughput if workers are being rate limited but the batch driver is not fully utilized.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14609:405,throughput,throughput,405,https://hail.is,https://github.com/hail-is/hail/pull/14609,1,['throughput'],['throughput']
Performance,"This adds a prometheus statefulset to track metrics like API request latency and uptime. It scrapes pods on a 15s interval and collects prometheus metrics from any container in a pod with `grafanak8sapp` label that exposes an https endpoint `/metrics`.; The batch front end was already exposing prometheus metrics, but I changed it up slightly. For any http endpoint there should be a single metric, `http_request_latency`. Prometheus adds app and namespace metrics so seeing latencies for batch in particular is just a filter applied to this single metric. You can track latency of an endpoint by adding the `@monitor_endpoint` decorator defined in `metrics.py`, which tracks latency as well as number of requests and status code per request, available in the `http_request_count` metric. I also added monitoring to all CI endpoints. This also includes an `up` metric for tracking uptime at the same 15s granularity. I'm not convinced prometheus will suit our finer-grained needs surrounding batch, but it should do well enough in the meantime for our more traditional SLIs and allows to focus on one problem at a time.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10165:69,latency,latency,69,https://hail.is,https://github.com/hail-is/hail/pull/10165,3,['latency'],['latency']
Performance,"This adds about 800KB total to the repo. I added a notion of supportedCodec, which includes all the codecs except DirectCodec (which currently depends on the in memory representation of region values which we don't want to promise). FYI @tpoterba I also added a micro optimization: if you get a field of a struct declaration, just grab out the AST for the field. This fixed some massive expression size blowup in create_all_values_datasets.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3043:268,optimiz,optimization,268,https://hail.is,https://github.com/hail-is/hail/pull/3043,1,['optimiz'],['optimization']
Performance,"This adds filter, filterCols, and filterRows to BlockMatrix, en route to moving kinship and LD matrix to from IRM to BlockMatrix. For example, filtering out rows/columns from a kinship matrix (e.g. to remove samples with missing covariate data); or filtering an LD matrix (e.g. to perform Leave-One-Chromosome-Out LMM analysis). Since the number of tasks equals the number of resulting partitions (blocks), these functions are suited to filtering out a small to medium subset (say, throwing out a few rows and columns, or half of the rows and columns) and may fail when filtering all but a small number of rows and columns due to all the blocks being sent to one executor. In the latter case, one can do better by writing the result of filterCols, then reading and applying filterRows. Testing locally, on a 16384 * 16384 matrix (block size 4096, 16 partitions, 128MB each), when only filtering columns (or rows), filterCols (or filterRows) tends to be faster than filter (likely due to less copying), and filterCols tends to be a bit faster than filterRows (which I think is due to lack of transpose and esp. better striding). When filtering both columns and rows, filter tends to be fastest but filter_columns followed by filter_rows is often comparable. It may be that the cost of an additional copy in the first step is offset by smaller strides resulting better caching in the second step. For example, filtering out every 31st row and column:; ```; filter; [9.550655126571655, 9.62321400642395]; filterCols.filterRows; [12.190248966217041, 12.07064700126648]; filterRows.filterCols; [14.239542007446289, 15.601837158203125]; ```; Filtering out every row and column with 0.5 probability, resulting in 4 partitions:; ```; filter; [4.625396966934204, 4.427164077758789, 4.435616970062256]; filterCols.filterRows; [4.647814035415649, 4.610292911529541, 4.655405044555664]; filterRows.filterCols; [4.866307973861694, 5.073392868041992, 5.470307111740112]; ```. On GCP, with 32 core highmem master and",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2647:281,perform,perform,281,https://hail.is,https://github.com/hail-is/hail/pull/2647,1,['perform'],['perform']
Performance,"This allows Spark 2 Hail to load partitioning information from VDSes; written by the Spark 1 version, but loading partitioning from previous; Spark 2 versions will now fail. Since there are far more VDSes in; uses written with Spark 1 in use, this seems like a good trade-off. There are now to serial version UIDs in the wild for Locus. I don't; see how to write code to load both of them (except maybe; loading/unloading different versions of the Locus class which seems; painful.) I would prefer a tool that converts the partitioning to; JSON instead (once support for JSON is ready). The partitioning information should be stored as JSON instead of Java; serialization, which is not a good long-term storage format.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1138:28,load,load,28,https://hail.is,https://github.com/hail-is/hail/pull/1138,4,['load'],"['load', 'loading']"
