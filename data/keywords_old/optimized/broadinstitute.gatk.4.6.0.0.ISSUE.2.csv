quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1 with -ERC GVCF. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; It would appear that variants covered by a spanning deletion are not output with phasing information even when surrounded by phased variants on either side. Since one of the alleles is covered by an upstream deletion phase is known, but the genotype itself is not phased and no phase set is attached. The following is a cut-down example from a gVCF:. ```; chr6 51618169 . GT G,<NON_REF> 948.60 . DP=94 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:32,39,0:71:3,4,0:29,35,0:99:0|1:51618169_GT_G:956,0,808,1054,926,1980:51618169:3,29,4,35; chr6 51618170 . T *,G,<NON_REF> 776.01 . DP=92 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:2,39,30,0:71:1,4,2,0:1,35,28,0:99:3533,786,723,1141,0,956,2837,916,1206,2757:1,1,6,63; chr6 51618171 . G <NON_REF> . . END=51618173 GT:DP:GQ:MIN_DP:PL 0/0:90:99:90:0,120,1800; chr6 51618174 . A G,<NON_REF> 1001.60 . DP=89 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:33,41,0:74:3,4,0:30,37,0:99:0|1:51618169_GT_G:1009,0,803,1108,926,2034:51618169:3,30,4,37; ```. You can see that the SNP at 51618170 is flanked by phased variants at 51618169 and 51618174, but is output with unphased genotype and no `PS` (or `PID/PGT`). I'm not entirely sure if this is on purpose for some reason I don't understand, or simply an edge case in the phasing code that's handled incorrectly. #### Steps to reproduce; Run HC on reads with three variants, starting with a deletion, a variant spanned by the deletion and a variant just beyond the deletion. FWIW I've requested permission to share an example case from real data and am awaiting an answer. #### Expected behavior; I think the spanned variant should be output with phasing information, e.g. in the above case I would expect (abbreviated):. ```; chr6 51618169 . GT G,<NON_REF> ... GT:DP:PS 0|1:71:51618169; chr6 51618170 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5651:131,release,release,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5651,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1.1.0. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 3/31/2019. ### Description ; It looks like PR #5840 did a lot of refactoring to the way F1R2/F2R1 annotations are computed. Along the way it looks like `OxoGReadCounts` was renamed to `OrientationBiasReadCounts`. This is, unfortunately for some, a non-backwards compatible change as any pipeline that uses `-A OxoGReadCounts` will now fail. I'm not sure if there's a deprecation mechanism for annotations that would inform users of this, and I'm not sure there's a whole lot to be done at this point. I'm logging this issue mainly so anyone else who runs into this will find the answer quickly. Might be nice to add a line to the 4.1.1.0 release notes though noting this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5848:120,release,release,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5848,3,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller `--annotation OrientationBiasReadCounts`. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]; - [ ] 4.2.2.0. ### Description; When specifying OrientationBiasReadCounts, HaplotypeCaller adds the description of F1R2 and F2R1 to the header, but does not calculate them. This was observed in GATK 4.2.2.0 and also in a test with 4.3.0.0 (the latest at the moment of this issue).; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=F1R2,Number=R,Type=Integer,Description=""Count of reads in F1R2 pair orientation supporting each allele"">; ##FORMAT=<ID=F2R1,Number=R,Type=Integer,Description=""Count of reads in F2R1 pair orientation supporting each allele"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT sample; 13 32911888 . A G 177.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.086;DP=21;ExcessHet=3.0103;FS=1.719;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=8.46;ReadPosRankSum=0.475;SOR=0.368 GT:AD:DP:GQ:PL 0/1:13,8:21:99:185,0,339; 13 32913055 . A G 402.06 . AC=2;AF=1.00;AN=2;DP=15;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=26.80;SOR=1.112 GT:AD:DP:GQ:PL 1/1:0,15:15:45:416,45,0; 13 32915005 . G C 378.06 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:GQ:PL 1/1:0,13:13:39:392,39,0; 13 32929232 . A G 168.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=16;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:GQ:PL 0/1:5,6:11:99:176,0,121; 13 32929387 . T C 209.02 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:153,release,release,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller when emitting physical phasing. ### Affected version(s); - [x] Latest public release version [4.1.4.1]; - [ ] Latest master branch as of [n/a]. ### Description ; When there are three SNPs in close proximity with the first having a homozygous-alt genotype and the other two being hets that are in trans, the GATK incorrectly outputs genotypes and phasing indicating they are in cis. I haven't tested more broadly (e.g. with > 3 variants or with indels etc.) but my suspicion is that it is to do with the first variant in the phase set being homozygous. This was seen happening on real data from a real sample, but I have also been able to reproduce this with synthetic test data that I can attach here. #### Steps to reproduce; I've attached [phasing.zip](https://github.com/broadinstitute/gatk/files/4237216/phasing.zip) to this issue. It contains a BAM file of synthetic data where I've introduced two variant haplotypes at 50 locations each separated by about 1000 bases. My goal in doing this was just to have a number of different sequence contexts and variant alleles in case that affected anything. It also contains the resulting VCF from running this GATK command using 4.1.4.1:. ```; gatk HaplotypeCaller -I phasing.bam -O phasing.g.vcf -ERC GVCF \; -R hg19.fasta -L chr2:179390700-179672150; ```. While the BAM clearly shows the two hets as in trans with one another:; ![hom_with_in_trans_hets](https://user-images.githubusercontent.com/1609210/75055826-edcfd300-5492-11ea-8bb7-b3c492140797.png). The resulting variant calls are given as in-cis:. ```; chr2 179393825 . C A,<NON_REF> 2686.03 . DP=60;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;RAW_MQandDP=216000,60 GT:AD:DP:GQ:PGT:PID:PL:PS:SB 1|1:0,60,0:60:99:0|1:179393825_C_A:2700,181,0,2700,181,2700:179393825:0,0,60,0; chr2 179393826 . T <NON_REF> . . END=179393826 GT:DP:GQ:MIN_DP:PL 0/0:60:99:60:0,120,1800; chr2 179393827 . T G,<NON_REF> 1386.60 . BaseQRankSum=0.000;DP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6463:143,release,release,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6463,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller with `--max-mnp-distance` filter. ### Affected version(s); - [x] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; I think there's a problem with the StrandOddsRatio (SOR) annotation and the `--map-mnp-distance` flag. I'm looking at a small region of NA24143 (one of the GIAB samples). There's a pair of SNPs in very close proximity. When called without the MNP output I get a pair of variants as follows (some info removed for clarity), coordinates are HG19:. ```; chr4 5743509 . C T 5903.03 . FS=0.000;QD=25.36;SOR=9.825 GT:AD:DP:GQ:PL 1/1:0,135:135:99:5917,406,0; chr4 5743512 . T C 2766.60 . FS=0.000;QD=21.12;SOR=0.983 GT:AD:DP:GQ:PL 0/1:57,74:131:99:2774,0,2060; ```. I'm trying to get permission to share the BAM over this region, but the key information is that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5698:145,release,release,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected [Version 4.1.5.0](https://github.com/broadinstitute/gatk/tree/4.1.5.0); - [GATK 4.1.5.0 ] public release version . ### Description ; 1. miscalled known germline MSH2 c.942+3A>T in sample1 (bam file in the zip file attached); 2. miscalled known germline DICER1 c.4206+9G>T in sample2 (bam file in the zip file attached). #### Steps to reproduce; The command used was. ```; java -Djava.io.tmpdir='' -jar gatk-4.1.5.0/gatk-package-4.1.5.0-local.jar HaplotypeCaller \; -R human_g1k_v37_fasta \; -L ROI.bed \; --dbsnp dbsnp_138.b37.vcf.gz \; -I sample1.bam \; -O sample1.4.1.5.0.gvcf.gz \; -ERC GVCF \; --native-pair-hmm-threads 16 \; --interval-padding 100 \; --min-base-quality-score 20 \; -bamout sample1.4.1.5.0.bamout.bam \; --minimum-mapping-quality 20; ```; The interval list file `ROI.bed` and the two bam files `sample1.bam` and `sample2.bam` are attached. Also attached are the gvcf files and bamout files we generated. . #### Expected behavior; There are two known germline variants in the two bam files. - **Sample 1**. miscalled germline MSH2 c.942+3A>T, rs193922376 in MSH2, NM_000251.1; intron 5 c.942+3A>T, heterozygous; chr2:47,641,560 A>T. ![Sample 1 bamout ](https://user-images.githubusercontent.com/56843518/77380922-e41be280-6d52-11ea-981f-140e719bece6.png). A screen shot of the variant loci in the bamout bam generated by HaplotypeCaller is provided above. Even though the variant was observed in the bam file, it was not reported by HaplotypeCaller. It seems that the variant was called at loci chr2:47641562 instead, as shown in the gvcf below; ```; C02ZC340LVDM:outputs_bamout xxx$ grep 47641562 sample1.4.1.5.0.gvcf; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT; 2 47641562 . A T,<NON_REF> 1044.64 . BaseQRankSum=0.829;DP=100;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.114;RAW_MQandDP=354425,100;ReadPosRankSum=0.692 GT:AD:DP:GQ:PGT:PID:PL:PS:SB 0|1:46,48,6:100:99:0|1:47641559_TAA_T:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6521:177,release,release,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6521,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - Latest public release version [4.2.5.0]. ### Description . #### Steps to reproduce; Specify `--mate-too-distant-length` parameter. #### Expected behavior; Use specified value to filter paired-end reads. #### Actual behavior; Java exception:. ```; Using GATK jar /home/rwilton/tools/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/rwilton/tools/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar HaplotypeCaller; --reference /datascope/rwilton/scratch/GATK/GRCh38/chr14.fna; --intervals chr14; --emit-ref-confidence GVCF; --sample-name HG002; --smith-waterman FASTEST_AVAILABLE; --native-pair-hmm-threads 48; --read-filter MateDistantReadFilter; --mate-too-distant-length 1500; --minimum-mapping-quality 10; --mapping-quality-threshold-for-genotyping 10; --input full.chr14.bam; --output vcf1/full.chr14.g.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: Couldn't set field value for mateTooDistantLength in org.broadinstitute.hellbender.engine.filters.MateDistantReadFilter@55fdf7f9 with value 1500.; at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValue(NamedArgumentDefinition.java:680); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setScalarValue(NamedArgumentDefinition.java:380); at org.broadinstitute.barclay.argparser.NamedArgumentDefinition.setArgumentValues(NamedArgumentDefinition.java:293); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.propagateParsedValues(CommandLineArgumentParser.java:490); at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:170); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:233,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7696:108,release,release,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7696,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [ ] 4.0.8.1; - [x] 4.0.9.0; - [x] Latest public release version [4.1.2.0]. ### Description ; This maybe a series of mistakes.; I guess it will happen when a new variant should be detected within a spanning deletion.; Below is an example:. in 4.0.8.1, a NMP ( CTTT>CAAAA ) was detected as two variants( CTTT>C + T>TAAAA ).; *vcf of 4.0.8.1*; ```vcf of 4.0.8.1; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944606	.	CTTT	C	9210.73	.	AC=1;AF=0.500;AN=2;BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;FS=0.518;MLEAC=1;MLEAF=0.500;MQ=60.03;MQRankSum=0.000;QD=11.81;ReadPosRankSum=0.295;SOR=0.728	GT:AD:DP:GQ:PL	0/1:423,357:780:99:9248,0,45245; chr13	32944609	.	T	TAAAA	14802.73	.	AC=1;AF=0.500;AN=2;BaseQRankSum=4.179;DP=787;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=18.93;ReadPosRankSum=0.241;SOR=0.689	GT:AD:DP:GQ:PL	0/1:411,371:782:99:14840,0,45112; ```; *gvcf of 4.0.8.1*; ```gvcf of 4.0.8.1; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944440	.	T	<NON_REF>	.	.	END=32944605	GT:DP:GQ:MIN_DP:PL	0/0:592:99:352:0,120,1800; chr13	32944606	.	CTTT	C,<NON_REF>	9210.73	.	BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQ=2929400.00;ReadPosRankSum=0.295	GT:AD:DP:GQ:PL:SB	0/1:423,357,0:780:99:9248,0,45245,10522,46330,56852:212,211,175,182; chr13	32944609	.	T	A,TAAAA,<NON_REF>	14802.73	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQ=2833200.00;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0:770:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. but from version 4.0.9.0 which `support for genotyping spanning deletions and a fix to the reference confidence calculation around indels`,; the second variants got filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5975:142,release,release,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [4.3.0.0 ] Latest public release version . ### Description ; I'm attempting to run HaplotypeCaller on Ultima flow based bams with the ""--flow-mode STANDARD"" and ""--likelihood-calculation-engine FlowBasedHMM"" arguments. However, I'm getting the following error ""java.lang.IllegalArgumentException: read must be flow based: 180652-BC94-0022826568 chr1:14585-14703"". The bams were created from fastqs provided directly from Ultima, so they are definitely flow-based. One question I have: how does HaplotypeCaller determine if a read is flow-based or not? Is this specified in the Read Groups?. #### Steps to reproduce; gatk --java-options ""-Xmx4g"" HaplotypeCaller -R hg38.fa --flow-mode STANDARD --likelihood-calculation-engine FlowBasedHMM -I [bam] -O [bam%.BQSRapplied.bam].GATK.vcf.gz. #### Expected behavior; HaplotypeCaller should complete successfully. #### Actual behavior; HaplotypeCaller fails, expecting flow-based reads",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8112:119,release,release,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8112,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [?] Latest public release version [version?]; - [x] Latest master branch as of Sept 10, 2019. ### Description ; Contamination estimate doesn't appear to be taken into account for reference blocks in GVCFs. #### Steps to reproduce; I'm looking at expected integration test results with uncontaminated (src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.calls.20.10100000-10150000.vcf) vs. 15% contaminated (src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.CEUTrio.HiSeq.WGS.b37.NA12878.CONTAMINATED.WITH.HCC1143.NORMALS.15PCT.20.10100000-10150000.postIndelRefConfUpdate.g.vcf). #### Expected behavior; Contaminated calls should have lower depth because the reads are being downsampled (in a biased way) by the contamination fraction. #### Actual behavior; In the expected HC integration test results I'm seeing for 0 contamination; 20 10132770 . A <NON_REF> . . END=10132770 GT:DP:GQ:MIN_DP:PL 0/0:57:99:57:0,120,1800. For 15% contamination:; 20 10132770 . A <NON_REF> . . END=10132770 GT:DP:GQ:MIN_DP:PL 0/0:56:99:56:0,120,1800. The pileup has 55 (I'm not going down the rabbit hole of the bonus reads), so I would expect the contaminated GVCF to have < 55 DP. The variants look good in some places and less good in others. Looking through the code, I don't see anywhere the contamination estimate would be used for reference confidence. I suspect @davidbenjamin has been harboring a desire to update the contamination model anyway.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6152:112,release,release,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6152,4,"['integrat', 'release', 'update']","['integration', 'release', 'update']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.1.8.1]; - [ ] Latest master branch as of [date of test?]. ### Description ; I have a sample with a complex variant that could be modeled either as a 1bp deletion followed by a 2bp MNP, or more likely as a 3bp deletion followed by a 2bp insertion (or, if you will, the replacement of three reference bases with two other bases). The changes are clearly visible in the following screenshot. The top track is the aligned/deduped BAM, and the bottom track is the assembly BAM generated by HaplotypeCaller:. ![missing-insertion igv-screenshot](https://user-images.githubusercontent.com/1609210/93133361-516e5780-f694-11ea-9b7d-7aa71f5623cc.png). When I call this region to generate a gVCF I get some fairly strange output despite HC clearly reconstructing the haplotype correctly (some annotations removed for readability):. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test-sample; chr13 32953865 . T <NON_REF> . . END=32953884 GT:DP:GQ:MIN_DP:PL 0/0:204:99:196:0,120,1800; chr13 32953885 . AGTT A,<NON_REF> 3110.60 . DP=213;MLEAC=1,0;MLEAF=0.500,0.00 GT:AD:DP:GQ:PL:SB 0/1:108,82,0:1; chr13 32953888 . T *,TAA,<NON_REF> 585.02 . DP=205;MLEAC=0,0,2;MLEAF=NaN,NaN,1.00;RAW_MQandDP=738000,205 GT:GQ:PL ./.:99:0,0,0,0,0,0,0,0,0,0; chr13 32953889 . A <NON_REF> . . END=32953905 GT:DP:GQ:MIN_DP:PL 0/0:211:99:205:0,120,1800; ```. When this is genotyped by `GenotypeGVCFs` the only resulting variant is the 3bp deletion:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test-sample; chr13 32953885 . AGTT A 3110.60 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.00;DP=213;ExcessHet=3.0103;FS=2.544;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.00;QD=16.37;ReadPosRankSum=-4.360e-01;SOR=0.506 GT:AD:DP:GQ:PL 0/1:108,82:190:99:3118,0,4288; ```. I've tried this with GATK 4.1.4.1, and also 4.1.7.0 and 4.1.8.1 and they all have the same issue (output above is from 4.1.8.1). I've also tried",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6817:112,release,release,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6817,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version [4.2.0.0]. ### Description ; HaplotypeCaller fails with the following java error:. ```*** Error in `java': munmap_chunk(): invalid pointer: 0x00007f1da5980f00 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7f3e4)[0x7f1daaec73e4]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(_Z21runSWOnePairBT_avx512iiiiPhS_iiaPcPs+0x338)[0x7f05b3b50f48]; /var/tmp/rwilton/libgkl_smithwaterman14257239252565866950.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)```. #### Steps to reproduce; Using properly-aligned paired-end reads from GIAB reference sample HG002 (NA24385) with GRCh38.p12. Please see the attached log file for parameterization and stderr log:; [vcall.swbug.log](https://github.com/broadinstitute/gatk/files/6275740/vcall.swbug.log). #### Expected behavior; No error. #### Actual behavior; See above and attached log file. Thank you in advance for having a look at this!. Richard Wilton; Johns Hopkins University,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7187:112,release,release,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7187,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); GATK 3.7 and GATK 4.0.11.0; ### Description . GATK Pipeline (HaplotypeCaller->gvcf-->importdb->GenotypeGVCF)is calling sites with a GQ=0. But these sites often have plenty of coverage and no obvious reason for such a low GQ score. Often the GQ should be 99 as the DP >40. This seems to be primarily an issue with homozygous reference calls. . The GT is accurate for the high DP sites but the inaccurate GQ is problematic for any genotype level qc on the pVCF. If the site is recoded from 0/0 to './.' for GQ <20, the result is higher missing rate due to the inaccurate GQ=0. . Directly calling the VCF with HaplotypeCaller without the gVCF intermediate gVCF file calculates the correct GQ score. Freebayes also calculates a correct GQ on these samples.; [rs429358_gq_dp.pdf](https://github.com/broadinstitute/gatk/files/2612419/rs429358_gq_dp.pdf). #### Steps to reproduce. I am seeing this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445:143,Pipeline,Pipeline,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445,1,['Pipeline'],['Pipeline']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller/ActiveProfile. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; In function findEndOfRegion (line 355 in src/main/java/org/broadinstitute/hellbender/utils/activityprofile/ActivityProfile.java), it tries to determine the end of an active region. . The problem happens here, (at line 356); ![activeregion](https://user-images.githubusercontent.com/34263164/205565469-84900a73-1180-48e1-ba9f-f96c23d91e11.PNG); There could be an edge case where stateList.size() = maxRegionSize + getMaxProbPropagationDistance(), the function processes forward for further calculation. Hence the end of active region is determined immediately. However, the end of region is determined earlier than we expected. If by coincidence location at maxRegionSize is determined as minimum, region end is determined here. IBut wait a sec... If location at maxRegionSize+50 (which is NOT involved in current code in the ""if"" judgement at line 356) has an active score larger than 0, it rises the probability value at location maxRegionSize. . Now you should understand what I said. The state of location at maxRegionSize+50 is not updated when you accessed it. Let's assume ; maxRegionSize = 300 and point at location 350 has active value > 0. We trasverse the region to find the minimum point where we could cut the region and we found location at 300 in current logic. However, location 350 can acturally increase the probability at point 300 but this is not considered (or not updated) when making region end decision. #### Expected behavior; Simply use less or equal to at line 356 in the above image would fix this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8118:126,release,release,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8118,3,"['release', 'update']","['release', 'updated']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); In the tutorial ""[(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092)"", users are asked to install R components using [install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R). . ### Affected version(s); Latest public release version [4.5.0.0]. ### Description ; Running the script with `Rscript install_R_packages.R` results in the following error:. `Error in download.file(p, destfile, method, mode = ""wb"", ...) : ; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz'; In addition: Warning message:; In download.file(p, destfile, method, mode = ""wb"", ...) :; cannot open URL 'http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz': HTTP status was '404 Not Found'`. This can be fixed by changing line [35 of install_R_packages.R](https://github.com/broadinstitute/gatk/blob/4.0.1.1/scripts/docker/gatkbase/install_R_packages.R#L35) from `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.tar.gz""` to `hmmUrl = ""http://cran.r-project.org/src/contrib/HMM_1.0.1.tar.gz""`. . The script runs as expected once this change is made. #### Steps to reproduce; Run `Rscript install_R_packages.R`. #### Expected behavior; Successfully installs all necessary R packages with the correct versions. #### Actual behavior; Fails to install the 'HMM' package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8638:233,install,install,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638,4,"['install', 'release']","['install', 'installs', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); JointGermlineCNVSegmentation. ### Affected version(s); - [x] Latest public release version [v4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; I get the following exception when running JointGermlineCNVSegmentation on an exome trio dataset:. ```; [January 19, 2023 at 6:59:29 AM CET] org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation done. Elapsed time: 0.82 minutes.; Runtime.totalMemory()=300941312; java.lang.IllegalStateException: Encountered genotype with ploidy 0 but 1 alleles.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.lambda$createDepthOnlyFromGCNVWithOriginalGenotypes$4(JointGermlineCNVSegmentation.java:666); at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); at java.base/java.util.ArrayList$Itr.forEachRemaining(ArrayList.java:1033); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.createDepthOnlyFromGCNVWithOriginalGenotypes(JointGermlineCNVSegmentation.java:667); at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8164:125,release,release,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8164,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark . ### Affected version(s); - Latest public release version [4.4.0.0]. ### Description . I am working on 40X human WGS data, running MarkDuplicatesSpark on the computation node of a cluster with 40 cores and 192GB RAM. MarkDuplicatesSpark usually hangs and never finish (even after few days) with log as below:. ```; 11:26:29.511 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.511 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:29.512 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:29.512 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:30.738 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:30.738 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.830 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.830 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:45.831 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:45.831 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 11:26:55.475 INFO FileOutputCommitter - File Output Committer Algorithm version is 1; 11:26:55.475 INFO FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folde",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8555:113,release,release,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8555,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5169:116,release,release,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); MergeVcfs (and potentially other Picard related tools). ### Affected version(s); - Latest public release version [4.1.7.0]. Here is my java version in case:; ```bash; $ java -version; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode); ```. ### Description ; When the upstream path of the current directory contains a whitespace **and** the VCFs are stored in a directory 2 level deeper, the VCF is not found. The bug does not happen if:; * VCFs are located in current directory or in a subdirectory (level 1) from the current working directory (see reproducible steps below).; * VCFs have themselves whitespace in their filenames (see reproducible steps below). Here is a GATK stacktrace example:; ```java; Using GATK jar $HOME/local/pckg/python/miniconda3/envs/test/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar $HOME/local/pckg/python/miniconda3/envs/test/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/a.vcf.gz -I data/calling/b.vcf.gz -O c.vcf.gz; 23:25:05.033 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:$HOME/local/pckg/python/miniconda3/envs/test/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Tue Jun 16 23:25:05 CDT 2020] MergeVcfs --INPUT data/calling/a.vcf.gz --INPUT data/calling/b.vcf.gz --OUTPUT c.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 16, 2020 11:25:05 PM shaded.c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664:147,release,release,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664,3,['release'],"['release', 'release-']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Mitochondria WDL. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; Mitochondria WDL still has `--genotyping-mode` argument: https://github.com/broadinstitute/gatk/blob/master/scripts/mitochondria_m2_wdl/AlignAndCall.wdl#L420. This argument doesn't exist in GATK version 4.1.1.0 (which is the one that is currently being used in the mitochondria WDL), so this argument should be changed to the new force-call argument which was added in #6090",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6286:113,release,release,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6286,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Mutect2 `--max-mnp-distance 0`. ### Affected version(s); - [X] Latest public release version [4.2.6.1]. ### Description; Same issue than described here: https://github.com/broadinstitute/gatk/issues/6473; ```; singularity exec docker://broadinstitute/gatk:4.2.6.1 gatk Mutect2 \; -R NC_000962.3.fa \; -I input.bam \; -O output.vcf \; --annotation StrandBiasBySample \; --num-matching-bases-in-dangling-end-to-recover 1 \; --max-reads-per-alignment-start 75 \; --max-mnp-distance 0; ```. And a MNP remains:; ```; grep -P ""NC_000962.3\t761155"" output.vcf; NC_000962.3 761155 . C T,G . . AS_SB_TABLE=0,0|9,9|0,2;DP=20;ECNT=1;MBQ=0,17,23;MFRL=0,311,334;MMQ=60,60,60;MPOS=31,40;POPAF=7.30,7.30;TLOD=44.10,3.01GT:AD:AF:DP:F1R2:F2R1:FAD:SB 0/1/2:0,18,2:0.807,0.143:20:0,5,0:0,4,1:0,15,2:0,0,9,11; ```. #### Expected behavior; ```; grep -P ""NC_000962.3\t761155"" output.vcf; NC_000962.3 761155 . C T [...]; NC_000962.3 761155 . C G [...]; ```. BAM, BAI, and VCF here: [files.zip](https://github.com/broadinstitute/gatk/files/8488204/files.zip). Cheers!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7782:127,release,release,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7782,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of 7/18/18. ### Description ; When running Mutect yesterday on Mitochondrial data I got the following error:; ```; java.lang.IllegalArgumentException: Invalid interval. Contig:chrM start:-4 end:65. 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:728); 	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); 	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:86); 	at org.broadinstitute.hellbender.transformers.PalindromeArtifactClipReadTransformer.apply(PalindromeArtifactClipReadTransformer.java:48); 	at org.broadinstitute.hellbender.transformers.ReadTransformer.lambda$andThen$f85d1091$1(ReadTransformer.java:20); 	at org.broadinstitute.hellbender.transformers.ReadTransformer$$Lambda$107/1786040872.apply(Unknown Source); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42); 	at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.fillDownsampledReadsCache(ReadsDownsamplingIterator.java:69); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.advanceToNextRead(ReadsDownsamplingIterator.java:55); 	at org.broadinstitute.hellbender.utils.downsampling.ReadsDownsamplingIterator.<init>(ReadsDownsamplingIterator.java:34); 	at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:149); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:109); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5036:104,release,release,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] Latest public release version 4.1.5.0. ### Description . #### Steps to reproduce; It seems that Mutect2 is emitting MNPs despite `--max-mnp-distance 0`. 1. Calling in tumor-only mode to prep PON:; ```; Using GATK jar /gatk/gatk-package-4.1.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx20G -jar /gatk/gatk-package-4.1.5.0-local.jar Mutect2 --max-mnp-distance 0 --input /mnt/data/input/gs/file.bam.cram --reference /mnt/data/input/gs/gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta --germline-resource /mnt/data/input/gs/ukbb_v2/projects/jamesp/data/mutect2/bravo-dbsnp-all-f5.exome.chr.sorted.reheader.vcf.gz --intervals /mnt/data/input/gs/gcp-public-data--broad-references/hg38/v0/exome_calling_regions.v1.interval_list --output /mnt/data/output/gs/file.bam.cram.unfiltered.vcf.gz; ```. 2. GenomicsDB creation for PON step:; ```; Using GATK jar /gatk/gatk-package-4.1.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx40G -jar /gatk/gatk-package-4.1.5.0-local.jar GenomicsDBImport --reference /mnt/data/input/gs/gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta --intervals chr1 --intervals chr2 --intervals chr3 --intervals chr4 --intervals chr5 --intervals chr6 --intervals chr7 --intervals chr8 --intervals chr9 --intervals chr10 --intervals chr11 --intervals chr12 --intervals chr13 --intervals chr14 --intervals chr15 --intervals chr16 --intervals chr17 --intervals chr18 --intervals chr19 --intervals chr20 --intervals chr21 --intervals chr22 --intervals chrX --genomicsdb-workspace-path pon_db {etc}. ***********************************************************************. A,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473:104,release,release,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] Latest public release version [2.1]; - [x] Latest master branch as of [2018-09-13]. ### Description ; The VCF header line; ""##Mutect Version=x.y""; causes problems for some VCF readers. Each header line is required to be a key-value pair and a space character is not expected in the key. (The VCF specification is not clear on this matter, but I've never encountered a space character in a VCF header key before.); Making VCF files that are easily readable by downstream tools should be in the interest of Mutect2. #### Steps to reproduce; Create a VCF file using Mutect2 and look at the header. #### Expected behavior; output; ""##MutectVersion=2.1"". #### Actual behavior; output; ""##Mutect Version=2.1""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5183:104,release,release,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5183,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] Latest public release version [4.1.4.1]; - [ ] Latest master branch as of [date of test?]. ### Description ; Mutect2 occasionally writes lines including INFO tag `MPOS=-2147483648`. This doesn't look sensible for ""median distance from end of read"", and the specific value is disallowed in [section 1.3 of the VCF specification](https://samtools.github.io/hts-specs/VCFv4.3.pdf). I've had a quick look at the code, and think the dubious value may be generated in [ReadPosition::getValueForRead](https://github.com/broadinstitute/gatk/blob/946f39/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ReadPosition.java#L57) when the result from [ReadPosRankSumTest.getReadPosition](https://github.com/broadinstitute/gatk/blob/946f39/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ReadPosRankSumTest.java#L53) is cast to an `int`. Looking at that function, it can [return `INVALID_ELEMENT_FROM_READ`](https://github.com/broadinstitute/gatk/blob/946f39/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ReadPosRankSumTest.java#L62) which is [defined as `Double.NEGATIVE_INFINITY`](https://github.com/broadinstitute/gatk/blob/946f39/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RankSumTest.java#L23). According to the [java documentation](https://docs.oracle.com/javase/specs/jls/se7/html/jls-5.html#jls-5.1.3), casting NEGATIVE_INFINITY to int will result in a value of `INT_MIN`. (Disclaimer: I haven't tested this, so it may be completely wrong...). #### Steps to reproduce; See attached .zip file which includes a smallish bam file that shows the problem. I ran mutect2 on it in the Docker container for the latest GATK release:; ```sh; unzip mpos_issue.zip; cd mpos_issue; ../gatk Mutect2 --input input/small.bam --reference input/small.fa --output small.vcf; grep MPOS=- small.vcf; ```. #### Expected behavior; `MPOS` should have a se",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6342:104,release,release,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6342,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] Latest public release version: 4.0.11.0; - [ ] Latest master branch as of [date of test?]. ### Description ; The output vcf for a few samples looks like this:. ```; chrM 151 . CT TC . PASS DP=3420;ECNT=23;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=14304.21 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB:SB 0/1:27,3242:0.992:3269:13,1545:14,1697:30,30:317,335:60:26:0:0.990,0.990,0.992:0.045,0.015,0.940:9,18,1541,1701; chrM 152 . T C . chimeric_original_alignment DP=3358;ECNT=23;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=46.40 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:PGT:PID:POTENTIAL_POLYMORPHIC_NUMT:SA_MAP_AF:SA_POST_PROB:SB 0/1:250,25:0.099:275:119,13:131,12:30,30:336,317:60:29:25:0|1:8660_C_T:true:0.091,0.061,0.091:2.722e-03,0.035,0.962:112,138,7,18; ```. ```; chrM 151 . CT TC . PASS DP=1867;ECNT=20;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=6145.34 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB:SB 0/1:13,1792:0.993:1805:7,879:6,913:30,30:441,442:60:39:0:0.990,0.990,0.993:0.026,0.024,0.950:5,8,745,1047; chrM 152 . T C . PASS DP=1847;ECNT=20;POP_AF=4.000e-03;P_CONTAM=0.00;TLOD=12.96 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB:SB 0/1:0,1755:0.999:1755:0,862:0,893:0,30:0,442:60:39:6:0.990,0.990,1.00:0.027,0.027,0.946:0,0,726,1029; ```. Note that site 152 is a T->C that is also captured in the MNP at site 151 CT->TC. In one case site 152 is filtered, but in the other it passes, but in both cases the MNP passes. . #### Steps to reproduce; @klaricch Could you please post the input BAMs into the Mutect task as well as the output VCFs from that task? Could you also post the ""script"" generated by Cromwell that will show what command Cromwell actually ran at this point? Thanks!. #### Expected behavior; I'm not sure what should happen in this case, but the two o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5513:104,release,release,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5513,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); 4.4.0.0. ### Description ; Hello GATK team,. I am working on a pipeline that calls variants using Mutect2 and uses those mutations for somatic variant calling in other samples. To achieve this I'm using the -allele flag in Mutect2 to pass the desired positions to genotype to Mutect2. However it appears that Mutect2 will output variants that it then is unable to parse as an -allele file. I believe this is because the REF column is too long. #### Steps to reproduce; Try to variant call using an -allele VCF containing this line:. ```; 5	283041	.	AGAAGACTCGGGGAGGAGCTGAGGTTCTAGTTTGAGGGTCGTGCACCTGGAGAACTGGACAGGAGCTGATGTTCTAGATTGAGCATCGTACAGCTGAAGACTTGGGGAGGAGCTTATGTTGTTCACTTTGAGGGTCTTTCAGCTGGAGACTCAGGCAGGAGCTGATGTTCTAGTTTGAGGATCTCGTAGCTGCAGAATCAGAGAGGAGCTGATGTTCTAGATTGAGGATCTTGTAGCTACAGACCCATAGAGGAGCTGATGATCTAGATTCAGGGTCATGCAGCT	A	.	.	AS_SB_TABLE=57,54|8,7;DP=126;ECNT=1;MBQ=30,31;MFRL=358,237;MMQ=60,60;MPOS=15;POPAF=7.30;TLOD=3.22	GT:AD:AF:DP:F1R2:F2R1:FAD:SB	0/1:111,15:0.028:126:17,1:18,5:86,9:57,54,8,7; ```. #### Expected behavior; Mutect2 should either not emit an invalid variant or it should be able to parse it; #### Actual behavior. ```; java.lang.ArrayIndexOutOfBoundsException: arraycopy: source index -152 out of bounds for byte[278]; 	at java.base/java.lang.System.arraycopy(Native Method); 	at java.base/java.util.Arrays.copyOfRange(Arrays.java:3823); 	at org.broadinstitute.hellbender.tools.walkers.annotator.TandemRepeat.getNumTandemRepeatUnits(TandemRepeat.java:54); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyRegionTrimmer.trim(AssemblyRegionTrimmer.java:189); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:273); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:304); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8699:147,pipeline,pipeline,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8699,1,['pipeline'],['pipeline']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); N/A. ### Affected version(s); - [ x] Latest public release version [4.5.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Dockerfile does not create unprivileged user account. #### Steps to reproduce; * git clone https://github.com/broadinstitute/gatk.git; * cd gatk; * git checkout 4.5.0.0; * docker build -t gatk .; * docker run ... #### Expected behavior; I'd expect the user to be in an unprivileged account in `/home/gatk` when the container is started. If there is a use case for enabling root (say for allowing system installs) this should be an option (config or a separate Dockerfile). #### Actual behavior; On `docker run` the user is root under `/gatk`. A container should not put the user in a root account upon startup. This is especially so in shared computing environments. I attempted to create a ""gatk"" account with `RUN useradd -d /home/gatk -ms /bin/bash gatk` (etc) in the Dockerfile but I get `Permission denied: '/root/.config/conda/.condarc'.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8856:101,release,release,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8856,2,"['install', 'release']","['installs', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); PlotModeledSegments. ### Affected version(s); Confirmed with 4.1.3, but also any version before this that uses optparse. ### Description ; The R package `optparse` is used in the R script `PlotModeledSegments.R` for plotting. The latest version of `optparse` (1.6.4) has been updated to include a check that the short-name of an option is only 1 character. ; See here: https://github.com/trevorld/r-optparse/commit/66acec58645f7401fc365bb769a72751671c2114; The `PlotModeledSegments.R` script in gatk has this line:; ```make_option(c(""--sample_name"", ""-sample_name""), dest=""sample_name"", action=""store""),```; which will now make `optparse` throw an error. #### Steps to reproduce; Install `optparse 1.6.4` and run `gatk PlotModeledSegments`. #### Expected behavior; The Rscript should parse the inputs and run. #### Actual behavior; PlotModeledSegments.R gives an error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6207:326,update,updated,326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6207,2,"['Install', 'update']","['Install', 'updated']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); PostprocessGermlineCNVCalls . ### Affected version(s); - [X ] Latest public release version [4.3.0.0]. ### Description ; Hello, I am a regular user of the gCNV pipeline of GATK4. Since version GATK 4.2.0.0, you have introduced the germline CNV calling joint which I wanted to try and I encountered several problems. So I used, in order, the DetermineGermlineContigPloidy and GermlineCNVCaller tools (cutting my target into 8 bins) version 4.3.0.0 on a cohort of 540 patients. Then I used the PostProcessGermlineCaller tool to produce the VCF files for these patients. Next, I used the JointGermlineCNVSegmentation beta tool to produce a multisample VCF which I reused with PostProcessGermlineCaller to produce joined VCFs. The problem is that the time needed to produce each VCF file has been multiplied by 20 (on average 120 minutes compared to 6), which makes it difficult to use on large cohorts. Here is an extract of the logs, from a sample without, then with the --clustered-breakpoints option: ; #PostprocessGermlineCNVCalls. 14:23:53.500 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:23:54.242 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:23:54.242 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.3.0.0; 14:23:54.242 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Executing as [tintest@dahu132.u-ga.fr](mailto:tintest@dahu132.u-ga.fr) on Linux v5.10.0-18-amd64 amd64; 14:23:54.262 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:23:54.263 INFO PostprocessGermlineCNVCalls - Start Date/Time: December 2, 2022 2:23:53 PM GMT; 14:23:54.263 INFO PostprocessGermlineCNVCall",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8183:126,release,release,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8183,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); ReadsPipelineSpark (HaplotypeCallerSpark) when running over a spark cluster. ### Affected version(s); - [x] Latest public release version [GATK v4.1.8.1]. ### Description . #### Tools used:; latest docker image from broadinstitute/gatk; latest hadoop (3.3.0); spark 2.3.1 without hadoop which is able to use the custom hadoop setup. #### Steps to reproduce. **Script run:**; ```; #!/bin/bash. export HADOOP_CONF_DIR=/etc/hadoop; export HADOOP_HOME=/mnt/hadoop-latest; export JAVA_HOME=/mnt/jre1.8.0_192; export SPARK_HOME=/mnt/spark-2.3.1-bin-without-hadoop; export HADOOP_USER_NAME=hadoop. # export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath). TEST_DIR=""hdfs://cromwellhadooptest:8020/user/hadoop/gatk/small""; COMMON_DIR=""hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common""; INPUT_DIR=""$TEST_DIR/input""; OUTPUT_DIR=""$TEST_DIR/output"". input_bam=""$INPUT_DIR/small_CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam""; output_vcf_basename=""$OUTPUT_DIR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21"". ref_fasta=""$COMMON_DIR/human_g1k_v37.20.21.fasta""; known_sites=""$COMMON_DIR/dbsnp_138.b37.20.21.vcf"". gatk ReadsPipelineSpark \; -R ${ref_fasta} \; -I ${input_bam} \; -O ${output_vcf_basename}.vcf \; --known-sites ${known_sites} \; -pairHMM AVX_LOGLESS_CACHING \; --spark-verbosity DEBUG \; -- --spark-runner SPARK --spark-master yarn-cluster \; # --conf 'spark.submit.deployMode=cluster'; ```. #### Expected behavior. ReadsPipelineSpark should be able to resolve the hdfs file path: `hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`. #### Actual behavior; The tool tries to access: `file:///user/hadoop/gatk/common/human_g1k_v37.20.21.fasta` even when the input is: `hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`. Verified that the file is accesible through hdfs:; ```; (gatk) root@2e738717b9c1:/gatk/mnt# $HADOOP_HOME/bin/hdfs dfs -ls hdfs://cromwellhadooptest:8020/user/hadoop/gatk/co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730:172,release,release,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es); ReadsPipelineSpark. ### Affected version(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinsti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5644:115,release,release,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644,2,"['pipeline', 'release']","['pipelines', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); Reblock | JointGenotype. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; We found a bug while running the latest JointGenotype pipeline (2.0.2). We are working with Dragen data (version 3.6.3); The error:. <details><summary>OPEN ERROR HERE</summary>; <p>. + gatk --java-options -Xms8g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-SplitIntervalList/glob-d928cd0f5fb17b6bd5e635f48c18ccfb/0073-scattered.interval_list --sample-name-map sample_name_map --reader-threads 5 --merge-input-intervals --consolidate; --; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/tmp/scratch/cromwell-dragen-us-west-2/cromwell-execution/GatkJointGenotyping/7dd18ebe-29ca-47b1-b71a-56b99c362789/call-ImportGVCFs/shard-73/tmp.9a65c1fc; 18:46:55.750 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 01, 2021 6:46:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:46:55.894 INFO GenomicsDBImport - ------------------------------------------------------------; 18:46:55.894 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.3.0; 18:46:55.895 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:46:55.895 INFO GenomicsDBImport - Executing as root@ip-10-10-156-13.us-west-2.compute.internal on Linux v4.14.243-185.433.amzn2.x86_64 amd64; 18:46:55.895 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 18:46:55.895 INFO GenomicsDBImport - Start Date/Time: December 1, 2021 6:46:55 PM GMT; 18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7589:120,release,release,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7589,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF (& how it interacts with GenotypeGVCFs). ### Affected version(s); - [x] 4.2.6.1 ; - [ ] Latest public release version (haven't tried yet, but doesn't look like ReblockGVCFs has been changed substantially since 4.2.6.1); - [ ] Latest master branch (haven't tried yet, but doesn't look like ReblockGVCFs has been changed substantially since 4.2.6.1). ### Description . Our samples run with ReblockGVCF appear to have unusually high numbers of variants with missing / not called genotypes (`./.`). These variants seem to have coverages that are good enough to successfully call variants  and, genotypes are called at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isnt giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but were seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8208:164,release,release,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208,3,"['pipeline', 'release']","['pipeline', 'release', 'releases']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x ] Latest public release version [version?] _**GATK 4.2.6.1**_; - [ ] Latest master branch as of [date of test?]. ### Description ; We ran ReblockGVCF in 549 samples with the newest GATK (4.2.6.1). 8 of them returned the error similar to the message below . `org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrM:1 [VC /tmp/scratch/prs-sabe-files/GRAR/2031812880_AJ.hard-filtered.gvcf.gz @ chrM:1 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=1} GT=[[2031812880_AJ G*/G* DP 1691 AD 112,1579 {MIN_DP=1691, SQ=0}]] filters=weak_evidence`. and right below, we could find in all of them; `Caused by: org.broadinstitute.hellbender.exceptions.UserException$BadInput: Bad input: Homozygous reference genotypes must contain GQ or PL. Both are missing for hom ref genotype at chrM:1`. All the ""failed samples"" produced a broken output, in this case, missing the chrM (and the alt chr, such as HLA, chr1_alt etc)... It was weird because on WDL it returned as **_Success_** job... We need all the samples with a proper output to run the JointGenotype pipeline with the Reblocked Dragen samples output. #### Steps to reproduce; I'll share with you the chrM:1 from GVCF from a sample with no error; `chrM	1	.	G	<NON_REF>	.	PASS	END=72	GT:AD:DP:GQ:MIN_DP:PL:SPL:ICNT	0/0:2441,2:2443:99:1613:0,120,1800:0,255,255:40,13`. And now, the chrM:1 from a sample with the error; `chrM	1	.	G	<NON_REF>	.	weak_evidence	END=1	GT:AD:DP:SQ:MIN_DP	0/0:112,1579:1691:0:1691`. #### Expected behavior; No broken output. #### Actual behavior; Failing in a few samples, breaking the expected output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7797:109,release,release,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF. ### Affected version(s); - [x] Latest public release version [4.2.6.1]; - [x] Latest master branch (probably). ### Description ; First position on a contig can be missing if that position is low quality in the input GVCF. #### Steps to reproduce; Run Reblock GVCF with the following parameters: --floor-blocks true --gvcf-gq-bands 20 --gvcf-gq-bands 30 --gvcf-gq-bands 40 --do-qual-score-approximation true --variant $inputVC -R $hg38. where inputVC contains; chr13	18173860	.	A	C,<NON_REF>	0	.	AS_RAW_BaseQRankSum=|-4.9,1|NaN;AS_RAW_MQ=51256.00|4709.00|0.00;AS_RAW_MQRankSum=|0.5,1|NaN;AS_RAW_ReadPosRankSum=|1.2,1|NaN;AS_SB_TABLE=23,2|1,1|0,0;BaseQRankSum=-4.896;DP=28;ExcessHet=0.0000;MLEAC=0,0;MLEAF=0.00,0.00;MQRankSum=0.564;RAW_MQandDP=59565,28;ReadPosRankSum=1.252	GT:AD:DP:GQ:PL:SB	0/0:25,2,0:27:61:0,61,946,75,951,965:23,2,1,1; appears to be dropped in output. Full input GVCF at gs://broad-dsde-methods-gauthier/reblocking-bug/. #### Expected behavior; QUAL 0 VC should be replaced with a GQ0 reference block. #### Actual behavior; Output GVCF is missing position chr13:18173860 and fails ValidateVCF task",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7884:108,release,release,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7884,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); SAMSequenceDictionary function in IndexUtils.java: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The SAMSequenceDictionary function always logs a warning when a file is passed in. Needs an if statement that validates whether or not the file is actually a sequence dictionary before logging the warning. #### Steps to reproduce; Run GATK's HaplotypeCaller with a --dbsnp option set, or just pass a sequence dictionary into the SAMSequenceDictionary function directly. #### Expected behavior; Should use the --dbsnp file that I pass in if valid, rather than log a warning and creating a separate sequence dictionary. #### Actual behavior; Logs a warning and instead tries to create a new sequence dictionary based on the index file it finds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5692:264,release,release,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5692,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); SV type inference pipeline. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 2018-06-26. ### Description ; Though we've never seen this case with `bwa mem`, it is theoretically possible&mdash;and happened in an experimental (mis-) run of another aligner&mdash; that a query sequence generates two alignment records, where the two alignments's overlap on the read and on the reference are of the same length. See illustration below.; ```; --------------------------sssssssssssssss READ ALN1; ssssssssssssssss------------------------- READ ALN2; | |; | |; | |; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ REF; ```. This scenario will trigger our code to resolve a complicated tandem duplication structure. ; But the code should check for such case, or better yet two alignments should really be stitched together. #### Steps to reproduce; Run the code with some funny alignments. #### Expected behavior; Two alignments should be merged into one, by the type inference pipeline. #### Actual behavior; Exception would be thrown currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4951:68,pipeline,pipeline,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4951,3,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); SelectVariants. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of Dec 14, 2020. ### Description ; I believe SelectVariants doesn't handle multi-allelic sites, which have more than one value per INFO field. The problem seems to be in coercing values like 0.00022456 and 2.496e-05 (note the decimal and scientific notation) into doubles, which happens in the apache commons code. But the problem is not limited to decimal valuesit fails to coerce integers like 0 and 9 (see below). . #### Steps to reproduce; gnomad=gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz; java -jar $gatkjar SelectVariants -V $gnomad -select ""AF > 0.05"" -O af-only-gnomad.hg38.contamination.vcf.gz. This one fails with `java.lang.ArithmeticException: Double coercion: java.util.ArrayList:([0.0002246, 2.496e-05])`. Also fails with the expression ""AC > 3"":. javadebug -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar $gatkjar SelectVariants -V $gnomad -select ""AC > 3"" -O af-only-gnomad.hg38.contamination.vcf.gz. `java.lang.ArithmeticException: Long coercion: java.util.ArrayList:([9, 1])`. #### Expected behavior; The tool should run to completion. #### Actual behavior; It doesn't.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6998:111,release,release,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6998,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); SelectVariants. ### Affected version(s); - [x] Latest public release version [4.5.0.0]. ### Description ; When trying to stream a reference file from a public URL, there is trouble interpreting the path when the file ends with `.fa.gz`, especially in finding the index. . #### Steps to reproduce; This was discovered trying to debug another Picard issue. To reproduce, run this command:; ```; gatk SelectVariants -L chr17:22477226-22477227 -V https://jmorp.megabank.tohoku.ac.jp/datasets/tommo-54kjpn-20230626-af_snvindelall/files/tommo-54kjpn-20230626r3-GRCh38-af-autosome.vcf.gz -R https://jmorp.megabank.tohoku.ac.jp/datasets/tommo-jg2.1.0-20211208/files/jg2.1.0.fa.gz -O subset.vcf.gz; ```; Here we try to stream a small region from a VCF using a public reference file. . #### Expected behavior; The file `subset.vcf.gz` should be written with just the regions given. #### Actual behavior; You get a stacktrace:; ```; org.broadinstitute.http.nio.HttpPath$CantDealWithThisException: Attempting to resolve this against a path which is relatve but looks like it has a scheme.; This: https://jmorp.megabank.tohoku.ac.jp/datasets/tommo-jg2.1.0-20211208/files; Other: https:/jmorp.megabank.tohoku.ac.jp/jg2.1.0.fa.gz.fai; Other interpretted as URI: https:/jmorp.megabank.tohoku.ac.jp/jg2.1.0.fa.gz.fai; This is a limitatation of the current implementation of resolve.; Please use choose a less horrible file name or get in touch with the developers to complain.; 	at org.broadinstitute.http.nio.HttpPath.resolve(HttpPath.java:381); 	at org.broadinstitute.http.nio.HttpPath.resolve(HttpPath.java:53); 	at java.base/java.nio.file.Path.resolveSibling(Path.java:549); 	at org.broadinstitute.http.nio.HttpPath.resolveSibling(HttpPath.java:418); 	at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getFastaIndexFileName(ReferenceSequenceFileFactory.java:262); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.checkFastaPath",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8751:111,release,release,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8751,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); StructuralVariationDiscoveryPipelineSpark. ### Affected version(s); GATK 4.1.0.0. ### Description . Running SV program generates a Java exception...; java.lang.IllegalArgumentException: provided start is negative: -1. #### Steps to reproduce; ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv//$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 80G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:733,deploy,deploy-mode,733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['deploy'],['deploy-mode']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684:1594,update,update,1594,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684,1,['update'],['update']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); TransferReadTags. ### Affected version(s); - [X] Latest public release version [gatk-4.3.0.0]. ### Description ; When traversing the reads in both aligned (target) and unaligned (the one with the desired tag) BAMs an error is thrown complaining about a read `found in the aligned bam is not found in the unmapped bam`. However the reads exists. It looks like the `traverse` function that uses the lexicographic order difference between both query names will find a _negative_ `diff` and assume that the read in the aligned BAM is missing in the uBAM. However, with Illumina read headers it seems almost guaranteed that this is going to be an issue since the y-coord (the last colon-separated field in the header) often has numbers with different number of digits. The lexicographical comparison will fail to adjust when comparing two read names where the length of the read in the target BAM is larger than the length of the read in the uBAM. . This is the `traverse` function that throws the error:; https://github.com/broadinstitute/gatk/blob/2b0a558fdb9fdf654e796d5d69a092e26345583b/src/main/java/org/broadinstitute/hellbender/tools/walkers/qc/TransferReadTags.java#L109-L145 . #### Steps to reproduce; Run `TransferReadTags` with an Illumina sequenced aligned BAM. I can provide dummy files if needed, but should be easy to reproduce. The following example should help illustrate the issue:. ```sh; $ /data/reddylab/software/gatk/gatk-4.3.0.0/gatk TransferReadTags \; --output /data/reddylab/Alex/tmp/TEST_BAM.with_umis.bam \; --read-tags RX \; --unmapped-sam /data/reddylab/Alex/tmp/TEST_BAM.umi.nsorted.ubam \; --input /data/reddylab/Alex/tmp/TEST_BAM.nsorted.bam; ```. Produces the following output:; ```; Using GATK jar /gpfs/fs1/data/reddylab/software/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8147:113,release,release,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8147,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); UpdateVCFSequenceDictionary. ### Affected version(s); - \[x] Latest master branch as of 8/3/18. ### Description ; I am running UpdateVCFSequenceDictionary on a vcf which should have a hg38 header but which (for unrelated unpleasant reasons) instead has a hg19 header. Using an hg38 dictionary as source dict to try to fix the header. If I ask to output a .vcf file, everything works fine. If I ask to output a .vcf.gz file, gatk crashes with; ```; java.lang.ArrayIndexOutOfBoundsException: 12922; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:89); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_as",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5087:50,Update,UpdateVCFSequenceDictionary,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087,2,['Update'],['UpdateVCFSequenceDictionary']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); VariantAnnotator -A ReferenceBases -A TandemRepeat. ### Affected version(s); - [x] Latest public release version [4.1.3]; - [ ] Latest master branch as of [date of test?]. ### Description ; VariantAnnotator doesn't seem to correctly generate annotations that are based off the reference. I've tried on a few different VCFs and with different genome builds, and get the same result every time. For `ReferenceBases` it seems to generate a string for each variant that is the ref allele, followed by `20-len(ref) * N`. E.g.:. ```; 1 118617 rs372912307 T C 50 PASS REF_BASES=TNNNNNNNNNNNNNNNNNNNN; 1 567239 rs78150957 CG C 50 PASS REF_BASES=CGNNNNNNNNNNNNNNNNNNN; ```. The STR annotations get their header lines added to the header, but not a single variant is flagged as an STR. I've tried processing the GIAB VCFs for NA12878 and NA24385 with the same results - even obvious STR variants are not flagged. I suspect this is related to the fact that REF_BASES isn't compute properly. #### Steps to reproduce; Take any decent size VCF without the above annotations (e.g. GIAB VCFs) and run something like:. gatk VariantAnnotator -V NA12878.vcf.gz -O NA12878.ann.vcf.gz -A TandemRepeat -A ReferenceBases -R hg38.fa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6095:147,release,release,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6095,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); VariantAnnotator. ### Affected version(s); - [x] Latest public release version 4.5.0.0; - [ ] Latest master branch as of [date of test?]. ### Description ; ```; Using GATK jar /directory_masked/programs/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /directory_masked/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar VariantAnnotator -I ../test.bam -V test.vcf -O test_2.vcf --reference /directory_masked/refs/hg19/ucsc.hg19.fasta --enable-all-annotations true -jdk-deflater true -jdk-inflater true; 14:02:45.344 INFO VariantAnnotator - ------------------------------------------------------------; 14:02:45.346 INFO VariantAnnotator - The Genome Analysis Toolkit (GATK) v4.5.0.0; 14:02:45.346 INFO VariantAnnotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:02:45.346 INFO VariantAnnotator - Executing as username@hostname.local on Mac OS X v14.2 aarch64; 14:02:45.346 INFO VariantAnnotator - Java runtime: OpenJDK 64-Bit Server VM v17.0.10+0; 14:02:45.346 INFO VariantAnnotator - Start Date/Time: April 30, 2024 at 2:02:45 PM HKT; 14:02:45.346 INFO VariantAnnotator - ------------------------------------------------------------; 14:02:45.346 INFO VariantAnnotator - ------------------------------------------------------------; 14:02:45.347 INFO VariantAnnotator - HTSJDK Version: 4.1.0; 14:02:45.347 INFO VariantAnnotator - Picard Version: 3.1.1; 14:02:45.347 INFO VariantAnnotator - Built for Spark Version: 3.5.0; 14:02:45.348 INFO VariantAnnotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:02:45.348 INFO VariantAnnotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:02:45.348 INFO VariantAnnotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:02:45.348 INFO V",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8800:113,release,release,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8800,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); VariantAnnotator. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . Throws an exception on a legal variant. java.lang.IllegalStateException: Allele in genotype G not in the variant context [G*, G, GT]; 	at htsjdk.variant.variantcontext.VariantContext$Validation.validateGenotypes(VariantContext.java:382); 	at htsjdk.variant.variantcontext.VariantContext$Validation.access$200(VariantContext.java:323); 	at htsjdk.variant.variantcontext.VariantContext$Validation$2.validate(VariantContext.java:331); 	at htsjdk.variant.variantcontext.VariantContext.lambda$validate$0(VariantContext.java:1384); 	at java.lang.Iterable.forEach(Iterable.java:75); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1384); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:489); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:647); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1329); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.trimAlleles(GATKVariantContextUtils.java:1285); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.getMinRepresentationBiallelics(VariantAnnotatorEngine.java:499); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateExpressions(VariantAnnotatorEngine.java:440); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:285); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotator.apply(VariantAnnotator.java:230); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689:113,release,release,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); - [X] Latest public release version [4.5.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; As of v1.3.0 the `scales` R package turns the use of deprecated values for the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] 1.3.0; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ; 1. base::source(""/path/to/rscript.r""); 2.  base::withVisible(eval(ei, envir)); 3.  base::eval(ei, envir); 4.  base::eval(ei, envir); 5. ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ggplot2::continuous_scale(...); 7.  ggplot2::ggproto(...); 8.  rlang::list2(...); 9. scales::seq_gradient_pal(low, high, space); 10. scales::pal_gradient_n(c(low, high), space = space); 11. lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. lifecycle:::deprecate_stop0(msg); 13. rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8664:116,release,release,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664,2,"['install', 'release']","['install', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); GATK 4.2.0.0 . ### Description . When running VariantRecalibrator on a joint-called gVCF with 2000 samples, the following java.lang.IllegalStateException occurs: **Gaussian mean vector does not have the same size as the list of annotations**. ```; 17:56:38.072 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 28, 2021 5:56:38 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:56:38.485 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.487 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:56:38.487 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:56:38.488 INFO VariantRecalibrator - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 17:56:38.488 INFO VariantRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 17:56:38.488 INFO VariantRecalibrator - Start Date/Time: July 28, 2021 5:56:38 PM EDT; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.490 INFO VariantRecalibrator - HTSJDK Version: 2.24.0; 17:56:38.491 INFO VariantRecalibrator - Picard Version: 2.25.0; 17:56:38.491 INFO VariantRecalibrator - Built for Spark Version: 2.4.5; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRIT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:454,install,install,454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['install'],['install']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _FilterAlignmentArtifacts_. ### Affected version(s); - [x] Latest public release version [4.1.4.1]. ### Description ; FilterAlignmentArtifacts consistently errors out with segmentation faults or IllegalArgumentExceptions. I've attached the log files for each of these errors below.; [invalid_interval.log](https://github.com/broadinstitute/gatk/files/4017907/invalid_interval.log); [seg_fault.log](https://github.com/broadinstitute/gatk/files/4017908/seg_fault.log). #### Steps to reproduce; The command to reproduce both errors is the same, and I have attached it below.; [realignment_filter.txt](https://github.com/broadinstitute/gatk/files/4017918/realignment_filter.txt); The BWA mem index I'm using is hg38, however the BAM that I am realigning from is hg19; thus, the reference argument is the hg19 fasta. I am happy to transfer zip files containing the other files need to reproduce this. Just let me know where to send them. #### Expected behavior; _FilterAlignmentArtifacts_. #### Actual behavior; _Errors_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6344:123,release,release,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6344,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_, _GencodeFuncotationFactory::createUtrFuncotation_. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of [20181019]. ### Description ; When determining whether a variant in a 5' UTR is a `DE_NOVO_START_IN_FRAME` or `DE_NOVO_START_OUT_FRAME`, Funcotator only checks whether the new start codon is in frame with the end of the current UTR (the UTR in which the variant occurs). Funcotator should account for the case that there are multiple 5' UTRs. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; All UTRs should be considered for whether or not the new start codon is in frame. #### Actual behavior; Only the UTR in which the variant occurs is considered for whether the new start codon is in frame.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5333:160,release,release,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5333,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_, _GencodeFuncotationFactory_. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The transcript position in Funcotator is always being populated as a single integer value. While this is correct for SNPs, it should be populated as a range - `<START_POS>_<END_POS>` for events spanning more than 1 base.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5375:138,release,release,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5375,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_. ### Affected version(s); - [ ] Latest public release version GATK 4.1.9.0 with data version funcotator_dataSources.v1.7.20200521g. ### Description . #### Steps to reproduce. I'm trying to run GATK Funcotator using the funcotator_dataSources.v1.7.20200521g data download. The command line that I'm using is:; ```; gatk Funcotator \; --variant cohort.vcf.gz \; --reference GRCh38.d1.vd1/GRCh38.d1.vd1.fa \; --ref-version hg38 \; --data-sources-path funcotator_dataSources.v1.7.20200521g \; --output cohort.funcotator.vcf.gz \; --output-file-format VCF; ```. If I run that command line without the `gnomad_*.tar.gz`'s expanded, it works fine and annotates my `cohort.vcf.gz` into `cohort.funcotator.vcf.gz`. . Following the directions at [Funcotator Information and Tutorial - 1.1.2.2.1: enabling gnomAD](https://gatk.broadinstitute.org/hc/en-us/articles/360035889931-Funcotator-Information-and-Tutorial#1.1.2.2.1), if I expand both `gnomAD_exome.tar.gz` and `gnomAD_genome.tar.gz`, funcotator dies at startup with a `400 Bad Request` error. This also happens if I expand either one of the `gnomad_*.tar.gz` files individually. . #### Expected behavior; Funcotator annotates my VCF and includes gnomAD annotations in the output VCF. . #### Actual behavior. Crash with 400 Bad Request:. ```; Using GATK jar /opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar Funcotator --variant cohort.vcf.gz --reference GRCh38.d1.vd1/GRCh38.d1.vd1.fa --ref-version hg38 --data-sources-path funcotator_dataSources.v1.7.20200521g --output cohort.funcotator.vcf.gz --output-file-format VCF; 14:24:33.589 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/share/gatk4-4.1.9.0-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926:109,release,release,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_. ### Affected version(s); - [x] Latest public release version [version v4.1.4.1]; - [ ] Latest master branch as of [date of test?]. ### Description . Hi @jonn-smith , I saw you often address Funcotator related issues, so I thought this might be of interest to you. I ran funcotator on a vcf created by mutect2 from RNA-seq data. The vcf includes a large deletion in the GABARAP gene, and when Funcotator processes this annotation, it dies with an error about a query that extends past the end of a contig:. > htsjdk.samtools.SAMException: Query asks for data past end of contig. Query contig ENST00000571253.1|ENS; G00000170296.9|OTTHUMG00000102156.3|OTTHUMT00000440082.2|AC120057.8-003|GABARAP|837|UTR5:1-753|CDS:754-8; 37| start:1 stop:895 contigLength:837; at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(Ca; chingIndexedFastaSequenceFile.java:316); at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource; .java:78); at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource; .java:64); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.; getFivePrimeUtrSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:744); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createUtrFuncotation(GencodeFuncotationFactory.java:1568); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnSingleTranscript(GencodeFuncotationFactory.java:983); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:805); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:78",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6345:109,release,release,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6345,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Funcotator_. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; When annotating a VCF, if the VCF already contains funcotations Funcotator will add a new funcotator line to the header. This will cause the parser to fail because it will not be able to get the correct line from the header. This is a bit of a pathological case (I can't currently see a good reason to funcotate a VCF twice), but since this behavior is valid it should be accounted for. The primary issue is how to resolve the two funcotation sets. Ideally we would leave them both in and somehow version them (to preserve all the information). Alternatively we can append to the existing funcotation list. This second method will likely involve a lot of work and probably isn't worth it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5368:109,release,release,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5368,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _GencodeFuncotationFactory_. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; When a trouble transcript comes up for an allele pairl, `GencodeFuncotationFactory::createFuncotationsHelper` does not create a default annotation for the allele pair. This will cause the parsing of funcotations to fail because not all the alleles are represented in the funcotation list. See the `todo` in `GencodeFuncotationFactory::createFuncotationsHelper`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5366:124,release,release,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5366,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Mutect2_. ### Affected version(s); - GATK 4.1.4.1. ### Description ; When running Mutect2 (from GATK v4.1.4.1) using the following command:. `gatk Mutect2 -R [path to grch37-1kg.fa] -I testcase.bam -O pon.vcf`. to create a PoN on NovaSeq WGS-data processed through the best practice pipeline (with the BQSR-steps run through the Spark-enabled tools, and bwa mem with -Y flag) I get the following error in multiple regions:. [Stacktrace](https://www.dropbox.com/s/d2n5zflj9u11oj8/stacktrace.png?dl=0). AFAIK this is related to the new code path introduced in #6240 and seem to be triggered when there are more than 2 reads supporting a fragment but all of them are either duplicate reads or supplemntary/secondary alignments. Any input is greatly appreciated. I guess a temporary fix is to use the --independent-mates flag (although haven't tried it yet -- how much worse mutation calling performance do one incur when using that flag?). #### Steps to reproduce; Use the following small test case .bam-file as input to the command specified above:. [Testcase](https://www.dropbox.com/s/hilcj3aj0jnjdmh/testcase.bam?dl=0). #### Expected behavior; Completion of mutect2 without Exception. #### Actual behavior; Early termination of the mutect2 run due to raising an exception when trying to create a fragment with no read data to back it up. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6310:334,pipeline,pipeline,334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6310,1,['pipeline'],['pipeline']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.1.7.0. ### Description . The following error message is output:. A USER ERROR has occurred: Bad input: Is the input a file of segment variant contexts? Variant context does not represent a copy number segment: [VC null @ 6:4130448-4130544 Q. of type=SYMBOLIC alleles=[C*, <DEL>] attr={END=4130544, Num_Probes=1, Segment_Call=-, Segment_Mean=-30.018694} GT=[] filters=. The local info in the segment file is:; 5 176563624 180687750 618 -0.053122 0; 6 203183 4128317 205 0.046724 0; 6 4130448 4130544 1 -30.018694 -; 6 4130545 6168103 42 -0.085445 0; 6 6174562 17463556 490 0.022415 0; 6 17493361 25510885 347 0.080520 0. This is a bad error message. The minimum size for a segment to be processed is 150 bases and that variant is only 96 bases, so it's failing that validation. #### Expected behavior; Should process variant without producing error. Hat tip: @jonn-smith for figuring out the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6575:138,release,release,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6575,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version 4.2.2.0. ### Description . Running apt-get inside docker image fails. #### Steps to reproduce. (base) fleharty@wm3b9-dfa docker % docker run -it broadinstitute/gatk:4.2.2.0; Unable to find image 'broadinstitute/gatk:4.2.2.0' locally; 4.2.2.0: Pulling from broadinstitute/gatk; a7fe112a8303: Already exists ; Digest: sha256:32175c3c7c1fb9f5bd6650183c9c5cf26fb822dddb0cad0123d48c33124b6065; Status: Downloaded newer image for broadinstitute/gatk:4.2.2.0; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# ; (gatk) root@bc90fdaf700c:/gatk# apt-get update; Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB] ; Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB] ; Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB] ; Get:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:138,release,release,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,2,"['release', 'update']","['release', 'update']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/CalculateContamination.java; /gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/ContaminationModel.java. ### Affected version(s); - [x] Latest public release version - v4.2.0.0 (also detected on previous versions) ; - [x] Latest master branch as of 03/30/2021. ### Description ; **ContaminationModel**; **Problem:**; Where errorDepth is greater than oppositeDepth, the output contamination is reported as **0 contamination** , which can be misinterpreted by the end user. calculateContaminationFromHoms receives the list of pileups PileupSummary; It iterates from 0.4 INITIAL_MAF_THRESHOLD down to zero. In each iteration pileups are selected using multiple, different strategies.; When the stdError exit condition is met (i.e., stdError < (contamination* MIN_RELATIVE_ERROR +MIN_ABSOLUTE_ERROR)), it reports out the contamination and stdError values. The issue is that this stdError exit condition is also met when contamination = 0, because in this case, stdError is also equal to 0, and thus is always less than the minimum value for (contamination * MIN_RELATIVE_ERROR [0.2] + MIN_ABSOLUTE_ERROR [0.001]), which cannot be less than 0.001. . final double stdError = homs.isEmpty() ? 1 : Math.sqrt(homs.stream().mapToDouble(ps -> {; final double d = ps.getTotalCount();; final double f = 1 - oppositeAlleleFrequency.applyAsDouble(ps);; return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);; }).sum()) / totalDepthWeightedByOppositeFrequency;. ** return (1 - f) * d * contamination * ((1 - contamination) + f * d * contamination);**. Root cause:; At the first MAF iteration where errorDepth is greater than oppositeDepth, contamination is set to 0 (according to the code logic shown below), the function exits the iteration process, and no further MAF thresholds are t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7177:348,release,release,348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7177,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. FilterAlignmentArtifacts. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. 4.3.0.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. ```; Using GATK jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -XX:+UseNUMA -jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar FilterAlignmentArtifacts -R /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.gz -O WGS-NA12878.FilterAlignmentArtifacts.vcf --tmp-dir . -V WGS-NA12878.filtered.vcf -I WGS-NA12878.sorted.dedup.recal.bam --bwa-mem-index-image /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.img; 11:24:09.761 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:24:09.942 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.942 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.3.0.0; 11:24:09.943 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:24:09.943 INFO FilterAlignmentArtifacts - Executing as root@D52BV-2U on Linux v4.15.0-202-generic amd64; 11:24:09.943 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_352-8u352-ga-1~18.04-b08; 11:24:09.943 INFO FilterAlignmentArtifacts - Start Date/Time: February 24, 2023 11:24:09 AM CST; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO Filter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8221:164,release,release,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. `SortSamSpark --sort-order coordinate`. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. `4.4.0.0`. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. An error occurs when using SortSamSpark to sort the large BAM file that contain long reads only (90x human wgs, min. read length>10kbp).; However, if the large BAM file contains short reads, it executes normally. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. ```shell; sysctl -w vm.max_map_count=2147483642; gatk SortSamSpark \; --input HG002-NA24385-GM24385.bam \; --output HG002-NA24385-GM24385.sorted.bam \; --sort-order coordinate \; --java-options ""-XX:+UnlockDiagnosticVMOptions -XX:GCLockerRetryAllocationCount=96 -XX:+UseNUMA -XX:+UseZGC -Xmx1794G"" \; --tmp-dir . \; -- \; --spark-runner LOCAL --spark-master local[96] --conf spark.local.dir=./tmp --conf spark.port.maxRetries=61495; ```. #### Expected behavior; _Tell us what should happen_. Output a sorted BAM file. #### Actual behavior; _Tell us what happens instead_. `java.lang.OutOfMemoryError: Required array length ? is too large`. The last lines of the log file.; ```; 11:00:42.884 INFO BlockManagerInfo - Removed taskresult_15758 on 172.20.19.130:43279 in memory (size: 10.5 MiB, free: 1076.2 GiB); 11:00:42.888 INFO TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool; 11:00:42.902 INFO DAGScheduler - ResultStage 0 (sortByKey at SparkUtils.java:165) finished in 1652.742 s; 11:00:42.915 INFO DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job; 11:00:42.916 INFO TaskSchedulerImpl - Killing all running ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:178,release,release,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; CalibrateDragstrModel; ### Affected version(s); - [ ] Latest public release version [gatk/4.2.0.0]. ### Description . gatk 4.2.0.0 CalibrateDragstrModel produces the following stacktrace.... ```; 13:55:31.187 INFO CalibrateDragstrModel - Initializing engine; 13:55:33.395 INFO CalibrateDragstrModel - Done initializing engine; 13:55:33.396 INFO ProgressMeter - Starting traversal; 13:55:33.396 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 13:55:42.364 INFO CalibrateDragstrModel - Shutting down engine; [April 4, 2021 1:55:42 PM EDT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 0.19 minutes.; Runtime.totalMemory()=2384986112; java.lang.IllegalArgumentException: Start cannot exceed end.; at htsjdk.samtools.util.IntervalTree.put(IntervalTree.java:74); at htsjdk.samtools.util.IntervalTree.merge(IntervalTree.java:137); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$ShardReadBuffer.add(CalibrateDragstrModel.java:949); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel$1.tryAdvance(CalibrateDragstrModel.java:798); at java.util.Spliterator.forEachRemaining(Spliterator.java:326); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:161,release,release,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Mutect2; ### Affected version(s); - [ ] 4.1.1.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; We have run mutect2 on the same sample using the same input crams, references, and intervals. The only discernible difference is the docker image used that we built. The difference between the first and second one is that the first one was built without samtools, the second one with samtools. The third is exactly the same as the second except it was re-built about a year or more later. Looking at a count of the `PASS` results based on each:. |Run type |var count|; |---------------------------|---------|; |docker no samtools | 8265 |; |docker yes samtools | 8283 |; |docker yes samtools rebuilt | 8273 |; |docker no samtools recently built | 8271 |. Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7269:1066,update,update,1066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269,1,['update'],['update']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; Picard IlluminaBasecallsToSam and IlluminaBasecallsToFastq. ### Affected version(s); - [X] Latest public release version [4.2.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; ""This bug has been fixed in Picard release https://github.com/broadinstitute/picard/releases/tag/2.25.4 - The version of gatk that you are using (4.2.0.0) was packaged with Picard https://github.com/broadinstitute/picard/releases/tag/2.25.0 in it (which has the bug)."" See [IlluminaBasecallsToSam and IlluminaBasecallsToFastq do not demultiplex NovaSeq barcoded reads](https://github.com/broadinstitute/picard/issues/1679) for details. The GATK release needs to be updated to contain Picard 2.25.4 or better. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; Run tools with NovaSeq run directory as input and many dual index barcodes. #### Expected behavior; _Tell us what should happen_; Reads having each of the dual index barcodes should be demultiplexed to separate files. #### Actual behavior; _Tell us what happens instead_; Reads having each of the dual index barcodes are all in the UNKNOWN files. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7254:198,release,release,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7254,6,"['release', 'update']","['release', 'releases', 'updated']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); _VariantEval, -O, --output_. ### Affected version(s); - [X] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Program starts then terminates with error:; `***********************************************************************`; `A USER ERROR has occurred: Couldn't read file file:///[.....]/X034.eval.grp. Error was: It doesn't exist.`; `***********************************************************************`. Note: The error is not thrown, and VariantEval completes successfully, if a zero-byte file with the name passed with the `-o` or `--output` arguments is created before executing VariantEval; For this example: `touch X034.eval.grp`. #### Steps to reproduce; Command line:; `gatk VariantEval -R $ref -L autosomes.list --eval W034.raw.annotated.vcf.gz --dbsnp dbsnp.vcf.gz -O X034.eval.grp`. #### Expected behavior; Expect the program to create its own output file (as other GATK tools do). #### Actual behavior; Terminates with error (above). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5674:124,release,release,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5674,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description . Functotator MAF output does not properly show genotypes from somatic multi-tumor VCF files produced using M2. It looks like only genotypes from the first tumor sample are shown, but there are no errors or messages to point the user to this fact. Only a single `tumor_sample` and `normal_sample` are reported in the MAF header:. ```; ##normal_sample=xxx; ##source=FilterMutectCalls; ##source=Funcotator; ##source=Mutect2; ##tumor_sample=xxx; ```. Conversely, instructing `Funcotator` to output VCF format properly adds the annotation to the INFO field while retaining the FORMAT-level genotypes works well. However, in this case the VCF header also only lists a single tumor_sample (take note: this is a separate bug, though mostly aesthetics) despite all tumor genotypes being included. #### Steps to reproduce; Run Funcotator with output format set to MAF on any multi-tumor VCF file. #### Expected behavior; Either one of:; 1. `Funcotator` should return an error when trying to process multi-tumor VCF to MAF output; 2. `Funcotator` MAF should output multiple lines per funcotation for each tumor sample, indicating the comparison in the `Tumor_Sample_Barcode` and `Normal_Sample_Barcode` columns.; 3. `Funcotator` should not output genotype information when processing multi-tumor VCF to MAF (this could also be an additional Funcotator parameter that must be switched on when requesting MAF output). #### Actual behavior; Funcotator runs without errors or warnings and the output file is missing genotypes for the other tumor samples",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5687:102,release,release,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5687,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description ; Funcotator returns a `NullPointerException` when trying to output compressed VCF:. ```; 15:35:26.085 INFO Funcotator - Creating a VCF file for output: XXXX; 15:35:26.125 INFO ProgressMeter - Starting traversal; 15:35:26.125 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.vcf.VcfFuncotationFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 15:35:26.328 INFO Funcotator - Shutting down engine; [February 15, 2019 3:35:26 PM EST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=3391094784; java.lang.NullPointerException; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:231); at org.broadinstitute.hellbender.tools.funcotator.vcfOutput.VcfOutputRenderer.close(VcfOutputRenderer.java:137); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:883); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:970); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5683:102,release,release,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5683,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); `GATK MarkDuplicatesSpark`; `GATK EstimateLibraryComplexity`. ### Affected version(s); - [x] Latest public release version 4.2.0.0. ### Description ; The metrics from `GATK MarkDuplicatesSpark` and `GATK EstimateLibraryComplexity` do not match, even though those from `GATK MarkDuplicatesSpark` are the same as `Picard MarkDuplicatesWithMateCigar`. #### Expected behavior; I'd expect that the metrics from `GATK MarkDuplicatesSpark` and `GATK EstimateLibraryComplexity` would be the same, since [here](https://gatk.broadinstitute.org/hc/en-us/articles/360050814112-MarkDuplicatesSpark) recommends to run `GATK MarkDuplicatesSpark` without metrics (it is faster) and run `GATK EstimateLibraryComplexity` afterwards. #### Actual behavior. EstimateLibraryComplexity ; ```; ## htsjdk.samtools.metrics.StringHeader; # EstimateLibraryComplexity INPUT=[temp/align/bwa_aln/c_lib1_L001.sorted.bam] OUTPUT=stats/align/estimate_library_complexity/c_lib1.metrics.txt MIN_IDENTICAL_BASES=5 MAX_DIFF_RATE=0.03 MIN_MEAN_QUALITY=20 MAX_GROUP_RATIO=500 MAX_READ_LENGTH=0 MIN_GROUP_COUNT=2 READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=2279706 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; ## htsjdk.samtools.metrics.StringHeader; # Started on: Wed Mar 24 21:31:32 CET 2021. ## METRICS CLASS picard.sam.DuplicationMetrics; LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; Unknown 0 9951 0 0 0 0 0 0. ## HISTOGRAM java.lang.Integer; duplication_group_count Unknown; 1 9951; ```. MarkDuplicatesSpark; ```; #",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7161:157,release,release,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7161,1,['release'],['release']
Deployability,## Bug Report. ### Affected tool(s) or class(es); `GermlineCNVCaller`. ### Affected version(s); - [x] Latest public release version [`4.3.0.0` and `4.4.0.0`]. ### Description ; `GermlineCNVCaller` pipeline provide different results with same GATK version (`4.3.0.0`) on different base Ubuntu images (`18.04` and `22.04`). Test results of GATK version `4.3.0.0` and `4.4.0.0` are same on Ubuntu 22.04 - I assume there are no changes in `GermlineCNVCaller` between `4.3.0.0` and `4.4.0.0`. #### Steps to reproduce; Command list:; ```sh; /soft/gatk-4.3.0.0/gatk PreprocessIntervals -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa --padding 0 -L chr1:10000-35000 -L chr22:198477-20003000 -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list. /soft/gatk-4.3.0.0/gatk AnnotateIntervals -L /outputs/gatk_intervals.interval_list -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -imr OVERLAPPING_ONLY -O /outputs/gatk_intervals.interval_list.annotated.tsv. /soft/gatk-4.3.0.0/gatk CollectReadCounts -I /inputs/E07002_normal_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_normal_alignment.bam.counts.hdf5; /soft/gatk-4.3.0.0/gatk CollectReadCounts -I /inputs/E07002_tumor_alignment.bam -R /ref/GRCh38.d1.vd1/GRCh38.d1.vd1.fa -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk DetermineGermlineContigPloidy -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --contig-ploidy-priors /outputs/a_valid_ploidy_priors_table.tsv.copy.tsv --output /outputs/COHORT_runDir --output-prefix COHORT --input /outputs/E07002_normal_alignment.bam.counts.hdf5 --input /outputs/E07002_tumor_alignment.bam.counts.hdf5. /soft/gatk-4.3.0.0/gatk GermlineCNVCaller --run-mode COHORT -L /outputs/gatk_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY --annotated-intervals /outputs/gat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8619:116,release,release,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8619,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); `Mutect2` and `FilterMutectCalls`. ### Affected version(s); GATK 4.1.0.0 release (though I suspect this was also in 4.0.x). ### Description ; M2 force-calling multi-allelic sites using GGA mode results in automatic `mappinq_quality` filter if one allele does not have any reads mapped, even if the other variant allele has good mapping quality. In my testing this happened when I force-called the following IDH1 mutations:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO; 2 209113112 . C A . . .; 2 209113112 . C T . . .; ```. The first listed is the common IDH1 R132H variant and the second listed is the less common but equally pathogenic R132L variant. The purpose for force-calling both variants is that one wants to absolutely exclude the possibility of either variant at this site. In most my tests, variants would fail `FilterMutectCalls` because only one of the multi-allelic alleles would be present, eg. *(representative except of `Mutect2` + `FilterMutectCalls` output)*. ```; VAR 	2	209113112	.	C	A,T; FILTER	mapping_quality;read_position; INFO	MMQ=60,0,60; FORMAT	GT:AD 0/1/2:56,0,47; ```. Only in rare cases would the variant site `PASS`, and that's when there would be at least one variant read in the second allele, eg:. ```; VAR 	2	209113112	.	C	A,T; FILTER	PASS; INFO	MMQ=60,60,60; FORMAT	GT:AD 0/1/2:28,1,16; ```. #### Steps to reproduce; To reproduce this specific example you will need a BAM file with a C>T mutation at `2:209113112`, and no reads with an `A` at this position. You will then need to run M2 and `FilterMutectCalls` with `--genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles ...` set to a VCF with both lines as shown in the description. #### Expected behavior; `FilterMutectCalls` should not apply a `mapping_quality` filter if one of the alleles would pass. Perhaps the mapping quality filter could ignore alleles with zero mapped reads, rather than assigning a MMQ of zero by default? *Note that this issue would also",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5695:123,release,release,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5695,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5935:136,release,release,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935,5,"['UPDATE', 'release', 'update']","['UPDATE', 'release', 'updated']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); `PrintReadsSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . I first encountered this type of error in a prototype tool I'm writing, so to dig further about what's happening, I run our simplest Spark tool&mdash;`PrintReadsSpark`. `PrintReadsSpark` reports errors when intervals are specified in a BED file (see command given in the stack trace). * Scenario 1: run with a WGS bam and give intervals via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:114,release,release,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); `SAMRecord` from `GATKRead`. ### Affected version(s); - [x] Latest master branch as of January 30, 2024. ### Description ; When I run a tool with a bam file as input, the following code will give me a null:; ```java; @Override; public void apply(GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext) {. // Build sets of read IDs for each file.; final SAMRecord samRecord = read.convertToSAMRecord(getHeaderForReads());; final SAMFileSource fileSource = samRecord.getFileSource();; System.out.println(fileSource);; ```. Output:; (a long list of `null`). #### Steps to reproduce; Create a ReadWalker that takes in a bam file. Here is an integration test that will replicate the issue:. ```java; public class ReadConcordanceIntegrationTest extends CommandLineProgramTest {. @Test; public void testTwoCrams() throws IOException {; final File output = createTempFile(""testReadConcordanceOutputFile"", "".txt"");; final File input = new File(GATKBaseTest.largeFileTestDir, ""expected.K-562.splitNCigarReads.chr20.bam"");. final ArgumentsBuilder args = new ArgumentsBuilder();. args.addInput(input);; this.runCommandLine(args.getArgsArray());; }; }; ```. #### Expected behavior; Output should be the file used in the read data source (bam file) for each read. #### Actual behavior; I get nulls instead",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8671:712,integrat,integration,712,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8671,1,['integrat'],['integration']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); `combine_tracks.wdl`, `CombineSegmentBreakpoints`. ### Affected version(s); - [ ] Latest master branch as of October 4, 2018. ### Description ; `CombineSegmentBreakpoints` does not have any notion of the num probes or num het columns. As a result, it cannot update those values as the segments are broken into pieces. ; This will get more complicated if both segment files have NUM_POINT columns.; #### Steps to reproduce; Seg File 1; ```; CONTIG START END NUM_POINTS_COPY_RATIO; 1 100 200 25; ```. Seg File 2; ```; CONTIG START END type; 1 100 150 centromere; ```. Result:; ```; CONTIG START END NUM_POINTS_COPY_RATIO type; 1 100 150 25 centromere; 1 151 200 25 ; ```. This result is incorrect, since we have suddenly doubled the number of points b/w 1:100-200. #### Expected behavior; Result:; ```; CONTIG START END NUM_POINTS_COPY_RATIO type; 1 100 150 12 centromere; 1 151 200 13 ; ```; Assuming that there are 12 points in 1:100-150 and 13 points in 1:151-200",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5280:308,update,update,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5280,1,['update'],['update']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.BreakpointsInference`, hence affecting the location of breakpoint output by the SV discovery pipeline. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . Micro-homology around breakpoints affects where we place breakpoints in the SV.; Take the simplest example of deletion; where (10A10G10A); ```; ......AAAAAAAAAAGGGGGGGGGGAAAAAAAAAA......; ```; becomes (10A); ```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4883:209,pipeline,pipeline,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); all tools, I would assume. ### Affected version(s); - [x] Latest public release version [version 4.1.7.0]. ### Description ; ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound:; - tk==8.5.18=0; - readline==6.2=2; - setuptools==36.4.0=py36_1; - certifi==2016.2.28=py36_0; ```; #### Steps to reproduce; Install a fresh miniconda3 install on Linux. Then run:; ```; conda env create -f gatkcondaenv.yml; ```. #### Expected behavior; The conda environment should just work. #### Actual behavior; ```; conda env create -f gatkcondaenv.yml; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound:; - tk==8.5.18=0; - readline==6.2=2; - setuptools==36.4.0=py36_1; - certifi==2016.2.28=py36_0; ```; ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; I would like a fixing of the `gatkcondaenv.yaml` file.; ----. ## Documentation request. ### Tool(s) or class(es) involved; Conda install. ### Description ; Amendment to README.md for installation if not a bug fix; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656:122,release,release,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656,5,"['Install', 'install', 'release']","['Install', 'install', 'installation', 'release']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); gatk GenotypeGVCFs. ### Affected version(s); - [X] Latest public release version [GATK 4.1.9.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Files generated by 'gatk GenotypeGVCFs' with french locale in February (Fvrier in french) August (Aot) or December (Dcembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7081:115,release,release,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); gatkcondaenv.yml from gatk-4.3.0.0.zip downloaded from https://github.com/broadinstitute/gatk/releases. ### Affected version(s); - [ ] Latest public release version [4.3.0.0]. ### Description ; I downloaded gatk-4.3.0.0.zip from https://github.com/broadinstitute/gatk/releases, unzip it on my linux server, and installed gatk by runnimg command line:; conda env create -n gatk -f gatkcondaenv.yml; After installing ended, I checked my installed gatk version and found it be 3.8-1-0-gf15c1c3ef but not installed 4.3.0.0. Any solution?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8120:144,release,releases,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8120,7,"['install', 'release']","['installed', 'installing', 'release', 'releases']"
Deployability,"## Bug Report. ### Affected tool(s) or class(es); org.broadinstitute.hellbender.utils.fragments.FragmentUtils. ### Affected version(s); - [x] Latest public release version 4.1.8.1; - [X] Latest master branch as of September 8th, 2020. ### Description ; At, https://github.com/broadinstitute/gatk/blob/12511551a3e273a1ad767253ccba9918d6eb45b9/src/main/java/org/broadinstitute/hellbender/utils/fragments/FragmentUtils.java#L93-L96. Both insertions and deletions use `getBaseInsertionQualities`. Deletions should instead use `getBaseDeletionQualities`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6801:156,release,release,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6801,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool(s) or class(es); particularly _SelectVariants_, but really anything that writes out a vcf. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [2/6/2020]. ### Description ; There are minimal/inconsistent checks that variants added to vcfWriters are correctly ordered (an issue with htsjdk I think). This leads to an insidious bug, where a sorted vcf can be fed through SelectVariants, and depending on the flavor of output vcf, either crash, or succeed but output an incorrectly sorted vcf. The issue is that in some circumstances SelectVariants will trim alleles to their minimal representation, which can change the location of a variant record, and thus reorder them. However, SelectVariants does nothing to account for the potential order change. Since vcfWriter implementations in htsjdk seem to do minimal/inconsistent checks on the order of variants being added to them, this may write out an incorrectly sorted vcf, or throw an exception, depending on the flavor of vcfWriter. . #### Steps to reproduce; With attached (zipped because github) vcf, run ; `gatk SelectVariants -V test.input.vcf -sn SAMPLE_01 -O test.output.vcf`; Tool will succeed, but output vcf will be incorrectly sorted. Somehow, this incorrectly sorted vcf will also be accompanied by an index! Though if you try to run `IndexFeatureFile` on the output vcf separately, it will fail. . run ; `gatk SelectVariants -V test.input.vcf -sn SAMPLE_01 -O test.output.vcf.gz`; tool will throw exception w/ stack trace:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=17148456, end=17148456, featureStartFilePosition=2460, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=17148447, end=17148457, featureStartFilePosition=2509, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6443:169,release,release,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6443,1,['release'],['release']
Deployability,"## Bug Report. ### Affected tool; Mutect2. ### Affected version(s); - [x] Latest public release version 4.1.9.0; - [x] Latest master branch as of 10/19/2020. ### Description; It looks like there may be a typo in Mutect2Engine.java that was introduced before the most recent release, https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2Engine.java#L392. The line currently reads:; ```java; if (bestNormalAltAllele.getLeft() == bestNormalAltAllele.getLeft()) {; ```; It seems like this line should instead be:; ```java; if (bestNormalAltAllele.getLeft() == bestTumorAltAllele.getLeft()) {; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6901:88,release,release,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6901,2,['release'],['release']
Deployability,"## Bug Report. ### Affected tool; Mutect2. ### Affected versions; - [x] Latest public release version 4.1.8.1; - [x] Latest master branch as of 9/20/2020. ### Description ; Mutect2s header defines `AS_FilterStatus` as follows:; ```text; ##INFO=<ID=AS_FilterStatus,Number=A,Type=String,Description=""Filter status for each allele, as assessed by ApplyRecalibration. Note that the VCF filter field will reflect the most lenient/sensitive status across all alleles."">; ```. `AS_FilterStatus` uses the pipe character `|` for per-allele concatenation and a comma `,` for filter concatenation. This causes records to have an incorrect number of values at sites with multiple filters or multiple alleles. Some examples:; ```text; chr1 826950 . G T . clustered_events;contamination;map_qual;strand_bias AS_FilterStatus=map_qual,strand_bias,contamination;AS_SB_TABLE=86,101|7,0;DP=199;ECNT=3;GERMQ=93;MBQ=35,34;MFRL=193,211;MMQ=33,27;MPOS=6;NALOD=1.28;NLOD=5.42;POPAF=1.39;ROQ=80;TLOD=8.79 GT:AD:AF:DP:F1R2:F2R1:SB 0/0:36,0:0.05:36:18,0:18,0:24,12,0,0 0/1:151,7:0.055:158:76,4:71,3:62,89,7,0; chr1 3633298 . GT G,GTT . contamination;multiallelic;normal_artifact;slippage;weak_evidence AS_FilterStatus=weak_evidence,contamination|weak_evidence,contamination;AS_SB_TABLE=89,7|7,0|7,0;DP=129;ECNT=1;GERMQ=67;MBQ=20,20,20;MFRL=0,0,0;MMQ=60,60,60;MPOS=17,31;NALOD=-0.2424,0.21;NLOD=6.4,6.36;POPAF=2.49,2.04;ROQ=93;RPA=11,10,12;RU=T;STR;STRQ=1;TLOD=3.04,4.6 GT:AD:AF:DP:F1R2:F2R1:SB 0/0:58,4,4:0.072,0.069:66:28,2,2:28,2,2:54,4,8,0 0/1/2:38,3,3:0.083,0.084:44:21,1,3:15,2,0:35,3,6,0; ```. A quick fix would be to define `Number=1` for `AS_FilterStatus` in the VCF header. Alternatively, using a pipe for filter concatenation and a comma for per-allele concatenation might be more compliant with the VCF specification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6857:86,release,release,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6857,1,['release'],['release']
Deployability,"## Bug Report. ### Affected version(s); - Latest master branch as of 1/12/2022. ### Description ; When I tried to build from the github repo, I received the following error:. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 688. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Could not resolve all files for configuration ':runtimeClasspath'.; > Could not find biz.k11i:xgboost-predictor:0.3.0.; Searched in the following locations:; - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; Required by:; project :. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. #### Steps to reproduce; `git clone https://github.com/broadinstitute/gatk.git`; `cd gatk/`; `./gradlew bundle`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7636:375,configurat,configuration,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636,1,['configurat'],['configuration']
Deployability,"## Bug Report. - OS: Arch Linux; - Java: 17. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Firstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8320:90,release,release,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320,4,"['continuous', 'release']","['continuous', 'release', 'release-notes']"
Deployability,"## Bug Report. Dear developers,. I tried to update the GENCODE database and used the getGencode.sh scripts to get the data. However, I was not able to index the feature-file: Do you have any idea why that happens and how to get it done?. Code:; /home/robby/Tools/NGS/gatk-4.2.0.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.113 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 08, 2021 6:53:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but err",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7134:44,update,update,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134,1,['update'],['update']
Deployability,"## Bug Report. Hi there,; So I downloaded the gatk-4.4-0.0.zip and unzipped it for using gatk. I also created the conda env using the gatkcondaenv.yml and used conda to install java ""1.7.0_91"". But when I run ./gatk --list I got this error message: . `; ./gatk --list; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8432:169,install,install,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432,1,['install'],['install']
Deployability,"## Bug Report. No changes have been made to the BQSR code that calculates qualities, but there are differences in quality scores from version to version. This is to understand why. ### Affected tool(s) or class(es); BQSR. ### Affected version(s); - [x] Latest public release version. #### Steps to reproduce. Ran GATK 4.1.8 and 4.1.3 on the same bam, got different quality scores. #### Expected behavior. Expect same quality scores across these versions. #### Actual behavior. These are quality distributions that differ from the two different versions on the same bam.; [qual.pdf](https://github.com/broadinstitute/gatk/files/4984005/qual.pdf); [qual.pdf](https://github.com/broadinstitute/gatk/files/4984007/qual.pdf)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6728:267,release,release,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6728,1,['release'],['release']
Deployability,"## Bug Report. Originally reported by @ldgauthier via slack. ### Affected tool(s) or class(es); CalibrateDragstrModel. ### Affected version(s); - [X ] Latest public release version [4.2.0.0]. ### Description ; An out-of-memory exception when running the aforementioned tool in at least one of many samples. Location where the error occur does not seem to be always the same but it was fixable by increasing memory over 15Gb. #### Steps to reproduce. Since I'm not sure the data is public I won't disclose its location nor ID in this issue. Let's call it the ""SAMPLE"" in ""SAMPLE.cram"":. ```; gatk --java-options ""-Dsamjdk.reference_fasta=Homo_sapiens_assembly38.fasta -Xmx2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" \; CalibrateDragstrModel \; -R Homo_sapiens_assembly38.fasta \; -I SAMPLE.cram \; -str Homo_sapiens_assembly38.str \; -O SAMPLE.final.cram.dragstr \; --parallel \; --verbosity DEBUG; ```. ```Homo_sapiens_assembly38.str``` depends only on the reference and ca be composed using this:. ```; gatk ComposeSTRFile -R Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.str; ```. #### Expected behavior; Completes without issues. #### Actual behavior; a Java Out-of-Memory error is throw and the execution finished without results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7189:165,release,release,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7189,1,['release'],['release']
Deployability,"## Bug Report. We're finding that in rare instances that `GenotypeGVCFs` can emit a variant with a spanned allele (`*`), and a genotype that references the spanned allele, but fail to emit the upstream spanning variant. This seems like a bug to me - either the spanning variant should be emitted _or_ the spanned allele should revert to a reference call. FWIW I have a sneaking suspicion that this is related to setting a non-zero value for `-stand-call-conf` (see #5793). My guess is that in one part of the code it determines the upstream variant _will_ be emitted so retains the allele as spanned, but then somewhere later the upstream variant is filtered out. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [not tested]. ### Description ; Here's the example from the VCF in the attached zip file:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test_sample; chr17 46806234 . TC T 148.64 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:208,25:239:116,16:90,8:99:156,0,6824; chr17 46806237 . TTCTCTCTCTCTC TTCTC,* 1528.04 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 1/2:3,60,33:174:1,29,20:1,21,11:99:3633,1088,2142,1538,0,3285; ```. You can see from this that a) the first variant does not have a spanned allele, implying that there cannot be a spanning event further upstream and b) the second variant has a spanned allele that is present in the `1/2` genotype. #### Steps to reproduce. The attached zip file contains a reduced test case with a 3-record gVCF and a 2-record VCF that exhibits the problem. To reproduce:. 1. Unzip the attached zip file; 2. Edit `command.sh` to put in the path to HG19; 3. Run `. command.sh` in the directory with the extracted files. #### Expected behavior; Either the spanning variant should be emitted, or the spanned allele should not be. #### Actual behavior; A spanned allele is emitted when there is no spanning variant!. ZIP file with test case: [spanned_allele_not_spanne",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6031:759,release,release,759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6031,1,['release'],['release']
Deployability,"## Bug Report; ### Version Information; GenomicsDBImport 4.1.9.0. ### Summary; A user posted on the forum with an error from GenomicsDBImport. @nalinigans @mlathara Can you determine what is causing this java.lang.IndexOutOfBoundsException?. This request was created from a contribution made by vivekruhela on January 12, 2021 19:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076396392-No-Output-from-GenomicsDBImport](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076396392-No-Output-from-GenomicsDBImport). \--. Dear GATK Team,. I am using GATK version 4.1.9.0 for my WES data pipeline. In order to get accurate somatic call, I am trying to generate the Panel of Normal (PON) using GenomicsDBImport module of GATK. While using GenomicsDBImport for PON generation, I am not getting any output from my command. Here is the command I used to print the stack trace:. ```; gatk GenomicsDBImport \\ ; ; \-R /gatk\_bundle/hg19\_v0\_Homo\_sapiens\_assembly19.fasta \\ ; ; \--variant normal1.vcf \\ ; ; \--variant normal2.vcf \\ ; ; \--variant normal3.vcf \\ ; ; \--variant normal4.vcf \\ ; ; \--variant normal5.vcf \\ ; ; \--variant normal6.vcf \\ ; ; \--variant normal7.vcf \\ ; ; \--variant normal8.vcf \\ ; ; \--variant normal9.vcf \\ ; ; \--variant normal10.vcf \\ ; ; \--variant normal11.vcf \\ ; ; \--variant normal12.vcf \\ ; ; \--variant normal13.vcf \\ ; ; \--variant normal14.vcf \\ ; ; \--variant normal15.vcf \\ ; ; \--variant normal16.vcf \\ ; ; \--variant normal17.vcf \\ ; ; .... ; ; \--variant normal80.vcf \\ ; ; \--genomicsdb-workspace-path pon\_db \\ ; ; \--tmp-dir /tmp1 \\ ; ; \-L /gatk\_bundle/hglft\_genome\_3bc14\_d6f440.bed \\ ; ; \--sequence-dictionary /gatk\_bundle/hg19\_v0\_Homo\_sapiens\_assembly19.dict \\ ; ; \--reader-threads 15 \\ ; ; \--java-options '-DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true'; ```. Here For interval list, I have downloaded the hg38 target interval from GATK resource bundle and converted into hg19 format us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037:622,pipeline,pipeline,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037,1,['pipeline'],['pipeline']
Deployability,"## Bug Report; GenotypeGVCFs stuck indefinitely at ""Initializing engine"" step. ### Affected tool(s) or class(es); gatk GenotypeGVCFs; ### Affected version(s); GATK v4.1.4.1 (installed in a `conda` convironment from the bioconda channel), on a RHEL server 7.6 (Maipo). ### Description ; Following the recommended pipeline of HaplotypeCaller, GenomicsDBImport and then GenotypeGVCFs, the last command hangs indefinitely and from the log file, it seems like it doesn't get past the ""Initialize engine"" step. This is an example of the standard error stream (after the `GenotypeGVCFs` job reached 20 hours wall time and was killed) :; ```; 22:28:44.293 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/export/user/home/miniconda3/envs/aDNA/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.; jar!/com/intel/gkl/native/libgkl_compression.so; Dec 17, 2020 10:28:44 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:28:44.639 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.4.1; 22:28:44.640 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:28:44.640 INFO GenotypeGVCFs - Executing as user@gc-prd-hpcn002 on Linux v3.10.0-957.27.2.el7.x86_64 amd64; 22:28:44.640 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 22:28:44.640 INFO GenotypeGVCFs - Start Date/Time: December 17, 2020 10:28:44 PM AEST; 22:28:44.640 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - HTSJDK Version: 2.21.0; 22:28:44.640 INFO GenotypeGVCFs - Picard Version: 2.21.2; 22:28:44.640 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7007:174,install,installed,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7007,2,"['install', 'pipeline']","['installed', 'pipeline']"
Deployability,"## Bug Report; Hi, we are using the dockstore version of the GATK variant calling pipeline that leverages mutect 2:; [github.com/broadinstitute/gatk/mutect2:4.1.8.1](https://dockstore.org/workflows/github.com/broadinstitute/gatk/mutect2:4.1.8.1). We're processing human glioma data, and currently we are making it through much of the pipeline, but failing on `GetPileupSummaries`. There's a thread about it on the discussion board [here] (https://gatk.broadinstitute.org/hc/en-us/community/posts/6179012337819-No-Pileup-Tables). . We are specifying a file for `variants_for_contamination`, and a file for `variants_for_contamination_idx` in the workflow, but the index is never passed to `GetPileupSummaries`, and it fails with this enigmatic error message:. ```; A USER ERROR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7935:82,pipeline,pipeline,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935,2,['pipeline'],['pipeline']
Deployability,"## Bug Report; I was running the JointDiscovery pipeline as a part of the GATK Best Practices pipeline. I am running this on many vcf files (~150) called by the HaplotypeCaller. I am getting this error: . ```; 19:01:58.009 WARN VariantDataManager - WARNING: Very large training set detected. Downsampling to 2500000 training variants.; 19:04:18.918 INFO VariantRecalibrator - Shutting down engine; [September 16, 2019 7:04:18 PM EDT] org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator done. Elapsed time: 912.93 minutes.; Runtime.totalMemory()=3204972544; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; 	at org.broadinstitute.hellbender.tools.walkers.vqsr.MultivariateGaussian.<init>(MultivariateGaussian.java:31); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.GaussianMixtureModel.<init>(GaussianMixtureModel.java:34); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibratorEngine.generateModel(VariantRecalibratorEngine.java:43); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:625); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. I believe this is derived from an error earlier in the log, since the `stderr` gives the same Java heap space error: ; ```; [2019-09-16 19:05:59,50] [error] WorkflowManagerActor Workflow 9f7a01a4-0632-4817-8622-aa51e520abf1 failed (during ExecutingWorkflowState): Job JointGe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165:48,pipeline,pipeline,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165,2,['pipeline'],['pipeline']
Deployability,"## Bug Report; JDK8 is no longer available for the current stable Debian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6053:73,release,release,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053,3,"['install', 'release']","['install', 'release']"
Deployability,"## Bug Report; When I use output files of CombineGVCFs to run GenotypeGVCFs, it seems no problem at beginning. However, several hours later, it suddenly shot down. The fatal error occur. ### Affected tool(s) or class(es); GenotypeGVCFs, only use arguments: -R, -V, -O, -all-sites. ### Affected version(s); GATK4 v4.1.9.0. ### Description . A fatal error has been detected by the Java Runtime Environment:. SIGBUS (0x7) at pc=0x00002acb0aee41d3, pid=14508, tid=0x00002acb0f80b700. JRE version: OpenJDK Runtime Environment (8.0_152-b12) (build 1.8.0_152-release-1056-b12); Java VM: OpenJDK 64-Bit Server VM (25.152-b12 mixed mode linux-amd64 compressed oops); Problematic frame:; C [libc.so.6+0x1501d3] __memmove_ssse3_back+0x1a13. Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again. If you would like to submit a bug report, please visit:; http://bugreport.java.com/bugreport/crash.jsp. --------------- T H R E A D ---------------. Current thread (0x00002acb10021800): GCTaskThread [stack: 0x00002acb0f70b000,0x00002acb0f80c000] [id=14511]. siginfo: si_signo: 7 (SIGBUS), si_code: 2 (BUS_ADRERR), si_addr: 0x00000003ea598000. Registers:; RAX=0x00000003ea593600, RBX=0x00002acb0f80aa00, RCX=0x00000000000059c8, RDX=0x0000000000000f48; RSP=0x00002acb0f80a928, RBP=0x00002acb0f80a950, RSI=0x000000044246f290, RDI=0x00000003ea597fa0; R8 =0x00000003ea593600, R9 =0x0000000057ed72f0, R10=0x00000003c0000000, R11=0x00002acb0af16b50; R12=0x0000000000000b3d, R13=0x00000000000059e8, R14=0x00002acb0f80aa00, R15=0x0000000010490000; RIP=0x00002acb0aee41d3, EFLAGS=0x0000000000010206, CSGSFS=0x0000000000000033, ERR=0x0000000000000006; TRAPNO=0x000000000000000e. Top of Stack: (sp=0x00002acb0f80a928); 0x00002acb0f80a928: 00002acb0ba575c6 00000003c5a14ae8; 0x00002acb0f80a938: 0000000000412400 000000001048e05a; 0x00002acb0f80a948: 00002acb0c077d00 00002acb0f80a9a0; 0x00002acb0f80a958: 00002acb0ba155e8 00002acb0c03f148; 0x00002a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7008:552,release,release-,552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7008,1,['release'],['release-']
Deployability,"## Bug Report; when I run the MarkDuplicatesSpark, it throws me an error: basically it shows the spark engine stopped when run this function. ; the part of the error log is here:; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/rnaseq_pipeline_app/Apps/GATK/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar) to method java.nio.Bits.unaligned(); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 21/01/12 15:50:31 INFO SparkContext: Running Spark version 2.4.5; 21/01/12 15:50:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 21/01/12 15:50:31 INFO SparkContext: Submitted application: MarkDuplicatesSpark; 21/01/12 15:50:31 INFO SecurityManager: Changing view acls to: root; 21/01/12 15:50:31 INFO SecurityManager: Changing modify acls to: root; 21/01/12 15:50:31 INFO SecurityManager: Changing view acls groups to: ; 21/01/12 15:50:31 INFO SecurityManager: Changing modify acls groups to: ; 21/01/12 15:50:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set(); 21/01/12 15:50:31 INFO Utils: Successfully started service 'sparkDriver' on port 36657.; 21/01/12 15:50:31 INFO SparkEnv: Registering MapOutputTracker; 21/01/12 15:50:31 INFO SparkEnv: Registering BlockManagerMaster; 21/01/12 15:50:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 21/01/12 15:50:31 INFO B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7035:697,release,release,697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7035,1,['release'],['release']
Deployability,"## Bug/Usability Report. ### Affected tool(s) or class(es); Mutect2 WDL. ### Affected version(s); - [x] Latest public release version [4.1.81]; - [x] Latest master branch as of October 28, 2021. ### Description ; The Mutect2 WDL's Funcotate task has an unintuitive setup with regard to setting memory for the Funcotate task. Funcotate task memory is defined [here](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L1108); ![image](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:118,release,release,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,1,['release'],['release']
Deployability,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6012:59,install,installation,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012,7,"['Install', 'install', 'release']","['Installing', 'install', 'installation', 'installed', 'installing', 'release', 'released']"
Deployability,"## Documentation request. ### Description ; I was unable to successfully follow the R setup instructions required to run integration tests locally. I don't know whether this is a general concern regarding initial setup on Mac OS X High Sierra 10.13.6, or the problem is specific to my system. . #### R installation itself; Expected: `brew install R` would install R with all necessary core functionality.; Actual: `brew install R` installed a version of R without X11 support. The binary I downloaded from [CRAN](https://cran.r-project.org/bin/macosx/) had the proper support. #### R package installation; Expected `sudo Rscript scripts/docker/gatkbase/install_R_packages.R` would install the necessary packages for R scripts needed.; Actual: Failure to compile source packages, with an error like `clang: error: unsupported option '-fopenmp'`. I made some attempts to update my local `clang` but was unsuccessful. Instead, I installed the packages at the R prompt:; ```; $ R; > install.packages('ggplot2'); > install.packages('reshape'); > install.packages('gplots'); > install.packages('gridExtra'); > install.packages('gsalib'); > install.packages('data.table'); > quit(); ```. After doing so, my test run `TEST_TYPE=integration ./gradlew shadowJar test` succeeded.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5389:121,integrat,integration,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389,17,"['install', 'integrat', 'update']","['install', 'installation', 'installed', 'integration', 'update']"
Deployability,"## Documentation request. ### Tool(s) or class(es) involved. In the document `docs/mutect/mutect.pdf` there are many references to filter names that have either changed or used shorthand names. We should update the documentation to reflect the actual filter names used in Mutect2. Some examples:; ""fragment_length"" in the document refers to ""fragment"" filter.; ""duplicate_evidence"" as ""duplicates""; ""base_quality"" as ""base_qual"". There are probably a number of other differences, we should ensure all are up to date.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6965:204,update,update,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6965,1,['update'],['update']
Deployability,"## Documentation request. ### Tool(s) or class(es) involved. LeftAlignIndels. ### Description . The example for LeftAlignIndels uses a parameter `-O` which doesn't exist for the tool - the parameter listed in the documentation is `--OUTPUT`. Either the example should be updated to use `--OUTPUT`, or the tool should be changed to accept `-O`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5072:271,update,updated,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5072,1,['update'],['updated']
Deployability,"## Documentation request. ### Tool(s) or class(es) involved; GermlineCNVCaller. ### Description ; I'm trying to get a pipeline running to call germline CNVs on small cohorts (20-40) PCR free whole genome samples sequenced to ~45X depth. I'm running into problems figuring out how wide to scatter the analysis, and how to allocate resources. It would be incredibly helpful to have some very clear guidelines about how number of samples and the number of intervals within each scatter affect both runtime and memory usage. Here's what I've been able to infer from the WDL pipelines, tool docs and experimentation (though I suspect some of it is wrong):. 1. Memory usage is approximately proportional to number of samples, number of intervals, number of bias covariates and max copy number. What the docs don't say is what the default is for the number of bias covariates _and_ how to take these numbers and project an approximate memory usage. 2. It would appear that GermlineCNVCaller will, by default, attempt to use all CPU cores available on the machine. From the WDL I see that setting environment variables `MKL_NUM_THREADS` and `OMP_NUM_THREADS` seems to control the parallelism? It would be nice if `GermlineCNVCaller` took a `--threads` and then set these before spawning the python process. 3. Runtime? This would be really nice to have some guidelines around as I get wildly varying results depending on how I'm running. My experimentation is with a) 20 45X WGS samples, b) bin size = 500bp, c) running on a 96-core general purpose machine at AWS with 384GB of memory. My first attempt a) scattered the genome into 48 shards of approximately 115k bins each, representing ~50mb of genome and b) ran 24 jobs concurrently but failed to set the environment variables to control parallelism. In that attempt the first wave of jobs were still running after 24 hours and getting close to finishing up the initial de-noising epoch, with 3/24 having failed due to memory allocation failures. My second",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166:118,pipeline,pipeline,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,## Documentation request. ### Tool(s) or class(es) involved; LearnReadOrientationModel. ### Description ; The tool LearnReadOrientationModel does not appear in the tool docs. We would like to update the documentation for this tool so that it is included with the other tool docs when GATK is updated and is more visible to users.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6862:192,update,update,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6862,2,['update'],"['update', 'updated']"
Deployability,"## Documentation request. ### Tool(s) or class(es) involved; Readme for M2 in https://github.com/broadinstitute/gatk/tree/master/scripts/mutect2_wdl. ### Description ; This is the text currently in the readme, it needs to be updated to feature Funcotator instead of Oncotator:. > Functional annotation (Oncotator); > ; > The M2 WDL can optionally run oncotator for functional annotation and produce a TCGA MAF from the M2 VCF. Oncotator is not a GATK4 tool and is provided in the M2 WDL as a convenience. There are several notes and caveats; > ; > Several parameters should be passed in to populate the TCGA MAF metadata fields. Default values are provided, though we recommend that you specify the values. These parameters are ignored if you do not run oncotator.; > ; > Several fields in a TCGA MAF cannot be generated by M2 and oncotator, such as all fields relating to validation alleles. These will need to be populated by a downstream process created by the user.; > ; > Oncotator does not enforce the TCGA MAF controlled vocabulary, since it is often too restrictive for general use. This is up to the user to specify correctly. Therefore, we cannot guarantee that a TCGA MAF generated here will pass the TCGA Validator. If you are unsure about the ramifications of this statement, then it probably does not concern you.; > ; > More information about Oncotator can be found at: http://archive.broadinstitute.org/cancer/cga/oncotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5889:225,update,updated,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889,1,['update'],['updated']
Deployability,"## Documentation request. ### Tool(s) or class(es) involved; [Mutect2 WDL's README](https://github.com/broadinstitute/gatk/tree/2e6045a259ed2ded3e9036a5b44a1f8ba330860d/scripts/mutect2_wdl) references several template JSONs that do not seem to exist in this repository, specifically:; * mutect2_multi_sample_template.json; * mutect2_template.json; * mutect2-replicate-validation_template.json. There is [one JSON](https://github.com/broadinstitute/gatk/blob/2e6045a259ed2ded3e9036a5b44a1f8ba330860d/scripts/mutect2_wdl/mutect_resources_json/mutect_resources_process_gnomAD_2.1.json) in the folder, although it's not immediately clear which of these three (if any) it is meant to replace. ### Description ; I did find some JSONs in the deprecated repo, although I'm not sure if they need to be updated. https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7596:793,update,updated,793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7596,1,['update'],['updated']
Deployability,"## Documentation system request. Currently, the tooldoc generation system does not separate arguments that relate to deployment decisions like compute platform (eg `--gcs-project-for-requester-pays`) from the ones that modify the analytical or processing behavior of the tools. This adds to the cognitive burden involved in sorting through all the options available for a given tool. We'd like to have a separate category for these arguments so that they would be isolated from the rest. . In addition, there are a bunch of convenience arguments in the common args section that have more to do with how we're running the tool than its analysis behavior, and could also be consolidated into this separate category (or their own category but that might be too granular). Examples below are from the popular tool [SelectVariants](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_variantutils_SelectVariants.php):. #### Arguments that would be stratified as platform args. `--cloud-index-prefetch-buffer`; `--cloud-prefetch-buffer`; `--disable-bam-index-caching`; `--gcs-max-retries`; `--gcs-project-for-requester-pays`. #### Arguments that would be stratified as convenience args. `--arguments_file` ; `--help` ; `--version` ; `--create-output-bam-index` ; `--create-output-bam-md5`; `--create-output-variant-index`; `--create-output-variant-md5`; `--gatk-config-file`; `--QUIET`; `--seconds-between-progress-updates`; `--tmp-dir`; `--use-jdk-deflater`; `--use-jdk-inflater`; `--verbosity`; `--showHidden` -> I thought we had got rid of hidden args??. These could also be stratified as convenience but one could argue they affect tool behavior qualitatively:. `--disable-sequence-dictionary-validation`; `--lenient`; `--read-validation-stringency`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5234:117,deploy,deployment,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5234,2,"['deploy', 'update']","['deployment', 'updates']"
Deployability,"## Feature request and Question. Original question was posted in GATK forum https://gatkforums.broadinstitute.org/gatk/discussion/12026/how-to-do-downsampling,; but it seems to me that the question should be posted here to ask the developer team. ### Tool(s) or class(es) involved; PrintReads. ### Description. In GATK4, printReads doesn't have an option to do downsample to coverage anymore. Is there any reason for that ? Or is there any update suggestions to do the same thing but migrating it from GATK3 to GATK4 ? The forum maintainer told me in original discussion that there is a `DownsampleSam` function in picard, but it can't be used to downsample to coverage directly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5075:440,update,update,440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5075,1,['update'],['update']
Deployability,"## Feature request. ### DRAGEN-GATK release?!!; Hello GATK team, ; - When will the DRAGEN-GATK version will be release officially? I have seen the online forum of Eric brans and Illumina head explaining GATK and DRAGEN about the open source version, a month back! I searched for DRAGEN-GATK update release in both the illumina website and GATK, couldn't find it? can anyone help me to get updated GATK.. and do I need dragen 3.4 for that? till 3.4.12 I haven't seen any mention of GATK in that.. which version I should download?. - Is there any feature in upcoming GATK to find STR variants.. If its in progress when it will get release?; ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6912:36,release,release,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6912,6,"['release', 'update']","['release', 'update', 'updated']"
Deployability,"## Feature request. ### Tool(s) or class(es) involved. (sv) VCF producing tool(s). ### Description. The VCF spec allows `POS` column to take value 0, when the suspected event is at a telomere.; The given example is in section 5.4.5 (see example event illustrated in Figure 6 and VCF records below the figure).; However, currently GATK writes VCF via `VariantContext`'s, which defines coordinate 0 as illegal.; I can of course push this feature request to htsjdk, if that is deemed more appropriate. **UPDATE**; Looking back at the error message, it is actually the `SimpleInterval` that I use for constructing the `VariantContext` throwing the error message.; Temporary workaround would be to ""hack"" the POS to be 1 or N, and warn using an INFO annotation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5915:501,UPDATE,UPDATE,501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5915,1,['UPDATE'],['UPDATE']
Deployability,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5111:58,pipeline,pipeline,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111,1,['pipeline'],['pipeline']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5855:97,pipeline,pipeline,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855,1,['pipeline'],['pipeline']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; Docker container environment for FireCloud / GATK. ### Description. I started a FireCloud instance with a GATK example. It looks like although conda does seem to be correctly pulling TensorFlow from the Anaconda repository, TensorFlow is still not being enabled with Intel MKL-DNN (which would make TensorFlow much faster on CPU). To test this, you can start a notebook or python session and do:. `import tensorflow; print(tensorflow.pywrap_tensorflow.IsMklEnabled())`. If True, then MKL-DNN is enabled in TensorFlow. Currently, this is showing up as false. I'm wondering whether I could work with someone (possibly Sam @lucidtronix) to update the conda environment. Also, the compute instances on FireCloud are using older CPU hardware (AVX-2). Is there any way to update this to a Skylake or Cascade Lake instance (AVX-512/VNNI)?. Thanks.; -Tony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6020:692,update,update,692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6020,2,['update'],['update']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; GATK PrintReads. ### Description; - Currently, this tool appears to consider reads independently of their mate, therefore if one partner is filtered and the other is not the SAM flags for the remaining read will be incorrect (indeed resulting BAMs from this tool fail GATK ValidateSamReads with error MATE_NOT_FOUND). ; - Here is a flagstat of one of these BAMs produced from this tool (note that there are no singleton reads listed, but they actually present -- you can even see this in the read1 and read2 counts; these counts should be equal if there are no supplementary, secondary, and/or singleton reads):. ```; 179466279 + 0 in total (QC-passed reads + QC-failed reads); 0 + 0 secondary; 0 + 0 supplementary; 0 + 0 duplicates; 179466279 + 0 mapped (100.00% : N/A); 179466279 + 0 paired in sequencing; 89740338 + 0 read1; 89725941 + 0 read2; 179466279 + 0 properly paired (100.00% : N/A); 179466279 + 0 with itself and mate mapped; 0 + 0 singletons (0.00% : N/A); 0 + 0 with mate mapped to a different chr; 0 + 0 with mate mapped to a different chr (mapQ>=5); ```. - I have two suggestions:; - Add a `--remove-mates` option that would ensure that if one read in a pair does not pass the read filters, the read pair will be filtered.; - Alternatively, add an `--update-flags` option that would update the filtered-in mate's SAM flags to be technically correct (i.e. if the read's partner was filtered, remove the 0x1, 0x2, 0x8, 0x20, 0x40, and 0x80 flags if they were present)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6839:1322,update,update-flags,1322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6839,2,['update'],"['update', 'update-flags']"
Deployability,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtim",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7324:183,update,update,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324,3,['update'],['update']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; LearnReadOrientation (& others I believe). ### Description; Wondering if you would consider modifying the exit status for 'java.lang.OutOfMemoryError` to reflect it being a memory-related error, perhaps `137`? This would help with pipelines that will retry with more memory in response to a memory-related error. The exit status is currently a generic `1`:. ```; Command exit status:; 1. ...; [March 23, 2023 at 5:50:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel done. Elapsed time: 2,210.83 minutes.; Runtime.totalMemory()=7796817920; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at org.apache.commons.math3.linear.Array2DRowRealMatrix.<init>(Array2DRowRealMatrix.java:61); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelEngine.<init>(LearnReadOrientationModelEngine.java:131); at org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModel.doWork(LearnReadOrientationModel.java:163); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8264:286,pipeline,pipelines,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8264,1,['pipeline'],['pipelines']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; M2 PoN Creation. ### Description; There is no progress meter when running `CreateSomaticPanelOfNormals`. This makes debugging harder and the tool could be accidentally identified as frozen. ### Proposed solution; `final Consumer<Locatable> progressUpdater,` as a parameter to the backend class.; The CLI ( `CreateSomaticPanelOfNormals`) can just pass in `l -> progressMeter.update(l)` as long as the CLI extends GATKTool.; When you want to disable the progress meter, you can simply pass in: `l -> {}`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5629:429,update,update,429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5629,1,['update'],['update']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; M2 WDL and FC deployment of M2. ### Description; We should specify the same file listed in the GATK forum (https://gatkforums.broadinstitute.org/gatk/discussion/4154/howto-install-and-run-oncotator-for-the-first-time), which can be downloaded from: https://personal.broadinstitute.org/lichtens/oncobeta/tx_exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt; This should be used as the default transcript selection list for funcotator (@jonn-smith I assume that funcotator and oncotator use the same format for this file. Please confirm.). The only time you would not want this file is if you are not running on hg19. For other references, ideally, we would want different lists. - This list needs to be put into a bucket (gatk-best-practices?); - Please notify @bshifaw for deployment in the FC featured workspace in the appropriate funcotator parameter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5841:69,deploy,deployment,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5841,3,"['deploy', 'install']","['deployment', 'install-and-run-oncotator-for-the-first-time']"
Deployability,"## Feature request. ### Tool(s) or class(es) involved; M2, at least. ### Description; With Cromwell v33, we should be able to merge the mutect2.wdl and mutect_nio.wdl into one WDL. There will need to be WDL modifications, for sure. https://github.com/broadinstitute/cromwell/releases",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4948:275,release,releases,275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4948,1,['release'],['releases']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; The docker image would need to be updated (gatkbase docker image). See line ~17 in `scripts/docker/gatkbase/Dockerfile`. ### Description; Can you include a later version of samtools in the GATK image? The current samtools version (1.7) does not support crams. ; I believe that you would need to update gatkbase to make this change. . Additional suggestion, which should not be a requirement for closing this issue: you may want to fix the versions of the software in gatkbase (lines ~6-25 in `scripts/docker/gatkbase/Dockerfile`). Note: I have only replicated this issue in `us.gcr.io/broad-gatk/gatk:4.2.6.1`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7886:89,update,updated,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7886,2,['update'],"['update', 'updated']"
Deployability,"## Feature request. ### Tool(s) or class(es) involved; ValidateBasicSomaticShortMutations . ### Description; It turns out that this tool is doing a subset of the CGA tool, MutationValidator. Originally, the understanding (by both DSP and CGA) was the the GATK tool was doing a different algorithm, but this turned out to be incorrect. We should rename the GATK tool perhaps to SomaticShortMutationValidator and cite MutationValidator. Any relevant WDL should be updated to prevent unnecessary workflow failures. @davidbenjamin . (citation does not exist as per last offline meeting with CGA)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5871:462,update,updated,462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5871,1,['update'],['updated']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4922:196,pipeline,pipeline,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator scripts_. ### Description; The scripts for Funcotator (`src/scripts/funcotator`) should all be refactored, if necessary, to allow for command-line arguments rather than internal configurations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5346:245,configurat,configurations,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5346,1,['configurat'],['configurations']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator, VcfFuncotationFactory_. ### Description; When using a VCF file as a data source, `Funcotator` should include the `ID` column as a separate funcotation. The specific use case is for CLINVAR, but should apply to all VCF data sources. The annotation can be added as `<NAME>_ID` in the VCF data source's output annotations. This feature is requested specifically for the clinical pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5186:444,pipeline,pipeline,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5186,1,['pipeline'],['pipeline']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator,GencodeFuncotationFactory_. ### Description; When annotating a variant and the `VariantClassification` is `SPLICE_SITE, INTRON`, Funcotator should include a non-empty `cDNA` annotation. This `cDNA` annotation should include the number of bases that the start of the variant is away from the exon, as well as the reference and alternate alleles of the variant in question. Concretely:; `c.e[EXON NUMBER][+|-][BASES FROM EXON][REF ALLELE]>[ALT ALLELE]`. For example:; c.e2-1A>G; Where:; 2 = the number of the closest exon (1-based); -1 = number of bases away from the exon (1 before); A = Reference allele; G = Alternate allele. This feature is requested for the clinical pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5187:737,pipeline,pipeline,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5187,1,['pipeline'],['pipeline']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_, _DataSourceFuncotationFactory_. ### Description; Funcotator should support NIO for data sources and data sources backing files.; In addition, the data source readers should be updated to support multiple backing files to support the `gnomAD` case (http://gnomad.broadinstitute.org/downloads).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5348:245,update,updated,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5348,1,['update'],['updated']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; _Funcotator_. ### Description; Currently the data sources for the clinical pipeline work contain ExAC. This must be updated to use gnomAD. The change will require a new release of the data sources which must be connected to the data source downloader tool. Additionally, these new data sources must be validated in four ways:; - By visually inspecting the gnomAD source file for correctness.; - By verifying that the source file for gnomAD does not contain special characters.; - By validating that the source file for gnomAD is a valid VCF (assuming it is used VCF format).; - By running a large file and spot checking at least 10 variants for correctness. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5259:130,pipeline,pipeline,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259,3,"['pipeline', 'release', 'update']","['pipeline', 'release', 'updated']"
Deployability,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_; _org.broadinstitute.hellbender.utils.nio.NioFileCopierWithProgressMeter_. ### Description; One `ProgressBar` to rule them all. One `ProgressBar` class hierarchy to bind them. Progress bars/meters should be consolidated into a single class hierarchy, with threaded updates that are triggered by both a `time interval` and a `percentage/# of records completed count` (whichever occurs first). This class hierarchy should have an abstract base `ProgressMeter` class, which has at least 2 concrete child classes - `GenomicProgressMeter` (equivalent to `ProgressMeter`), and `NumericProgressMeter` (which encapsulates the progress meter functionality inside `NioFileCopierWithProgressMeter`). . The progress meter functionality inside `NioFileCopierWithProgressMeter` should be replaced with the resulting progress meter class. The `ProgressMeter` class should be similarly updated / replaced within `GATKTool` to leverage the new class hierarchy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5178:373,update,updates,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5178,2,['update'],"['updated', 'updates']"
Deployability,"## Feature request. ### Tool(s) or class(es) involved; `SelectVariants`. ### Description; In order to run SelectVariants with VCF inputs that are in separate locations from their index files or to stream SelectVariants using https from Azure blob storage, we need a way to provide the index file in a separate argument from the `-V` input. @jamesemery started thinking this through (copying this from slack):. > In `featureDataSource.getTribbleFeatureReader()` we currently initialize the datasources in `getFeatureReader()` which gets called by `VariantWalker.initializeDrivingVariants()` . You could stick an override into that where you thread down the path for the index source through that path and optionally (only if the index is explicitly supplied by the user) push it down into the `getTribbleFeatureReader()` calls at the bottom of the stack there. @droazen any thoughts on this? @VJalili Would adding this feature to `SelectVariants` be useful for your pipelines at all?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8568:965,pipeline,pipelines,965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8568,1,['pipeline'],['pipelines']
Deployability,"## Feature request. ### Tool(s) or class(es) involved; gatk 4.1.4.0, PlotModeledSegments. ### Description; In exploring the CNV pipeline discussed [here](https://gatkforums.broadinstitute.org/gatk/discussion/11683), I ran into an issue where some of my CNVs that were at much higher copy ratios (upwards of 20/2) were not being plotted, since it appears the R script for part of this tool only limits to a maximum copy ratio of 4. . I think the best case would be to add an additional flag to PlotModeledSegments where a user can specify the exact height they want their output graph to go to. Alternatively, it might be useful to determine the highest copy ratio in the supplied `modelFinal.seg` file and pass that to `SetUpPlot()`. . I ended up rewriting part of `WriteModeledSegmentsPlot()` to use this for my own project, and this reflects my second suggestion of determining the highest ratio and plotting using that. This might not be ideal for all users, since you'll lose detail in the lower copy ratios, but I hope this is a good start! . --- ; ```; WriteModeledSegmentsPlot = function(sample_name, allelic_counts_file, ; denoised_copy_ratios_file, modeled_segments_file, ; contig_names, contig_lengths, output_dir, output_prefix) {; modeled_segments_df = ReadTSV(modeled_segments_file); max_log2_ratio = max(modeled_segments_df$LOG2_COPY_RATIO_POSTERIOR_10, ; modeled_segments_df$LOG2_COPY_RATIO_POSTERIOR_50,; modeled_segments_df$LOG2_COPY_RATIO_POSTERIOR_90); max_copy_ratio = (2^max_log2_ratio) + 1; ; num_plots = ifelse(all(file.exists(c(denoised_copy_ratios_file, allelic_counts_file))), 2, 1); png(output_file, 12, 3.5 * num_plots, units=""in"", type=""cairo"", res=300, bg=""white""); par(mfrow=c(num_plots, 1), cex=0.75, las=1); ; if (file.exists(denoised_copy_ratios_file) && denoised_copy_ratios_file != ""null"") {; denoised_copy_ratios_df = ReadTSV(denoised_copy_ratios_file); ; #transform to linear copy ratio; denoised_copy_ratios_df[[""COPY_RATIO""]] = 2^denoised_copy_ratios_df[[""LOG2_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6391:128,pipeline,pipeline,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6391,1,['pipeline'],['pipeline']
Deployability,"## Feature request. Make joint genotyping functionality available as a publicly accessible function to another class/program without passing through the GATK command line interface. ### Tool(s) or class(es) involved; VariantContext; GenotypeGVCFs; Underlying engine classes. ### Description; For various use cases where our pipelines produce in-memory VariantContext objects it would be faster and easier to pass these directly to a joint genotyping function and extract the results back into memory rather than writing to VCF, running the GenotypeGVCFs pipeline via the command line interface and then re-ingesting the resultant VCFs. From discussions during the GATK Working Group meetings it appears this request is similar in principle to existing functionality for the HaplotypeCaller that was implemented by ""extracting the engine"" from the HaplotypeCaller walkers so that it can be instantiated outside the command line utility. Ideally, this implementation should make it possible to instantiate any necessary engine classes pass VariantContext objects directly to the GenotypeGVCFs.apply or GenotypeGVCFs.regenotypeVC and receive the re-genotyped VariantContext objects back for further processing from Java code. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5910:324,pipeline,pipelines,324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,## Feature request. Mitochondria Pipeline. ### Description; Many of the users on Terra that work with the GATK workflows get stuck working with requester-pays data. (e.g. [forum post](https://gatk.broadinstitute.org/hc/en-us/community/posts/360067820111/comments/360011055131)). This request asks that the workflow include the requester pays option like the [pathseq](https://github.com/broadinstitute/gatk/blob/5e5747b76fa98a3b6731dbc328e292fa941f269b/scripts/pathseq/wdl/pathseq_pipeline.wdl#L83) workflow.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6828:33,Pipeline,Pipeline,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6828,1,['Pipeline'],['Pipeline']
Deployability,"## Feature request. Since CombineVariants will not be ported, we need equivalent functionality to its ability to annotate ""set"", ie which callset(s) a site is present in. Here is an excerpt from a tutorial that describes this functionality in action:. ----. To find out which set each variant belongs to, we can use CombineVariants. CombineVariants has a way to annotate each site with which set the site belongs to. For example, if a site is in GIAB and failed hard filtering but passed VQSR, CombineVariants will annotate the site with set=G-filterInH-V. The ""filterIn"" flag before the filtering method tells us the site failed the filtering method, hence it was ""filtered"" in the set. java -jar GenomeAnalysisTK.jar \; 	-T CombineVariants \; 	-R ref/human_g1k_b37_20.fasta \; 	-V:G truth_dataset/NA12878.GIAB.vcf \; 	-V:H vcfs/NA12878.hard.filtered.vcf \; 	-V:V vcfs/NA12878.VQSR.filtered.vcf \; 	-o sandbox/NA12878.Combined.vcf . The set-annotated VCF looks like this:. ````; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT INTEGRATION NA12878; 20 61795 rs4814683 G T 2034.16 PASS AC=2;AF=0.500;AN=4;(...);set=Intersection  GT:AD:ADALL:DP :GQ:PL 0/1:218,205:172,169:769:99 0/1:30,30:.:60:99:1003,0,1027; ````. In this record, ""set=Intersection"" indicates this record was present and unfiltered in all callsets considered. Here is a key of all the possible combinations for this 3-way venn:. | Meaning | Annotation |; |:-|:-|; | In GIAB only | G |; | In GIAB and failed VQSR only | G-H-filterInV |; | In GIAB and failed both hard filtering and VQSR | G-filterInH-filterInV |; | In GIAB and failed hard filtering only | G-filterInH-V |; | In GIAB and passed both hard filtering and VQSR | Intersection |; | Not in GIAB and failed VQSR only | H-filterInV |; | Not in GIAB and failed both hard filtering and VQSR | FilteredInAll |; | Not in GIAB and failed hard filtering only | filterInH-V |; | Not in GIAB and passed both hard filtering and VQSR | H-V |",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2489:1026,INTEGRAT,INTEGRATION,1026,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2489,1,['INTEGRAT'],['INTEGRATION']
Deployability,## Feature request. The mitochondria pipeline should have new annotations and filters in Mutect2 and FilterMutectCalls. This is being addressed in #5193. An accompanying best practices WDL should also be developed and eventually be available in Firecloud. I'll update here once the PR is merged and has been released.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5310:37,pipeline,pipeline,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5310,3,"['pipeline', 'release', 'update']","['pipeline', 'released', 'update']"
Deployability,"## System. * GATK4 a1eee32e84c21c2f265d248c5f47789ae0ba2b37; * Mac OS X 10.11.6 x86_64; * machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 DTES64 MON DSCPL VMX SMX EST TM2 SSSE3 CX16 TPR PDCM SSE4.1 SSE4.2 POPCNT; * machdep.cpu.extfeatures: SYSCALL XD EM64T LAHF RDTSCP TSCI; * java version ""1.8.0_60""; * Java(TM) SE Runtime Environment (build 1.8.0_60-b27); * Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode). ## Error. When updating a downstream project with the latest master and running the integration/unit tests with gradle, it generates the following error. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x00000001236427f4, pid=4010, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression7227189416687158431.dylib+0x17f4] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid4010.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Error report file: [hs_err_pid4010.log.txt](https://github.com/broadinstitute/gatk/files/1259963/hs_err_pid4010.log.txt). ## Forcing other GKL versions. * 0.5.2 (working); * 0.5.3 (failing); * 0.5.5 (failing); * 0.5.6 (failing); * 0.5.7 (failing); * 0.5.8 (failing)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3532:606,integrat,integration,606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3532,1,['integrat'],['integration']
Deployability,"## System; * Mac OS X 10.11.6 x86_64; * Java HotSpot(TM) 64-Bit Server VM 1.8.0_60-b27. ## Problem; I'm trying to update my project ([ReadTools](https://github.com/magicDGS/ReadTools)) to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2315:114,update,update,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2315,1,['update'],['update']
Deployability,"## Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Ser",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7324:934,update,update-workspace-path,934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324,1,['update'],['update-workspace-path']
Deployability,"## Update numpy\scipy\pymc3 python package. ### Tool(s) or class(es) involved. python\gcnvkernel; python\vqsr_cnn. ### Description; want to use the newer numpy 1.19.4, but I found that gatk uses conda-force to install the older numpy 1.17.5, and it is not allowed to upgrade numpy because of scipy version restrictions. And scipy cannot be upgraded because of the version limitation of pymc3. I think we should use the new version of the software (in the new version, some bugs are fixed, the performance is better), we need to deal with the difficulties and help the software upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6978:3,Update,Update,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6978,5,"['Update', 'install', 'upgrade']","['Update', 'install', 'upgrade', 'upgraded']"
Deployability,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/192. ### **Commit Summary**; - Updated StorageAPIAvroReader.java; > - To handle the use case if no data is returned from BQ. ### **Tested**; - Updated BigQueryUtilsUnitTest.java; > - testQueryWithEmptyDatasetStorageAPI() function to test code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7082:105,Update,Updated,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7082,6,['Update'],['Updated']
Deployability,### **Addresses** ; https://github.com/broadinstitute/dsp-spec-ops/issues/246. ### **Commit Summary**; - Created AvroFileReader ; - Created a sampleAvroFile; - Update ExtractCohort and ExtractCohortEngine to accept a AvroFile. Testing:; Created a test for AvroFileReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7174:160,Update,Update,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7174,1,['Update'],['Update']
Deployability,### **Addresses** ; https://github.com/broadinstitute/gatk/pull/7115. ### **Commit Summary**; -Created AvroFileReader; - Update ExtractCohort and ExtractCohortEngine to accept a AvroFile. Testing:; Created a test for AvroFileReader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7127:121,Update,Update,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7127,1,['Update'],['Update']
Deployability,"### Affected class(es); All test classes in GATK (and downstream projects) extending `BaseTest`. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch. ### Description ; The GATK toolkit assumes `US` locale (set in a `Main` static method), which in turn produces all the test files using the `US` locale; if the test suite is run in a different locale, it might fail unexpectedly. For example, if the locale has a comma-separated decimals instead of dot-separated, comparing the expected file output with `US` locale against the generated by the tests fail. . #### Expected behavior; `BaseTest` should set the locale in a `@BeforeSuite` method (or static method) to set the assumptions of the toolkit to all tests (also for downstream toolkits). #### Actual behavior; `BaseTest` picks default locale.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5012:142,release,release,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5012,1,['release'],['release']
Deployability,"### Affected tool(s) or class(es); Funcotator, but could be future tools as well. ### Affected version(s); - [ ] Latest master branch as of [June 14, 2018]. ### Description ; Currently, if you want to read a MAF the GATK will use AnnotatedIntervalCodec. This is fine in the majority of cases. However, under the hood, it is using a configuration setup that has an aliasing scheme. This alias scheme is fairly permissive and can lead to conflicts. For example, if a MAF has a column named ""END"", the MAF will not parse, since the default configuration will attempt to use the ""END"" column instead of ""End_Position"". This can be fixed if we have a MAF codec, but some decisions need to be made. For example, should it produce AnnotatedIntervals? Variant may be too difficult.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4897:332,configurat,configuration,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4897,2,['configurat'],['configuration']
Deployability,"### Affected tool(s) or class(es); HaplotypeCallerSpark. gatk HaplotypeCallerSpark -R GRCh38_full_analysis_set_plus_decoy_hla.fa -I GatherBamFiles.bam -O g.vcf.gz. The HaplotypeCaller works, but not HaplotypeCallerSpark.; Tried to use the docker image, and different server; tried to build the newest gatk, same error message. ### Affected version(s); - [ x ] Latest public release version 4.1.8.1; java version ; ```; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode); ```. ### Description ; keep get the error message like below. ```; Using GATK jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Dsamjdk.compression_level=5 -Xms10G -jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCallerSpark -R GRCh38_full_analysis_set_plus_decoy_hla.fa -I SRR1573206.GatherBamFiles.bam -O SRR1573206.g.vcf.gz -G StandardAnnotation -G StandardHCAnnotation -G AS_StandardAnnotation -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -ERC GVCF; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 09:38:05.655 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 15, 2020 9:38:05 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:38:05.911 INFO HaplotypeCallerSpark -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6750:374,release,release,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6750,3,['release'],"['release', 'release-']"
Deployability,### Affected tool(s) or class(es); _EstimateDragstrParameters_. ### Affected version(s); - [ ] Latest public release version [version?]; - [X] Latest master branch as of [after PR 6634 has been merged in]. ### Description . Look for usages of ```Utils.runInParallel```. Change those to use Spark instead. There is a possibility of removing multi-threading all together if we change the way we decimate and filter sites.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6876:109,release,release,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6876,1,['release'],['release']
Deployability,"### Affected tool(s) or class(es); docker version GATK:4.1.1.0. ### Affected version(s); ; latest release. ### Description ; Funcotator shuts down part way through job. A configuration problem @ google?; [funcotator_crash.txt](https://github.com/broadinstitute/gatk/files/3652568/funcotator_crash.txt). RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; ; ### Description . 04:13:19.667 INFO ProgressMeter - 15:85753672 1834.2 199000 108.5; 04:17:42.593 INFO VcfFuncotationFactory - dbSNP 9606_b150 cache hits/total: 0/0; 04:17:42.593 INFO VcfFuncotationFactory - gnomAD_exome 2.1 cache hits/total: 0/1402; 04:17:42.593 INFO VcfFuncotationFactory - gnomAD_genome 2.1 cache hits/total: 0/162233; 04:17:42.665 INFO Funcotator - Shutting down engine; [September 25, 2019 4:17:42 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1,845.78 minutes.; Runtime.totalMemory()=4523032576; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:318); at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182:98,release,release,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"### Affected tool; - CollectReadCounts. ### Affected version; - The Genome Analysis Toolkit (GATK) v4.2.3.0; - HTSJDK Version: 2.24.1. Downloaded from https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip. ### Description ; When calling `CollectReadCounts` tool with symlink as input BAM-file, `java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE` occurs. Symlinks seem to work fine with Picard BAM-tools as well as `HaplotypeCaller` and `Mutect2`. Probably HTSJDK level issue, but popped up exception is kind of misleading. #### Stacktrace:; ```; java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:863); at htsjdk.samtools.MemoryMappedFileBuffer.<init>(MemoryMappedFileBuffer.java:23); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:931); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7579:190,release,releases,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579,1,['release'],['releases']
Deployability,"### Bug Report. Hi, after installing the conda environment and running `conda activate gatk` without errors, I seem to still have a problem importing the gcnvkernel module. Is there a way I can install it through pip or what is something I may have done wrong? I already went over the README and standard documentation, and don't think I missed a step. ### Affected tool(s) or class(es); gvnvkernel, other expected modules. #### Expected behavior; Generate output file from my VCF. #### Actual behavior; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/gamer456148/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar GermlineCNVCaller --input var.vcf --run-mode CASE --contig-ploidy-calls X/prefix-calls --output-prefix regular.vcf --output testfile.vcf; 21:21:12.277 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/gamer456148/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 23, 2020 9:21:12 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:21:12.543 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:21:12.544 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.4.1; 21:21:12.544 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:21:12.544 INFO GermlineCNVCaller - Executing as gamer456148@gamer456148-Inspiron-15-7579 on Linux v4.15.0-88-generic amd64; 21:21:12.544 INFO GermlineCNVCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_191-b12; 21:21:12.544 INFO GermlineCNVCaller - Start Date/Time: February 23, 2020 9:21:12 PM EST; 21:21:12.544 INFO GermlineCNVCaller - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6467:26,install,installing,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6467,2,['install'],"['install', 'installing']"
Deployability,"### Feature request. ### Tool(s) or class(es) involved; Mitochondria pipeline (Mutect2). ### Description; This is a user request from the forum:. Other mitochondria tools notate the difference between heteroplasmy and homoplasmy variant calls. This is based only on the estimated AF so should be simple to implement and add an additional annotation. It is allele specific, so would fit better in the FORMAT field.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6257:69,pipeline,pipeline,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6257,1,['pipeline'],['pipeline']
Deployability,"### Goal; Our team try to explore the usage of GATK Spark tool for our internal WES and WGS data. ## Bug Report. ### Affected tool(s) or class(es); BwaSpark and ReadsPipelineSpark. ### Affected version(s); - [ X ] Latest public release version [gatk 4.0.11.0]. ### Description ; For both tools, we encountered an issue when the driver shutdown the command as the screenshot. However, for the bwaspark, the alignment ratio seems to be unaffected by the error, but the lines number of the VCF file from ReadsPipelineSpark varies randomly, and quite different from the non-spark version of GATK 4.0.5.2. #### Steps to reproduce; Before running the tool, we generated the image index for whole genome by using the fasta file from GATK official ftp site, and uploaded the reference file to Hadoop HDFS. ``` bash; gatk-4.0.11.0/gatk BwaMemIndexImageCreator -I Homo_sapiens_assembly38.fasta -O Homo_sapiens_assembly38.fasta.img; ```. and then, we preprocess our pair end fastq files into unaligned ubam file as, ; ``` bash; java -jar picard.jar FastqToSam \; F1=R1.fastq.gz; F2=R2.fastq.gz; O=unaligned_reads.bam \; SM=sample001 \; PL=illumina \; RG=rg001; ```. For BwaSpark, we used,; ``` {bash}; ../gatk-4.0.11.0/gatk --java-options ""-Dgatk.spark.debug=true -XX:+PrintGCDetails"" BwaSpark -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test3.bam -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --spark-runner SPARK --spark-master spark://master:7077 -- --num-executors 4 --driver-memory 4g --executor-cores 10 --executor-memory 20g; ```. For ReadsPipelineSpark, we used, ; ``` {bash}; time_gatk ""ReadsPipelineSpark --tmp-dir /tmp --align true -I hdfs://ns/user/root/test/unaligned_reads.bam -O hdfs://ns/user/root/test/test10.vcf -R hdfs://ns/user/root/Homo_sapiens_assembly38.fasta --known-sites hdfs://ns/user/root/Homo_sapiens_assembly38.dbsnp138.vcf -pairHMM AVX_LOGLESS_CACHING --max-reads-per-alignment-start 50"" 4 44 88g 12g; ```. #### Expected behavior; Both tool s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5481:228,release,release,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481,1,['release'],['release']
Deployability,"### Instructions. ## Bug Report. ### Affected tool(s) or class(es); FilterMutectCalls. ### Affected version(s); - [ ] Latest public release version [4.1.4.1]. ### Description ; Header is missing description for ""##FILTER=<ID=PASS"" , it causes inaccurate parsing of VCF . #### Steps to reproduce; Run the FilterMutectCalls on any mutect2 VCF available . Thanks, ; Nick",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6426:132,release,release,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6426,1,['release'],['release']
Deployability,"### Instructions. ## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [v ] Latest public release version [4.4.0.0]. ### Description ; Since switching to 4.4.0.0 we are experiencing an increased memory consumption of MarkDuplicatesSpark. We see it on large BAM/CRAM files, not tested on small files. #### Steps to reproduce; This command: ; ```; java -Xmx190g -jar /usr/gitc/GATK_ultima.jar MarkDuplicatesSpark \; --spark-master local[24] \; --input 019242_old.ua.aln.bam \; --output 019242_old.aligned.sorted.duplicates_marked.bam \; --create-output-bam-index true \; --spark-verbosity WARN \; --verbosity WARNING \; --flowbased; ```; required 90GB memory on 4.3.0.0. The input BAM is large: 270GB; However on the 4.4.0.0 it requires >160GB RAM. #### Expected behavior; The memory requirement is not expected to change ; #### Actual behavior; Significantly increased memory requirement",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8307:135,release,release,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8307,1,['release'],['release']
Deployability,"### Instructions. ## Bug Report. Hi GATK team , I'm afraid I found an exception in gatk HC related to https://github.com/broadinstitute/gatk/issues/6516. ### Affected tool(s) or class(es). GATK HC v4.3.0.0 . ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description . ```; + gatk --java-options '-Xmx5g -Djava.io.tmpdir=TMP' HaplotypeCaller -R /LAB-DATA/BiRD/resources/species/human/cng.fr/hs38me/hs38me_all_chr.fasta --minimum-mapping-quality 10 --sample-ploidy 2 --do-not-run-physical-phasing --alleles TMP/jeter.vcf.gz -L TMP/jeter.vcf.gz -I /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz Using GATK jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5g -Dj; ava.io.tmpdir=TMP -jar /LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar HaplotypeCaller -R /LAB-DATA/BiRD/resources/species/human/cng.fr/hs38me/hs38me_all_chr.fasta --minimum-mapping-quality 10 --sample-ploidy 2 --do-not-run-physical-phasing --alleles TMP/jeter.vcf.gz -L TMP/jeter.vcf.gz -I /SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/87/5fa0df303dc4f06212547353be621c/BAMS/cluster.aaaaaaacx.bam.list -O TMP/jeter2.vcf.gz; 18:15:19.107 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.j; ar!/com/intel/gkl/native/libgkl_compression.so ; 18:15:21.727 INFO HaplotypeCaller - ------------------------------------------------------------ ; 18:15:21.728 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.3.0.0 ; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8106:253,release,release,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106,1,['release'],['release']
Deployability,"### Instructions. ## Bug Report; ### Affected tool(s) or class(es); - tools: HaplotypeCaller perhaps Mutec. ; - classes: AlleleLikelihoods. ### Affected version(s); - [ X] Latest public release version [version?]; - [ X] Latest master branch as of [date of test?]. ### Description ; Right before calling annotators HC engine adds filtered reads as additional evidence in the AlleleLikelihoods instance that is passed down to the annotators. The code requests the new evidence to have 0.0 likelihoods so label them as uninformative. However due to an error in how the lk arrays are ""extended"" inside the AlleleLikelihoods these reads inherit past reads (removed) zombie likelihoods instead. Fix is easy. as simple as remove this enclosing ```if``` in AlleleLikelihoods, and simply executed its body; always:. ```; line 793:; if (initialLikelihood != 0.0) // the default array new value.; {; for (int a = 0; a < alleleCount; a++) {; Arrays.fill(sampleValues[a], sampleEvidenceCount, newSampleEvidenceCount, initialLikelihood);; }; }; ```. #### Steps to reproduce. Debug and active region with filtered reads. . #### Expected behavior. Those reads won't contribute to AD or DP. #### Actual behavior. They do contribute, at random, to those count annotations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7153:186,release,release,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7153,1,['release'],['release']
Deployability,"### Instructions. In one of the six samples that the DSP pipelines team ('lantern') uses for scientific testing, found bug in GATK 4.1.7.0's HaplotypeCaller. 'java.lang.IllegalArgumentException: evidence provided is not in sample'. Full stack trace below. This is found for Sample NA17-308, Shard 49.; https://cromwell.gotc-dev.broadinstitute.org/api/workflows/1/83938362-b9b5-49f3-a65d-715065d6eabd/metadata; Execution bucket is:; broad-gotc-dev-cromwell-execution (results will stay there for 30 days before being automatically cleaned up). ----. ## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - 4.1.7.0. ### Description ; Stack trace:; java.lang.IllegalArgumentException: evidence provided is not in sample; 	at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.lambda$removeEvidence$9(AlleleLikelihoods.java:1124); 	at java.util.stream.ReferencePipeline$4$1.accept(ReferencePipeline.java:210); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.IntPipeline.toArray(IntPipeline.java:504); 	at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.removeEvidence(AlleleLikelihoods.java:1128); 	at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.contaminationDownsampling(AlleleLikelihoods.java:315); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:173); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:608); 	at org.broadinstitute.hellbender.tools.walkers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586:57,pipeline,pipelines,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586,1,['pipeline'],['pipelines']
Deployability,"### Instructions. Initially reported by a user on the [forum](https://gatkforums.broadinstitute.org/gatk/discussion/13680/variants-with-ad-0-0-and-dp-0#latest)... Aparently some variants with non-zero quals have 0 AD and DPs. Other annotations are also missing from the INFO columns. . After some debugging it turns out that the criteria to determine whether a read should be considered for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434:955,configurat,configuration,955,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434,1,['configurat'],['configuration']
Deployability,"### Instructions. The github issue tracker is for bug reports, feature requests, and API documentation requests. General questions about how to use the GATK, how to interpret the output, etc. should be asked on the [official support forum](http://gatkforums.broadinstitute.org/gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es). FilterMutectCalls. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description . If there is germline variant within one read length of a somatic variant, the clustered_events filter can filter out the somatic variant. Clustered events, intention is to filter out noisy sites that are likely artifactual, but should ignore contributions from neighboring germline sites.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6532:1282,release,release,1282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6532,1,['release'],['release']
Deployability,"### Instructions; I use PathSeqPipelineSpark to analyze 10x Visium spatial transcribed data.; I did not download the data from the database on the GATK official website. But I prepared the database according to the tutorial [https://gatk.broadinstitute.org/hc/en-us/articles/360035889911--How-to-Run-the-Pathseq-pipeline] by myself.; The analysis has no results, and I don't know the reason for the lack of results. ## software / environment / log file informations; Using GATK jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx750g -jar /mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar PathSeqPipelineSpark --input CRC_16/outs/possorted_genome_bam.bam --filter-bwa-image hsa_GRCh38/genome.fa.img --kmer-file hsa_GRCh38/genome.hss --min-clipped-read-length 60 --microbe-dict 16SrRNA/bacteria.16SrRNA.dict --microbe-bwa-image 16SrRNA/bacteria.16SrRNA.fa.img --taxonomy-file 16SrRNA/16SrRNA.db --output pathseq/CRC_16.pathseq.complete.bam --scores-output pathseq/CRC_16.pathseq.complete.csv --is-host-aligned false --filter-duplicates false --min-score-identity .7 --tmp-dir pathseq/tmp; 13:19:23.776 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/icfs/work/singlecelldevelopment/software/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:19:28.982 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 13:19:28.982 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.3.0.0; 13:19:28.982 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:19:28.983 INFO PathSeqPipelineSpark - Executing as singlecellproject@d01.capitalbiotech.local on Linux v3.10.0-514.16.1.el7.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8339:312,pipeline,pipeline,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339,1,['pipeline'],['pipeline']
Deployability,"### Updates; A record with SVTYPE=CTX and CPX_TYPE=CTX_INV was added to a recent GATK-SV VCF after manual curation. The following changes were made to be able to properly annotate this type of event.; * CPX_TYPE will be checked for CTX records, and if it is CTX_INV, the INV interval from CPX_INTERVALS will be added to the annotation segments.; * Additionally, instead of annotating two breakpoint intervals CHROM:POS-END and CHR2:END2-END2+1 for CTX events, we will now annotate 4 individual breakpoints to cover the case where END != POS+1. Those 4 breakpoints are CHROM:POS-POS, CHROM:END-END, CHR2:END2-END2, and CHR2:END2+1-END2+1.; * In the future, to be able to represent intervals on CHR2, POS2 may be added. SVAnnotate will need to be updated accordingly at that time. ### Testing; * Unit tests for CTX_INV added; * Unit tests for other CTX updated; * A one-line VCF was created to test the real-life example CTX_INV event that was curated, and it was annotated correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8693:4,Update,Updates,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8693,3,"['Update', 'update']","['Updates', 'updated']"
Deployability,### Updates; Some GATK-SV VCFs contain MEI deletions with ALT in the format <DEL:ME:ALU> or <DEL:ME>. This change will allow SVAnnotate to recognize and annotate those records as deletions. ### Testing; * Added unit test with MEI DEL; * Ran all unit and integration tests for SVAnnotate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8125:4,Update,Updates,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8125,2,"['Update', 'integrat']","['Updates', 'integration']"
Deployability,"#6329 Bug Report. ### Affected tool(s) or class(es); Mutect2 / FilterMutectCalls. ### Affected version(s); - [x] Latest public release version [version?]. ### Description . Variants with alternative representations in gnomad are not recognized as being the same as called variants in some cases. This results in variants that are called and not filtered, but they should be filtered by ""germline"". As an example of this, in gnomad the site hg19 16:72991715 is represented as:; 16	72991715	.	ACCG	GCCG,*,AGCCGCCG	14986622.13	PASS	AC=33700,10,4;AF=0.83,2.463E-4,9.852E-5. But in M2 it is called as:; A->G. Although ACCG->GCCG and A->G are equivalent, they are not recognized as identical. #### Expected behavior; Mutect2 should filter these variants as germline. #### Actual behavior; Mutect2 does not filter these variants.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6972:127,release,release,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6972,1,['release'],['release']
Deployability,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:17094,update,update,17094,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,8,"['Update', 'update']","['Update', 'update']"
Deployability,"#Bug Report. ### Affected tool(s) or class(es); CNNScoreVariant. ### Affected version(s); 4.0.7.0. ### Description ; For 1,297,033 variant sites, the CNN_1D=-16.118. . #### Steps to reproduce; /run_cnn.sh adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; Using GATK jar /share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /share/pkf/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O cnn/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; 11:29:43.339 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:29:43.467 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.467 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.0.7.0; 11:29:43.467 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:29:43.468 INFO CNNScoreVariants - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 11:29:43.468 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:29:43.469 INFO CNNScoreVariants - Start Date/Time: August 13, 2018 11:29:43 AM UTC; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Version: 2.16.0; 11:29:43.469 INFO CNNScoreVariants - Picard Version: 2.18.7; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:29:43",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5101:310,install,install,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101,2,['install'],['install']
Deployability,"' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these tasks: gatkTabComplete, installDist, gatkDoc, shadowJar, sparkJar. Seems like it should be easy to fix, I'm not sure how we didn't see them when doing the upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7625:2320,install,installDist,2320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625,2,"['install', 'upgrade']","['installDist', 'upgrade']"
Deployability,"(Linked to #7988); Feature additions (and integration tests) for CompareReferences tool, including:; * ability to run base-level comparison modes on specified sequences (not just detected mismatching sequences) using ""sequences-to-align"" option ; * changed wording for missing MD5 compatibility status ('COMPATIBLE' to 'MAYBE_COMPATIBLE,' or something similar) in compatibility tool ; * option to ignore case level differences in base level comparison modes . NOTE: integration test on using an equivalent sequences input file with more than one line (ie. specifying more than one sequences) not yet tested, and can probably do some refactoring to clean up the code for the equivalent sequence comparisons",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8163:42,integrat,integration,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163,2,['integrat'],['integration']
Deployability,"(SV) consolidate logic in simple chimera inference, update how variants are represented in VCF emitted by new code path",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663:52,update,update,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663,2,['update'],['update']
Deployability,"(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testPerfectUnpairedMapping SKIPPED; ```. This test fails because some JAR wasn't built:; ```; Running Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest); Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one by running. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest > testPipeForPicardTools STANDARD_ERROR; No local jar was found, please build one by running; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err:. Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar. /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: or. or; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: export GATK_LOCAL_JAR=<path_to_local_jar>. export GATK_LOCAL_JAR=<path_to_local_jar>; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one by running. No local jar was found, please build one by running; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err:. Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:3958,Pipeline,PipelineSupportIntegrationTest,3958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,1,['Pipeline'],['PipelineSupportIntegrationTest']
Deployability,(this may be related to recent upgrade of our cluster?). running the simples example blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(Comm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1389:31,upgrade,upgrade,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389,7,"['deploy', 'upgrade']","['deploy', 'upgrade']"
Deployability,") or class(es); GATK LiftoverVcf. ### Affected version(s); gatk/4.1.7.0. ### Description . The LiftoverVcf generates the following error. The error occurs with SVs where the INFO/END is not also lifted over. This results in INFO/END before the site start position which triggers the error.; ```; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar LiftoverVcf -I b37/HG002_SVs_Tier1_v0.6.vcf.gz -O b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz -CHAIN grch37_to_grch38.over.chain.gz --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; 10:20:35.165 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Sun Jul 26 10:20:35 EDT 2020] LiftoverVcf --INPUT b37/HG002_SVs_Tier1_v0.6.vcf.gz --OUTPUT b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz --CHAIN grch37_to_grch38.over.chain.gz --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz --REFERENCE_SEQUENCE /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --WARN_ON_MISSING_CONTIG false --LOG_FAILED_INTERVALS true --WRITE_ORIGINAL_POSITION false --WRITE_ORIGINAL_ALLELES false --LIFTOVER_MIN_MATCH 1.0 --ALLOW_MISSING_FIELDS_IN_HEADER false --RECOVER_SWAPPED_REF_ALT false --TAGS_TO_REVERSE AF --TAGS_TO_DROP MAX_AF --DISABLE_SORT false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jul 26, 2020 10:20:35 AM shaded",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:1028,install,install,1028,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,1,['install'],['install']
Deployability,"). Ah, ok. So keep it in the GATK3 repo then, for now? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260642385). Yes. I believe @lucidtronix is doing VQSR development in GATK3 and we'll port later. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505). @ldgauthier @lucidtronix Any update on this since I heard VQSR got ported to GATK4?. ---. @ldgauthier commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287905381). It's unlikely the behavior has changed. For gnomad we used hard filters to; address the problem, which is probably a good global recommendation. On Mar 20, 2017 1:35 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @lucidtronix; > <https://github.com/lucidtronix> Any update on this since I heard VQSR; > got ported to GATK4?; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLRwdezIkmt3uPqIABWLggVjRN3yks5rnrjegaJpZM4Dt4t7>; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287919609). OK, that makes sense, thanks. Do you want me to migrate the issue to GATK4? Otherwise I'll just close it out here as WONTFIX. ---. @ldgauthier commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-288223457). Somewhere we need a record of updates to filtering best practices until we; publish a new thing, so yeah, please migrate. On Mar 20, 2017 6:36 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > OK, that makes sense, thanks. Do you want me to migra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2508:8725,update,update,8725,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2508,1,['update'],['update']
Deployability,"); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:2302,update,update,2302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['update'],['update']
Deployability,* Add a table of contents.; * Update out-of-date information.; * Merge in information from the old gatk-protected README; * Add section on git-lfs; * Add section on downloading GATK4; * Add section on documentation generation; * Add section on zenhub; * Remove no-longer-needed protected-root directory. Resolves #2775; Resolves #2978; Resolves #2487; Resolves #2461,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3158:30,Update,Update,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3158,1,['Update'],['Update']
Deployability,* Add new parameter to set filtered genotypes to no-calls to ExtractCohort; * Modified ExtractCohortEngine to optionally set genotypes that are filtered (FT flag set - at the genotype leve) to no-calls.; * Renamed VQSR Classic to 'VQSR'; * Renamed VQSR Lite to 'VETS'; * Updated VCF and pgen tests for code changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8797:271,Update,Updated,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8797,2,['Update'],['Updated']
Deployability,"* Added liftover chain file creation script.; * Added WDLs and some arguments to lift over gnomAD; * Added chain file for b37->hg38 and arguments for liftover.; * Limited to 1000 records in memory.; * Added stack trace option to all wdls and sub tasks.; * Fixed output to be consistent with local files for indexing.; * Added timing information on wdls.; * Added a wdl/json to create a TSV from gnomAD allele freq data.; * Updated indexFeatureFile wdl, added params for run to index gnomAD.; * Added json file for indexing a large gnomad file.; * Fixed critical issues with NIO data sources.; * Updates to the test script to save output and point to full cloud data.; * Added some logging hooks to SeekableByteChannelPrefetcher. I haven't reviewed this since I made the changes to it to see what should stay, so it may need a fair bit of work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5514:423,Update,Updated,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5514,2,['Update'],"['Updated', 'Updates']"
Deployability,"* Added new option --unfilteredBreakpointEvidenceDir; When set, this option dumps all evidence (even evidence that is; ultimately rejected) in an easy to parse text format. Some additional; info (cigarString, mappingQuality) is stored in ReadEvidence to output; information related to read quality.; * Updated option --readMetadata; When set, will additionaly output the map from contig number to contig; name. Added non-null ParitionBounds to readMetadata in; ReadMetadataTest::testEverything to prevent crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3691:302,Update,Updated,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3691,1,['Update'],['Updated']
Deployability,"* Added support for annotating 5'/3' flanks via new FIVE_PRIME_FLANK and THREE_PRIME_FLANK funcotations. * Added --five-prime-flank-size and --three-prime-flank-size arguments to control the size of each flanking region. * Refactored datasource classes to allow for padded/custom queries to make this feature possible. * We now emit IGR funcotations in more cases (in particular, when a gene has no basic transcripts, and when the basic transcripts do not fully span a gene and the flank size is small). * Added comprehensive unit tests, and updated integration test data. Resolves #4771",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5403:542,update,updated,542,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5403,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,* Adding a beta version of http-nio which allows streaming http files and seeking within them.; * This allows using https urls including signed urls to access remote files.; * Bams/crams can be read by specifying the index manually. Automatic index resolution does not work correctly at the moment.; * known caveats; * some methods are not implement in the nio filesystem library yet; * failures are not retryied. I'm currently fighting with sonatype to get a real release pushed out... it seems close...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6526:465,release,release,465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6526,1,['release'],['release']
Deployability,* Adding the dataproc-cluster-ui script to the release bundle so users can access it.; * Fixes #5400,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5401:47,release,release,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5401,1,['release'],['release']
Deployability,* Centralizes Docker image versioning to top-level WDLs; * Does away with GATK override jar in all cases except integration tests (override jar can still be specified during feature development and/or for emergencies); * Docker image versions can be captured as the inputs to tasks; * Freshens Variants Docker image. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/815ef8ea-8cfe-47b6-be80-54250d1f180b),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8457:112,integrat,integration,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8457,2,"['Integrat', 'integrat']","['Integration', 'integration']"
Deployability,* Change extract so that when we filter at the genotype level (with FT) the VCF header has the FT filter definition in the comment/unspecified field.; * Also minor renaming of ExtractCohort argument.; * Point to updated truth.; ; [Here's](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/94129da8-6faf-419b-ab75-a46c228b1bbe) an integration test run. Passing everything except ValidateVDS because `reference_data` not being written due to issues beyond my control.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8850:212,update,updated,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8850,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,* Have GvsCreateVATfromVDS.wdl take sites-only-vcf as an optional input.; * Added logic to allow/disallow CopyFile to overwrite. [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/bb8906d4-7111-4fd1-a723-b5616b354c23) is a passing run using an existing sites-only VCF.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/9c8be4d5-f707-4c54-bde5-18d9d23cde66) is a run where it tried to generate the sites-only VCF. Failing because of Echo issues with creating VDS.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8f8cc493-b0ff-4d8c-8813-6c463dbf17c0) is an integration test. It's failing in ValidateVDS on two paths (the ones that create VDSes) since this is based off of EchoCallset branch - this is expected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8866:671,integrat,integration,671,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8866,1,['integrat'],['integration']
Deployability,* Optionally extract to bgz format.; * Set bgzipping to be off (everywhere) by default.; * Update assert_identical_outputs to handle bgzipped outputs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8820:91,Update,Update,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8820,1,['Update'],['Update']
Deployability,* Successful run in PMI land that finds nothing to clean up [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20First%20Look/job_history/ba861882-96ee-4635-b522-2fe9489b0076).; * Successful run in Integration land that finds loads to clean up [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/01667ae7-fd85-4a12-abcb-69e892500fa3).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8644:225,Integrat,Integration,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8644,1,['Integrat'],['Integration']
Deployability,* The previous attempt to fix requester pays didn't fix it in many cases.; This incorporates a newer version of the NIO library with several patches to fix; edge cases we were hitting.; * https://github.com/googleapis/java-storage-nio/issues/849; * https://github.com/googleapis/java-storage-nio/issues/856; * https://github.com/googleapis/java-storage-nio/issues/857; * upgrade com.google.cloud:google-cloud-nio:0.123.23 ->0.123.25; * fixes https://github.com/broadinstitute/gatk/issues/7716,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7730:141,patch,patches,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7730,2,"['patch', 'upgrade']","['patches', 'upgrade']"
Deployability,* This fixes an issue while installing gcloud on travis due to permissions in the root directories. @ldgauthier This should fix the issue you were seeing where test files weren't being uploaded.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7525:28,install,installing,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7525,1,['install'],['installing']
Deployability,* This updates us from commons-text:1.6.0 -> 1.10.0 to fix a vulnerability; * fixes https://github.com/broadinstitute/gatk/issues/8060,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8071:7,update,updates,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8071,1,['update'],['updates']
Deployability,* Upating disq 0.3.3 -> 0.3.4; * This release makes use of new features in htsjdk 2.21.0 which were previously part of disq itself.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6252:38,release,release,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6252,1,['release'],['release']
Deployability,* Update Picard 2.23.0 -> 2.25.0; * Add serialVersionUID to classes now marked as Serializable in picard.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7075:2,Update,Update,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7075,1,['Update'],['Update']
Deployability,* Update htsjdk 2.20.1 -> 2.20.2; * This release fixes https://github.com/broadinstitute/gatk/issues/6091,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6094:2,Update,Update,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6094,2,"['Update', 'release']","['Update', 'release']"
Deployability,"* Update htsjdk to 2.7.0; * Remove usage of deprecated `SAMRecordUtil`; * Remove usage of deprecated `IndexFactory.writeIndex(index, indexFile)`; * Cleaning `IndexFeatureFile`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2247:2,Update,Update,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2247,1,['Update'],['Update']
Deployability,* Update picard 2.21.1 -> 2.21.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6253:2,Update,Update,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6253,1,['Update'],['Update']
Deployability,* Update picard from 2.20.7 -> 2.21.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6205:2,Update,Update,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6205,1,['Update'],['Update']
Deployability,* Updates htsjdk to 2.7.0; * Use `Index.write(File)` for write the index independently of type; * Removed unused imports,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2244:2,Update,Updates,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2244,1,['Update'],['Updates']
Deployability,"* Updating htsjdk 2.18.1 -> 2.18.2; * Remove deprecated method use; * Changing IntervalUtilsUnitTest due to changes in IntervalList; * IntervalList now rejects certain invalid intervals that it previously didn't and throw IllegalArgumentException.; * This ends up changing which exceptions are thrown in some cases, updated some tests to accept both MalformedFile and MalforedGenomeLoc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5585:316,update,updated,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5585,1,['update'],['updated']
Deployability,* `JointVariantCalling` [does set](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f3f98f0e-2a7f-460b-886f-3442551140a8) `tighter_gcp_quotas`.; * Integration tests [do not set](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/51213d40-7583-49f1-a101-1842180a6470) `tighter_gcp_quotas`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8540:170,Integrat,Integration,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8540,1,['Integrat'],['Integration']
Deployability,* add a new deploy key for travis to use to authenticate to github,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7524:12,deploy,deploy,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7524,1,['deploy'],['deploy']
Deployability,* add an new option to VariantsToTable to allow output VCF style numeric GT fields; previously it always output the actual bases of the Allele in the GT spot; * resolves https://github.com/broadinstitute/gatk/issues/8160; * updates htsjdk to 3.0.5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8219:224,update,updates,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8219,1,['update'],['updates']
Deployability,"* added a reference parameter to FeatureData source and FeatureManager methods; * genomicsDB requires a reference, previously this was being passed; through by hardcoding it in the required json files; * json files are now autogenerated by the importer tool, but the; reference wasn't being handled correctly. * updated the various walkers to pass the reference through if available. * gendb:// paths now point to the workspace directory instead of a; directory of jsons. * removed the ability to specify array, vidmap.json, and; callset.json paths in the importer tool since we now rely on the; structure and naming of the files when loading; moved some constants to GenomicsDBConstants. * updated GenomicsDBIntegration tests to use the new importer instead of a; prepackaged and very brittle set of json files. fixed a bug in GenomicsDBImporterIntegrationTests that made both tests; write to the same workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2626:312,update,updated,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626,2,['update'],['updated']
Deployability,"* changing the key hash we use to download R package keys on travis from an insecure 32 bit hash that has been compromised to a more secure longer hash; * we will no longer be installing the ""Totally Legit Signing Key""; * see https://evil32.com/ for a summary of the problem",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5214:176,install,installing,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5214,1,['install'],['installing']
Deployability,* com.intel.gkl:gkl:0.8.8 -> 0.8.10. @droazen @kachulis Maybe we should update to the newest version and rerun the tests?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8181:72,update,update,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8181,1,['update'],['update']
Deployability,* disables tests that use the now defunct google genomics reference API; * update BaseRecalibratorSparkIntegrationTest.testBQSRFailWithIncompatibleReference to not use the reference API; * fixes #4163; * these tests should be revisted and removed or replaced in #4166,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4178:75,update,update,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4178,1,['update'],['update']
Deployability,* improvements leading to tieout of cohort extract; * WDLs for running WARP pipeline for tieout; * tweaks to WDLs (memory size) for running extract,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7062:76,pipeline,pipeline,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7062,1,['pipeline'],['pipeline']
Deployability,* update gradle wrapper 8.2.1 -> 8.10.2; * remove 'versions' plugin because we don't use it; * update gradle plugins to new versions; * shadow plugin changed maintainers and coordinates com.github.johnrengelman.shadow:8.1.1 -> com.gradleup.shadow:8.3.3; * git-version 0.5.1 -> 3.1.0; * sonatype scan 2.6.1 -> 2.8.3; * download 5.4.0 -> 5.6.0; * use tasks.register() which is the newer style,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8998:2,update,update,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8998,2,['update'],['update']
Deployability,* update setup-gcloud@v0 -> v2 since v0 is deprecated,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8651:2,update,update,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8651,1,['update'],['update']
Deployability,* updating Intel-GKL from 8.5 -> 8.6; * this is a very minor update that only changes a log message; * fixes https://github.com/broadinstitute/gatk/issues/5393,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5463:61,update,update,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5463,1,['update'],['update']
Deployability,* updating htsjdk 2.16.1 -> 2.18.0; * the most noticable change is that we will now produce bam 1.6 instead; of 1.5; * some test files updated to have the new version since they were being; compared with exact match tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5424:135,update,updated,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5424,1,['update'],['updated']
Deployability,* upgrade protobuf-java 3.19.4 -> 3.21.6,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8036:2,upgrade,upgrade,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8036,1,['upgrade'],['upgrade']
Deployability,****; org.broadinstitute.hellbender.exceptions.UserException$MissingReference: The specified fasta file (file:///user/hadoop/gatk/common/human_g1k_v37.20.21.fasta) does not exist.; at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.checkFastaPath(CachingIndexedFastaSequenceFile.java:173); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:143); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:125); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:110); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:148); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:277); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:546); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730:3624,pipeline,pipelines,3624,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730,1,['pipeline'],['pipelines']
Deployability,"**Brief issue description:** ; When following the tutorial https://gatk.broadinstitute.org/hc/en-us/articles/360035531092--How-to-part-I-Sensitively-detect-copy-ratio-alterations-and-allelic-segments, the #4 Plot standardized and denoised copy ratios with PlotDenoisedCopyRatios have different results than the tutorial. Through the control vectors test, it seems that the samples that are used in step #2 to generate CNV PON used in the tutorial are different from the files stored in the tutorial.; **Results:**; Following steps 1 to 4, the resulting plots; ![hcc1143_T_clean denoised](https://github.com/broadinstitute/gatk/assets/89409924/3bce4382-5109-4c6e-b34d-1c6e365dcf62); ![hcc1143_T_clean denoisedLimit4](https://github.com/broadinstitute/gatk/assets/89409924/9d23987c-2747-43af-b72c-4e3754015531); The results have values However, the values in the tutorial are 0.134 and 0.125.; **Tests**; Using the files provided in the tutorial and script generated `cnvponC.pon.hdf5`, which seems to lead to this inconsistency result.; Using:; gatk --java-options ""-Xmx6500m"" CreateReadCountPanelOfNormals \; -I HG00133.alt_bwamem_GRCh38DH.20150826.GBR.exome.counts.hdf5 \; -I HG00733.alt_bwamem_GRCh38DH.20150826.PUR.exome.counts.hdf5 \; -I NA19654.alt_bwamem_GRCh38DH.20150826.MXL.exome.counts.hdf5 \; --minimum-interval-median-percentile 5.0 \; -O sandbox/cnvponC.pon.hdf5; **Files**; The script used to generate this result are attached. ; [gatk_tutorial11682_issue.zip](https://github.com/user-attachments/files/15930567/gatk_tutorial11682_issue.zip). Please help me understand this difference in reproducing the tutorial result. It will be extremely helpful for me to use the pipelines on our lab-generated data. Thank you very much!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8884:1682,pipeline,pipelines,1682,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8884,1,['pipeline'],['pipelines']
Deployability,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1935:10,integrat,integration,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935,3,"['Update', 'integrat']","['Updated', 'integration']"
Deployability,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5919:136,upgrade,upgraded,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919,2,"['pipeline', 'upgrade']","['pipeline', 'upgraded']"
Deployability,**UPDATE**; Add proposed heuristic alignment filtering/picking of long reads for later cpx SV resolving.; Solves #3221 . . Changed `AlignedContig` by adding a boolean field to signal if several equally good alignment configurations exist for downstream analysis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3432:2,UPDATE,UPDATE,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3432,2,"['UPDATE', 'configurat']","['UPDATE', 'configurations']"
Deployability,*ON HOLD*: Update SharedSequenceMerger.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4214:11,Update,Update,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4214,1,['Update'],['Update']
Deployability,"+ blaunch -no-wait -z hpcgenomicn24 /spark-1.6.2-bin-hadoop2.6//bin/spark-class org.apache.spark.deploy.worker.Worker spark://hpcgenomicn24:6311 -c 16; + echo --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:97,deploy,deploy,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['deploy'],['deploy']
Deployability,", [chr1] and [chrM, chr1].; 	at htsjdk.tribble.index.tabix.TabixIndexMerger.processIndex(TabixIndexMerger.java:47); 	at htsjdk.tribble.index.tabix.TabixIndexMerger.processIndex(TabixIndexMerger.java:19); 	at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); 	at org.disq_bio.disq.impl.formats.vcf.VcfSink.save(VcfSink.java:120); 	at org.disq_bio.disq.HtsjdkVariantsRddStorage.write(HtsjdkVariantsRddStorage.java:150); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariantsSingle(VariantsSparkSink.java:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delega",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5997:1148,pipeline,pipelines,1148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997,1,['pipeline'],['pipelines']
Deployability,"- Adds the gatk conda environment, with dependencies defined by the file scripts/gatkcondaenv.yml.; - Updates the docker image to include the activated conda environment.; - Adds a new entry to the travis test matrix for running tests that depend on Python and the conda environment.; - Adds a python test group. Any tests for tools or functionality that are dependent on Python should be put into this group. Tests in this group will be executed in a docker container on travis in the python build matrix entry only.; - The existing WDL tests are unchanged; so although they execute in the context of the docker container and conda environment, there are no tests (yet) that are actually dependent on the conda environment and run through cromwell.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912:104,Update,Updates,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912,1,['Update'],['Updates']
Deployability,- ApplyBQSR adapted to fit into the Skeleton pipeline; - command-line version still works and passes tests (including cloud); - BaseRecalibrator's testPlottingWorkflow now passes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/815:45,pipeline,pipeline,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/815,1,['pipeline'],['pipeline']
Deployability,- CEUTrio.HiSeq.WGS.b37.ch20.4379150-4379157.bam is a very small input that triggers the bug.; - TestMath is a small demonstration of the underlying problem (order of operations changes the answer); - RecalDatum.java is updated to fix the problem; - BaseRecalibratorDataflowIntegrationTest runs the new code and confirms it's OK.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/878:220,update,updated,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/878,1,['update'],['updated']
Deployability,"- Clarify tool documentation:; - Update `joint posteriors (JL)` to `joint posteriors (JP)`; - Remove statistical notes and provide link to GATK Article#11074 for background and math; - Consolidate Notes and Caveats sections; - Clarify at top the three different sources of priors and tool behavior regarding these; - Clarify for family priors the tool only considers trio groups; - Add Laura's comment that recent updates allow the tool to appropriately apply priors to indels; - Change logger.info to logger.warn for situation where trio pedigree file is incomplete; - Note that in this situation, in the absence of other refinement, the results are identical to the input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5601:33,Update,Update,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601,2,"['Update', 'update']","['Update', 'updates']"
Deployability,- Closes #5114 ; - Updates the WDL as well to expose the new files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5115:19,Update,Updates,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5115,1,['Update'],['Updates']
Deployability,- Creating PR for update to Funcotator documentation and to show off CARROT integration.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6920:18,update,update,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6920,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"- DF_BaseRecalibrator tool, can be called from the command line. Same syntax as BaseRecalibrator.; - BaseRecalibrator's integration tests ported to this Dataflow version.; - Small changes to make types serializable.; - Note that this pull request is an intermediate step as it only works for local computations. (this depends on [PR#522](https://github.com/broadinstitute/hellbender/pull/522))",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/523:120,integrat,integration,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/523,1,['integrat'],['integration']
Deployability,- Extracted the order validation for GVCF files into a separate method and included; a check to reset the counter when a new contig is found. Contigs have to; occur in continuous blocks; validation for files in which contigs occur; alternatingly is not supported.; - Added a set of integration tests for GVCF files with two and three contigs. Fixes #6023,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6028:168,continuous,continuous,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6028,2,"['continuous', 'integrat']","['continuous', 'integration']"
Deployability,"- Fixes https://github.com/broadinstitute/gatk/issues/4696, https://github.com/broadinstitute/gatk/issues/4342, https://github.com/broadinstitute/gatk/issues/4443, https://github.com/broadinstitute/gatk/issues/4444.; - Use a second FIFO for command acknowledgement instead of relying on prompt synchronization.; - Add a Python module for managing the Python side of GATK/Python interaction.; - Removed all timeouts.; - Install a Python exception handler for handling uncaught Python exceptions.; - Update CNNScoreVariants to use the new protocol.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4757:419,Install,Install,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757,2,"['Install', 'Update']","['Install', 'Update']"
Deployability,- M2 WDL has explicit optional parameter for a list of fields that should be excluded from the output.; - Both M2 WDL files are updated. Manually tested mutect2.wdl on local backend. Closes #5141,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5242:128,update,updated,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5242,1,['update'],['updated']
Deployability,"- MAF is the output of Funcotator in M2 WDL. Closes #4935 ; - Updated mutect2.wdl manually tested locally and manually tested in FireCloud.; - Updated mutect2_nio.wdl manually tested in FireCloud.; - Updated automatic Cromwell WDL tests. Closes #4807 ; - Empty MAFs will be devoid of variants, not a file of 0 bytes. Closes #4937 ; - Fixed issue where multiple transcripts could be selected in edge cases, even when CANONICAL or BEST_EFFECT was selected. Closes #4952",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4941:62,Update,Updated,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941,3,['Update'],['Updated']
Deployability,- Modified the files so that we can build libVectorLoglessPairHMM.so on Ubuntu/ppc64le platform; - Restored and modified the files for 128-bit vector that are on GATK3; - Added a new file to replace AVX with POWER8 vector instructions; - [Question] Is any unit test included in the repository to test the library?; - Confirmed that the library was built on Ubuntu 15.10/ppc64le. ```; ./gradlew installAll; :downloadGsaLibFile UP-TO-DATE; :extractIntelDeflater; :compileJava; :processResources; :classes; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; :linkVectorLoglessPairHMMSharedLibrary; :copySharedLib; :jar; :startScripts; :installDist; :sparkJar; :installSpark; :installAll. BUILD SUCCESSFUL; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1748:394,install,installAll,394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748,4,['install'],"['installAll', 'installDist', 'installSpark']"
Deployability,- Move command line parser integration of read filters up to GATKTool (https://github.com/broadinstitute/gatk/issues/2175); - Added fromList method to ReadFilter and CountingReadFilter (https://github.com/broadinstitute/gatk/issues/2198); - Minor change/rationalization of naming and implementation of BQSR filter methods to match the rest of the framework; - Made a small change to the implementation of the base read filter class to improve clarity/testability; - Opportunistic removal of extraneous imports in unrelated classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2218:27,integrat,integration,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2218,1,['integrat'],['integration']
Deployability,"- Moved tools to ""Metagenomics"" program group; - Updated tool docs; - Changed tool arguments to kebab-case; - Defined argument strings as static variables that are cross-referenced in integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3918:49,Update,Updated,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3918,2,"['Update', 'integrat']","['Updated', 'integration']"
Deployability,- Now will count downloads of all artifacts in github releases instead of just the first one.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8418:54,release,releases,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8418,1,['release'],['releases']
Deployability,"- Now will detect variants on mitochondrial contigs and will use the; correct, alternate coding sequence to create protein change strings for; such variants.; - Added MT sequences to Gencode data source.; - Added tests for MT protein change strings.; - Now `FuncotatorUtils::getMitochondrialAminoAcidByCodon` has more; complete tests and handles special cases for known initiation site; differences by genus.; - Updated scripts to detect the directory in which the scripts are run.; - Added MT variants to integration tests.; - Added MT genes to gencode testing data source. Fixes #4863",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5361:412,Update,Updated,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5361,2,"['Update', 'integrat']","['Updated', 'integration']"
Deployability,"- Push to `gvs` repo in `broad-dsde-methods` rather than `variantstore`.; - Fix the way image IDs are discovered.; - Support both `alpine` and non-`alpine` image types, setting up support for plink2. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/c1c2fc65-104b-46af-a536-882d7c1e8954).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8791:211,integrat,integration,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8791,1,['integrat'],['integration']
Deployability,"- Refactored GencodeGtfCodec to enable parsing of ENSEMBL GTF files.; - Created AbstractGtfCodec and EnsemblGtfCodec.; - Updated Funcotator and Funcotation Factories to allow ENSEMBL-based; GTF files.; - Added an e. coli data sources folder, reference, VCF, and expected; data for testing.; - Added tests for ENSEMBL GTF files. Fixes #6180",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6477:121,Update,Updated,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6477,1,['Update'],['Updated']
Deployability,- Relaxes restrictions for allowed samples in SVConcordance: the tool can now accept eval/truth VCFs with arbitrary sample sets and will have genotype concordance metrics computed on the intersection of the sample sets. All available samples are still used for AF/AC annotations. Integration tests added for cases when the samples sets are overlapping but not equal.; - Small additional improvements for sites-only VCFs: concordance annotations will now be `.` instead of `NaN` for example. Integration test added for this case.; - Improved behavior for eval AF annotations: these will not be recalculated if they already exist.; - Improved behavior for truth AF annotations: these will now only be recalculated if they don't exist in the input truth VCF.; - Updated tool doc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8211:280,Integrat,Integration,280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8211,3,"['Integrat', 'Update']","['Integration', 'Updated']"
Deployability,"- Released new data sources to google bucket and FTP site for both somatic and germline (clinical pipeline); - Updated data source download URL to point to the bucket for v1.6.20190124; - Updated minimum version of data sources to v1.6.20190124. With the release and these changes, the following issues are addressed:. Fixes #5259 ; Fixes #5428 ; Fixes #5429",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5614:2,Release,Released,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5614,5,"['Release', 'Update', 'pipeline', 'release']","['Released', 'Updated', 'pipeline', 'release']"
Deployability,- Remove some unused VCF header fields from ExtractFeatures; - Renamed VQSR Lite fields to their original naming (e.g. AS_VQS_SENS becomes CALIBRATION_SENSITIVITY). Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/67a00690-5b74-40fd-a0fb-5ab2b0407a4d) - uses updated truth. Example outputs can be found in [this](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/22134bb6-e4b5-4252-b674-860a1168fb6c) Extract run.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8412:173,integrat,integration,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8412,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,- Removed positive-negative training from TrainVariantAnnotationsModel along with associated integration and WDL tests.; - Added ability to run positive-unlabeled training by passing unlabeled annotations to a custom python backend (although no example backend or tests were added).; - Cleaned up some WDL arguments to allow distinct training and scoring python scripts.; - Removed the `useAlleleSpecificAnnotations` argument; we instead infer whether to run in allele-specific mode from the VCF header.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131:93,integrat,integration,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131,1,['integrat'],['integration']
Deployability,"- Running in mitochonrial mode; - Link to liquid biopsy blogpost will need to be added once it is posted; - Specifying tumor name is no longer necessary; - Af-of-alleles-not-in-resource is dynamically adjusted for modes; - Joint calling on multiple tumor and normal samples; - Mention and/or link to FilterMutectCalls, Funcotator and CreateSomaticPanelOfNormals; - Remove caveat and state instead M2's ability to handle extreme high depths; - Update HaplotypeCaller link from v3 to current; - Add 'Notes' section and make ordered list; - Move example AF resource snippet to 'Notes' section",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5769:443,Update,Update,443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5769,1,['Update'],['Update']
Deployability,- Tool creates histograms to reflect differences in the composition reference blocks in GVCF files; - Integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6802:102,Integrat,Integration,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6802,1,['Integrat'],['Integration']
Deployability,- Update htsjdk to 2.6.1; - Fix #2080,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2131:2,Update,Update,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2131,1,['Update'],['Update']
Deployability,- Updated data sources to include variant sites for symbolic alleles.; - Fixed tests to be correct for new logic.; - Now has tests for symbollic alternate alleles and masked alleles. Fixes #5402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5406:2,Update,Updated,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5406,1,['Update'],['Updated']
Deployability,"- User defined transcripts were being used as a filter rather than a priority order. The filtering step has been eliminated. Closes #4918 ; - Fixed previously unidentified issue where locus level ranking was being reversed. Updated tests. This was identified thanks to the thousands of tests in Funcotator (only one failed, but that was all it took).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4931:224,Update,Updated,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931,1,['Update'],['Updated']
Deployability,"- [ ] Currently [BaseRecalibrator runs with 4g of memory](https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/PairedSingleSampleWf.wdl#L535-L554) in the production pipeline. We should check that it still does if we have bam index caching on. . - [ ] Similarly, make sure [ApplyBQSR runs with 3g of memory](https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/PairedSingleSampleWf.wdl#L576-L597)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2380:97,pipeline,pipelines,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2380,3,['pipeline'],"['pipeline', 'pipelines']"
Deployability,- added GvsAssignIds to .dockstore.yaml; - added logic to GvsAssignIds to prevent bug from empty input; - updates to Quickstart README directions. Closes https://broadworkbench.atlassian.net/browse/VS-183,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7463:106,update,updates,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7463,1,['update'],['updates']
Deployability,- added up-to-date docker image for prepare step; - updated documentation to refer to the right past steps,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7522:52,update,updated,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7522,1,['update'],['updated']
Deployability,- error message was not updated when arguments were changed....,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5969:24,update,updated,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5969,1,['update'],['updated']
Deployability,"- improved baits count annotator (""lazy"" post processing); - included bait counts as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper; - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts; - missing test resource files from gatk-protected repo; - address PR review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3183:341,integrat,integration,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183,2,['integrat'],['integration']
Deployability,"- matches the bootstrap updates for formatting ; - minor text tweaks, displays version clearly; - adds version switching menu. These changes will make it easier to upload new version docs quickly. This should definitely go in before the next version release. . Resulting docs are live at https://software.broadinstitute.org/gatk/documentation/tooldocs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4805:24,update,updates,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4805,2,"['release', 'update']","['release', 'updates']"
Deployability,"- new ""lazy"" annotation mode in TargetAnnotator (a hack for generating annotations that can not be done with a state-less FeatureWalker); - baits count target annotation; - included bait count as a multiplicative bias in TargetCoverageSexGenotypeCalculator; - improved info and warn log messages in TargetCoverageSexGenotyper; - interval exclusion via CLI args for TargetCoverageSexGenotyper (PAR regions can not be blacklisted via CLI arguments); - soft target filtering using masks; - more extensive unit/integration tests for TargetCoverageSexGenotyper; - integration test for annotate targets w/ bait counts",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2813:507,integrat,integration,507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2813,2,['integrat'],['integration']
Deployability,"- new version of BaseRecalibratorDataflow that fits into the skeleton framework; - new command-line BaseRecalibratorDataflow that uses the same code; - tests and test inputs for BaseRecalibrator. They pass, locally and on the cloud.; - fix for issue #791 via a new genomics-dataflow-java release; - smaller changes, like == -> .equals in SequenceDictionaryUtils and using getInstance() to follow the singleton pattern.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/812:288,release,release,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/812,1,['release'],['release']
Deployability,- restore job stats collection to create_ranges_cohort_extract_data_table.py; - add writing of cost info to BigQuery table to create_ranges_cohort_extract_data_table.py and populate_alt_allele_table.py; - add task to GvsQuickstartIntegration.wdl that checks that expected cost data was written to BigQuery table; - tweaked schema for cost_observability table to include descriptions; ; Integration test run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/30a2d8ee-13dd-4829-b3a8-4e6a67409705; Closes https://broadworkbench.atlassian.net/browse/VS-480,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7915:386,Integrat,Integration,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7915,1,['Integrat'],['Integration']
Deployability,- update to gradle 8.4 and build with java 21; - fixing or supressing various warnings; - in progress; - allow warnings and update actions to 21 so we can run tetsts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8589:2,update,update,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8589,2,['update'],['update']
Deployability,"- updated integration test (removed that argument so it defaults to the same thing); - tested that using ""NONE"" as an argument works",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7206:2,update,updated,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7206,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,"- uses GvsExtractCohortFromSampleNames.wdl to generate VCFs for calculating P & S all in one WDL; - allows for use of an interval_list in P & S; - updates to docs; - Successful run here: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/2537ea7b-f635-4609-8fbb-7eaec41a6df8; - integration run, since I touched the bulk ingest and extract VCF WDLs: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f71941ba-f02f-4472-a04c-aedff24fdd14",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8707:147,update,updates,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8707,2,"['integrat', 'update']","['integration', 'updates']"
Deployability,"-- ; ; 00:12:21.142 INFO BaseRecalibrator - HTSJDK Version: 2.24.1 ; ; 00:12:21.143 INFO BaseRecalibrator - Picard Version: 2.27.1 ; ; 00:12:21.143 INFO BaseRecalibrator - Built for Spark Version: 2.4.5 ; ; 00:12:21.143 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 00:12:21.143 INFO BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 00:12:21.143 INFO BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 00:12:21.143 INFO BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 00:12:21.143 INFO BaseRecalibrator - Deflater: IntelDeflater ; ; 00:12:21.144 INFO BaseRecalibrator - Inflater: IntelInflater ; ; 00:12:21.144 INFO BaseRecalibrator - GCS max retries/reopens: 20 ; ; 00:12:21.144 INFO BaseRecalibrator - Requester pays: disabled ; ; 00:12:21.144 INFO BaseRecalibrator - Initializing engine ; ; 00:12:21.485 INFO FeatureManager - Using codec VCFCodec to read file file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz ; ; 00:12:21.565 INFO FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz ; ; 00:12:21.688 INFO FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz ; ; 00:12:21.797 WARN IndexUtils - Feature file ""file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file ; ; 00:12:21.895 WARN IntelInflater - Zero Bytes Written : 0 ; ; 00:12:21.966 INFO BaseRecalibrator - Done initializing engine ; ; 00:12:21.969 INFO BaseRecalibrationEngine - The covariates being used here: ; ; 00:12:21.969 INFO BaseRecalibrationEngine -   ReadGroupCovariate ; ; 00:12:21.969 INFO BaseRecalibrationEngine -   QualityScoreCovari",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:17815,pipeline,pipeline,17815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"---------------------------------------------------; 01:07:02.003 INFO GenomicsDBImport - ------------------------------------------------------------; 01:07:02.004 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 01:07:02.005 INFO GenomicsDBImport - Picard Version: 2.22.8; 01:07:02.005 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 01:07:02.005 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 01:07:02.005 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:07:02.005 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:07:02.005 INFO GenomicsDBImport - Deflater: IntelDeflater; 01:07:02.005 INFO GenomicsDBImport - Inflater: IntelInflater; 01:07:02.006 INFO GenomicsDBImport - GCS max retries/reopens: 20; 01:07:02.006 INFO GenomicsDBImport - Requester pays: disabled; 01:07:02.006 INFO GenomicsDBImport - Initializing engine; 01:07:02.331 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 01:07:02.702 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 01:07:02.868 INFO IntervalArgumentCollection - Processing 135534747 bp from intervals; 01:07:02.869 INFO GenomicsDBImport - Done initializing engine; 01:07:02.870 INFO GenomicsDBImport - Callset Map JSON file will be re-written to /paedwy/disk1/yangyxt/wes/healthy_bams_for_CNV/using_v6_probe/genomicdbimport_chr10/callset.json; 01:07:02.870 INFO GenomicsDBImport - Incrementally importing to workspace - /paedwy/disk1/yangyxt/wes/healthy_bams_for_CNV/using_v6_probe/genomicdbimport_chr10; 01:07:02.871 INFO ProgressMeter - Starting traversal; 01:07:02.871 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 01:07:03.006 INFO GenomicsDBImport - Shutting down engine; [August 29, 2020 at 1:07:03 AM HKT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:12254,update,update-workspace-path,12254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['update'],['update-workspace-path']
Deployability,"---------------------------------------------------; 10:49:12.233 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.233 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 10:49:12.233 INFO GenomicsDBImport - Picard Version: 2.22.8; 10:49:12.234 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:12.234 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:12.234 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:49:12.234 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:49:12.234 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:49:12.234 INFO GenomicsDBImport - Inflater: IntelInflater; 10:49:12.234 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:49:12.234 INFO GenomicsDBImport - Requester pays: disabled; 10:49:12.235 INFO GenomicsDBImport - Initializing engine; 10:49:12.577 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 10:49:12.938 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 10:49:13.163 INFO IntervalArgumentCollection - Processing 51304566 bp from intervals; 10:49:13.163 INFO GenomicsDBImport - Done initializing engine; 10:49:13.164 INFO GenomicsDBImport - Callset Map JSON file will be re-written to /mnt/mone/OMICS/Project/Joint_call/GATK_GenomicDB/test_database/test_overwrite_1/callset.json; 10:49:13.164 INFO GenomicsDBImport - Incrementally importing to workspace - /mnt/mone/OMICS/Project/Joint_call/GATK_GenomicDB/test_database/test_overwrite_1; 10:49:13.164 INFO ProgressMeter - Starting traversal; 10:49:13.164 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 10:49:13.231 INFO GenomicsDBImport - Shutting down engine; [June 18, 2021 10:49:13 AM KST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport don",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7324:3164,update,update-workspace-path,3164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324,1,['update'],['update-workspace-path']
Deployability,---------------------------------------------; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Version: 2.24.0; 01:22:35.483 INFO GenomicsDBImport - Picard Version: 2.25.0; 01:22:35.483 INFO GenomicsDBImport - Built for Spark Version: 2.4.5; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:22:35.483 INFO GenomicsDBImport - Deflater: IntelDeflater; 01:22:35.483 INFO GenomicsDBImport - Inflater: IntelInflater; 01:22:35.483 INFO GenomicsDBImport - GCS max retries/reopens: 20; 01:22:35.483 INFO GenomicsDBImport - Requester pays: disabled; 01:22:35.484 INFO GenomicsDBImport - Initializing engine; 01:24:58.683 INFO FeatureManager - Using codec BEDCodec to read file file:///lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - I,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7598:2814,update,update,2814,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598,1,['update'],['update']
Deployability,"--------------------------------; 22:28:44.640 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - HTSJDK Version: 2.21.0; 22:28:44.640 INFO GenotypeGVCFs - Picard Version: 2.21.2; 22:28:44.640 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:28:44.641 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:28:44.641 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:28:44.641 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:28:44.641 INFO GenotypeGVCFs - Deflater: IntelDeflater; 22:28:44.641 INFO GenotypeGVCFs - Inflater: IntelInflater; 22:28:44.641 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 22:28:44.641 INFO GenotypeGVCFs - Requester pays: disabled; 22:28:44.641 INFO GenotypeGVCFs - Initializing engine; ```. #### Steps to reproduce; I've followed the recommendation to process my genome in parallel, each chromosome at a time, so I created the commands based on the following pipeline:; ```; # HC; gatk HaplotypeCaller -R GRCh38.fasta -I sample.dedup.rg.csorted.bam -O sample1.HC.gvcf -ERC GVCF; gatk HaplotypeCaller -R GRCh38.fasta -I sample.dedup.rg.csorted.bam -O sample2.HC.gvcf -ERC GVCF; gatk HaplotypeCaller -R GRCh38.fasta -I sample.dedup.rg.csorted.bam -O sample3.HC.gvcf -ERC GVCF. # GenomicsDBImport for Chr1; export TILEDB_DISABLE_FILE_LOCKING=1 ; gatk GenomicsDBImport --java-options ""-Xmx4g -Xms4g"" -V sample1.HC.gvcf -V sample2.HC.gvcf -V sample3.HC.gvcf --genomicsdb-workspace-path GenomicsDB_1 --tmp-dir /tmp -L 1. # GenotypeGVCFs; gatk GenotypeGVCFs --java-options ""-Xmx12g -Xms12g"" -R GRCh38.fasta -V gendb://GenomicsDB_1 --tmp-dir /tmp -O samples.1.vcf; ```; The jobs were send to the HPC scheduler and were allocated 2 CPUs and up to 16GB of RAM each. Everything till the last genotype calling step worked fine (and quite quickly) ; #### Expected behavior; The tool should call variants from the G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7007:2740,pipeline,pipeline,2740,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7007,1,['pipeline'],['pipeline']
Deployability,"----. ## Bug Report. ### Affected tool(s) or class(es); - GATK; - gcnvkernel ; - theano. ### Affected version(s); - GATK 4.1.0.0; - gcnvkernel 0.0.7; - theano 0.9.0; - GCC 7.3.0. ### Description ; I have installed the python package theano(which is a requirement of gcnvkernel) with python 3.6.6 which is compiled with gcc 7.3.0. I am not using the conda environment to install these packages.; Then i tried to run theano-nose, but is giving me the following error:. ```sh. $ theano-nose; --; ; You can find the C code in this temporary file: /tmp/theano_compilation_error_gp0ar1kx; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x1a): is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x6b): is not found.; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:204,install,installed,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,3,"['INSTALL', 'install']","['INSTALLDIRGATK', 'install', 'installed']"
Deployability,"----. ## Bug Report. ### Affected tool(s) or class(es); GATK HaplotypeCaller and GenomicsDBImport . ### Affected version(s); Version=""4.1.1.0"". ### Description ; In 7% of 8M variants in a 9 sample variant calling there is a discordance at least once between the GT and PGT field of a sample. The discordance between the GT and PGT fields can be found in the GVCF files created by the HaplotypeCaller and in the multi-sample VCF created by GenomicsDBImport. . This issue and pullrequest might be related, but have not been updated since March. ; https://github.com/broadinstitute/gatk/issues/5727; https://github.com/broadinstitute/gatk/pull/5772. I also already created this post on the forum. ; https://gatkforums.broadinstitute.org/gatk/discussion/24465/how-can-a-homozygous-reference-0-0-genotype-gt-have-a-heterozygous-phased-genotype-pgt-of-0-1#latest. Just thought it might help to (also) ask here, for me and other people who encounter this issue. . #### Steps to reproduce. Run the script below on any multi-sample VCF file created by GenomicsDBImport. ; The GT and PGT discordance is already in the GVCF files. But I did not test this script on any GVCF file. . The most important bit of the script is this comparison between the allele sets of the GT and PGT field. ; ```; if gt_allele_set == {0} and pgt_allele_set == {0,1}:; variant_with_phase_homref_to_het_issue = True; if gt_allele_set == {2} and pgt_allele_set == {1}:; variant_with_phase_hom22_to_hom11_issue = True; elif gt_allele_set == {0,2} and pgt_allele_set == {0,1}:; variant_with_phase_het02_to_het01_issue = True; ```. ```; from cyvcf2 import VCF, Writer. path = ""/DA_1458/VSDA_1458-gatk-haplotype-joint-annotated.bcf""; output_path_hom_ref_to_het = ""/DA_1458/hom_gt_het_phase_issue/hom_gt_het_phase_issue_homref_to_het.vcf""; output_path_hom22_to_hom11 = ""/DA_1458/hom_gt_het_phase_issue/hom_gt_het_phase_issue_hom22_to_hom11.vcf""; output_path_het02_to_het01 = ""/DA_1458/hom_gt_het_phase_issue/hom_gt_het_phase_issue_het02_to_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6220:522,update,updated,522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6220,1,['update'],['updated']
Deployability,----. ## Bug Report. ### Affected tool(s) or class(es); GATK LiftoverVcf. ### Affected version(s); gatk/4.1.7.0. ### Description . The LiftoverVcf generates the following error. The error occurs with SVs where the INFO/END is not also lifted over. This results in INFO/END before the site start position which triggers the error.; ```; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar LiftoverVcf -I b37/HG002_SVs_Tier1_v0.6.vcf.gz -O b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz -CHAIN grch37_to_grch38.over.chain.gz --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; 10:20:35.165 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Sun Jul 26 10:20:35 EDT 2020] LiftoverVcf --INPUT b37/HG002_SVs_Tier1_v0.6.vcf.gz --OUTPUT b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz --CHAIN grch37_to_grch38.over.chain.gz --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz --REFERENCE_SEQUENCE /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --WARN_ON_MISSING_CONTIG false --LOG_FAILED_INTERVALS true --WRITE_ORIGINAL_POSITION false --WRITE_ORIGINAL_ALLELES false --LIFTOVER_MIN_MATCH 1.0 --ALLOW_MISSING_FIELDS_IN_HEADER false --RECOVER_SWAPPED_REF_ALT false --TAGS_TO_REVERSE AF --TAGS_TO_DROP MAX_AF --DISABLE_SORT false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:377,install,install,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,2,['install'],['install']
Deployability,----. ## Bug Report. ### Affected tool(s) or class(es); GenomicsDBImport. ### Affected version(s); - [X] Latest public release version [4.1.9.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File opening error; path=/storage/home/data/gendb/chr13/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/RAW_MQandDP.tdb; errno=122(Disk quota exceede; d); [TileDB::WriteState] Error: Cannot write segment to file.; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File opening error; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__book_keeping.tdb.gz; errno=122(Disk quota e; xceeded); [TileDB::utils] Error: (write_to_file_after_compression) Could not write compressed bytes to internal buffer; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__book_keeping.tdb.gz; ; errno=122(Disk quota exceeded); [TileDB::BookKeeping] Error: Cannot finalize book-keeping; Failure to write to file /storage/home/data/gendb/chr13/chr13$32310639$32310731/.__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__book_keeping.tdb.gz.; [TileDB::FileSystem] Error: (create_file) Failed to create file; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087/__tiledb_fragment.tdb; errno=122(Disk quota exceeded); [TileDB::utils] Error: (create_fragment_file) Failed to create fragment file; path=/storage/home/data/gendb/chr13/chr13$32310639$32310731/__7a3cf8dc-ea9d-4bf9-9e33-c87b91d94b0546913384130304_1605025432087; errno=122(Disk quota exceeded); 11:23:52.390 erro NativeGenomicsDB - pid=57964 tid=57984 VariantStorageManagerException exception : Error while finalizing TileDB array chr13$32310639$32310731; TileDB error message : [TileDB::WriteState] ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6950:119,release,release,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6950,1,['release'],['release']
Deployability,"----. ## Bug Report. ### Affected tool(s) or class(es); ReblockGVCF . ### Affected version(s); 4.2.0.0. ### Description ; When running ReblockGVCF the following exception occurs:. `java.lang.IllegalArgumentException: cannot add a genotype with GQ=-1 because it's not within bounds [0,20); `. #### Steps to reproduce. Using a gVCF created with 4.2.0.0 HaplotypeCaller... `gatk ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz`. #### Expected behavior; Should run to completion and create reblocked GVCF. #### Actual behavior; ```; Reblocking gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334:870,install,install,870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334,1,['install'],['install']
Deployability,"----. ## Bug Report; Hi, I'm trying the CNV detection pipeline from GATK: https://gatk.broadinstitute.org/hc/en-us/categories/360002310591; However, when running the Determine Germline Contig Ploidy step, I stumble upon this error. Please guide me to solve this problem. ### Affected tool(s) or class(es); ```; gatk DetermineGermlineContigPloidy \; -L /home/nguyen/RB1/RB1.cohort.gc.filtered.interval_list \; --interval-merging-rule OVERLAPPING_ONLY \; -I ... (63 tsv files output from CollectReadCounts); ```. ### Affected version(s); - GATK 4.1.6.1; ### Description ; Full error log:; ```; Traceback (most recent call last):; File ""/tmp/cohort_determine_ploidy_and_depth.380621677219090732.py"", line 119, in <module>; ploidy_task.engage(); File ""/home/nguyen/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/tasks/inference_task_base.py"", line 339, in engage; converged_continuous = self._update_continuous_posteriors(); File ""/home/nguyen/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/tasks/inference_task_base.py"", line 395, in _update_continuous_posteriors; assert not np.isnan(loss), ""The optimization step for ELBO update returned a NaN""; AssertionError: The optimization step for ELBO update returned a NaN; 11:09:59.446 DEBUG ScriptExecutor - Result: 1; 11:09:59.447 INFO DetermineGermlineContigPloidy - Shutting down engine; [April 28, 2020 11:09:59 AM ICT] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=623902720; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python /tmp/cohort_determine_ploidy_and_depth.380621677219090732.py --sample_coverage_metadata=/tmp/samples-by-coverage-per-contig8606344533091962323.tsv --output_calls_path=/home/nguyen/Exec/gatk-4.1.6.0/ploidy-calls --mapping_error_rate=1.000000e-02 --psi_s_scale=1.000000e-04 --mean_bias_sd=1.000000e-02 --psi_j_scale=1.000000e-03 --learning_rate=5.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6573:54,pipeline,pipeline,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6573,1,['pipeline'],['pipeline']
Deployability,"----. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; CombineGVCFs. ### Affected version(s); - [ ] Latest public release version [version?] Yes; - [ ] Latest master branch as of [date of test?] singularity. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; I am trying to combine GVCFs for joint-calllings and I am using the latest singularity release of GATK. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I am trying to run GATK and CombineGVCF failed.; I am using the following code:; singularity exec /fs/scratch/PHS0338/appz/GVCF/gatk_latest.sif \; gatk CombineGVCFs -R /users/PHS0338/jpac1984/data/Autosome.fasta \; --variant PA113.vcf.gz --variant PA113corr.vcf.gz --variant PA112.vcf.gz --variant PA112corr.vcf.gz --variant IN33.vcf.gz\; --variant IN33corr.vcf.gz --variant AL82.vcf.gz \; -O test.vcf.gz; It has all the parameters as mentioned in the website: https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs. #### Expected behavior; _Tell us what should happen_; According to the website (https://gatk.broadinstitute.org/hc/en-us/articles/360037593911-CombineGVCFs) about combineGVCF, it should have worked fine without any problems... I got the following error log:. 20:11:34.701 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 13, 2021 8:11:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 20:11:35.527 INFO CombineGVCFs - ------------------------------------------------------------; 20:11:35.527 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 20:11:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7311:143,release,release,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7311,2,['release'],['release']
Deployability,"-Added a ""large files"" directory to src/test/resoureces containing files managed; by ""git lfs"" rather than checked directly into the hellbender repository.; Updated setup instructions in README appropriately to reflect new requirement; for git lfs. -Added a bam with ~600,000 reads from chromosomes 20 and 21, as well as ~50000; unmapped reads. -Added a snippet of the b37 reference with all of chromosomes 20 and 21. -Added a DBSNP vcf containing variants overlapping the reads in the bam above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/839:157,Update,Updated,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/839,1,['Update'],['Updated']
Deployability,-Changed sampling of denoised copy ratios to address memory spike and updated output formats and filenames. Partially addresses #5754.; -Updated theano version to 1.0.4 and changed numpy install source to conda defaults to enable MKL.; -Updated theano flags to use MKL and OpenMP elemwise. Closes #5764.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5781:70,update,updated,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781,4,"['Update', 'install', 'update']","['Updated', 'install', 'updated']"
Deployability,"-Created a new class of tool, IntervalWalker, that processes a single interval at a time,; with the ability to query optional overlapping sources of reads, reference data, and/or; features/variants. Current implementation is simple/naive with no special caching;; performance issues will be addressed once we port this traversal type to dataflow. -Added the ability for VariantWalkers to access contextual reads/reference/feature data. -To enable the above changes, migrated most of the engine to use SimpleIntervals rather; than GenomeLocs. This allows for the creation of Context objects in traversals where there; is not necessarily a sequence dictionary available (eg., VariantWalker). -Moved shared arguments/code from Walker classes up into GATKTool. Still some issues; related to marking engine-wide arguments as optional/required on a per-traversal or; per-tool basis, but tickets have been created for these. -Since there isn't yet an htsjdk release that contains SimpleInterval, temporarily; checked a copy of it into our repo, which we can remove the next time we; rev htsjdk. TODOs:. -We currently still require a sequence dictionary to actually parse intervals in; IntervalArgumentCollection. This is due entirely to our support of intervals without; specific stop positions (eg., ""chr1"" and ""chr1:1+"") -- for these intervals we must; look up the stop position in a sequence dictionary. This means that IntervalWalkers; currently require at least one input that contains a sequence dictionary (although; VariantWalkers do not). We should look into ways of relaxing this restriction. Resolves #109",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/297:951,release,release,951,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/297,1,['release'],['release']
Deployability,-ERC BP_RESOLUTION mode in HaplotypeCaller needs an integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6833:52,integrat,integration,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6833,1,['integrat'],['integration']
Deployability,"-O output.bam. Mostly, I just get an identical 9GB bam over and over again (as confirmed by md5). However, sometimes (~10% of the time it seems), I get a MUCH larger bam, more like ~45GB. In runs where I get these larger output files, they are not always the same size, sometimes 45GB, sometimes 47GB (still always with the same input file, same commandline, same wdl task, etc). The runs that produce these larger bam also take much longer, with slower reads per minute rate). They report exactly the same number of reads processed in the logs as the normal runs. Looking inside the large output bams with gsutil cat, I see the header suddenly transitioning from compressed looking jibberish to a plaintext header, and then after a bit back to compressed looking jibberish again. Additionally, if I run these large bams through samtools view to get samtools to write them as a bam (ie samtools view big.bam -o samtools_out.bam) the resulting bam is much smaller ~6GB. It kind of seems like sometimes gatk will just stop compressing the output, and then start back up again, seemingly randomly??. I suspect this may be an issue with all gatk tools, I first encountered this recently with PostProcessReadsForRSEM, and then confirmed the behavior in PrintReads as a minimal example. Maybe its something to do with google hardware, Ive only seen this in Terra so far (not that Ive tried to reproduce it anywhere else).; seeing this in 4.2.6.0. Summary of investigative results:; * reproducible on very small files (at about same rate of ~10%); * appears to be related to intel deflater. when running with jdk deflater (--use-jdk-deflater) all 100/100 runs result in same sized bam. Ive run a version sweep, and it looks like the behavior begins in 4.2.1.0, but does not occur in earlier versions. Looking at the 4.2.1.0 release notes, **it seems highly likely that the issue was introduced by the upgrade from gkl 0.8.6 to gkl 0.8.8 in https://github.com/broadinstitute/gatk/pull/7203/files**",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8141:2027,release,release,2027,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8141,2,"['release', 'upgrade']","['release', 'upgrade']"
Deployability,"-Prints the current locus, the elapsed time, number of records processed,; and the rate at which records are being processed. -Hooked up for ReadWalkers, VariantWalkers, and IntervalWalkers. -A new command-line arg in GATKTool allows control over the frequency of; progress meter updates. -Tweaked the log4j output format to create more screen space for logger output. Resolves #974 (for alpha purposes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1037:280,update,updates,280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1037,1,['update'],['updates']
Deployability,"-Reduce memory usage of AssemblyRegion traversal by an order of magnitude; by loading the reads for each shard more lazily. -Add a sharding mode that creates one shard per user interval (or per contig,; if there are no explicit intervals), and make it the default for both HaplotypeCaller; and Mutect2. -When determining active regions, only consider loci within the user's intervals (but; still include surrounding reads in the final region). This mimics GATK3.x behavior. -Serve up empty pileup objects for uncovered loci (this also mimics GATK3.x behavior).; The fact that we weren't doing this before was responsible for much of the remaining; difference vs. the GATK 3.x HaplotypeCaller. -Ported GATK 3 PR 1389 (use median rather than the second-best likelihood for the; NON_REF allele). -Ported a change to the ReferenceConfidenceModel from GATK3. -Fixed a bug in ReadLikelihoods that was causing ArrayIndexOutOfBoundsException. -Added special handling of RawMQ to HaplotypeCaller (mirrors the handling of RawMQ; from GenotypeGVCFs). -Added updated concordance test data generated with HaplotypeCaller 3.8-4-g7b0250253f. Resolves #1950; Resolves #3516; Resolves #3517; Resolves #3518; Resolves #3233; Resolves #2848",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3519:1047,update,updated,1047,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3519,1,['update'],['updated']
Deployability,"-Require that the reference dictionary be a superset of the reads dictionary; only when there is at least one CRAM input. -When determining whether a superset relationship exists, do not take extended; attributes (like the sequence MD5) into account; only consider the contig names; and lengths. -Do not require common contigs to occur at the same absolute indices across; dictionaries (but do require that they occur in the same relative order).; Contig indices were an issue for GATK3, but since hellbender relies on contig; names for queries we can afford to disable this annoying check. If we later find; that we need to turn it back on, we can easily do so. -Updated tests appropriately:; -Added test cases showing that extended attributes are ignored when checking; for a superset. ```; -Added test cases for various combinations of the new boolean options; requireSuperset and checkContigIndices. -The existing integration test CRAMSupportIntegrationTest.testWrongRef(); shows that we throw when a CRAM is provided as input with a reference; that does not contain all of its contigs.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/877:664,Update,Updated,664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/877,2,"['Update', 'integrat']","['Updated', 'integration']"
Deployability,"-Tools can now customize the progress meter to use a different word than; ""records"" in its output (eg., ""reads"", ""regions"", etc.). -Updated standard walker classes to specify appropriate labels. -Hooked up GenomicsDBImport to the progress meter (it was always reporting; ""Processed 0 records"" at traversal end). Resolves #1943; Resolves #2683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2690:132,Update,Updated,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2690,1,['Update'],['Updated']
Deployability,"-a-reference-with-alternate-contigs-like-grch38> for the implications. This section also gives the first hack--remove the `0x1` SAM flag to circumvent the now `MateOnSameContigOrNoMappedMateReadFilter`. ; - The second hack is in the current Mutect2 hands-on tutorial where I have users `--disableReadFilter MateOnSameContigOrNoMappedMateReadFilter`. This latter hack is particularly germane to somatic analyses where there can be many fusion events. The `MateOnSameContigOrNoMappedMateReadFilter` filter asks HaplotypeCaller or Mutect2 to ignore reads whose mate maps to a different contig. This filter is not at the engine level but rather deep within the assembler and was made disable-able in the summer. I do not know the reasoning behind ignoring read pairs that map across chromosomes. My assumption is that (at least previously) these types of mappings tended to be artifactual and so we wanted to discount them to improve specificity. I think it prudent we assess whether this still holds true for more recent sequencing data and processing pipelines.; - For example, I also know that BWA prefers mappings that place mates within a standard insert distance, e.g. on the same contig. ; - Also, for chimeric reads produced by weird sequencer bridging reactions, we have dual barcodes that would then discount such reads in the `0x200` QCFAIL pool. **Here, I am asking for a simple feature at the engine level**; What I would like is an option for tools that employ the `MateOnSameContigOrNoMappedMateReadFilter` to count mates on what should be molecularly contiguous (but represented as different contigs in the reference) as on the same contig for ALT-aware alignments. The dictionary section of the header will indicate ALT-aware alignment with an AH tag and an asterisk if processed through MergeBamAlignment. Corresponding ALT to primary assembly pairings are given by the `.alt` file used in alt-aware alignment and post-processing and the parameter would ask for this. What this feature e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3764:2411,pipeline,pipelines,2411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3764,1,['pipeline'],['pipelines']
Deployability,"-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:3260,INSTALL,INSTALLDIRGCC,3260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGCC']
Deployability,". Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:2493,INSTALL,INSTALLDIRGATK,2493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability,". Out of curiosity, we tried building the docker again without samtools, so in theory, the only possible change is that when each docker is built, apt update is run. The differences are small, but is that expected? That with and without samtools, and if `apt` packages change, mutect2 could be influenced?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_; I can't replicate scenario 1 in the table because it was built in 2019, so apt packages were different then.; Scenario 2 dockerPull: `docker pull kfdrc/gatk:4.1.1.0`; Scenario 3 dockerPull: `docker pull pgc-images.sbgenomics.com/d3b-bixu/gatk:4.1.1.0`; Scenario 4 dockerPull: `docker pull migbro/gatk:4.1.1.0L`. No samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. Yes samtools Dockerfile:; ```; FROM ubuntu:18.04; LABEL maintainer=""Miguel Brown (brownm28@email.chop.edu)"". ENV GATK4_VERSION 4.1.1.0. RUN apt update && apt install -y openjdk-8-jdk python wget unzip libgomp1 tabix samtools; \; wget -q https://github.com/broadinstitute/gatk/releases/download/${GATK4_VERSION}/gatk-${GATK4_VERSION}.zip; \; unzip gatk-${GATK4_VERSION}.zip; \; mv gatk-${GATK4_VERSION}/gatk* . && rm -rf gatk-${GATK4_VERSION}*; \; apt remove -y wget; ```. #### Expected behavior; _Tell us what should happen_; All `PASS` var counts are the same; #### Actual behavior; _Tell us what happens instead_; `PASS` var counts vary slightly +/- samtools and year docker built; ----; Thank you for your time!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7269:1888,update,update,1888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7269,6,"['install', 'release', 'update']","['install', 'releases', 'update']"
Deployability,"..............................................(BUG 001).......................................................... Traceback (most recent call last):; File ""/tmp/cohort_determine_ploidy_and_depth.3351404099122294482.py"", line 8, in <module>; import gcnvkernel; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 5, in <module>; from .continuous import get_tau_sd, Normal, Flat; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/continuous.py"", line 16, in <module>; from pymc3.theanof import floatX; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/theanof.py"", line 89, in <module>; empty_gradient = tt.zeros(0, dtype='float32'); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/tensor/basic.py"", line 2558, in zeros; return alloc(np.array(0, dtype=dtype), *shape); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/tensor/basic.py"", line 3091, in __call__; ret = super(Alloc, self).__call__(val, *shapes, **kwargs); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 955, in make_thunk; no_recycling); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 858, in make_c_thunk; outpu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235:2287,continuous,continuous,2287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235,1,['continuous'],['continuous']
Deployability,"....................................................(BUG 002)..........................................................; Stderr: Traceback (most recent call last):; File ""/tmp/segment_gcnv_calls.3402406683372415608.py"", line 9, in <module>; import gcnvkernel; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 5, in <module>; from .continuous import get_tau_sd, Normal, Flat; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/continuous.py"", line 16, in <module>; from pymc3.theanof import floatX; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/theanof.py"", line 89, in <module>; empty_gradient = tt.zeros(0, dtype='float32'); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/tensor/basic.py"", line 2558, in zeros; return alloc(np.array(0, dtype=dtype), *shape); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/tensor/basic.py"", line 3091, in __call__; ret = super(Alloc, self).__call__(val, *shapes, **kwargs); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 955, in make_thunk; no_recycling); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 858, in make_c_thunk; outpu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235:10354,continuous,continuous,10354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235,1,['continuous'],['continuous']
Deployability,./gradlew install - turn off javadoc checks,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1955:10,install,install,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1955,1,['install'],['install']
Deployability,".076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2; 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater; 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater; 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INFO HaplotypeCaller - Shutting down engine; [January 18, 2020 1:13:17 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2216689664; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:463); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:457); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(Sequenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6384:1808,patch,patch,1808,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6384,1,['patch'],['patch']
Deployability,".433;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2871800,797;ReadPosRankSum=0.433	GT:AD:DP:GQ:PL:SB	0/2:414,2,357,0:773:99:14672,11361,50781,0,41338,45124,13972,52387,44158,56529:206,208,177,182; chr13	32944608	.	T	A,*,<NON_REF>	0	.	BaseQRankSum=5.453;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2869200,797;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I targe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5975:5839,pipeline,pipeline,5839,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975,1,['pipeline'],['pipeline']
Deployability,.6.1-local.jar ; ; Running: ; ;   java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:11:11.813 INFO Base,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:9170,pipeline,pipeline,9170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,".77	.	BaseQRankSum=4.433;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQ=2871800.00;ReadPosRankSum=0.433	GT:AD:DP:GQ:PL:SB	0/2:414,2,357,0:773:99:14672,11361,50781,0,41338,45124,13972,52387,44158,56529:206,208,177,182; chr13	32944608	.	T	A,*,<NON_REF>	14811.77	.	BaseQRankSum=5.453;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQ=2869200.00;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	14802.73	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQ=2833200.00;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. And in the latest public release version [4.1.2.0], variant A>TAAAA was filtered as the QUAL is zero I guessed.; *vcf of 4.1.2.0*; ```vcf of 4.1.2.0; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944606	.	CTTT	C	9240.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=1.37;DP=813;ExcessHet=3.0103;FS=0.518;MLEAC=1;MLEAF=0.500;MQ=60.03;MQRankSum=0.00;QD=11.85;ReadPosRankSum=0.295;SOR=0.728	GT:AD:DP:GQ:PL	0/1:423,357:780:99:9248,0,45245; ```; *gvcf of 4.1.2.0*; ```gvcf of 4.1.2.0; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944440	.	T	<NON_REF>	.	.	END=32944605	GT:DP:GQ:MIN_DP:PL	0/0:592:99:352:0,120,1800; chr13	32944606	.	CTTT	C,<NON_REF>	9240.60	.	BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2929400,813;ReadPosRankSum=0.295	GT:AD:DP:GQ:PL:SB	0/1:423,357,0:780:99:9248,0,45245,10522,46330,56852:212,211,175,182; chr13	32944607	.	T	A,*,<NON_REF>	0	.	BaseQRankSum=4.433;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5975:3907,release,release,3907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975,1,['release'],['release']
Deployability,".794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:1966,release,release,1966,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['release'],['release']
Deployability,.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/08/02 20:24:09 INFO util.ShutdownHookManager: Shutdown hook called; 16/08/02 20:24:09 INFO util.ShutdownHookManager: Deleting directory /ssd_hdfs2/spark_tmp/spark-42d0223f-b492-43e3-a6fa-d4edd98b2324,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2073:2874,deploy,deploy,2874,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073,6,['deploy'],['deploy']
Deployability,.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:8299,Continuous,ContinuousBuildActionExecuter,8299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buil,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:7447,Continuous,ContinuousBuildActionExecuter,7447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,".BwaMemIndex.createReferenceIndex(Native Method); at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImageFromFastaFile(BwaMemIndex.java:227); at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImageFromFastaFile(BwaMemIndex.java:196); at org.broadinstitute.hellbender.BwaMemIntegrationTest.loadIndex(BwaMemIntegrationTest.java:49); Running Test: Test method testChimericUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testChimericUnpairedMapping SKIPPED; Running Test: Test method testPerfectUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testPerfectUnpairedMapping SKIPPED; ```. This test fails because some JAR wasn't built:; ```; Running Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest); Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one by running. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest > testPipeForPicardTools STANDARD_ERROR; No local jar was found, please build one by running; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err:. Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar. /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: or. or; Test: Test method testPipeForPicardTools(org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:3328,Pipeline,PipelineSupportIntegrationTest,3328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,1,['Pipeline'],['PipelineSupportIntegrationTest']
Deployability,.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [42925293-731b-47bb-8e5e-7f375d9c3490] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:6561,deploy,deploy,6561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,6,['deploy'],['deploy']
Deployability,.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:23217,deploy,deploy,23217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['deploy'],['deploy']
Deployability,.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:2149,deploy,deploy,2149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,".broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:4479,deploy,deploy,4479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(Di,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:2639,deploy,deploy,2639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,".broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; *****************************************************************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:7375,deploy,deploy,7375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,".broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2216:2611,deploy,deploy,2611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216,1,['deploy'],['deploy']
Deployability,".broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.makeMeansTable(VariantRecalibrator.java:986); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.writeModelReport(VariantRecalibrator.java:887); at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:680); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms100g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:10025,install,install,10025,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['install'],['install']
Deployability,.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3012,configurat,configuration,3012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['configurat'],['configuration']
Deployability,.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:19415,deploy,deploy,19415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['deploy'],['deploy']
Deployability,.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30; ); at; org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at; org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.jav; a:179); at; org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at; org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at; org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at; org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928); at; org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203); at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90); at; org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: scala.Product$class; at java.lang.ClassLoader.findClass(ClassLoader.java:523); at; org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35); at java.lang.ClassLoader.loadClass(ClassLoader.java:418); at; org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40); at; org.apache.spark.util.ChildFirstURLClassLoader.loadClass(ChildFirstURLClassLoader.java:48); at java.lang.ClassLoader.loadClass(ClassL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6644:4933,deploy,deploy,4933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6644,1,['deploy'],['deploy']
Deployability,".engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseSt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:7625,deploy,deploy,7625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['deploy'],['deploy']
Deployability,".file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312:1877,install,installedProviders,1877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312,2,['install'],['installedProviders']
Deployability,.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:3447,deploy,deploy,3447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,".hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFile",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:5618,deploy,deploy,5618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,".jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:1256,deploy,deploy,1256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['deploy'],['deploy']
Deployability,".jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.678 INFO BaseRecalibrator - ------------------------------------------------------------ ; ; 00:09:41.679 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:09:41.679 INFO BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:09:41.679 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:09:41.679 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:09:41.680 INFO BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:09:41 AM CST ; ; 00:09:41.680 INFO BaseRecalibrator - --------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:2753,pipeline,pipeline,2753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,".java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-248086238). @huangk3 Unfortunately Adam moved on to a different job so he's longer working on GATK. . I believe this is the same problem as https://github.com/broadinstitute/gatk/issues/2026 and has been patched in gatk public with https://github.com/broadinstitute/gatk/pull/2028. You might try using FastqToSam in the public repo, or wait and try a new version of protected that incorporates an updated gatk public (coming soon..)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2868:3307,patch,patched,3307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868,2,"['patch', 'update']","['patched', 'updated']"
Deployability,.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.remote.RemoteTestNG.run(RemoteTestNG.java:111); at org.testng.remote.RemoteTestNG.initAndRun(RemoteTestNG.java:204); at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:175); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:125); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140); Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at com.google.cloud.dataflow.sdk.Pipeline.run(Pipeline.java:166); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.runPipeline(DataflowCommandLineProgram.java:145); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.doWork(DataflowCommandLineProgram.java:107); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:151); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:78); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:75); at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); ... 33 more; Caused by: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at com.google.cloud.genomics.dataflow.readers.bam.Reader.proc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/866:3044,Pipeline,Pipeline,3044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/866,1,['Pipeline'],['Pipeline']
Deployability,".python.PythonScriptExecutorException: A nack was received from the Python process (most likely caused by a raised exception caused by): nkm received. ```; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/regmova/miniconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/models.py"", line 22, in start_session_get_args_and_model; K.clear_session(). AttributeError: module 'keras.backend' has no attribute 'clear_session'; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForAck(StreamingPythonScriptExecutor.java:222); 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:183); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.initializePythonArgsAndModel(CNNScoreVariants.java:557); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:317); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1056); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289) ; ```. #### Steps to reproduce; `conda activate gatk; gatk CNNScoreVariants -V VCF.vcf.gz -R reference.fa -O VCF.CNNscored.vcf `. #### Expected behaviour; CNNScoreVariants should generate an annotated VCF. #### Actual behavior; CNNScoreVariants crashes. #### What I tried; I ran into this issue with an older build based on v4.1.9. I upgraded to v4.2.0, removed and built the Conda environment again, but the issue persisted. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7250:5663,upgrade,upgraded,5663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250,1,['upgrade'],['upgraded']
Deployability,.runTool(MarkDuplicatesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waitin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183:4939,deploy,deploy,4939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183,1,['deploy'],['deploy']
Deployability,".scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Cli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:6801,deploy,deploy,6801,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,".tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace ``. Crashes: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf.gz --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace `` . This is caused by the Tabix tools in htsjdk being given the dictionary from the input vcf to use for indexing instead of the source-dictionary. Can be fixed by overriding ``getBestAvailableSequenceDictionary()`` in UpdateVCFSequenceDictionary to return ``sourceDictionary``. However, this requires making ``VariantWalkerBase::getBestAvailableSequenceDictionary()`` non-final, so perhaps there is a better option.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5087:1794,Update,UpdateVCFSequenceDictionary,1794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087,3,['Update'],['UpdateVCFSequenceDictionary']
Deployability,"//github.com/broadinstitute/gatk/files/4091802/log.txt). ```; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... done. Downloading and Extracting Packages. keras-preprocessing- | 36 KB | ########## | 100%; astor-0.8.0 | 46 KB | ########## | 100%; setuptools-36.4.0 | 563 KB | ########## | 100%; termcolor-1.1.0 | 8 KB | ########## | 100%; protobuf-3.11.2 | 635 KB | ########## | 100%; keras-applications-1 | 33 KB | ########## | 100%; readline-6.2 | 606 KB | ########## | 100%; libgfortran-ng-7.3.0 | 1006 KB | ########## | 100%; numpy-1.13.3 | 3.1 MB | ########## | 100%; ```. numpy-1.13.3 is corectly installed . but then . ```; Collecting numpy (from biopython==1.70->-r /root/gatk-4.1.4.0/condaenv.g1uyq0ce.requirements.txt (line 1)); Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB); ```. that does . ```; Found existing installation: numpy 1.13.3; Uninstalling numpy-1.13.3:; Successfully uninstalled numpy-1.13.3; ```. this causes ```gatk DetermineGermlineContigPloidy ```; to exit with an error related to numpy.testing.decorators which is deprecated since numpy 1.15.0 see https://docs.scipy.org/doc/numpy-1.15.0/release.html. ```; Deprecations. Aliases of builtin pickle functions are deprecated, in favor of their unaliased pickle.<func> names:; numpy.loads; numpy.core.numeric.load; numpy.core.numeric.loads; numpy.ma.loads, numpy.ma.dumps; numpy.ma.load, numpy.ma.dump - these functions already failed on python 3 when called with a string.; Multidimensional indexing with anything but a tuple is deprecated. This means that the index list in ind = [slice(None), 0]; arr[ind] should be changed to a tuple, e.g., ind = [slice(None), 0]; arr[tuple(ind)] or arr[(slice(None), 0)]. That change is necessary to avoid ambiguity in expressions such as arr[[[0, 1], [0, 1]]], currently interpreted as arr[array([0, 1]), ar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6396:1381,install,installation,1381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6396,1,['install'],['installation']
Deployability,"/GATK/bundle/2.8/b37/human_g1k_v37.fasta -I /humgen/gsa-scr1/schandra/bgrenier_MixingAndMatchingGVCFAndBPRES/ind2.bam -L 9 -o Sheila.HaplotypeCallerGVCF.g.vcf -ERC GVCF`. GenotypeGVCFs:; `java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T GenotypeGVCFs -R /humgen/gsa-hpprojects/GATK/bundle/2.8/b37/human_g1k_v37.fasta -V Sheila.HaplotypeCallerGVCF.g.vcf -o Sheila.GenotypeGVCFsGVCF.vcf`. ---. @chandrans commented on [Sat Dec 03 2016](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-264640941). User asked about this. Probably won't get to it anytime soon, but I told him/her I would check in. ---. @vdauwera commented on [Mon Dec 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-264964313). I responded that we're waiting on the tie-outs. . ---. @ldgauthier commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-265162286). Tie-outs?. I started on this, but I definitely won't finish it before the release. ---. @vdauwera commented on [Tue Dec 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-265166347). Functional equivalence of GATK4 ports. That's my party line for now. Didn't realize you had actually started on this... but yeah, 3.7 is going out today. ---. @ronlevine commented on [Mon Jan 23 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-274513719). @davidbenjamin Any thoughts on this? Laura thought she implemented a fix but it have any effect. I am becoming familiar with the HC code but it's slow going. While stepping though the code, I noticed it correctly identifies the events for the deletion and SNP. ---. @davidbenjamin commented on [Mon Jan 23 2017](https://github.com/broadinstitute/gsa-unstable/issues/1499#issuecomment-274536654). @ronlevine I would put in a few breakpoints to see where the spanning deletion allele gets lost from the SNP site `VariantContext`. I just ran things wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2960:3144,release,release,3144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2960,1,['release'],['release']
Deployability,/PathSeqBuildKmers/exampleFASTA.hss; src/test/resources/org/broadinstitute/hellbender/tools/spark/pathseq/PathSeqBuildReferenceTaxonomy/genbank_test.dict; src/test/resources/org/broadinstitute/hellbender/tools/spark/pathseq/PathSeqBuildReferenceTaxonomy/genbank_test.fasta.fai; src/test/resources/org/broadinstitute/hellbender/tools/spark/pathseq/PathSeqBuildReferenceTaxonomy/test.dict; src/test/resources/org/broadinstitute/hellbender/tools/spark/pathseq/PathSeqBuildReferenceTaxonomy/test.fasta.fai; src/test/resources/org/broadinstitute/hellbender/tools/spark/pathseq/PathSeqPipelineSpark/e_coli_k12_mini.dict; src/test/resources/org/broadinstitute/hellbender/tools/spark/pathseq/PathSeqPipelineSpark/pipeline_output.bam.splitting-bai; src/test/resources/org/broadinstitute/hellbender/tools/spark/pathseq/PSBuildReferenceTaxonomyUtils/test.tar.gz; src/test/resources/org/broadinstitute/hellbender/tools/spark/pathseq/PSFilter/hg19mini_test_reads.bam; src/test/resources/org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark/flag_stat.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark/SVBreakpointsTest.assembly.0; src/test/resources/org/broadinstitute/hellbender/tools/spark/sv/sga/RunSGAViaProcessBuilderOnSpark; src/test/resources/org/broadinstitute/hellbender/tools/spark/sv/sga/RunSGAViaProcessBuilderOnSpark/4.raw.fastq; src/test/resources/org/broadinstitute/hellbender/tools/spark/sv/sga/RunSGAViaProcessBuilderOnSpark/4.raw.pp.ec.fa; src/test/resources/org/broadinstitute/hellbender/tools/spark/sv/sga/RunSGAViaProcessBuilderOnSpark/4.raw.pp.ec.filter.pass.fa; src/test/resources/org/broadinstitute/hellbender/tools/spark/sv/sga/RunSGAViaProcessBuilderOnSpark/4.raw.pp.ec.filter.pass.merged.fa; src/test/resources/org/broadinstitute/hellbender/tools/spark/sv/sga/RunSGAViaProcessBuilderOnSpark/4.raw.pp.ec.filter.pass.merged.rmdup-contigs.fa; src/test/resources/org/broadinstitute/hellbender/tools/spark/sv/sga/RunSGA,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:42462,pipeline,pipelines,42462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['pipeline'],['pipelines']
Deployability,/coveragemodel/learning_sample_read_depth.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/learning_sample_sex_genotypes.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_contig_anots.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_HMM_priors_table.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_model; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_model/mean_bias_covariates_matrix.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_model/target_specific_mean_log_bias.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_model/target_specific_unexplained_variance.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_targets.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/acnv-segments-from-allelic-integration.seg; src/test/resources/org/broadinstitute/hellbender/tools/exome/af-params-from-allelic-integration.af.param; src/test/resources/org/broadinstitute/hellbender/tools/exome/allelic-pon-test-pulldown-1.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/allelic-pon-test-pulldown-2.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/allelic-pon-test-pulldown-3.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/allelic-pon-test-pulldown-4.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/dupReadsMini.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12778.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12872.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12878.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:29138,integrat,integration,29138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['integrat'],['integration']
Deployability,"/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. ```; 1 <?xml version=""1.0"" encoding=""UTF-8""?>; 2 <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">; 3 <modelVersion>4.0.0</modelVersion>; 4 ; 5 <!--; 6 This pom is parent for all gatk poms; 7 See also:; 8 http://maven.apache.org/pom.html#Inheritance_v; 9 http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Project_Inheritance_vs_Project_Aggregation; 10 http://stackoverflow.com/questions/1992213/maven-parent-pom-vs-modules-pom; 11 -->; 12 ; 13 <groupId>org.broadinstitute.gatk</groupId>; 14 <artifactId>gatk-root</artifactId>; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>3.8-2-SNAPSHOT</version>; 19 >>>>>>> 0450e2531ee021e28bd7c5e92b5ba736d530d9af; 20 <packaging>pom</packaging>; 21 <name>GATK Root</name>; 22 ; 23 <prerequisites>; 24 <maven>3.0.4</maven>; 25 </prerequisites>; ```. Please make a new release, preferably 3.8.2 which unpacks as usual into `./gatk-3.8.2/`. Thank you. While making a bugfix release? because it is the last version supporting `old` syntax.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:3022,release,release,3022,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,2,['release'],['release']
Deployability,"/home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk AnalyzeCovariates --before-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/before.recal.FMF-248.table --after-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/after.recal.FMF-248.table --plots-report-file /home/detagen/Desktop/pipeline/playground//NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; Using GATK jar /home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar AnalyzeCovariates --before-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/before.recal.FMF-248.table --after-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/after.recal.FMF-248.table --plots-report-file /home/detagen/Desktop/pipeline/playground//NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; 12:57:16.643 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 17, 2020 12:57:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:57:16.774 INFO AnalyzeCovariates - ------------------------------------------------------------; 12:57:16.775 INFO AnalyzeCovariates - The Genome Analysis Toolkit (GATK) v4.1.8.1; 12:57:16.775 INFO AnalyzeCovariates - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:57:16.775 INFO AnalyzeCovariates - Executing as detagen@detagen on Linux v5.4.0-58-generic amd64; 12:57:16.775 INFO AnalyzeCovariates - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1+1-Ubuntu-0ubuntu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7006:112,pipeline,pipeline,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7006,5,['pipeline'],['pipeline']
Deployability,"/share/man/man1/git-lfs-locks.1.gz; git-lfs usr/share/man/man1/git-lfs-logs.1.gz; git-lfs usr/share/man/man1/git-lfs-ls-files.1.gz; git-lfs usr/share/man/man1/git-lfs-merge-driver.1.gz; git-lfs usr/share/man/man1/git-lfs-migrate.1.gz; git-lfs usr/share/man/man1/git-lfs-pointer.1.gz; git-lfs usr/share/man/man1/git-lfs-post-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-post-commit.1.gz; git-lfs usr/share/man/man1/git-lfs-post-merge.1.gz; git-lfs usr/share/man/man1/git-lfs-pre-push.1.gz; git-lfs usr/share/man/man1/git-lfs-prune.1.gz; git-lfs usr/share/man/man1/git-lfs-pull.1.gz; git-lfs usr/share/man/man1/git-lfs-push.1.gz; git-lfs usr/share/man/man1/git-lfs-smudge.1.gz; git-lfs usr/share/man/man1/git-lfs-standalone-file.1.gz; git-lfs usr/share/man/man1/git-lfs-status.1.gz; git-lfs usr/share/man/man1/git-lfs-track.1.gz; git-lfs usr/share/man/man1/git-lfs-uninstall.1.gz; git-lfs usr/share/man/man1/git-lfs-unlock.1.gz; git-lfs usr/share/man/man1/git-lfs-untrack.1.gz; git-lfs usr/share/man/man1/git-lfs-update.1.gz; git-lfs usr/share/man/man1/git-lfs.1.gz; git-lfs usr/share/man/man5/; git-lfs usr/share/man/man5/git-lfs-config.5.gz; ```. Then I run ; ```; git lfs pull --include src/main/resources/large; ./gradle localJar; ```; then; ```; error transferring ""1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46"": [0] remote missing object 1d70940bd9d7c6c862304c66d64233726dc30342ae7032a4636939e8249cbf46; error transferring ""bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d"": [0] remote missing object bd17c3a98f7651b4e7ee54d875c47ec12e18b75daf79b3744a2590ddb0d6b44d; error transferring ""6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c"": [0] remote missing object 6f663a2fdbcde0addc5cb755f7af5d4c19bed92dccfd20e25b2acf2bc8c2ca7c; error transferring ""e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6"": [0] remote missing object e38e09cfe7b7ffbc80dce4972bc9c382148520147d46738a3f6f3235b2d876c6; error transferring ""4ed7feb034",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8320:3327,update,update,3327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320,1,['update'],['update']
Deployability,0); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.refreshAccessToken(ComputeEngineCredentials.java:137); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591:3252,deploy,deploy,3252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591,1,['deploy'],['deploy']
Deployability,"0/0:38,1:39:82:0,82,1231; 0/0:26,0:26:78:0,78,869; 0/0:22,0:22:66:0,66,734; 0/0:28,1:29:52:0,52,878; 0/0:36,0:36:99:0,108,1221; 0/0:39,0:39:99:0,117,1315; 0/0:31,1:32:86:0,86,1008; 0/0:38,0:38:99:0,114,1286; 0/0:37,0:37:99:0,111,1266; 0/0:35,0:35:99:0,105,1131; 0/0:32,0:32:96:0,96,1071; 0/0:46,0:46:99:0,138,1508; 0/0:27,0:27:81:0,81,918; 0/0:19,0:19:57:0,57,620; 0/0:45,0:45:99:0,135,1474; 0/0:50,0:50:99:0,150,1687; 0/0:29,0:29:87:0,87,968; 0/0:30,0:30:90:0,90,992; 0/0:55,1:56:99:0,158,1830; 0/1:8,2:10:19:19,0,248; 0/1:5,1:6:17:17,0,145; 0/1:4,1:5:10:10,0,124; 0/1:5,1:6:17:17,0,155; 0/1:3,2:5:34:34,0,79; 0/1:5,3:8:45:45,0,150; 0/1:2,1:3:26:26,0,61; 0/0:13,0:13:39:0,39,431; 0/0:17,0:17:51:0,51,571; 0/0:28,0:28:84:0,84,993; 0/0:10,0:10:30:0,30,328 . Freebayes has been run on these 57 samples and also get '0/0' but with GQ in the 140-160 range for most samples and are in line with the results with the HaplotypeCaller VCF direct results (see below). #### Actual behavior. Pipeline 2 is incorrectly setting GQ=0. This is the output for the 57 GQ=0 samples+ 1 extra sample GQ=99 with pipeline 2. The extra sample is needed to produce a variant record otherwise all the records would be homoz refer with GQ=0. . AC=1;AF=8.621e-03;AN=116;BaseQRankSum=-8.310e-01;DP=2213;ExcessHet=41.0061;FS=1.957;InbreedingCoeff=-0.3410;MLEAC=10;MLEAF=0.086;MQ=60.00;MQRankSum=0.00;QD=12.17;ReadPosRankSum=0.616;SOR=1.080; GT:AD:DP:GQ:PL; 0/1:9,8:17:99:227,0,272; 0/0:41,0:41:0:0,0,1097; 0/0:51,0:51:0:0,0,1216; 0/0:61,0:61:0:0,0,1373; 0/0:54,0:54:0:0,0,962; 0/0:49,0:49:0:0,0,1156; 0/0:53,0:53:0:0,0,729; 0/0:44,0:44:0:0,0,1161; 0/0:38,0:38:0:0,0,963; 0/0:68,0:68:0:0,0,1518; 0/0:33,0:33:0:0,0,841; 0/0:54,0:54:0:0,0,687; 0/0:44,0:44:0:0,0,1003; 0/0:33,0:33:0:0,0,709; 0/0:54,0:54:0:0,0,580; 0/0:31,0:31:0:0,0,790; 0/0:36,0:36:0:0,0,843; 0/0:49,0:49:0:0,0,978; 0/0:34,0:34:0:0,0,669; 0/0:39,0:39:0:0,0,898; 0/0:60,0:60:0:0,0,1270; 0/0:48,0:48:0:0,0,908; 0/0:30,0:30:0:0,0,780; 0/0:44,0:44:0:0,0,778; 0/0:24,0:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445:4039,Pipeline,Pipeline,4039,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445,1,['Pipeline'],['Pipeline']
Deployability,"003 http://projecteuclid.org/download/pdf_1/euclid.aos/1056562461. -Add Metropolis-Hastings univariate sampler as alternative to SliceSampler. -Add Metropolis-Hastings/nested/etc. multivariate samplers as alternatives to GibbsSampler. This should only be tackled if a model/dataset necessitates it. -Implement hierarchical/multilevel models in an OOP way. Currently, the samplers operate on lists of global parameters and lists of lists of ""local"" parameters (i.e., segment-level or site-level parameters), which is a bit clunky. -Add convergence diagnostics (e.g., autocorrelation time). -Add ability to make trace plots and corner plots. -Implement more flexible discarding of burn-in. Currently, samples from all iterations are aggregated in memory. Depending on the maximum number of iterations we want to allow, it might be better to write samples to disk, only store samples in memory after burn-in, etc. so we don't run into memory issues. -Parallelization (again, only if a model/dataset necessitates it). ---. @LeeTL1220 commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153466956). @samuelklee Do we need this for the beta release?. ---. @samuelklee commented on [Tue Nov 03 2015](https://github.com/broadinstitute/gatk-protected/issues/126#issuecomment-153471237). I'd say no to pretty much all of the points, except for whatever @davidbenjamin ends up needing to implement for the allele-fraction model (David, last time I looked at your branch there was some MH sampling going on?). Some of them will probably be relatively easy to address before beta (e.g., the first point about fixing up the SliceSampler), but I think they are low priority. The only thing that we'll definitely have to decide on for beta release is how to store/plot the MCMC chains (i.e., the posterior samples). If all people want to see is posterior point estimates + credible intervals, we can just discard the chains, but this seems somewhat wasteful to me.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2824:1522,release,release,1522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824,2,['release'],['release']
Deployability,033_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_YO_0034_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_ZC_0035_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_GV_0036_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_CW_0037_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_DL_0038_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_KS_0039_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_OF_0040_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_WR_0041_chr26.raw.g.vcf --variant /usr/users/geibel/chicken/pool_sequence_nov2016/data/gVCF/pl_RI_0042_chr26.raw.g.vcf -nt 10 --max_genotype_count 1024 -L chr26 --dbsnp /usr/users/geibel/chicken/chickenrefgen/ENSEMBL_20170106/Gallus_gallus.updated.vcf -o /usr/users/geibel/chicken/pool_sequence_nov2016/data/rawVCF/IndandPool_chr26.raw.vcf ; ```. The user actually includes a shell script in the test data bundle called `JointGenotyping_chr26.sh`. ---; ### The error shows:; ```; ##### ERROR --; ##### ERROR stack trace ; java.lang.IllegalArgumentException: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; 	at org.broadinstitute.gatk.tools.walkers.genotyper.GenotypeLikelihoodCalculators.getInstance(GenotypeLikelihoodCalculators.java:319); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.mergeRefConfidenceGenotypes(ReferenceConfidenceVariantContextMerger.java:461); 	at org.broadinstitute.gatk.tools.walkers.variantutils.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:164); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2946:9678,update,updated,9678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946,1,['update'],['updated']
Deployability,"0424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now RUNNING; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:10 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.16:49734 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:55:05 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/0 is now RUNNING; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/6 is now RUNNING; 18/04/24 17:55:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.25:54754) with ID 2; 18/04/24 17:55:07 INFO BlockManagerMast",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:13906,update,updated,13906,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['update'],['updated']
Deployability,"08.890 INFO PrintReadsSpark - Start Date/Time: July 24, 2018 9:02:08 PM UTC; 21:02:08.890 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.891 INFO PrintReadsSpark - HTSJDK Version: 2.16.0; 21:02:08.891 INFO PrintReadsSpark - Picard Version: 2.18.7; 21:02:08.891 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:02:08.892 INFO PrintReadsSpark - Deflater: IntelDeflater; 21:02:08.892 INFO PrintReadsSpark - Inflater: IntelInflater; 21:02:08.892 INFO PrintReadsSpark - GCS max retries/reopens: 20; 21:02:08.892 INFO PrintReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:02:08.892 WARN PrintReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:5719,patch,patch,5719,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['patch'],['patch']
Deployability,"092 INFO HaplotypeCaller - Start Date/Time: January 15, 2018 12:18:41 PM GMT; 12:18:42.092 INFO HaplotypeCaller - ------------------------------------------------------------; 12:18:42.092 INFO HaplotypeCaller - ------------------------------------------------------------; 12:18:42.093 INFO HaplotypeCaller - HTSJDK Version: 2.13.2; 12:18:42.093 INFO HaplotypeCaller - Picard Version: 2.17.2; 12:18:42.093 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 12:18:42.093 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:18:42.093 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:18:42.093 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:42.093 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:18:42.093 INFO HaplotypeCaller - Inflater: IntelInflater; 12:18:42.093 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:18:42.093 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:42.093 INFO HaplotypeCaller - Initializing engine; 12:18:42.597 INFO FeatureManager - Using codec VCFCodec to read file file:///beegfs/work/zxmai83/Reference/dbs/b37/dbsnp_138.b37.vcf; 12:18:42.723 INFO HaplotypeCaller - Done initializing engine; 12:18:42.732 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 12:18:42.732 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:18:43.546 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:18:43.549 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:18:43.599 WARN IntelPairHmm - Flush-to-zero (FTZ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:2769,patch,patch,2769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['patch'],['patch']
Deployability,1); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Fi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517:1978,deploy,deploy,1978,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517,1,['deploy'],['deploy']
Deployability,"1. We need a script to take snapshots of the [GATK total downloads page](https://somsubhra.com/github-release-stats/?username=broadinstitute&repository=gatk) every month and report the number of times each release was downloaded.; 2. We should change the GATK download link on [this page of the GATK website](https://gatk.broadinstitute.org/hc/en-us) and have it point at a script that records the IP addresses of users before redirecting to github. That way we can have another source of GATK downloads metrics.; 3. Github reports the traffic on GATK repo [here](https://github.com/broadinstitute/gatk/graphs/traffic). It tracks the number of clones and the number of visitors. However, github saves this data only for one week, so we will need an automated script to take snapshots of the page and store the metrics.; 4. In the same way track the total number of download from bioconda from [this](https://anaconda.org/bioconda/gatk4) page.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6946:102,release,release-stats,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6946,2,['release'],"['release', 'release-stats']"
Deployability,"10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:6513,configurat,configuration,6513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['configurat'],['configuration']
Deployability,"1073); > at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2278); > at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2274); > at java.security.AccessController.doPrivileged(Native Method); > at javax.security.auth.Subject.doAs(Subject.java:422); > at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1924); > at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2272); > ; > org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file hdfs://cloudera08/gatk-test2/WES2019-022_S4_out.vcf because writing failed with exception concat: target file /gatk-test2/WES2019-022_S4_out.vcf.parts/output is empty; > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInternal(FSNamesystem.java:2303); > at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInt(FSNamesystem.java:2257). #### Steps to reproduce; The user's command line was. > nohup /opt/gatk/gatk-4.1.4.0/gatk ReadsPipelineSpark --spark-runner SPARK --spark-master yarn --spark-submit-command spark2-submit -I hdfs://cloudera08/gatk-test2/WES2019-022_S4.bam -O hdfs://cloudera08/gatk-test2/WES2019-022_S4_out.vcf -R hdfs://cloudera08/gatk-test1/ucsc.hg19.fasta --known-sites hdfs://cloudera08/gatk-test1/dbsnp_150_hg19.vcf.gz --known-sites hdfs://cloudera08/gatk-test1/Mills_and_1000G_gold_standard.indels.hg19.vcf.gz --align true --emit-ref-confidence GVCF --standard-min-confidence-threshold-for-calling 50.0 --conf deploy-mode=cluster --conf ""spark.driver.memory=2g"" --conf ""spark.executor.memory=18g"" --conf ""spark.storage.memoryFraction=1"" --conf ""spark.akka.frameSize=200"" --conf ""spark.default.parallelism=100"" --conf ""spark.core.connection.ack.wait.timeout=600"" --conf ""spark.yarn.executor.memoryOverhead=4096"" --conf ""spark.yarn.driver.memoryOverhead=400"" > WES2019-022_S4.out. #### Expected behavior; The tool should terminate normally and produce an output variants file. #### Actual behavior; The tool crashes with exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6218:3152,deploy,deploy-mode,3152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6218,1,['deploy'],['deploy-mode']
Deployability,"11:05:38.056 INFO CountVariantsSpark - Shutting down engine; [May 12, 2016 11:05:38 AM AST] org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=3114270720; htsjdk.tribble.TribbleException: Input stream does not contain a BCF encoded file; BCF magic header info not found, at record 0 with position 0:; at htsjdk.variant.bcf2.BCF2Codec.error(BCF2Codec.java:492); at htsjdk.variant.bcf2.BCF2Codec.readHeader(BCF2Codec.java:153); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:109); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:89); at org.seqdoop.hadoop_bam.VCFInputFormat.addGuessedSplits(VCFInputFormat.java:254); at org.seqdoop.hadoop_bam.VCFInputFormat.fixBCFSplits(VCFInputFormat.java:242); at org.seqdoop.hadoop_bam.VCFInputFormat.getSplits(VCFInputFormat.java:221); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:95); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910); at org.apache.spark.rdd.RDD.count(RDD.scala:1121); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:445); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:47); at org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark.runTool(CountVariantsSpark.java:39); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1815:134,pipeline,pipelines,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1815,1,['pipeline'],['pipelines']
Deployability,"1st step towards bringing in fermi-lite as alternative option for local assembly.; Updated other classes accordingly.; Some teeny clean ups elsewhere. @cwhelan mind reviewing? Mostly trivial changes, should be very quick.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2136:83,Update,Updated,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2136,1,['Update'],['Updated']
Deployability,"2 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.; 2-45-ga30af5a-SNAPSHOT; 15:47:28.835 INFO BwaAndMarkDuplicatesPipelineSpark - Defaults.BUFFER_SIZE : 131072; 15:47:28.835 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171:1447,pipeline,pipelines,1447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171,1,['pipeline'],['pipelines']
Deployability,2 changes to gatk-launch; - gcloud now requires arguments to the spark job to be separated from arguments to gcloud by `--`; - `--sparkMaster yarn-client` has been replaced with `--sparkMaster yarn --deploy-mode client`; this only requires the sparkMaster to be changed in gatk-launch gcs because the client mode is set by gcloud,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2263:200,deploy,deploy-mode,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2263,1,['deploy'],['deploy-mode']
Deployability,"210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5975:6349,release,releases,6349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975,1,['release'],['releases']
Deployability,"211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTabl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:2108,INSTALL,INSTALLDIRGATK,2108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability,"248, chrUn_gl000249]; reads contigs = []; 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:163); 	at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:469); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:361); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18/01/09 18:31:26 INFO util.ShutdownHookManager: Shutdown hook called; 18/01/09 18:31:26 INFO util.ShutdownHookManager: Deleting directory /tmp/sun/spark-5a3e539e-2e2b-4da2-b218-2bda166bd4c0; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:35945,deploy,deploy,35945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,6,['deploy'],['deploy']
Deployability,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5975:6542,release,releases,6542,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975,3,['release'],['releases']
Deployability,"295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utiliz",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:1730,configurat,configuration,1730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,1,['configurat'],['configuration']
Deployability,2:35.484 INFO GenomicsDBImport - Initializing engine; 01:24:58.683 INFO FeatureManager - Using codec BEDCodec to read file file:///lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7598:3661,update,update,3661,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598,1,['update'],['update']
Deployability,"2:66:0,66,734; 0/0:28,1:29:52:0,52,878; 0/0:36,0:36:99:0,108,1221; 0/0:39,0:39:99:0,117,1315; 0/0:31,1:32:86:0,86,1008; 0/0:38,0:38:99:0,114,1286; 0/0:37,0:37:99:0,111,1266; 0/0:35,0:35:99:0,105,1131; 0/0:32,0:32:96:0,96,1071; 0/0:46,0:46:99:0,138,1508; 0/0:27,0:27:81:0,81,918; 0/0:19,0:19:57:0,57,620; 0/0:45,0:45:99:0,135,1474; 0/0:50,0:50:99:0,150,1687; 0/0:29,0:29:87:0,87,968; 0/0:30,0:30:90:0,90,992; 0/0:55,1:56:99:0,158,1830; 0/1:8,2:10:19:19,0,248; 0/1:5,1:6:17:17,0,145; 0/1:4,1:5:10:10,0,124; 0/1:5,1:6:17:17,0,155; 0/1:3,2:5:34:34,0,79; 0/1:5,3:8:45:45,0,150; 0/1:2,1:3:26:26,0,61; 0/0:13,0:13:39:0,39,431; 0/0:17,0:17:51:0,51,571; 0/0:28,0:28:84:0,84,993; 0/0:10,0:10:30:0,30,328 . Freebayes has been run on these 57 samples and also get '0/0' but with GQ in the 140-160 range for most samples and are in line with the results with the HaplotypeCaller VCF direct results (see below). #### Actual behavior. Pipeline 2 is incorrectly setting GQ=0. This is the output for the 57 GQ=0 samples+ 1 extra sample GQ=99 with pipeline 2. The extra sample is needed to produce a variant record otherwise all the records would be homoz refer with GQ=0. . AC=1;AF=8.621e-03;AN=116;BaseQRankSum=-8.310e-01;DP=2213;ExcessHet=41.0061;FS=1.957;InbreedingCoeff=-0.3410;MLEAC=10;MLEAF=0.086;MQ=60.00;MQRankSum=0.00;QD=12.17;ReadPosRankSum=0.616;SOR=1.080; GT:AD:DP:GQ:PL; 0/1:9,8:17:99:227,0,272; 0/0:41,0:41:0:0,0,1097; 0/0:51,0:51:0:0,0,1216; 0/0:61,0:61:0:0,0,1373; 0/0:54,0:54:0:0,0,962; 0/0:49,0:49:0:0,0,1156; 0/0:53,0:53:0:0,0,729; 0/0:44,0:44:0:0,0,1161; 0/0:38,0:38:0:0,0,963; 0/0:68,0:68:0:0,0,1518; 0/0:33,0:33:0:0,0,841; 0/0:54,0:54:0:0,0,687; 0/0:44,0:44:0:0,0,1003; 0/0:33,0:33:0:0,0,709; 0/0:54,0:54:0:0,0,580; 0/0:31,0:31:0:0,0,790; 0/0:36,0:36:0:0,0,843; 0/0:49,0:49:0:0,0,978; 0/0:34,0:34:0:0,0,669; 0/0:39,0:39:0:0,0,898; 0/0:60,0:60:0:0,0,1270; 0/0:48,0:48:0:0,0,908; 0/0:30,0:30:0:0,0,780; 0/0:44,0:44:0:0,0,778; 0/0:24,0:24:0:0,0,531; 0/0:40,0:40:0:0,0,714; 0/0:50,0:50:0:0,0,794; 0/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445:4149,pipeline,pipeline,4149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445,1,['pipeline'],['pipeline']
Deployability,"3#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:2127,release,release,2127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,2,['release'],['release']
Deployability,3); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2503:1256,deploy,deploy,1256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503,1,['deploy'],['deploy']
Deployability,3); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3303:4424,deploy,deploy,4424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303,1,['deploy'],['deploy']
Deployability,3); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.get,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2793:2081,deploy,deploy,2081,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793,1,['deploy'],['deploy']
Deployability,"3.0; 13:54:10.423 INFO ProgressMeter - chr22:41712333 9.2 6226000 676191.6; 13:54:20.447 INFO ProgressMeter - chrX:39799780 9.4 6342000 676514.9; 13:54:30.520 INFO ProgressMeter - chrX:91818371 9.5 6453000 676246.2; 13:54:40.591 INFO ProgressMeter - chrX:143619069 9.7 6568000 676399.8; 13:54:50.640 INFO ProgressMeter - chrUn_KI270743v1:125398 9.9 6674000 675662.2; 13:55:00.673 INFO ProgressMeter - chr20_KI270869v1_alt:62679 10.0 6792000 676161.8; 13:55:10.679 INFO ProgressMeter - chr19_GL949752v1_alt:485077 10.2 6910000 676673.7; 13:55:26.149 INFO ProgressMeter - HLA-DRB1*11:01:02:3272 10.5 6938356 662718.7; 13:55:26.149 INFO ProgressMeter - Traversal complete. Processed 6938356 total records in 10.5 minutes.; 13:55:26.149 INFO ComposeSTRTableFile - Shutting down engine; [April 4, 2021 1:55:26 PM EDT] org.broadinstitute.hellbender.tools.dragstr.ComposeSTRTableFile done. Elapsed time: 10.52 minutes.; Runtime.totalMemory()=1128792064; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentia",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:11212,install,install,11212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['install'],['install']
Deployability,3:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.CREATE_MD5 : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.CUSTOM_READER_FACTORY :; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3351,Configurat,Configuration,3351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Configurat'],['Configuration']
Deployability,"4). If you organize the inputs into blocks and keep all such knobs together at the end it's not too bad. A lot of our users will need to be able to tweak those settings -- and the others can ignore them. . See here for an example of how we do it: https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.inputs.json. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607287). So we are a slimmer version of what @vdauwera has. @takutosato I agree with your frustrations, but then we have to hardcode to the worst case, which will be quite expensive (in the cloud), underutilized (in all backends), and have trouble dispatching (in SGE). . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607541). @vdauwera I will happily accept comments on our json templates. . https://github.com/broadinstitute/gatk-protected/tree/master/scripts/mutect2_wdl. ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286607739). @davidbenjamin @takutosato The more I think about it, the more important I think this issue is. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/925#issuecomment-286613100). Yeah we need to parameterize the heck out of all our WDLs. If anything, the example I linked to is not parameterized nearly as much as I'd like (it's derived from the prod pipeline so we're a bit constrained). . It's not that much clutter if you make those parameters task-level and organize the JSONs clearly. And it makes it waaaay easier for people to adjust what they need without touching the WDL itself. This becomes even more important once you move the WDL into a platform like FireCloud, where changing the WDL is a huge pain, whereas tweaking parameters (via a method config) is trivial.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2949:2337,pipeline,pipeline,2337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2949,1,['pipeline'],['pipeline']
Deployability,4.0.0]; at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:986) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:825) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:93) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:233) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:119) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:374) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.tools.spark.pipelines.SortSamSpark.runTool(SortSamSpark.java:114) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:546) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.broadinstitute.hellbender.Main.mainEntry(Main.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:17926,pipeline,pipelines,17926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['pipeline'],['pipelines']
Deployability,"4.1.8.1/gatk AnalyzeCovariates --before-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/before.recal.FMF-248.table --after-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/after.recal.FMF-248.table --plots-report-file /home/detagen/Desktop/pipeline/playground//NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; Using GATK jar /home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar AnalyzeCovariates --before-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/before.recal.FMF-248.table --after-report-file /home/detagen/Desktop/pipeline/playground//BACKUP/FMF-248_Backup/after.recal.FMF-248.table --plots-report-file /home/detagen/Desktop/pipeline/playground//NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; 12:57:16.643 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/detagen/Desktop/programlar/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Dec 17, 2020 12:57:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:57:16.774 INFO AnalyzeCovariates - ------------------------------------------------------------; 12:57:16.775 INFO AnalyzeCovariates - The Genome Analysis Toolkit (GATK) v4.1.8.1; 12:57:16.775 INFO AnalyzeCovariates - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:57:16.775 INFO AnalyzeCovariates - Executing as detagen@detagen on Linux v5.4.0-58-generic amd64; 12:57:16.775 INFO AnalyzeCovariates - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1+1-Ubuntu-0ubuntu1.18.04; 12:57:16.775 INFO AnalyzeCova",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7006:1034,pipeline,pipeline,1034,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7006,1,['pipeline'],['pipeline']
Deployability,41.330011002Z 13:13:41.328 INFO GenotypeGVCFs - HTSJDK Version: 2.13.2; 2018-03-09T13:13:41.330022980Z 13:13:41.328 INFO GenotypeGVCFs - Picard Version: 2.17.2; 2018-03-09T13:13:41.330030226Z 13:13:41.329 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 2018-03-09T13:13:41.330036559Z 13:13:41.329 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 2018-03-09T13:13:41.330045071Z 13:13:41.329 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 2018-03-09T13:13:41.330051564Z 13:13:41.329 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 2018-03-09T13:13:41.330068542Z 13:13:41.329 INFO GenotypeGVCFs - Deflater: IntelDeflater; 2018-03-09T13:13:41.330102470Z 13:13:41.329 INFO GenotypeGVCFs - Inflater: IntelInflater; 2018-03-09T13:13:41.330111084Z 13:13:41.329 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 2018-03-09T13:13:41.330117286Z 13:13:41.329 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 2018-03-09T13:13:41.330127806Z 13:13:41.329 INFO GenotypeGVCFs - Initializing engine; 2018-03-09T13:13:44.528605497Z 13:13:44.528 INFO GenotypeGVCFs - Done initializing engine; 2018-03-09T13:13:45.237843760Z 13:13:45.235 INFO ProgressMeter - Starting traversal; 2018-03-09T13:13:45.237903383Z 13:13:45.235 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 2018-03-09T13:13:56.665869885Z 13:13:56.662 INFO ProgressMeter - chr13:82078938 0.2 13000 68265.4; 2018-03-09T13:14:07.517952300Z 13:14:07.517 INFO ProgressMeter - chr13:82096938 0.4 31000 83475.5; 2018-03-09T13:14:17.546110604Z 13:14:17.545 INFO ProgressMeter - chr13:82123938 0.5 58000 107706.6; 2018-03-09T13:14:28.760222694Z 13:14:28.759 INFO ProgressMeter - chr13:82144938 0.7 79000 108905.4; 2018-03-09T13:14:39.292149466Z 13:14:39.289 INFO ProgressMeter - chr13:82169938 0.9 104000 115440.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518:2766,patch,patch,2766,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518,1,['patch'],['patch']
Deployability,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:19072,update,updates,19072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,8,"['Update', 'pipeline', 'update']","['Update', 'pipeline', 'update', 'updates']"
Deployability,5); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionRe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:4083,configurat,configuration,4083,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['configurat'],['configuration']
Deployability,"5.033 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:$HOME/local/pckg/python/miniconda3/envs/test/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Tue Jun 16 23:25:05 CDT 2020] MergeVcfs --INPUT data/calling/a.vcf.gz --INPUT data/calling/b.vcf.gz --OUTPUT c.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 16, 2020 11:25:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Tue Jun 16 23:25:05 CDT 2020] Executing as xxxx on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0; [Tue Jun 16 23:25:05 CDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=605028352; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to create BasicFeatureReader using feature file , for input source: file:///tmp/test%20a/data/calling/a.vcf.gz; at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:124); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:81); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:148); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:98); at picard.vcf.MergeVcfs.doWork(MergeVcfs.java:174); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664:2282,release,release-,2282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664,1,['release'],['release-']
Deployability,533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.RuntimeException: Could not serialize lambda; 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:70); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 44 more; Caused by: java.lang.NoSuchMethodException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$94/1029586776.writeReplace(); 	at java.lang.Class.getDeclaredMethod(Class.java:2130); 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:61); 	... 46 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6091:4074,deploy,deploy,4074,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091,6,['deploy'],['deploy']
Deployability,"635 INFO HaplotypeCaller - Start Date/Time: October 29, 2018 10:25:20 PM EDT; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.635 INFO HaplotypeCaller - ------------------------------------------------------------; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Version: 2.14.3; 22:25:20.635 INFO HaplotypeCaller - Picard Version: 2.17.2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:25:20.635 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:25:20.636 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:25:20.636 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:25:20.636 INFO HaplotypeCaller - Inflater: IntelInflater; 22:25:20.636 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:25:20.636 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 22:25:20.636 INFO HaplotypeCaller - Initializing engine; 22:25:21.061 INFO HaplotypeCaller - Done initializing engine; 22:25:21.069 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:25:21.069 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:25:21.070 INFO HaplotypeCaller - Shutting down engine; [October 29, 2018 10:25:21 PM EDT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1506344960; USAGE: HaplotypeCaller [arguments]. Call germline SNPs and indels via local re-assembly of haplotypes; Version:4.0.3.0. ***********************************************************************. A USER ERROR has occurred: Argument --emitRefConfidence has a bad value: Can onl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5372:2819,patch,patch,2819,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372,1,['patch'],['patch']
Deployability,"66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:16839,pipeline,pipelines,16839,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['pipeline'],['pipelines']
Deployability,"6](https://github.com/broadinstitute/gsa-unstable/issues/1438). ### Instructions. Follow up to #1432.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. ---; ## Feature request; ### Tool(s) involved. Any tool using `IntervalUtils. intervalFileToList()` ; ### Description. Once this change is made, -1 length intervals will be validated and an exception will be thrown. ---. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927). From what I understand of the referenced thread, the ""incorrect"" interval list may always be around, so we may never be able to just blow up on it. Would it perhaps be more viable to add an option to toggle the level of stringency, ie choose in the command line whether to blow up or skip on these invalid intervals? . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260496001). @yfarjoun will want to opine on this, I think. . ---. @yfarjoun commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260513266). I hope that when we move exomes to hg38 we will correct this silly thing; and a few decades later we will no need this code (hehe). Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; > ; > You are receiving this because y",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2520:1007,toggle,toggle,1007,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520,1,['toggle'],['toggle']
Deployability,"6d11bef1c81f885c26b2b56c8616b7a705171e4f from [https://github.com/droazen/google-cloud-java/tree/dr\\\_all\\\_nio\\\_fixes](https://github.com/droazen/google-cloud-java/tree/dr\_all\_nio\_fixes) ; ; 12:52:16.269 INFO GenotypeGVCFs - Initializing engine ; ; terminate called after throwing an instance of 'VariantQueryProcessorException' ; ; what(): VariantQueryProcessorException : Could not open array genomicsdb\_array at workspace: /home/WES-VCFQC/S2\_GenomicsDBImport/temporary/tmp4. Hi, I used GenomicsDBImport to combined 2000 GVCFs. To speed up, I split the bed file and concatenated multiple intervals into a contig. I also met the file locking problem which can be solved by setting TILEDB\_DISABLE\_FILE\_LOCKING=1 in my Linux system. Currently, I experience some issues with GenotypeGVCFs in GATK version 4.0.3.0. It cannot open ""genomicsdb\_array"" although the directory of genomicsdb\_array does exist. I found someone else has reported this issue here: [https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000](https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000), but except for using the latest version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7442:5208,a/b,a/broadinstitute,5208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442,1,['a/b'],['a/broadinstitute']
Deployability,7); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:12493,deploy,deploy,12493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['deploy'],['deploy']
Deployability,"7:1024); 	at picard.sam.AbstractAlignmentMerger.mergeAlignment(AbstractAlignmentMerger.java:557); 	at picard.sam.SamAlignmentMerger.mergeAlignment(SamAlignmentMerger.java:186); 	at picard.sam.MergeBamAlignment.doWork(MergeBamAlignment.java:368); 	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Despite the fact that option `-SORT_ORDER ""unsorted""` is being used and the two BAM files have the reads in the same order:; ```; $ samtools view unmapped.bam | cut -f1; NB500989:333:HKYJNAFX2:1:11101:24447:1024; NB500989:333:HKYJNAFX2:1:11101:24447:1024; NB500989:333:HKYJNAFX2:1:11101:10000:1915; NB500989:333:HKYJNAFX2:1:11101:10000:1915. $ samtools view aligned.unmerged.bam | cut -f1; NB500989:333:HKYJNAFX2:1:11101:24447:1024; NB500989:333:HKYJNAFX2:1:11101:24447:1024; NB500989:333:HKYJNAFX2:1:11101:10000:1915; NB500989:333:HKYJNAFX2:1:11101:10000:1915; ```; Is there a way to run `MergeBamAlignment` letting the tool know that the order of the reads in the input BAMs is the same?. It seems then that the only workaround is to let `FastqToSam` sort the reads, which seems an unnecessary computational step since eventually the reads will be sorted by coordinate anyway. This problem was observed when running together the WDL pipelines [paired-fastq-to-unmapped-bam.wdl](https://github.com/gatk-workflows/seq-format-conversion/blob/master/paired-fastq-to-unmapped-bam.wdl) and [processing-for-variant-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl) where the former runs `FastqToSam` and the latter runs `SamToFastq` and `MergeBamAlignment`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7398:4739,pipeline,pipelines,4739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398,1,['pipeline'],['pipelines']
Deployability,"7;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2869200,797;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5975:6134,release,releases,6134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975,1,['release'],['releases']
Deployability,7a5236e879818910d9c9. and the stacktrace:. ```; java.util.ServiceConfigurationError: java.nio.file.spi.FileSystemProvider: Provider com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider could not be instantiated; at java.util.ServiceLoader.fail(ServiceLoader.java:232); at java.util.ServiceLoader.access$100(ServiceLoader.java:185); at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384); at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); at java.util.ServiceLoader$1.next(ServiceLoader.java:480); at java.nio.file.spi.FileSystemProvider.loadInstalledProviders(FileSystemProvider.java:119); at java.nio.file.spi.FileSystemProvider.access$000(FileSystemProvider.java:77); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:169); at java.nio.file.spi.FileSystemProvider$1.run(FileSystemProvider.java:166); at java.security.AccessController.doPrivileged(Native Method); at java.nio.file.spi.FileSystemProvider.installedProviders(FileSystemProvider.java:166); at java.nio.file.Paths.get(Paths.java:141); at org.broadinstitute.hellbender.engine.spark.datasources.NioProviderExceptionUnitTest.test(NioProviderExceptionUnitTest.java:12). Caused by:; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:122); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:208); at com.google.cloud.HttpServiceOptions.<init>(HttpServiceOptions.java:153); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:69); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:27); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:64); at com.google.cloud.storage.StorageOptions.defaultInstance(StorageOptions.java:91); at com.google.cloud.storage.contrib.nio.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2110:1259,install,installedProviders,1259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110,1,['install'],['installedProviders']
Deployability,8); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:53727,deploy,deploy,53727,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['deploy'],['deploy']
Deployability,"86); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay un",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:31331,update,update,31331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['update'],['update']
Deployability,"8:01.493 INFO HaplotypeCaller - Start Date/Time: May 17, 2018 6:58:01 PM PDT. 18:58:01.493 INFO HaplotypeCaller - ------------------------------------------------------------. 18:58:01.493 INFO HaplotypeCaller - ------------------------------------------------------------. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Version: 2.14.1. 18:58:01.494 INFO HaplotypeCaller - Picard Version: 2.17.2. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true. 18:58:01.494 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false. 18:58:01.494 INFO HaplotypeCaller - Deflater: IntelDeflater. 18:58:01.494 INFO HaplotypeCaller - Inflater: IntelInflater. 18:58:01.494 INFO HaplotypeCaller - GCS max retries/reopens: 20. 18:58:01.494 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes. 18:58:01.494 INFO HaplotypeCaller - Initializing engine. 18:58:02.043 INFO HaplotypeCaller - Done initializing engine. 18:58:02.053 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output. 18:58:02.053 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output. 18:58:02.886 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_utils.so. 18:58:02.888 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so. 18:58:02.932 WARN IntelPairHmm - Flush-to-zero (FTZ) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4788:2864,patch,patch,2864,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788,1,['patch'],['patch']
Deployability,"9). However, the bug wasn't reported when I didn't assign the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;   java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:12:21.140 INFO BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.141 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:15113,pipeline,pipeline,15113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"9); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///proj",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:4688,deploy,deploy,4688,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,1,['deploy'],['deploy']
Deployability,"96.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:45.788 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:40:45.788 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:45.789 INFO HaplotypeCaller - Start Date/Time: February 10, 2021 2:40:45 PM EST; 14:40:45.789 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.789 INFO HaplotypeCaller - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:6032,install,install,6032,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,1,['install'],['install']
Deployability,9:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:1234,deploy,deploy,1234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"9:4040; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Stopped; 17/10/11 14:19:38 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/10/11 14:19:38 INFO storage.MemoryStore: MemoryStore cleared; 17/10/11 14:19:38 INFO storage.BlockManager: BlockManager stopped; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 17/10/11 14:19:38 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/10/11 14:19:38 INFO spark.SparkContext: Successfully stopped SparkContext; 14:19:38.600 INFO PrintReadsSpark - Shutting down engine; [October 11, 2017 2:19:38 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.48 minutes.; Runtime.totalMemory()=986185728; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:31828,pipeline,pipelines,31828,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['pipeline'],['pipelines']
Deployability,9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7598:3859,update,update,3859,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598,1,['update'],['update']
Deployability,: IntelDeflater; 01:22:35.483 INFO GenomicsDBImport - Inflater: IntelInflater; 01:22:35.483 INFO GenomicsDBImport - GCS max retries/reopens: 20; 01:22:35.483 INFO GenomicsDBImport - Requester pays: disabled; 01:22:35.484 INFO GenomicsDBImport - Initializing engine; 01:24:58.683 INFO FeatureManager - Using codec BEDCodec to read file file:///lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error;,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7598:3450,update,update,3450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598,1,['update'],['update']
Deployability,": v4.1.9 ; ; b) Exact command used:. /gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar \\ ; ; VariantEval \\ ; ; \-R /$PATH\_TO\_REFERENCE/chm13/t2t-chm13.20200921.withGRCh38chrY.chrEBV.chrYKI270740v1r.fasta \\ ; ; \--eval /$PATH\_TO\_VCF/1kgp.chrX.recalibrated.snp\_indel.pass.vcf.gz \\ ; ; \--pedigree /$PATH\_TO\_PED/1kgp\_trios.ped \\ ; ; \-no-ev -no-st -ST Family \\ ; ; \-EV MendelianViolationEvaluator \\ ; ; \-O 1kgp.chrX.recalibrated.snp\_indel.pass.MVs.byFamily.table. c) Entire error log:. 19:35:29.408 INFO VariantEval - ------------------------------------------------------------ 19:35:29.408 INFO VariantEval - The Genome Analysis Toolkit (GATK) v4.1.9.0 19:35:29.409 INFO VariantEval - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) 19:35:29.409 INFO VariantEval - Executing as root@0b79b5044551 on Linux v5.4.104+ amd64 19:35:29.409 INFO VariantEval - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 19:35:29.409 INFO VariantEval - Start Date/Time: May 27, 2021 7:35:29 PM UTC 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.409 INFO VariantEval - ------------------------------------------------------------ 19:35:29.410 INFO VariantEval - HTSJDK Version: 2.23.0 19:35:29.410 INFO VariantEval - Picard Version: 2.23.3 19:35:29.410 INFO VariantEval - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 19:35:29.410 INFO VariantEval - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false 19:35:29.410 INFO VariantEval - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true 19:35:29.410 INFO VariantEval - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false 19:35:29.410 INFO VariantEval - Deflater: IntelDeflater 19:35:29.410 INFO VariantEval - Inflater: IntelInflater 19:35:29.410 INFO VariantEval - GCS max retries/reopens: 20 19:35:29.410 INFO VariantEval - Requester pays: disabled 19:35:29.411 WARN VariantEval - \[1m\[31m !!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7304:1606,release,release-,1606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304,1,['release'],['release-']
Deployability,"://192.168.0.104:9000/user/jacky/marked_dup.bam -M hdfs://192.168.0.104:9000/user/jacky/marked_dup_metrics.txt -- --spark-runner SPARK --deploy-mode cluster --spark-master yarn; Using GATK jar /home/jacky/Exec/gatk/build/libs/gatk-spark.jar; Running:; /home/jacky/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.executor.memoryOverhead=600 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --deploy-mode cluster /home/jacky/Exec/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I hdfs://192.168.0.104:9000/user/jacky/NA12878.mapped.illumina.mosaik.CEU.exome.20110411.bam -O hdfs://192.168.0.104:9000/user/jacky/marked_dup.bam -M hdfs://192.168.0.104:9000/user/jacky/marked_dup_metrics.txt --spark-master yarn; 20/10/22 12:02:26 INFO client.RMProxy: Connecting to ResourceManager at /192.168.0.104:8032; 20/10/22 12:02:26 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers; 20/10/22 12:02:26 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1536 MB per container); 20/10/22 12:02:26 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 20/10/22 12:02:26 INFO yarn.Client: Setting up container launch context for our AM; 20/10/22 12:02:26 INFO yarn.Client: Setting up the launch environment for our AM container; 20/10/22 12:02:26 INFO yarn.Client: Preparing resources for our ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:1307,deploy,deploy-mode,1307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,1,['deploy'],['deploy-mode']
Deployability,":121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.clus",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:6876,deploy,deploy,6876,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,:995); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:406); at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:986); at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:825); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:93); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:233); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:119); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:374); at org.broadinstitute.hellbender.tools.spark.pipelines.SortSamSpark.runTool(SortSamSpark.java:114); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:546); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.lang.OutOfMemoryError: Required arr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:26519,pipeline,pipelines,26519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['pipeline'],['pipelines']
Deployability,":; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.uti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:3067,deploy,deploy,3067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['deploy'],['deploy']
Deployability,; 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2349:3618,Integrat,IntegrationTestSpec,3618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349,1,['Integrat'],['IntegrationTestSpec']
Deployability,; 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:119); 	at org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark.runTool(BwaSpark.java:49); 	... 17 more,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2503:1478,deploy,deploy,1478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503,2,['deploy'],['deploy']
Deployability,"; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/ramadugulab/luy/SNPcallingBreeding/core.1058615); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. --------------- S U M M A R Y ------------. Command Line: -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /bigdata/operations/pkgadmin/opt/linux/centos/8.x/x86_64/pkgs/gatk/4.6.0.0/gatk-package-4.6.0.0-local.jar HaplotypeCaller -R /rhome/luy/bigdata/genomes/Cclementina_182_v1_2.fa -I AlignedCalToCcl_Scaffolds_MarkDupOut.bam -O AlignedCalToCcl_Scaffolds.vcf.gz -ERC GVCF. Host: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz, 64 cores, 20G, Rocky Linux release 8.8 (Green Obsidian); Time: Sat Sep 28 04:11:19 2024 PDT elapsed time: 58592.788414 seconds (0d 16h 16m 32s). --------------- T H R E A D ---------------. Current thread (0x00007f06e4025b70): JavaThread ""main"" [_thread_in_native, id=1058616, stack(0x00007f06edc7a000,0x00007f06edd7b000)]. Stack: [0x00007f06edc7a000,0x00007f06edd7b000], sp=0x00007f06edbe6458, free space=18014398509481393k; Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code); C [libc.so.6+0xcf291] __memset_avx2_erms+0x11; C [libgkl_pairhmm_omp5311772482084658743.so+0x1500f] Java_com_intel_gkl_pairhmm_IntelPairHmm_computeLikelihoodsNative._omp_fn.0+0xcf. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); J 8942 com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V (0 bytes) @ 0x00007f06d563401c [0x00007f06d5633fa0+0x000000000000007c]; J 10003 c2 com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988:1784,release,release,1784,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988,1,['release'],['release']
Deployability,"; - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:23797,integrat,integration,23797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['integrat'],['integration']
Deployability,"; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:2098,deploy,deploy,2098,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['deploy'],['deploy']
Deployability,"; ```. 3. java.lang.NullPointerException occurs. ; 4. No variants output into VCF. This is the log:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:1-105581 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-1-105581.vcf.gz; 00:05:54.259 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 00:05:54.319 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:05:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:05:54.582 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.583 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 00:05:54.583 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:05:54.583 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:05:54.583 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:05:54.584 INFO GenotypeGVCFs - Start Date/Time: August 25, 2021 12:05:54 AM EDT; 00:05:54.584 INFO GenotypeGVCFs - ------------------------------------------------------------; 00:05:54.584 INFO GenotypeGVCFs - --------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437:1673,install,install,1673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437,1,['install'],['install']
Deployability,; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:1444,Configurat,ConfigurationResolver,1444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['Configurat'],['ConfigurationResolver']
Deployability,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:3882,Install,Installing,3882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,8,"['Install', 'install', 'update']","['Installing', 'install', 'installAll', 'installer', 'update']"
Deployability,"<img width=""745"" alt=""Screenshot 2023-11-14 at 12 44 24 AM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/8390c9a8-f343-4c9b-9636-15b4dfc5aefa"">. Integration run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/3aa1545f-7f11-4919-9ecf-8b3a5900441a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8581:159,Integrat,Integration,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8581,1,['Integrat'],['Integration']
Deployability,"=com.google.protobuf:protobuf-java&package-manager=gradle&previous-version=3.23.4&new-version=3.25.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/broadinstitute/gatk/network/alerts). </details>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9004:4040,upgrade,upgrade,4040,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9004,3,['upgrade'],['upgrade']
Deployability,"> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171:1224,install,install,1224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171,1,['install'],['install']
Deployability,@LeeTL1220 . * Updates command line arguments to conform to new GATK standard.; * Removes M2 wdl hacks no longer needed as of Cromwell 27,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3584:15,Update,Updates,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3584,1,['Update'],['Updates']
Deployability,@LeeTL1220 This fixes the bug preventing Beri from updating to 4.0.8.0. I will put in an integration test but could you start looking at it now?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5121:89,integrat,integration,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5121,1,['integrat'],['integration']
Deployability,"@LeeTL1220 This fixes the issue. I tested it on SGE and the M2 integration tests now have a DREAM bam where I switched the sample to ""tumor sample"".",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4360:63,integrat,integration,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4360,1,['integrat'],['integration']
Deployability,@LeeTL1220 This is worth getting in before the release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4848:47,release,release,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4848,1,['release'],['release']
Deployability,"@LeeTL1220 This should fail earlier in situations like the one you ran into with the low coverage test data. I don't *think* there should be any unintended side effects. I'm fine if this doesn't make it in before the point release if you want to run some sanity checks on real data with it (since users should be able to figure out what is going on relatively quickly if they run into the issue), but I'll leave it up to you.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4292:223,release,release,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4292,1,['release'],['release']
Deployability,@LeeTL1220 Your call if this is worth merging pre-release. @sooheelee @chandrans In case some users want to know how to generate the M2 resources for themselves.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4069:50,release,release,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4069,1,['release'],['release']
Deployability,"@LeeTL1220 commented on [Fri Jan 15 2016](https://github.com/broadinstitute/gatk-protected/issues/308). This may be as simple as adding a `ctx.close()` statement after the spark calculations are complete.; - [ ] Confirmed on our spark cluster that this is fixed... ---. @LeeTL1220 commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-175139853). Cannot close without the unit tests falling over. Putting this off for a later release. ---. @lbergelson commented on [Tue Jan 26 2016](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-175154352). Possible solutions involve running `SparkContext.KillExecutors()` but I haven't looked into how it works exactly... ---. @samuelklee commented on [Thu May 11 2017](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300793994). @LeeTL1220 should I keep this open?. ---. @LeeTL1220 commented on [Thu May 11 2017](https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300797579). I think this should be kept open, but low priority. On Thu, May 11, 2017 at 9:46 AM, samuelklee <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> should I keep this open?; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/308#issuecomment-300793994>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk06fX-Z26myWvz9Shn_c5e4I0xHqks5r4xEigaJpZM4HGA9T>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2833:476,release,release,476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2833,1,['release'],['release']
Deployability,@LeeTL1220 commented on [Tue Oct 18 2016](https://github.com/broadinstitute/gatk-protected/issues/742). Should now be `CallAllelicSplits`. Rename the task configuration as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2903:155,configurat,configuration,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2903,1,['configurat'],['configuration']
Deployability,@LeeTL1220 commented on [Tue Oct 18 2016](https://github.com/broadinstitute/gatk-protected/issues/743). Now called `CallAllelicSplits` . This should happen after (or right before) the next release.; - [ ] Update forum posts; - [ ] Add forum post explaining the output of the cnb_called files. See issue #585,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2904:189,release,release,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2904,2,"['Update', 'release']","['Update', 'release']"
Deployability,"@LeeTL1220 commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1053). And upgrade ubuntu to 16.04. And leverage the ``broadinstitute/gatk:gatkbase-1.0`` image. Actually, this will handle the move to OpenJDK8 and ubuntu 16.04.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2990:109,upgrade,upgrade,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2990,1,['upgrade'],['upgrade']
Deployability,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1768:47,configurat,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768,1,['configurat'],['configuration']
Deployability,"@anowlcalledjosh and @bbimber Thanks for your contributions to GATK - I've updated the AUTHORs list with your names - can you verify that it looks correct ? (@bbimber not sure I got your name right, and let me know if you have an email address to include). Thx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5033:75,update,updated,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5033,1,['update'],['updated']
Deployability,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1063). Due to a bug in Nd4j library, INDArray.get() method does not work correctly for row or column vectors (that we work with when number of samples is 1). In these cases a call to `getNDArrayByIndices(array, indX, indY, int)` is made.; Note that this bug was fixed in later version of Nd4j (currently used version is 0.5.0), so the method can be removed when the dependency is updated",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3000:480,update,updated,480,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3000,1,['update'],['updated']
Deployability,"@asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065). ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302528347). @mbabadi could you do it?. ---. @sooheelee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302564888). Is the test <src/test/java/org/broadinstitute/hellbender/tools/coveragemodel/germline/GermlineCNVCallerIntegrationTest.java> not the integration test?. ---. @asmirnov239 commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-302565368). @sooheelee It's a collection of different tests, but it's missing some use cases. ---. @mbabadi commented on [Mon May 22 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303249272). @asmirnov239 it covers PoN creation and calling (from the created PoN, and from the ""exact"" PoN). It certainly does not cover all combination of all advanced arguments, and we do not intend to do that either. Perhaps we should extend the test to include w/ and w/o ARD, and w/ and w/o bias covariates. I'm open to suggestions. ---. @asmirnov239 commented on [Wed May 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1065#issuecomment-303858968). @mbabadi What I meant is to write an extra test for a use case of calling events on a single sample (as it is a requirement for our workflows). Just a single test with most generic arguments should suffice I think.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3002:535,integrat,integration,535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002,1,['integrat'],['integration']
Deployability,"@bshifaw found this bug in the M2 wdl where it requests eg 3500 GB of RAM instead of 3500 MB, causing disastrous ""no machines available"" errors. @LeeTL1220 any way to get this into a release?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4321:183,release,release,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4321,1,['release'],['release']
Deployability,"@chandrans commented on [Mon Jan 29 2018](https://github.com/broadinstitute/dsde-docs/issues/2881). User reports BaseRecalibratorSpark in gatk-4.beta.6-17 took 3.79 minutes vs 40 minutes in official release. ----; User Report; ----. Dear GATK_team, I'd like to run Spark-enabled GATK tools on a Spark cluster. Precisely I am launching a Spark cluster in the standalone mode submitting the `BaseRecalibratorSpark` application via Slurm. Before the official release, I was running the `gatk-4.beta.6-17` version, with the following allocated resources, and the following command line for the Spark arguments: `./gatk-launch BaseRecalibratorSpark \ --sparkRunner SPARK --sparkMaster spark://${MASTER} --driver-memory 80g --num-executors 16 --executor-memory 8g`. The speed-up achieved was 3.79 min. However, with the official release GATK-4.0.0.0, with the same datafiles and the same Spark arguments I don't see the same nice speed-up anymore (~ 40 min). Am I missing something with the new version? Or with the invoking command line? Thanks in advance for your time and kind answer. Best, Giuseppe. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11260/gatk-4-0-0-0-baserecalibratorspark-low-performance/p1. ---. @chandrans commented on [Mon Jan 29 2018](https://github.com/broadinstitute/dsde-docs/issues/2881#issuecomment-361324925). @droazen @lbergelson Hi David and Louis. Do you have any comments? I was supposed to put this in gatk but put it in dsde-docs. Thanks. ---. @droazen commented on [Mon Jan 29 2018](https://github.com/broadinstitute/dsde-docs/issues/2881#issuecomment-361342262). @chandrans Could you move this ticket to the gatk repo so that we can remember to have a look at the tool? Someone will have to re-profile.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4300:199,release,release,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4300,3,['release'],['release']
Deployability,"@cmnbroad I updated VariantQC and identified one minor difference in behavior associated with VariantEvalEngine. Contig stratification assigns level based on all the contigs. If user-supplied contigs are given, it should defer to these. This PR addresses this, and adds a test case. Note: I put the getContigNames() method into VariantEvalEngine, but it would also be possible to keep this in Config, but expose a getter for userSuppliedIntervals. It seemed marginally better to keep that private.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7238:12,update,updated,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238,1,['update'],['updated']
Deployability,"@cmnbroad correct me if I'm wrong, but we currently just update the conda env if it is already present. This will add new packages that were added to the yml but will not remove old packages that were removed from the yml. Note that the update --prune option seems like it should take care of this, but see https://github.com/conda/conda/issues/7279.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5776:57,update,update,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5776,2,['update'],['update']
Deployability,"@cmnbroad, [a researcher has pointed out](https://gatkforums.broadinstitute.org/gatk/discussion/1319/collected-faqs-about-interval-lists#latest) that although GATK accepts both types of intervals lists (Picard-style & BED), Picard tools called through the GATK errors with a BED intervals list. Is it possible to amend this behavior so any intervals list GATK accepts, Picard-called-through-GATK also accepts? If not, please let us know (myself and @rcmajovski) so that we can update documentation. . Given BED is the more widely-used intervals format, it would be great if we enabled its use consistently in our tools. The downside is the lack of reference match checking. However, it seems the decision has already been made with GATK's acceptance of BED intervals. Let me know your thoughts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5472:477,update,update,477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5472,1,['update'],['update']
Deployability,"@cwhelan @tedsharpe @vruano @mwalker174 , I've organized the scripts for running the whole sv pipeline as it exists right now. There used to be a PR (if I recall correctly) but since that's outdated, why not an up-to-date one. Here's how to run it. 1. To create the cluster. ```; ./create_cluster.sh broad-dsde-methods sv-methods-1 broad-dsde-methods/sv; ```; where the 1st argument is the project name, 2nd argument is the cluster name, and the 3rd name is the place where input data lives. 2. To run the whole pipeline. ```; ./svCall.sh /Users/shuang/GATK/gatk sv-methods-1 /user/shuang/NA12878_PCR-_30X; ```; where the 1st argument is the location of my GATK directory and the 3rd argument is the location of all outputs on the cluster. So change them as necessary. The different stages called by the master script `svCall.sh` are (in order) `scanBam.sh` -> `assembly.sh` -> `alignAssembly.sh` -> `callVariants.sh`, which all take the same arguments. 3. To delete the whole cluster. ```; ./delete_cluster.sh sv-methods-1; ```; This avoids having to wait for the web-based Console's confirmation. One thing to note though, is that I've copied everything to a bucket at; ```; gs://broad-dsde-methods/sv/; ```; under a different project. We used to be developing under the project ""broad-dsde-dev"", but we are asked to move to project ""broad-dsde-methods"". So to run these scripts, you might need to switch to a different project via. ```; gcloud config set project broad-dsde-methods; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435:94,pipeline,pipeline,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435,2,['pipeline'],['pipeline']
Deployability,"@cwhelan @tedsharpe please review. The pipeline tools are:; 1. PathSeqFilterSpark : quality/low-complexity/host read filtering; 2. PathSeqPathogenAlignSpark : bwa-mem aligner; 3. PathSeqClassifyReadsSpark : quantifies pathogen abundance. These are supported by utilities:; 4. PathSeqKmerSpark - creates kmer library (either a Hopscotch set or Bloom filter) used by Filter tool; 5. PathSeqBuildReferenceTaxonomy - creates a file containing taxonomic information for a given reference, required by ClassifyReads. tools.spark.pathseq package:; Contains all the tools. Static helper functions were put into PS<ToolName>Utils classes, eg PSFilterUtils contains functions used by PathSeqFilterSpark. PSUtils contains mostly functions that are used by more than one tool. There are a number of other ""PS"" classes for doing Bwa, taxonomy bookkeeping, and read classification. Also has the kmer and host alignment read filters. tools.spark.sv package:; Added base masking to SVKmerShort class. tools.spark.utils package:; Hopscotch set and Bloom filter for long primitives. Each type has a ""Large"" class for sets exceeding the maximum JVM array size (~2B).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2646:39,pipeline,pipeline,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2646,1,['pipeline'],['pipeline']
Deployability,"@davidbenjamin Discovered that importing hellbender as a dependency fails now that we have add spark dependencies. This is easily fixed by including the following (or it's non-gradle equivalent) in your build file, but it shouldn't be necessary. ```; maven {; url ""https://repository.cloudera.com/artifactory/cloudera-repos/"" // spark-dataflow; }; ```. We should update our artifact so that it includes the necessary information to download the spark dependencies.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/779:363,update,update,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/779,1,['update'],['update']
Deployability,"@davidbenjamin Intellij pointed out this if statement to me as suspicious and I think it is. There are two arms of the second if statement that are guarded by `includeNonVariants`. However the second one can never be hit because if `includeNonVariants` you will already have chosen the first clause. Seems suspicious...; ```; if (regenotypedVC == null || (!GATKVariantContextUtils.isProperlyPolymorphic(regenotypedVC) && !includeNonVariants)) {; return null;; }; if (GATKVariantContextUtils.isProperlyPolymorphic(regenotypedVC) || includeNonVariants) {; // Note that reversetrimAlleles must be performed after the annotations are finalized because the reducible annotation data maps; // were generated and keyed on the un reverseTrimmed alleles from the starting VariantContexts. Thus reversing the order will make; // it difficult to recover the data mapping due to the keyed alleles no longer being present in the variant context.; final VariantContext withGenotypingAnnotations = addGenotypingAnnotations(originalVC.getAttributes(), regenotypedVC);; final VariantContext withAnnotations = annotationEngine.finalizeAnnotations(withGenotypingAnnotations, originalVC);; final int[] relevantIndices = regenotypedVC.getAlleles().stream().mapToInt(a -> originalVC.getAlleles().indexOf(a)).toArray();; final VariantContext trimmed = GATKVariantContextUtils.reverseTrimAlleles(withAnnotations);; final GenotypesContext updatedGTs = subsetAlleleSpecificFormatFields(outputHeader, trimmed.getGenotypes(), relevantIndices);; result = new VariantContextBuilder(trimmed).genotypes(updatedGTs).make();; } else if (includeNonVariants) {; result = originalVC;; } else {; return null;; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6109:1414,update,updatedGTs,1414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6109,2,['update'],['updatedGTs']
Deployability,"@davidbenjamin We might be able to share some code with contamination calculation, etc. Tangentially related, we also should unify the pileup-based tools at some point. Low priority, we can discuss after release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915:204,release,release,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915,1,['release'],['release']
Deployability,"@davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432). Before integrating this with probabilistic segmentation, which is more complicated for somatic than for germline, we can simply replace the current tangent normalization step with the mode of the likelihood (as a function of copy ratio) resulting from the generative coverage model. This requires issues https://github.com/broadinstitute/gatk-protected/issues/429 and https://github.com/broadinstitute/gatk-protected/issues/430 to be completed. ---. @davidbenjamin commented on [Thu Mar 31 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-204015562). Also, this should only be attempted if we get good results from doing the equivalent in the germline code, issue https://github.com/broadinstitute/gatk-protected/issues/431. After it is done somatic and germline will share a PoN and all associated code, including tangent normalization. ---. @LeeTL1220 commented on [Mon Jun 06 2016](https://github.com/broadinstitute/gatk-protected/issues/432#issuecomment-223992660). This only needs to be done if we stick with CBS for segmentation. . At the very least, we need to implement this for checking performance against HMM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2842:115,integrat,integrating,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2842,1,['integrat'],['integrating']
Deployability,"@davidbenjamin commented on [Wed Feb 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903). Since most of the work is in setting up the necessary tools and pipelines to evaluate, I will lump the actual act of evaluating on specific data into this single ticket. We need to:. * CRSP specificity: apply Takuto's `mutect2-replicate-validation.wdl` on the CRSP NA12878 replicates.; * CRSP sensitivity: apply the (currently in-progress) hapmap sensitivity pipeline to CRSP data.; * cfDNA: run cfDNA samples and matched solid tumor samples (which we already have from Viktor) and run the concordance tool.; * FFPE: run FFPE and matched non-FFPE samples and run the concordance tool.; * tumor-only: run some TCGA samples with and without their matched normal and run the concordance tool. ---. @davidbenjamin commented on [Wed Mar 15 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-286795503). Update: CRSP sensitivity and specificity have been run several times, cfDNA is currently running for the first time. FFPE and tumor-only will use the same wdl as cfDNA, so we'll run those once cfDNA finishes. ---. @davidbenjamin commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gatk-protected/issues/903#issuecomment-287675783). Update: everything done except FFPE and tumor-only. FFPE will use the same wdl as cfDNA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2942:172,pipeline,pipelines,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2942,4,"['Update', 'pipeline']","['Update', 'pipeline', 'pipelines']"
Deployability,"@dpmccabe commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/1008). (Very low-priority enhancement request). Allow GetBayesianHetCoverage's matched tumor-normal mode to run on multiple tumor samples matched to a single normal. The normal coverage pulldown and likelihood calculations really only need to be calculated and written to a file once. Alternatively, allow the user to specify a `normalHets` file instead of a BAM if one has already been generated. Thanks!. ---. @samuelklee commented on [Thu Apr 27 2017](https://github.com/broadinstitute/gatk-protected/issues/1008#issuecomment-297704915). We're slowly rebuilding the entire somatic pipeline. One change on the allelic side will be to simply collect allelic counts at all specified sites, rather than performing genotyping on all sites in matched normals and then collecting the corresponding tumor counts at het sites. . The CLI tool to do this (CollectAllelicCounts) is already merged, if you'd like to start using it. You'd only have to run this once on each BAM. The ultimate idea is that resulting allelic count files, along with the corresponding coverage files, could then be passed to a SomaticCNVCaller tool, along with the necessary annotations denoting whether they are tumor or normal. For now, you could probably insert a simple script that performs the genotyping step if you still want to use the rest of the old pipeline but avoid pulling down the normal multiple times.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2977:681,pipeline,pipeline,681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2977,2,['pipeline'],['pipeline']
Deployability,@droazen I plugged this into the `HaplotypeCaller` integration tests and it looks really good. The concordance tests fail due to a handful of false positives but these are _all_ obvious true positive multiallelics that the old qual model behaves stupidly on. I'm not sure who the right reviewer for this is but I'm guessing it's @ldgauthier and/or @vruano.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2098:51,integrat,integration,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098,1,['integrat'],['integration']
Deployability,"@droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473). This capability would be useful if it turns out that the CNV tools (for example) need to be released much more frequently than the GATK as a whole. We don't want a release of the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:194,release,released,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,4,['release'],"['release', 'released']"
Deployability,"@fleharty This fixes #6744, deferring a more principled solution for later. Can we get it in for Friday's release?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6871:106,release,release,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6871,1,['release'],['release']
Deployability,"@fleharty this is a rebased version of the 17cadfa399643877c70ba830d0b4abf9e5b159a9 branch used to generate the Pf7 CNV call set. There are two minor changes: a) one to remove spurious negative dCR estimates reported by gCNV, which were negatively affecting genotyping of HRP2/3 deletions, and b) updating sklearn to the version used for clustering, so that we can reproduce everything exactly using just the GATK Docker. The latter change probably isn't absolutely necessary, but it doesn't seem to break anything so I'm going to go ahead with it. We might want to update to an even more recent version later on (especially if we make any breaking/non-refactoring improvements to the malaria genotyping code after the initial PR), but unfortunately this slightly changes the clustering assignment for a few samples. @mwalker174 @asmirnov239 we discussed the first change some time ago, but just a heads up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7261:566,update,update,566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7261,1,['update'],['update']
Deployability,"@jamesemery This is related to #6617. We've been using GATK4 DepthOfCoverage, and noticed that since it inherits from LocusWalkerByInterval, -L is now required. To this point:. 1) the usage examples still say -L is optional, at minimum this should be updated. 2) It would be nice if it was not required. Perhaps if omitted, all intervals (inferred from genome) would be used?. 3) Alternately, perhaps there could be a shortcut way to pass ""all intervals in the genome"" to GATK in the -L argument? While one can convert a .dict file to intervals manually, it would be convenient if this were more seamless. Thanks",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6648:251,update,updated,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6648,1,['update'],['updated']
Deployability,"@jean-philippe-martin noticed that the performance of his optimized version of spark BQSR took a nosedive during one of the rebases of his branch. Since he's on leave, one of us will have to profile it in order to find out what the bottleneck is and submit a patch. This is a prerequisite to being able to do the broadcast vs. manual sharding comparison called for in https://github.com/broadinstitute/gatk/issues/995",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1006:259,patch,patch,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1006,1,['patch'],['patch']
Deployability,"@kdatta @kgururaj It seems like we're losing rsID's in the input gvcf when we load them into genomics db. Is this deliberate to save space? Is it a bug? Is it a configuration option that isn't exposed by `GenomicsDBImport`? . I don't think it's important for production because they pass in a dbSNP at genotyping time so that can be recomputed, but it's causing issues in some of my tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2636:161,configurat,configuration,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2636,1,['configurat'],['configuration']
Deployability,"@lbergelson @jamesemery The newest version of Barclay (not yet released) checks for and rejects dangling mutex argument references. `MarkDuplicatesSparkArgumentCollection` has a few args that are defined as mutually exclusive with other arguments that are defined directly in `MarkDuplicatesSpark`, and outside of the arg collection, but these fail in the other contexts where `MarkDuplicatesSparkArgumentCollection` collection is used (`ReadsPipelineSpark`, `BwaAndMarkDuplicatesPipelineSpark`, etc). This PR moves the referenced args into `MarkDuplicatesSparkArgumentCollection`, which resolves the compile time and parse time issues, but these args aren't actually honored in the other contexts, so this may not be the right fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5538:63,release,released,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5538,1,['release'],['released']
Deployability,@lbergelson commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gatk-protected/issues/763). Update to the current version of public. This requires an update to spark 2.0 which means it needs some extra testing before merging. . @LeeTL1220 would like to run some wdl's to check that things are still working.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2911:105,Update,Update,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2911,2,"['Update', 'update']","['Update', 'update']"
Deployability,@lbergelson commented on [Tue Jun 21 2016](https://github.com/broadinstitute/gatk-protected/issues/580). Update genotype gvcfs so includeNonVariantSites can be enabled. This may be easiest by implementing a different walker type to match the old LocusWalker behavoir but could also be done by accumulating sites as it goes. ---. @lbergelson commented on [Fri Apr 07 2017](https://github.com/broadinstitute/gatk-protected/issues/580#issuecomment-292661735). this is blocked by broadinstitute/gatk#2429,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2865:105,Update,Update,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2865,1,['Update'],['Update']
Deployability,"@ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508). ## Bug Report. ### Affected tool(s); HaplotypeCaller. ### Affected version(s); - [x] Latest public release version -- at least back to 3.1; - [x] Latest development/master branch as of Nov. 10, 2016; ### Description ; Addition of a rare SNP to a common deletion changes the representation of the deletion.; ![image](https://cloud.githubusercontent.com/assets/6578548/20180442/825d4a14-a728-11e6-9caa-bfad9e20378d.png); Top bam is the BWA-aligned bam for the sample with deletion and SNP, middle bam is the BWA-aligned bam for the sample with just the deletion, bottom bam is the bamout from calling the two together (artificial haplotypes in the top readgroup). The haplotype with the 9bp deletion stays the same, but the haplotype with the SNP turns into two separate deletions -- one of 1bp and one of 8bp. #### Steps to reproduce; Data where this was reported is sensitive, but I added a unit test that reproduces the problem using the haplotype sequences from the data (SWPairwiseAlignmentUnitTest::testLongHapDeletionNearSNP in the branch ldg_SWparamExamples). #### Expected behavior; CIGARs for the deletion haplotype sequences with and without the SNP should be the same. #### Actual behavior; CIGARs for the deletion haplotype sequences with and without the SNP have different deletions. Specifically, in this case, the SNP causes the deletion to be represented as two separate deletions seemingly to avoid the mismatch penalty. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-259727960). I did find that using SWParameterSet.STANDARD_NGS as the parameters instead of CigarUtils.NEW_SW_PARAMETERS will resolve this particular case, but a more comprehensive analysis would be required before we make the change since this will impact a lot of other events too. ---. @ldgauthier commented on [Thu Nov 10 2016](https://g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2498:203,release,release,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498,1,['release'],['release']
Deployability,"@ldgauthier this finishes what we started in #4858 and is necessary for the pileup-calls-on-bamouts MC3 validation. The cause is the same, in that Pair-HMM has a tiny bias in favor of shorter haplotypes and thus it prefers deletion haplotypes when reads end inside STRs. In #4858 we broke near-ties in favor of the reference; this PR fixes the case where two alt haplotypes share a SNV and one of them has a spurious deletion. One important sanity check was that when I set `cigarTerm` to zero in `AssemblyBasedCallerUtils.java` no tests broke. This means that the refactoring needed to set up the change didn't affect behavior. I looked at most of the sites where `PL`s and/or `DP`s changed in the integration test vcfs and in every case the difference was from a fake deletion that this PR fixed. I also went through the diff of the bamouts in IGV and found the same thing. Finally, the changes to test vcfs in `GenotypeGVCFsIntegrationTest` and `GenomicsDBImporterIntegrationTest` are a consequence of changes to the `HaplotypeCallerIntegrationTest` vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5359:699,integrat,integration,699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5359,1,['integrat'],['integration']
Deployability,"@mbabadi commented on [Fri Feb 17 2017](https://github.com/broadinstitute/gatk-protected/issues/914). The current coverage tool in the GATK CNV pipeline, i.e. `CalculateTargetCoverage` has important caveats that make the ab-initio modeling of coverage data and subsequent CNV analysis inaccurate. We need to write a principled tool for calculating read depth histograms from targetted sequencing data. Given that a major source of coverage variance in targetted sequencing stems from the variance in bait efficiencies, the most reasonable read-depth calculation scheme is to associate **inserts to baits** rather than **single reads to targets**. No more arbitrary interval padding (which was a hacky way to get away not thinking about inserts). There is a subtle problem, though: inserts often overlap with more than one bait. In such cases, we need to have a model for estimating the probability that the insert is captured by either of the overlapping baits. The modeling can be done in the following semi-empirical fashion (thanks @yfarjoun), which needs to be done only once for each capture technology (Agilent, ICE):; - We locate isolated baits (i.e. those that are separated from one another by a few standard deviations of the average insert size); - We take a number of BAMs and calculate the empirical distribution of inserts around the isolated baits; - We fit a simple parametric distribution to the obtained empirical distributions, parameterized by bait length and insert length; we probably don't need to go all-in here, though the reference context of the bait is also likely to be an important covariate. Once these distributions are known, we can easily calculate the membership share of each bait in ambiguous cases and give each bait the appropriate share. Bonus:; -------. The empirical distribution of inserts around baits also allows us to associate a more reasonable GC content to each bait. Since GC bias is a property of the fragments that are pulled by the baits, a reasona",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2947:144,pipeline,pipeline,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947,1,['pipeline'],['pipeline']
Deployability,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3004:1280,release,release,1280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004,1,['release'],['release']
Deployability,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1058). - [ ] good choice of default parameters; - [ ] double-check assertion coverage in `CoverageModelArgumentCollection.validate()`; - [ ] if a model is provided, ARD and number of PCs must be overridden (currently, an exception is thrown if there is a discrepancy between model parameters and arguments). Relevant discussion:; **Mehrtash**: We may be able to get rid of a number of these parameters. Though, generally speaking, I'd rather expose more than less, with good default values and bold advanced disclaimers w/ proper documentation as you suggested. This is the case with sophisticated tools like HaplotypeCaller, StarAligner, etc. Soon enough, we will get strange errors from various users many of which can be resolved by changing a certain advanced parameter. Without exposing them, we will have to create patches for them and/or build custom jars.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2995:917,patch,patches,917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2995,1,['patch'],['patches']
Deployability,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2996:243,release,release,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996,1,['release'],['release']
Deployability,"@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1064). At the moment, the maximum copy number in gCNV is set by the dimension provided transition matrices. One needs to work with copy number states up to 10 ~ 20 in order to capture high copy number states seen in certain regions. It is desirable to limit the number of states, however, to designate a special state that represents _all copy number states at and above CN_max_. This can be achieved by adjusting the emission probability to integrate over all higher copy number states. There are other challenges too (e.g. calculating copy number posteriors from such states).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3001:538,integrat,integrate,538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3001,1,['integrat'],['integrate']
Deployability,"@meganshand, the [1.5.1 release](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.5.1) should contain all the changes we made for azure to support your use case. There is no need to use `TILEDB_NUM_THREADS=1` env anymore as that is the default now.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8470:24,release,release,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8470,2,['release'],"['release', 'releases']"
Deployability,"@mwalker174's idea (my interpretation, might be slightly off):; SV pipeline performs local assembly at active regions where seemingly a structural variant is present. Pathogen integration into (human) host generates similar signal and it makes sense for the SV pipeline to help identify such sites.; A tool for extracting the locally assembled contigs and their alignments (if any) that potentially useful for this purpose is desired.; And since we output VCF for SV, the potential location of integration would be helpful too.; But for this feature we need #3192 dealt with first, which is now being handled by PR #3457 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3458:67,pipeline,pipeline,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3458,4,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"@nalinigans I have some very long-running jobs that I'm trying to track, but my stderr is choked with buffer resize outputs. Would it be possible to turn those off by default or add a flag to turn them off? It makes for huge log files and it's hard to track if the log is still being updated for jobs that run >24hrs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5568:284,update,updated,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5568,1,['update'],['updated']
Deployability,"@ronlevine commented on [Fri Jul 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438). ### Instructions. Follow up to #1432.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. ---; ## Feature request; ### Tool(s) involved. Any tool using `IntervalUtils. intervalFileToList()` ; ### Description. Once this change is made, -1 length intervals will be validated and an exception will be thrown. ---. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927). From what I understand of the referenced thread, the ""incorrect"" interval list may always be around, so we may never be able to just blow up on it. Would it perhaps be more viable to add an option to toggle the level of stringency, ie choose in the command line whether to blow up or skip on these invalid intervals? . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260496001). @yfarjoun will want to opine on this, I think. . ---. @yfarjoun commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260513266). I hope that when we move exomes to hg38 we will correct this silly thing; and a few decades later we will no need this code (hehe). Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2520:296,release,released,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520,1,['release'],['released']
Deployability,@ruchim @LeeTL1220 Can you confirm that this works before we merge and cut a release? @droazen . Closes #4319.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4324:77,release,release,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4324,1,['release'],['release']
Deployability,"@samuelklee I'm running into an error for the `cnv_germline_case_scattered_workflow` WDL pipeline to create a Panel of Normals. It seems during the _GermlineCNVCallerCohortMode_ step, the pipeline opens up tens-of-thousands of files that it doesn't close, causing the system to crash. This seems to happen for me both with the Docker image and Standalone GATK4.1.0.0 jar. It reminds me [of this issue mentioned on the forums from GATK3.8](https://gatkforums.broadinstitute.org/gatk/discussion/12791/too-many-open-files) but the error still occurs even if I limit that step to a single thread. I'm running on a Red Had HPC with 16 threads and 200GB of RAM available and using Cromwell v34. After checked with the manager for my cluster it seems the error occurred when over 60K files were opened simultaneously so this looks to me more like a memory leak than a ulimit issue. Here's the output from a typical error file:. ```Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023; 23:36:58.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:36:58.940 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/libgkl_compression7867300459324040837.so; 23:37:00.969 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:37:00.970 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:37:00.970 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:37:00.970 INFO GermlineCNVCaller - Executing as user@e15b680c0241 on Linux v3.10.0-327.36.1.el7.x86_64 amd64; 23:37:00.970 INFO Germ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:89,pipeline,pipeline,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,2,['pipeline'],['pipeline']
Deployability,"@samuelklee commented on [Mon Feb 01 2016](https://github.com/broadinstitute/gatk-protected/issues/344). Segment class that was reintroduced in the germline code requires reference to collection of Targets in constructor and also stores a call, segment mean, and number of targets. ModeledSegment (used in CNV) now extends this and adds methods to transform CR/log2CR, and in turn ACNVModeledSegment (used in ACNV) extends ModeledSegment in https://github.com/broadinstitute/gatk-protected/pull/329. However, this is awkward because ACNVModeledSegment does not store a call, segment mean, or number of targets. I think we decided in https://github.com/broadinstitute/gatk-protected/issues/57, https://github.com/broadinstitute/gatk-protected/issues/61, https://github.com/broadinstitute/gatk-protected/issues/70, https://github.com/broadinstitute/gatk-protected/issues/71, etc. that Segments should simply query the relevant collection of Targets, especially for things like number of targets (which, correct me if I'm wrong, is only needed upon output to file), and that we should use SimpleInterval to represent a segment whenever possible. This obviates the need to update internally held fields when merging segments, etc. @LeeTL1220 @vruano @davidbenjamin we should probably get together and decide how these classes should be structured before moving them over into public. I expect that some of this will also resolve once CNV's output is more along the lines of ACNV's (i.e., when it outputs posterior summaries).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2836:1169,update,update,1169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2836,1,['update'],['update']
Deployability,"@samuelklee commented on [Thu Sep 01 2016](https://github.com/broadinstitute/gatk-protected/issues/662). See comments in #660. Runtime is now about 30-40 minutes for an exome with default parameters, which is a little high. I think we could explore implementing a version of the AF model where the reference bias is not integrated out. This might require more iterations to converge, but on the other hand conditional likelihoods might be cheaper to calculate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2884:320,integrat,integrated,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2884,1,['integrat'],['integrated']
Deployability,"@sooheelee Ready for your review (after the break, of course). Documentation for the CLIs should be considered a rough first cut; I tried to highlight all of the possible workflows in a non-technical manner, while linking to the technical details in the Javadoc for helper classes for those who are interested. Feel free to make changes directly to the branch. @LeeTL1220 This includes that change to kernel-variance-allele-fraction that we discussed. I need to make some non-trivial code changes to PreprocessIntervals (see #3981), so I'll make some further updates to the docs for that tool in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4010:559,update,updates,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4010,1,['update'],['updates']
Deployability,"@takutosato In addition to the bug fix, an integration test I should have written long ago.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6137:43,integrat,integration,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6137,1,['integrat'],['integration']
Deployability,"@takutosato Most of the files changed are just due to a change in method signature. Any significant block of ""new"" code is just your code moved to a different class. This PR does a few things:. * Move logic from the orientation bias annotation into the filter.; * Package F1R2 counts and learned orientation bias models in .tar.gz files to simplify command lines and accomodate multiple samples.; * Make all orientation bias tools fully multi-sample.; *Extract a backend for CollectF1R2Counts and use this backend inside Mutect2. I have tested the new pipeline on Firecloud. Do you have time to review this before the release tomorrow? If not, I can ask Lee.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5840:552,pipeline,pipeline,552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"@takutosato Most of this PR is refactoring to make filtering work for multiple samples while leaving single-pair output unchanged. For example, moving FORMAT annotations to the INFO field, keeping track of the sample of orientation bias priors etc. I'm leaving the wdl unchanged for now. It will still work with the new release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5560:320,release,release,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5560,1,['release'],['release']
Deployability,"@takutosato See the new comment in the M2 task command for explanation. This occurred in the Broad blood biopsy pipeline for scatter count of 75, so it's not far-fetched by any means.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6216:112,pipeline,pipeline,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6216,1,['pipeline'],['pipeline']
Deployability,"@takutosato This dramatically improves `CalculateContamination` by giving more care to distinguishing hom alts from hets. It makes an especially big difference in our tumor-only HCC1143 validations, where the accuracy is now very good (and BTW, ContEst gets these all completely wrong even *with* a matched normal). It also makes the tool work better in targeted panels where there might not be enough hom alt sites by adding a backup hom ref mode that gets triggered automatically. This is based on a Broadie request. I will file a separate issue to update the docs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5413:551,update,update,551,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5413,1,['update'],['update']
Deployability,@thebkaufman1995 encountered the following warning when trying to use TSV count files in the gCNV pipeline:. ```; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fsuper.c line 277 in H5F_super_read(): file signature not found; major: File accessibilty; minor: Not an HDF5 file; ```. My guess is this is because we first try to open counts files as HDF5 and then fall back to TSV (catching the relevant exception). Perhaps slightly different versions of the HDF5 library result in these warnings being emitted.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4482:98,pipeline,pipeline,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4482,1,['pipeline'],['pipeline']
Deployability,"@tomwhite and @lbergelson ; I've been testing the 4.0.0.0 release with the bgzip output fix for HaplotypeCallerSpark (#3725). Thanks again for the work on that bug. I'm now hitting a similar issue with empty output VCFs. It will correctly output bgzipped VCFs when there are variants in the region, but if we call in a region with no variants we end up with plain text output. Here is a self contained test case that demonstrates the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk/gatk4_haplotypecallerspark_plain.tar.gz. The first example command line produces plain text because it includes no variants, while the second is correct:; ```; gatk-launch --java-options '-Xms1g -Xmx2g' HaplotypeCallerSpark --reference hg19.2bit -I Test1-sort-recal.bam -L chrM:1-70 --spark-master local[1] --output plain_text.vcf.gz; gatk-launch --java-options '-Xms1g -Xmx2g' HaplotypeCallerSpark --reference hg19.2bit -I Test1-sort-recal.bam -L chrM:1-75 --spark-master local[1] --output correct_gzip.vcf.gz; ```; Thank you again for all the help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4197:58,release,release,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4197,1,['release'],['release']
Deployability,"@tomwhite can you review? We have CHD 5.7 running now and 1.6 is available on the cloud so no reason to not upgrade AFAIK. For some reason, the lists returned from `.collect` are no longer mutable so i have to make copies in 1 test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1834:108,upgrade,upgrade,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1834,1,['upgrade'],['upgrade']
Deployability,"@vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/951). @lfrancioli commented on [Thu Oct 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489). ## Bug Report; ### Affected tool(s). HaplotypeCaller; ### Affected version(s). Cloud production pipeline; ### Description. This apparently happen when a longer allele was at the position but is no emitted. This issue can be e.g. found in the first VCF of gnomAD (gnomAD_20k.filtered.0.vcf.gz): . ```; 1 10153 . AC CC,*,GC 42563.06 PASS ; ```. As can be seen the last 'C' is superfluous.; #### Steps to reproduce. This was in a production VCF in the cloud ; #### Expected behavior. Minimal representation, e.g. ```; 1 10153 . A C,*,G 42563.06 PASS ; ```; #### Actual behavior. Extra 'C'. ---. @ldgauthier commented on [Thu Oct 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252063664). I think this happened when there was a AC ->C deletion originally called that got subset out because it didn't meet the QUAL threshold, but the trimming that was done on the final allele set didn't work right because of the *. ---. @vdauwera commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252204232). Do we have GVCF snippets to reproduce this? . ---. @ldgauthier commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496). I don't know what intermediates we save on the cloud but maybe @yfarjoun is willing to help. ---. @yfarjoun commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252258477). I don't have special privileges on the cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2959:303,pipeline,pipeline,303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959,1,['pipeline'],['pipeline']
Deployability,"@vdauwera reports that the following `ApplyBQSRSpark` command fails with an NIO-related error on dataproc:. ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $APIKEY_ORTMP \; -- \; --sparkRunner GCS \; --cluster gvda-test-bqsr \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 20g; ```. ```; java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; at java.nio.file.Paths.get(Paths.java:147); at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:30); at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:231); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287:619,install,installed,619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287,1,['install'],['installed']
Deployability,"@vruano ; Since there's some a major change of implementation following your suggestions (single class instead of abstract-base-and-sole-inheritor, remove over-classing), I'm issuing this PR to replace #5117, so the comments you made there are easier to be kept track of. Basically, ; * the first commit is trivial; * the second commit is to address some comments you have about various utils classes; * the third commit is what's contained in #5117 ; * the fourth commit is the re-implementation, which replaces the two old classes with a new class so it's easier to read; * the fifth commit is a simple integration test for this new tool. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5164:605,integrat,integration,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5164,1,['integrat'],['integration']
Deployability,@vruano commented on [Wed Jun 17 2015](https://github.com/broadinstitute/gatk-protected/issues/39). There some issues in the documentation text in package-info.java that was not updated properly after a last minute refactoring. This task is neither nor not urgent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2823:178,update,updated,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2823,1,['update'],['updated']
Deployability,"@vruano commented on [Wed Oct 14 2015](https://github.com/broadinstitute/gatk-protected/issues/149). Plots required to choose some of the parameters use along the pipeline:; To have an idea how they look like and how they would be used you can refer to XHMM tutorial:; https://atgu.mgh.harvard.edu/xhmm/tutorial.shtml. These can be totally in R and you may choose to reuse XHMM original code make reference to the appropriate license; they are quite simple so probably it is not necessary:; - min and max average sample coverage (to filter extreme samples).; - Plot a histogram of the average sample target coverage to choose this cut-offs. ; - min and max std dev. coverage across targets per sample (to filter extreme targets).; - Plot another histogram but in this case of the std .dev target coverage.; - min and max average and std. dev target coverage (to filter extreme targets); - Basically the ""transpose of the two plots above so that we can filter extreme targets:; - Histogram of the mean coverage per target across samples; - Histogram of the std. dev coverage per target across samples.; - Principal components variance explained plot.; - Y is the variance explained by the component (~ eigen value).; - X is the component index where 0 is the first component and i is the ith component.; Consequently this graph is monotonic decreasing.; - Would be nice to get the component vs covariate plot to find out whether we are getting rid ; of known biases like GC content but this one may take a bit more time an might not be necessary for now in practice. . The first few plots could be done by a script that takes in a read counts file.; The principal components one may access the .pon file directly perhaps using a cran package to read hdf5 files. Otherwise you might need to write a simple tool to extract those variances from the .pon. ---. @samuelklee commented on [Wed Aug 17 2016](https://github.com/broadinstitute/gatk-protected/issues/149#issuecomment-240525897). The new germline ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2826:163,pipeline,pipeline,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2826,1,['pipeline'],['pipeline']
Deployability,A PR updating beta documentation to reflect the changes in user experience with bulk ingestion. Do not merge until workspace is ready to be updated to reflect this documentation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8397:140,update,updated,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8397,1,['update'],['updated']
Deployability,"A collection of changes in non-GVS packages required to build a working version of GVS against master:. 1. Support for an optional monitoring script for VQSR Lite `JointVcfFiltering.wdl`.; 2. `VQS_SENS_FAILURE_PREFIX ` VCF header value updated for correctness.; 3. Moved all BigQuery classes under a `gvs` package to make clear these are currently considered to be GVS specific.; 4. Added method to BigQueryUtils.; 5. ~ExcessHet calculation fixes for the case of no PLs.~ Removed, no longer required with Annotation changes in `ExtractTool`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8362:236,update,updated,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8362,1,['update'],['updated']
Deployability,"A consolidated PR containing many fixes for issues and obviating their associated PRs. Created to do a single review of the main Funcotator branch with updates, rather than several smaller ones. This is because due to timing constraints, the branches all were based off of each-other (as opposed to waiting for the complete review process, merging to master, then branching from master). The issues fixed are as follows:. Fixes issues #3753, #3784, #3780, #3781, #3782, #3859, #3783, #3757, #3906, #3898 ; Obviates PRs: #3843, #3846, #3858, #3863, #3879, #3897, #3908, #3943",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3952:152,update,updates,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3952,1,['update'],['updates']
Deployability,"A few M2 WDL 1.0 updates, like structs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6144:17,update,updates,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6144,1,['update'],['updates']
Deployability,"A few interrelated issues:. -The install_R_packages.R script is copied and installed in the base Docker image. However, it is currently also copied (but not installed) in the non-base Docker image for some reason. @jamesemery may be able to comment (#4251).; -Different R packages are installed in that script in different ways. Some are pegged to older versions sourced from http://cran.r-project.org/src/contrib/Archive URLs; this is to prevent the http://cran.r-project.org/src/contrib URLs for the most recent versions from breaking out under us, which has happened frequently in the past. Other packages are simply installed using `dependencies = ...`; -We should perhaps consider moving the R dependencies into the conda environment, see discussion in #4209.; -R dependencies are cached in a `site-library` folder in the Travis build to avoid intermittent connection issues with the aforementioned URLs. This can cause tests to break after the fact if the cache is not cleared every time a dependency is removed. If we decide to cache pip installs similarly, we will also run into this issue.; -Requiring the base Docker image to be updated every time an R dependency is changed is also fragile. If it is accidentally not updated when dependencies are removed, tests can continue to pass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4250:75,install,installed,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4250,7,"['install', 'update']","['installed', 'installs', 'updated']"
Deployability,"A few things worth mentioning. 1) This skeleton is mostly a direct port of the existing Dataflow pipeline.; 2) I had to modify some test data because there is a (masked) bug in the Dataflow code, see #795.; 3) Serialization was a slight pain and I had to bump the kryo version to the latest 2.x, as well as add two custom Serializers. If there's a cleaner way to handle any of that I'm all ears.; 4) I'm using Hadoop-bam for reading and writing reads and variants.; 5) There are unit tests for all code except for the skeleton itself. I've run it on a cluster on my machine successfully, but haven't written the tests (which will take a little while to design and get right).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/850:97,pipeline,pipeline,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/850,1,['pipeline'],['pipeline']
Deployability,A full bam pipeline scattered n ways.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/280:11,pipeline,pipeline,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/280,1,['pipeline'],['pipeline']
Deployability,"A gradle plugin may be able to solve the problem of how people building extensions to gatk can create sparkJar's without a lot of confusing custom configuration. People building projects on top of gatk would apply the plugin to their own build script, and it would automatically configure a sparkJar target.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1453:147,configurat,configuration,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1453,1,['configurat'],['configuration']
Deployability,"A quick patch to help out the Variants team, which is struggling with a problematic callset. Note that a similar regularization to the effective number per component probably should have been applied to solve the issue in https://github.com/broadinstitute/gatk/pull/6425. I'm not sure if the lack of this regularization will still lead to convergence issues, but I would hope that the fix that was implemented instead (treating vanishing components as a special case and skipping computation) suffices. As discussed there, we may also want to eventually remove the idiosyncratic finalize step; its likely this is the source of issues here, since the correct Bayesian M step is already regularized by the prior. The covariance regularization term added here is standard (c.f. e.g. https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/mixture/_gaussian_mixture.py#L154), but it may result in non-negligible changes to VQSLODs. As just discussed with the Variants team, we can probably use the WARP validation to convince ourselves that results are functionally equivalent. I updated the exact-match tests without much close examination (by simply forcing IntegrationTestSpec.assertEqualTextFiles to overwrite the old expected files), so someone may want to sanity check them. There were also a few more interactions between the integration tests for different tools than I anticipated. Some tests use output generated by an upstream tool as input and break encapsulation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709:8,patch,patch,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709,4,"['Integrat', 'integrat', 'patch', 'update']","['IntegrationTestSpec', 'integration', 'patch', 'updated']"
Deployability,"A request from @eitanbanks and @yfarjoun :. ""Yossi and I are just looking at our production processing costs and the HaplotypeCaller is the biggest culprit right now. That's because it currently requires these high memory machines. If we could somehow get it to use a max of 3 GB RAM then we'd cut 10% off of the entire pipeline. Even 6GB would be okay, but 3 would be huge.; What do you think -- will it be possible?""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2591:320,pipeline,pipeline,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2591,1,['pipeline'],['pipeline']
Deployability,"A seemingly large change PR, but most changes are trivial.; The non-trivial part:. * a new tool `StructuralVariantionDiscoveryPipelineSpark` to run the whole process of SV discovery, by delegating works to `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSAMSpark`, both of which are refactored to accommodate the new tool;; * class `AlignmentRegion` is effectively moved into a new class `AlignedAssembly` (named quite close to the existing class `AlignedAssemblyOrExcuse` but will be moved into a different sub-package in a sequential PR).; * integration tests (local mode and on MiniClusters/hdfs) for all 5 major tools `FindBreakpointEvidenceSpark`, `DiscoverVariantsFromContigAlignmentsSAMSpark`, `StructuralVariantionDiscoveryPipelineSpark`, `AlignAssembledContigsSpark` and `DiscoverVariantsFromContigAlignmentsSGASpark`; a draw back is these integration tests do not test correctness of results but simple tests if these tools run.; * various unit tests. The two paths involving use of Fermi-lite are tested to be running and generating compatible results. The path involves using SGA as the assembler is also running but generates significantly less variants. (see attached run logs).; [differentVersions.txt](https://github.com/broadinstitute/gatk/files/956271/differentVersions.txt). The access levels of the various classes and methods are not optimal now because a serial PR that simply repackaging these classes (hence access levels must be changed) is expected to be generated immediately after this PR is approved.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2621:567,integrat,integration,567,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621,2,['integrat'],['integration']
Deployability,"A shortcut in the code returns empty values if there are no reads supporting the reference, but if an alt has no reads then the rank sum test from Apache Math returns NaN. Output for invalid rank sum test Z-scores should return null/empty rather than NaN. (Pipe-delimited raw annotations don't have to follow the VCF spec using `.` for missing.) This will involve updating a lot of exact match integration test results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7512:394,integrat,integration,394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7512,1,['integrat'],['integration']
Deployability,"A user gets an issue with the HDF5 library when running DenoiseReadCounts on an arm64 processor. We would like to create a fallback solution for this tool, since in this case the tool is not working with HDF5 files. This request was created from a contribution made by dbpiero on June 01, 2021 10:22 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor](https://gatk.broadinstitute.org/hc/en-us/community/posts/360078197412-Error-running-DenoiseReadCounts-on-arm64-processor). \--. If you are seeing an error, please provide(REQUIRED) : ; ; a) GATK version used: 4.2.0.0. b) Exact command used: gatk DenoiseReadCounts -I sample.counts.tsv --annotated-intervals annotated\_intervals.tsv --standardized-copy-ratios sample.standardizedCR.tsv --denoised-copy-ratios sample.denoisedCR.tsv. c) Entire error log: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. Dear Administrators,. I try to run DenoiseReadCounts on new apple silicon chip (M1) with arm64 architecture, but I got this error: A USER ERROR has occurred: Cannot load the required HDF5 library. HDF5 is currently supported on x86-64 architecture and Linux or OSX systems. I created a docker with ubuntu 20.04 to launch gatk and I have already installed libhdf5-103:arm64 library and hdf5-tools inside but launching DenoiseReadCounts i get the same error. Is there a way to solve this issue?. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/161375'>Zendesk ticket #161375</a>)<br>gz#161375</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7297:1374,install,installed,1374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7297,1,['install'],['installed']
Deployability,"A user has identified an issue with variants near regions of the reference with `N` bases:. https://gatk.broadinstitute.org/hc/en-us/community/posts/360072168572-Funcotator-errors-?page=1#community_comment_360012539271. If Funcotator gets to a codon sequence with `N` bases in it, right now it throws an exception because it cannot decode the `N` bases into a valid amino acid. Funcotator needs to be updated to provide a symbolic protein prediction stating that it was ambiguous because of reference IUPAC bases. The variant in question is from **HG19**:; ```; 4	9274640	.	A	ATCACTG,ATCCTG	.	.	BETA=0.989,0.141;FRACTION=0.022; ```. The reference around this variant is:. ![image](https://user-images.githubusercontent.com/11667487/91493420-4a1e1000-e885-11ea-97f5-820a44a054bc.png). ### Stack Trace:; ```; ***********************************************************************. A USER ERROR has occurred: Unknown file is malformed: File contains a bad codon sequence that has no amino acid equivalent: CNN. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$MalformedFile: Unknown file is malformed: File contains a bad codon sequence that has no amino acid equivalent: CNN; 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createAminoAcidSequenceHelper(FuncotatorUtils.java:1195); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createAminoAcidSequence(FuncotatorUtils.java:1158); 	at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.createProteinSequences(ProteinChangeInfo.java:125); 	at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:52); 	at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:371); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:2045); 	at org.broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6774:401,update,updated,401,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6774,1,['update'],['updated']
Deployability,"A user on the GATK Forum submitted a request to make the INFO field easier to manipulate through creating a table. At the GATK Office Hours meeting 11/8, we discussed the two ideas and favored the first idea to make a new tool, similar to VariantsToTable, that would unpack the INFO field. This request was created from a contribution made by Shahryar Alavi on October 30, 2020 19:54 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360073983291-VariantsToTable-not-extracting-INFO-sub-fields-#community\_comment\_360013343072](https://gatk.broadinstitute.org/hc/en-us/community/posts/360073983291-VariantsToTable-not-extracting-INFO-sub-fields-#community_comment_360013343072). \--. But MAF output is somewhat different from VCF; and I think the VCF output format is better for germline variant annotation. With Funcotator we get an integrated (and minuter) ""variant calling - annotation"" workflow. But the problem is ""vertical bar"" separated INFOs are not easy for downstream text processing. I have two suggestions for the GATK Team:. You may want to develop a new tool (like VariantsToTable) to separate each ""sub-info"" in the FUNCOTATION INFO, and put them into separate columns with corresponding headers when creating the tab-delimited table. Or add a feature to Funcotator to create multiple INFOs with FUNCOTATION prefix in their IDs; e.g. #INFO=<ID=FUNCOTATION\\\_Gencode\\\_34\\\_hugoSymbol,...> ; ; #INFO=<ID=FUNCOTATION\\\_Gencode\\\_34\\\_ncbiBuild,...>. instead of. #INFO=<ID=FUNCOTATION,...,Description=""Funcotation fields are: Gencode\\\_34\\\_hugoSymbol|Gencode\\\_34\\\_ncbiBuild|..."">. Thanks<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/45403'>Zendesk ticket #45403</a>)<br>gz#45403</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7556:854,integrat,integrated,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7556,1,['integrat'],['integrated']
Deployability,"A user rightly [points out](http://gatkforums.broadinstitute.org/gatk/discussion/comment/32631#Comment_32631) that different versions of HaplotypeCaller may produce GVCFs that are not directly compatible, causing weirdness when you joint-genotype them with GenotypeGVCFs. . Obviously this is primarily a data management problem (user should control what's in their pipeline) -- but it would be good to provide an additional safety layer by having GenotypeGVCFs, CombineGVCFs or whatever demon is used to invoke TileDB at least emit a WARN message if they see GVCFs produced by different versions of HC within the same input cohort. . Note that the VCF version number is not directly useable for this purpose since changes in the contents of GVCFs can arise within the same version of VCF spec. Also, one could argue that the GVCFs really should all be produced using exactly the same command line arguments -- but validating the entire command line would probably be overkill...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2129:365,pipeline,pipeline,365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2129,1,['pipeline'],['pipeline']
Deployability,"A very quick update to our AoU docs to mention the ticket for deletion. Note: yes, this branch is incorrectly named because it's VS-1206 and not VS-1207. It wasn't important enough to rename the branch for its short lifetime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8679:13,update,update,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8679,1,['update'],['update']
Deployability,"A while back I added a .dockstore.yml file to the gatk repo so that gatk workflows in the /script/ folder would be automatically synced in Dockstore after every push or release. This also allowed the workflows in every branch/release to be readily available in Terra. However, GATKs 700+ branches has been causing problems for Dockstore syncs and in some instances associated released tags to be missing ([forum discussion](https://discuss.dockstore.org/t/dockstore-could-not-find-a-workflow-in-git-using-yml-though-it-worked-previously/4255/5)). . This PR adds filters to the dockstore.yml so that only the master branch and the releases gets synced to dockstore, also any future branches arent automatically synced. If anyone wants to sync their branch theyll have to add their branch name to the dockstore.yml file in their branch.; More info on dockstore yml and filters can be found [here](https://docs.dockstore.org/en/develop/getting-started/github-apps/github-apps.html)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7217:169,release,release,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7217,4,['release'],"['release', 'released', 'releases']"
Deployability,"ABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12; 12:52:37.522 INFO GenomicsDBImport - Start Date/Time: 28 February 2019 12:52:35 WET; 12:52:37.522 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.522 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.523 INFO GenomicsDBImport - HTSJDK Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5740:1459,install,install,1459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740,1,['install'],['install']
Deployability,"AC:. While in a Notebook, I can run Hail 120 and I can specifically run the new vcf_combiner code; merge_alleles() works in the 120; and we must use array_elements_required=False in import_VCF to get around the missing data issue Dan pointed out; ![image](https://github.com/broadinstitute/gatk/assets/6863459/3da27122-8b6b-4bec-b785-55846e671cff). Hail version 120 has been pinned in the Integration test -- pinned!. Integration test with above pin has succeeded.; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/a1c5e4c4-2058-4dad-8261-87b23c8bb0f3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8502:389,Integrat,Integration,389,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8502,2,['Integrat'],['Integration']
Deployability,ACAF updates [VS-1109] [VS-1366],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8941:5,update,updates,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8941,1,['update'],['updates']
Deployability,AH - implement changes for mitochondrial pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6399:41,pipeline,pipeline,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6399,1,['pipeline'],['pipeline']
Deployability,AH - update mitochondria wdl for new pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6597:5,update,update,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6597,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,AOU deliverables doc updates [VS-1073],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8551:21,update,updates,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8551,1,['update'],['updates']
Deployability,"ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:36:22.398 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:36:22.398 INFO Funcotator - Deflater: IntelDeflater; 16:36:22.398 INFO Funcotator - Inflater: IntelInflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. *****************************************************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:4058,pipeline,pipelines,4058,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,1,['pipeline'],['pipelines']
Deployability,"Accompanying this branch is and will be an official new release of the Funcotator Datasource bundles: ; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg19.20230908s.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908g.tar.gz; gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.8.hg38.20230908s.tar.gz; ; Note that the format of the datasources bundles has changed somewhat, importantly they are now split into separate hg19 and hg38 bundles to cut down on size. In this branch are:; - The necessary changes to the FuncotatorDownloaderScript to accomidate the new bundles; - Changes to the various Funcotator datasource downloader scripts to point to newer releases of bundled sources used in this release; - A fix for the MAF output renderer to handle Gencodev43 datasources. Fixes #8296",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8512:56,release,release,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8512,3,['release'],"['release', 'releases']"
Deployability,"According to #2858, the new [GATK CNV pipeline](https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/somatic) is intended to replace AllelicCNV (because it now segments jointly on total copy ratio and allelic fraction). We've found the segmentation to be great for WGS data, but the new workflow does not create the same outputs as AllelicCNV - in particular, AllelicCNV generated a *-sim-final.acs.seg that could be used for [ABSOLUTE](https://software.broadinstitute.org/cancer/cga/absolute) and [DeTiN](https://github.com/getzlab/deTiN). We'd like to run these tools - Is there any way to get the equivalent of this file from the workflow's outputs? None of the outputs look like *-sim-final.acs.seg. . If not, I had planned to simply run AllelicCNV (or AllelicCapseg) using files from the new workflow. The only issue is that the input files are unclear to me - I've provided a table below with what I believe the matchups relative to the old GATK CNV workflow to be, but it would be great to get clarification!. Name of file | Old GATK CNV (task) | New GATK CNV (task); -- | -- | --; tumorHets | *.tumor.hets.tsv (GetHetCoverage) | *.hets.tsv (ModelSegments); segments | *.seg (PerformSegmentation) | *.modelFinal.seg (ModelSegments); tangentNormalized | *.tn.tsv (NormalizeSomaticReadCounts) | ????? (maybe .denoisedCR.tsv from DenoiseReadCounts?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6685:38,pipeline,pipeline,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6685,1,['pipeline'],['pipeline']
Deployability,"According to @vdauwera, the most-requested GenomicsDB feature from GATK users is the ability to do incremental updates to an existing GenomicsDB.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4773:111,update,updates,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4773,1,['update'],['updates']
Deployability,Adam moved the code for SplitNCigarReads and all supporting utils over.; I've updated the tests so that they all use a small snippet of the reference instead of the entire hg19 (hg19mini.fasta),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/91:78,update,updated,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/91,1,['update'],['updated']
Deployability,Add CRAM integration tests for HaplotypeCaller.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3681:9,integrat,integration,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3681,1,['integrat'],['integration']
Deployability,Add HC/Mutect2 integration tests for --min-base-quality-score,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4136:15,integrat,integration,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4136,1,['integrat'],['integration']
Deployability,Add R group to integration tests that use R.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/752:15,integrat,integration,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/752,2,['integrat'],['integration']
Deployability,Add VCF input and support for allelic counts from indels to ModelSegments pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4903:74,pipeline,pipeline,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4903,1,['pipeline'],['pipeline']
Deployability,Add VCF output to ModelSegments pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4114:32,pipeline,pipeline,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4114,1,['pipeline'],['pipeline']
Deployability,Add VCF output to gCNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4113:23,pipeline,pipeline,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4113,1,['pipeline'],['pipeline']
Deployability,Add VDS Validation to the hail integration split,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8343:31,integrat,integration,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8343,1,['integrat'],['integration']
Deployability,Add a command-line argument to toggle using NIO on reading for Spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6008:31,toggle,toggle,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6008,2,['toggle'],['toggle']
Deployability,Add a few more tests to ModelSegments pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3916:38,pipeline,pipeline,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3916,1,['pipeline'],['pipeline']
Deployability,Add a file-based configuration mechanism to GATK (with ability to override),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368:17,configurat,configuration,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368,1,['configurat'],['configuration']
Deployability,Add a read/write roundtrip Spark integration test for a CRAM and reference on HDFS.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6618:33,integrat,integration,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6618,1,['integrat'],['integration']
Deployability,"Add ability to specify the RELEASE arg in build_docker_remote.sh, and add a release_prebuilt_docker_image.sh script",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8247:27,RELEASE,RELEASE,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8247,1,['RELEASE'],['RELEASE']
Deployability,"Add additional integration tests for BaseRecalibratorSpark that use --joinStrategy BROADCAST, now that it's going to be the default",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1140:15,integrat,integration,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1140,1,['integrat'],['integration']
Deployability,"Add additional validation around duplicated rows in the VAT; <img width=""1418"" alt=""duplicate_AN_or_AC_values"" src=""https://user-images.githubusercontent.com/6863459/220667710-a416ab64-4f9b-475b-9268-ef7b86bfa81e.png"">. This has a successful run (except for one failure that is because it's being run on way less data); https://job-manager.dsde-prod.broadinstitute.org/jobs/07ddde58-ac0d-4229-9f96-d093f5c11682; The failed test is:; SpotCheckForAAChangeAndExonNumberConsistency. Perhaps we want to update this to not run this test if there are less than 10k samples?; Yes we do:; Here's the ticket for that:; https://broadworkbench.atlassian.net/browse/VS-878",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8175:498,update,update,498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8175,1,['update'],['update']
Deployability,Add an initial stub end-to-end integration test for the ReadsPreProcessingPipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/772:31,integrat,integration,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/772,1,['integrat'],['integration']
Deployability,Add better error message when Java is not installed to the launch script,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5992:42,install,installed,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5992,1,['install'],['installed']
Deployability,Add better integration tests to HaplotypeCaller for less-common arguments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7632:11,integrat,integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632,1,['integrat'],['integration']
Deployability,Add bq util classes and update dependencies; add local sort classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6759:24,update,update,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6759,1,['update'],['update']
Deployability,Add default tool header information to the output of PostprocessGermlineCNVCalls and update the integration test accordingly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4267:85,update,update,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4267,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"Add dependency on jbwa snapshot, script to release future jbwa snapshots, and method to load the library at runtime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1847:43,release,release,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847,1,['release'],['release']
Deployability,Add good integration test for GCS -L and -R support,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4102:9,integrat,integration,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4102,1,['integrat'],['integration']
Deployability,Add good integration tests for BwaSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2523:9,integrat,integration,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2523,1,['integrat'],['integration']
Deployability,Add hap.py/som.py benchmarking results to docs for each release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/9019:56,release,release,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9019,1,['release'],['release']
Deployability,Add integration test for BGE samples [VS-1225],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8920:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8920,1,['integrat'],['integration']
Deployability,Add integration test that checks filtering for CreatePanelOfNormals for rounding.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3181:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181,1,['integrat'],['integration']
Deployability,Add integration test using -maxNumPLValues for GenotypeGVCFs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1895:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1895,1,['integrat'],['integration']
Deployability,Add integration tests for GCS support in VariantWalkers,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2396:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2396,2,['integrat'],['integration']
Deployability,Add integration tests for correctness for DetermineGermlineContigPloidy and GermlineCNVCaller.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4375:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4375,1,['integrat'],['integration']
Deployability,Add integration tests in Funcotator for out of bounds variants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7523:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523,1,['integrat'],['integration']
Deployability,Add integration tests showing that bai-indexed traversal by intervals on cram files works,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/860:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/860,1,['integrat'],['integration']
Deployability,Add link to GATK4 release instructions to README,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3399:18,release,release,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3399,1,['release'],['release']
Deployability,"Add more test cases for the errors seen in issue #6289 . Specifically we should add both more unit tests and integration tests. One variant that should be added in integration tests is the following:; ```; ##fileformat=VCFv4.1; ##contig=<ID=chr9,length=138394717>; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; chr9	67726241	.	TCA	TCACACA,TCACACACA	182	PASS	.	GT	./.; ```. The issue with adding more tests is that we don't have the full Funcotator datasources in our `git-lfs` repo because of size concerns. To add tests we'll need to add more intervals to our datasources to support the variants' loci, or create variants in the regions we already cover.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7523:109,integrat,integration,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7523,2,['integrat'],['integration']
Deployability,Add new configuration entry for plugins,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4611:8,configurat,configuration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611,1,['configurat'],['configuration']
Deployability,"Add new scripts to gatk/scripts/sv/ folder, and alter action (but not; passed parameters) of older scripts to make running sv spark jobs; more convenient.; Added:; -copy_sv_results.sh: copy files to time and git-stamped folder on; google cloud storage; -> results folder on cluster; -> command line arguments to SV discover pipeline; -> console log file (if present). -manage_sv_pipeline.sh: create cluster, run job, copy results, and; delete cluster. Manage cluster naming, time and git-stamping,; and log file production. Altered:; -create_cluster.sh: control GCS zone and numbers of workers via; environmental variables. Defaults to previous hard-coded values. -runWholePipeline.sh: accept command-line arguments for sv; discovery pipeline, work with clusters having NUM_WORKERS != 10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3370:324,pipeline,pipeline,324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370,2,['pipeline'],['pipeline']
Deployability,Add script to extract sample & subpop data from pipeline TSV [VS-150],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7387:48,pipeline,pipeline,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7387,1,['pipeline'],['pipeline']
Deployability,Add table size check to quickstart integration test [VS-501],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7970:35,integrat,integration,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7970,1,['integrat'],['integration']
Deployability,Add tests to CPX variants in SV pipeline and fixing bugs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4330:32,pipeline,pipeline,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4330,1,['pipeline'],['pipeline']
Deployability,Add the Dockstore integration app to the repo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6887:18,integrat,integration,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6887,1,['integrat'],['integration']
Deployability,Add toggle for treatment of MNPs in VETS.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8733:4,toggle,toggle,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8733,1,['toggle'],['toggle']
Deployability,Add utilites to update Echo filters and reference genotypes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8867:16,update,update,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8867,1,['update'],['update']
Deployability,"Added .dockstore.yml file to the root directory of the gatk repo to allow automatic syncing to occur with workflows in [Dockstore](https://dockstore.org/organizations/BroadInstitute/collections/GATKWorkflows). ; **Problem:** ; The GATK workflows are currently organized in [Dockstore](https://dockstore.org/organizations/BroadInstitute/collections/GATKWorkflows), maintenance of the workflows requires manually refreshing the workflow profile in Dockstore in order to view the latest releases of the workflows. Also some workflows like germline_cnv fails to sync because Dockstore has trouble handling the number of branches/tags in the gatk repo.; **Solution:** ; Adding the dockstore yml file allows this syncing to happen automatically whenever there is a push to the gatk repo. This may also help focus which branches to sync and prevent Dockstore from failing during sync. . See [doc](https://docs.dockstore.org/en/develop/getting-started/github-apps/github-apps.html) for description.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6770:484,release,releases,484,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6770,1,['release'],['releases']
Deployability,Added CLIs and WDL for python gCNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925:35,pipeline,pipeline,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925,1,['pipeline'],['pipeline']
Deployability,Added FilterIntervals to perform annotation-based and count-based filtering in the gCNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5307:88,pipeline,pipeline,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5307,1,['pipeline'],['pipeline']
Deployability,Added Gencode's GeneTranscriptType as an annotation field in GencodeFunctotation. Updated unit tests after adding new annotation field. resolves #4408,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7343:82,Update,Updated,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7343,1,['Update'],['Updated']
Deployability,Added GitHub Actions workflow file for CARROT integration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6917:46,integrat,integration,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6917,1,['integrat'],['integration']
Deployability,Added PDF of release note about VETS in the context of GVS. Updated documentation to reference VETS and the relevant GATK tools instead of VQSR. Updated png of gvs diagram to be more generic and not mention VQSR.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8466:13,release,release,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8466,3,"['Update', 'release']","['Updated', 'release']"
Deployability,Added SAM headers to ModelSegments CNV pipeline output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3914:39,pipeline,pipeline,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3914,1,['pipeline'],['pipeline']
Deployability,Added VCF generation tool for the gCNV pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4254:39,pipeline,pipeline,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4254,1,['pipeline'],['pipeline']
Deployability,Added WDLs to subset the INFO field annotations to only the allele frequency fields. Also modified the `mutect2` wdl to produce a gnomAD with only AF fields. @davidbenjamin - you may want to double-check my updates to the M2 support wdl.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5604:207,update,updates,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5604,1,['update'],['updates']
Deployability,"Added XGBoostEvidenceFilter, an alternative BreakpointEvidence filter; based on XGBoost classifier.; - Default is still BreakpointDensityFilter. Switch by passing; ""--sv-evidence-filter-type XGBOOST"" instead of ""DENSITY"".; - Decisions based on evidence overlap or coherence are now scaled based; on coverage depth (in both filter types).; - Multiple avenues for supplying saved classifier binary file,; including built-in resource, local file, and google cloud storage.; - BreakpointEvidence updated to carry information necessary for; classifier. Unit tests were correspondingly updated.; - Data from genome tracts used for some classifier features. From the; hg38 genome: gaps, centromeres, and umap s100. Additional changes to convenience scripts; - Updated sanity_checks.sh to return error signal when exiting; - Bugfixes to manage_sv_pipeline.sh for linux compatibility; - Update run_whole_pipeline.sh to detect preemptible workers and; thus set NUM_EXECUTORS correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769:492,update,updated,492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769,4,"['Update', 'update']","['Update', 'Updated', 'updated']"
Deployability,Added a script to publish GATK tool WDLs for each release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6980:50,release,release,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6980,1,['release'],['release']
Deployability,"Added a test to verify that https://github.com/broadinstitute/gatk/issues/3154 is fixed now that we've upgraded htsjdk (though we should keep that ticket open until @sooheelee can verify her particular incarnation of this issue). Also note that while the CRAM MD5 slice calculation is fixed, GATK users can still have problem reading CRAMs made from references containing ambiguity codes if the .dict accompanying the reference was generated with samtools. This is tracked by https://github.com/broadinstitute/gatk/issues/3306, but is really a samtools issue. The simple workaround is to recreate the .dict using CreateSequenceDictionary, which is what I've done to create the test in this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3430:103,upgrade,upgraded,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3430,1,['upgrade'],['upgraded']
Deployability,Added ability for user to override to annotate again. Wrote unit and integration tests for new feature and override ability. resolves #5679,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7349:69,integrat,integration,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7349,1,['integrat'],['integration']
Deployability,"Added ability to call gatk tools to collect variant calling metrics.; It is optional, and defaulted to not collect. Passing Extract run with metrics collected [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/97411a89-bd75-48dd-8dba-00d6af5373e7).; Passing Extract run where we don't (default) collect metrics [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/dfe6a72f-f2e0-4999-a7c9-f8243e0f1586).; Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/b11f03a9-16ef-4b1e-9b1c-c281f41b43a1)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8968:492,Integrat,Integration,492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8968,1,['Integrat'],['Integration']
Deployability,Added abstract classes for unifying table formats for ModelSegments CNV workflow and updated AllelicCountCollection accordingly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3716:85,update,updated,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3716,1,['update'],['updated']
Deployability,Added additional workflow and README updates for Quickstart [VS-183],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7463:37,update,updates,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7463,1,['update'],['updates']
Deployability,Added an ExampleAssemblyRegionWalker to provide a full integration test in public for assembly region traversal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2179:55,integrat,integration,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2179,1,['integrat'],['integration']
Deployability,Added and updated documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4003:10,update,updated,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4003,1,['update'],['updated']
Deployability,Added check for non-finite copy ratios in ModelSegments pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4292:56,pipeline,pipeline,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4292,1,['pipeline'],['pipeline']
Deployability,Added code and WDL to complete ModelSegments CNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:49,pipeline,pipeline,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['pipeline'],['pipeline']
Deployability,"Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an issue with the new VariantClassification code. Fixed issue #4410. Fixed #4022. Fixed #4420. Fixed #3922",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4472:1541,integrat,integration,1541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472,2,"['Update', 'integrat']","['Updated', 'integration']"
Deployability,Added denoising tools for ModelSegments CNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3820:44,pipeline,pipeline,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3820,1,['pipeline'],['pipeline']
Deployability,"Added documentation and kebab-case updates for ModelSegments pipeline (excluding PreprocessIntervals), along with some WDL updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4010:35,update,updates,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4010,3,"['pipeline', 'update']","['pipeline', 'updates']"
Deployability,Added gCNV integration test to detect numerical differences in the outputs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7889:11,integrat,integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889,1,['integrat'],['integration']
Deployability,Added google apps script to automatically update GATK release stats.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7637:42,update,update,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7637,2,"['release', 'update']","['release', 'update']"
Deployability,Added in Owner style configuration file with some basic hooks.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3447:21,configurat,configuration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3447,1,['configurat'],['configuration']
Deployability,Added in a script to pull down the latest Gencode data source.; Fixed an issue in 5' UTR processing that would cause variant alleles with length > 1 to throw an exception (issue 4712).; Added three test cases to prevent regression of issue 4712.; Updated Gencode codec to be compatible with latest Gencode release (v28).; Fixed a bug in the version detection for Funcotator data sources that would prevent newer data source versions from being detected as compatible (date comparison error).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4770:247,Update,Updated,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4770,2,"['Update', 'release']","['Updated', 'release']"
Deployability,"Added in argument for MAF out.; Added more ""required"" MAF fields.; Added Funcotation::getDataSourceName; Added contig name overrides for hg19 VS B37; Added in code to convert VCF INDEL positions to MAF INDEL positions.; Added start/stop positions for IGRs.; Added argument to ignore filtered variants at the front of processing to save time.; Added a script to fully retrieve the COSMIC data sources. Fixed how the MafOutputRenderer handles mapping fields to values.; Fixed a bug in LocatableXsv and COSMIC parsers (was missing name and; version).; In Gencode: Now TumorSeqAllele1 is the refAllele, not the AltAllele.; Fixed some problems with VCF output.; Updated VCF outputs to have better header info.; Refactored header output for OutputRenderers.; Changed the logic for creating alt protein sequences.; Fixed a bug in the LocatableXsvFuncotationFactory that caused annotations to be incorrectly associated with a factory.; Fixed several bugs in the GencodeFuncotationFactory.; Fixed bugx in the handling of UTR variants.; Fixed the Transcript Selection Mode ordering.; Fixed an issue with splice sites. Minor speed fix to GencodeFuncotationFactory. Now CosmicFuncotationFactory opens the database in read-only mode. Bugfix - now LocatableXsvFuncotationFactories use overrides. Now the reference should properly align with ALL indels regardless of; length. ReferenceContext now always rendered on + strand. Now will create funcotations for transcripts without fasta sequences. Minor changes to FuncotatorIntegrationTest. - Added in more integration test files. These are as yet unused - must; refactor the files themselves to actually reflect what should be correct; as far as produced funcotations. - Updated LocatableXsvFuncotationFactoryUnitTest.java and SimpleKeyXsvFuncotationFactoryUnitTest.java; to reflect the change to funcotation factories to always produce the; expected funcotations (rather than only producing funcotations when; there are data that match the target variant). Fixed an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4472:657,Update,Updated,657,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4472,1,['Update'],['Updated']
Deployability,Added in code to pull config elements from the Owner configuration.; Hooked the configuration into the classes where it is necessary.; Added in a config file with defaults.; Added in utilities and consolidated hooks for configuration code.; Added in help text for configuration file options in gatk-launch. Basic configuration options have been implemented and hooked; into files where appropriate. Configuration values in properties files cannot currently have; trailing spaces - this results in a parsing error. There is a; workaround that has not yet been implemented. Fixes #3126,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3447:53,configurat,configuration,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3447,6,"['Configurat', 'configurat']","['Configuration', 'configuration']"
Deployability,"Added more logging functionality to better inform users. Fixed NullPointerException when onTraversalStart throws.; Now logs version number information for data sources.; Now checks minimum data sources version and requires that the user; upgrade if the version is too old. Fixes #4521 in a roundabout way - data sources need to be re-released for this version to run at all, but these new data sources contain a fix for the HG38 issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4563:238,upgrade,upgrade,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4563,2,"['release', 'upgrade']","['released', 'upgrade']"
Deployability,Added numerical-stability tests and updated test data for all ModelSegments single-sample and multiple-sample modes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652:36,update,updated,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652,1,['update'],['updated']
Deployability,Added some websites with tool information and updated the command,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3045:46,update,updated,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3045,1,['update'],['updated']
Deployability,Added the new VariantContext comparison functionality from #6417 but without yet integrating it into any existing tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7021:81,integrat,integrating,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7021,1,['integrat'],['integrating']
Deployability,Added toggle for selecting resource-matching strategies and miscellaneous minor fixes to new annotation-based filtering tools.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8049:6,toggle,toggle,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8049,1,['toggle'],['toggle']
Deployability,Added validateSampleNameMap command line parameter; Added a unit test; Updated genomicsdb version to 0.6.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2733:71,Update,Updated,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733,1,['Update'],['Updated']
Deployability,Adding CRAM integration tests for CountReads.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/870:12,integrat,integration,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/870,1,['integrat'],['integration']
Deployability,Adding NIO and updating to WDL 1.0 to MT pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6074:41,pipeline,pipeline,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6074,1,['pipeline'],['pipeline']
Deployability,Adding a dataflow pipeline for Read only tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/443:18,pipeline,pipeline,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/443,1,['pipeline'],['pipeline']
Deployability,"Adding a new method `getVariantCacheLookAheadBases` to `VariantWalkerBase` which allows subclasses to set how far to look ahead when caching variants. This may help reduce memory use in GenotypeGVCFs. This also changes the side inputs to use FeatureDataSource.DEFAULT_QUERY_LOOKAHEAD_BASES which is `1000`, this is the value used by the other tools. I'm not sure if that's the right thing to do, but it makes variant walkers more consistent with other tools. Alternatively we could add a separate configuration method that lets tools change the side input value. We could also expose an optional parameter in the feature input that lets you set that on a per input basis if we need it. . This doesn't seem to have any negative effect on performance for genotypegvcfs, but it's hard to tell from short runs. It's also hard to tell if it's improving memory usage. It doesn't seem to make an appreciable difference at random places in the genome, but I'm hoping it will make a difference in very bad locations that have a lot of variation. Ideally our caches would be based on size rather than number of variants, but that's a more complicated change. fixes #3471",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480:497,configurat,configuration,497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480,1,['configurat'],['configuration']
Deployability,"Adding an R dependency for a package that improves performance and memory usage for reading large TSV files. This is convenient for generating CNV plots from WGS data. Note: The current CNV plotting code has other issues that make it unsuitable for WGS data; these are addressed in the sl_wgs_acnv dev branch for the new pipeline by new versions of the plotting tools. However, I do not plan on making these fixes to the old versions of the plotting tools. The real purpose of this PR is just to get the additional R dependency merged into master so I can build a new docker for the dev branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3693:321,pipeline,pipeline,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3693,1,['pipeline'],['pipeline']
Deployability,"Adding in a parser that handles text files that are delimited with some known separator (i.e. commas, tabs, words). The new class is called `SimpleKeyXsvFuncotationFactory` and it is usable with the rest of `Funcotator` using new command-line arguments. Added new integration tests that include this new data source factory. Fixes #3757",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3897:264,integrat,integration,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3897,1,['integrat'],['integration']
Deployability,"Adding in the option for using a compressed representation of our reference data. The flag use_compressed_references needs to be passed into ingest and also export, and doing so causes us to use a different internal schema for the ref_ranges table that can save ~40% on space. On the VCF path, the packed data is expanded while we populate the prepare tables before extract. On the VDS path, the packed data is expanded while we extract from the tables to create the Avro files. I updated integration tests to take use_compressed_refs as an option, and saw what I needed. Integrations tests for the VCF and VDS paths go to completion and fail, but in the expected way. AssertIdenticalOutputs, the important part, passes. AssertTableSizesAreExpected fails because the ref_ranges table is so much smaller (255884464 vs 431805033 expected) and AssertCostIsTrackedAndExpected fails because the PrepareRangesCallsetTask.GvsPrepareRanges.BigQuery Query Scanned is similarly smaller (340787200 vs 515899392). Given the nature of the change and the fact that it is an optional flag that defaults to zero, updating the integration tests to cover this path did not seem sound at this time. But using them to verify that the _content_ of the final vcfs wasn't affected by the change makes more sense. VCF integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/e9896786-9d3f-48b9-ab6f-9a76d9fafafe; Hail integration run: https://job-manager.dsde-prod.broadinstitute.org/jobs/dbdd0934-50c3-4477-a191-3282341eacd3. another post-merge integration run with same results: identical outputs, failure due to lower table sizes and query scan costs; https://job-manager.dsde-prod.broadinstitute.org/jobs/edabdac3-1856-4e2a-aadc-3bcd4c353956",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8543:481,update,updated,481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8543,7,"['Integrat', 'integrat', 'update']","['Integrations', 'integration', 'updated']"
Deployability,Adding median coverage metric to mitochondria pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7253:46,pipeline,pipeline,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7253,1,['pipeline'],['pipeline']
Deployability,Additional Dependency updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9006:22,update,updates,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9006,1,['update'],['updates']
Deployability,"Addresses #6242. Current behavior: when all the reads in a read group are filtered in the base recalibration step, the read group is not logged in the recal table. Then ApplyBQSR encounters these reads, can't find the read group in the recal table, and throws an error. New behavior: if `--allow-read-group` flag is set to true, then ApplyBQSR outputs the original quantities (after quantizing). . I avoided the alternative approach of collapsing (marginalizing) across the read groups, mostly because it would require a complete overhaul of the code. I also think that using recal data from other read groups might not be a good idea. In any case, using OQ should be good enough; I assume that these ""missing"" read groups are low enough quality to be filtered out and are likely to be thrown out by downstream tools. I also refactored the BQSR code, mostly to update the variable and class names to be more accurate and descriptive. For instance:. ReadCovariates.java -> PerReadCovariateMatrix.java; EstimatedQReported -> ReportedQuality",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9020:861,update,update,861,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9020,1,['update'],['update']
Deployability,"Addresses [219](https://github.com/broadinstitute/dsp-spec-ops/issues/219). Major changes. - calculate site level metrics in `feature_extract.sql`; - extract metrics, apply thresholds, and set filter field in ExtractFeature; - CreateSiteFilteringFiles to translate from input VCF with filter fields into format for BQ loading, especially `location` fields; - update WDL to call CreateSiteFilteringFiles and upload results to BQ. Minor changes; - added call_GQ to alt_allele creation; - reduced memory requirements in WDL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7197:359,update,update,359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7197,1,['update'],['update']
Deployability,Adds NIO to the SubsetBam task and converts all the MT pipeline WDLs to WDL 1.0. I ran this on one sample in Terra and it worked.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6074:55,pipeline,pipeline,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6074,1,['pipeline'],['pipeline']
Deployability,"Adds a WDL that replaces the ""serial"" SnpThenIndel joint filtering workflow added in #7932. This simplified replacement only runs one iteration of the extract-train-score toolchain, rather than running one iteration for SNPs followed by another for INDELs. The original SnpThenIndel workflow (used for Ultima) will be updated and moved to the WARP repo. (EDIT: I was originally confused here, the WDL that was replaced in this PR simply ran SNPs and indels separately, rather than serially. Curious that things still tied out, but Im not sure its worth looking into at this point.). Test files have also been subset to chr21-22 and slimmed down. A test for the positive-negative was also added, as well as tests of an empty shard. The first commit contains the original workflow (JointVcfFilteringOriginal.wdl), as well as a reimplementation (JointVcfFilteringSnpThenIndel.wdl) that calls the simplified workflow (JointVcfFiltering.wdl). I've verified that both the original and reimplemented SnpThenIndel workflows tie out on the original test data. The second commit then removes the original and the reimplementation, leaving only the simplified workflow. It may thus be easier to review the first commit, second commit, or the overall changes, depending on what you are looking at. @meganshand can you take a look and let me know if there's any missing functionality, or if this otherwise won't work for Ultima and/or importing in WARP? Apologies for the delay!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074:318,update,updated,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074,1,['update'],['updated']
Deployability,"Adds a new ReadWalker tool, `RealignSoftClippedReads`, that realigns soft-clipped fragments using BWA. This tool is motivated by a specific artifact produced by Illumina DRAGEN v3.7.8 in which reads containing small indels are erroneously soft-clipped, often within mobile element contexts (LINE, SINE, ALU, SVA, etc). This is particularly problematic for mobile element insertion callers such as [Scramble](https://github.com/GeneDx/scramble) that rely on soft-clips for identifying potential insertion sites but do not perform a local assembly. In some cases, these soft-clipped reads are aligned to the incorrect region (confirmed by BLAT query and comparison to BWA alignments). An example of a false positive site produced by Scramble is shown below. <img width=""1008"" alt=""Screenshot 2023-11-16 at 2 09 45 PM"" src=""https://github.com/broadinstitute/gatk/assets/5686877/9d2c1dfd-9673-49f0-9372-c4c9cf6ffd9f"">. This PR includes the new tool and unit/integration tests and some minor refactoring to expose non-Spark BWA read mapping. This tool should be considered experimental until thorough benchmarking and analysis can be performed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8588:954,integrat,integration,954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8588,1,['integrat'],['integration']
Deployability,"Adds a new tool that prints any of the SV evidence file types: read count (RD), discordant pair (PE), split-read (SR), or B-allele frequency (BAF). This tool is used frequently in the gatk-sv pipeline for retrieving subsets of evidence records from a bucket over specific intervals. Evidence file formats comply with the current specifications in the existing gatk-sv pipeline. The tool is implemented as a FeatureWalker, which needed to be modified slightly to retrieve the Feature file header. Thus each evidence type has its own classes implementing a Feature and a codec. There are also new OutputStream classes for conveniently writing Features in compressed (and indexed) or plain text formats. The existing PairedEndAndSplitReadEvidenceCollection tool has been modified to use these OutputStream classes. The IntegrationSpec class can now also check for the existence of expected index file output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7026:192,pipeline,pipeline,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7026,3,"['Integrat', 'pipeline']","['IntegrationSpec', 'pipeline']"
Deployability,"Adds a tool for calling SVs using various SV evidence (BNDs, SV pipeline DEL calls, discordant read pairs, split reads) and copy number calls from gCNV. Possible haplotypes are enumerated using a traversal of the sequence graph supported by the SV evidence. The copy number posteriors are integrated to call likely events associated with the possible genotypes. Unit and integration tests forthcoming.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5161:64,pipeline,pipeline,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5161,3,"['integrat', 'pipeline']","['integrated', 'integration', 'pipeline']"
Deployability,Adds dynamic sizing to m2 wdl as well as standardization of each task in terms of what control a user can have over it. This is the first step pegging spec ops to a commit and then merging with the mutect.wdl proper once a release is to be made.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3653:223,release,release,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3653,1,['release'],['release']
Deployability,"Adds pathogen abundance scoring tool for the PathSeq pipeline. . Tool input:; 1) BAM of paired reads; 2) BAM of unpaired reads; 3) Taxonomy database file generated from PathSeqBuildReferenceTaxonomy. Tool output:; 1) Tab-delimited table of taxa scores; 2) Combined BAM with tags indicating which taxa each read was assigned to (optional). In Spark, each read pair / single read is paired with a list of taxa to which it aligned (with sufficient coverage/identity). These lists are collected to the driver and transformed into a map from taxonomic ID to scores (abundance score, number of reads, number of unambiguous reads). Details of the scoring can be found in the tool header comments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3114:53,pipeline,pipeline,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3114,1,['pipeline'],['pipeline']
Deployability,"Adds support for online documentation generation through Barclay (https://github.com/broadinstitute/gatk/issues/2211) via the gatkDoc gradle task. The first commit contains only annotation updates (to target selected classes as documentation targets). A more complete audit/update pass will need to be done; but we need some for now in order to be able to exercise the doc generation process. The second commit contains that actual code and templates for documentation, and the final one upgrades to a Barclay snapshot that has the necessary dependent classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2327:189,update,updates,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327,3,"['update', 'upgrade']","['update', 'updates', 'upgrades']"
Deployability,"Adds two small Python scripts for processing output from the PathSeq pipeline:. 1. aggregate_results.py : combines abundance scores and metrics across multiple samples. It outputs several files, one for metrics, one for scores, etc. each containing a table of values across all the samples. 2. extract_unmapped_reads.py : reads the output SAM from the pipeline from stdin and writes unmapped records (as determined by the absence of a YZ tag) to stdout.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4144:69,pipeline,pipeline,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4144,2,['pipeline'],['pipeline']
Deployability,"Adds updated tools for creating the host reference kmer set (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3115:5,update,updated,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115,1,['update'],['updated']
Deployability,"After #5688 we need to adjust some of the default values when in mitochondria mode. I tested these parameters with the mixture samples and it looked good. In particular note that the TLOD divided by depth filter is no longer needed, so the default would now be 0. Again, would love to get this in before the release on Tuesday (fyi @droazen). @ldgauthier @davidbenjamin @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5827:308,release,release,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827,1,['release'],['release']
Deployability,"After banging my head on GetPileupSummaries for a bit, I think I've figured out a problem with the tool: it assumes that the AF INFO annotation is always present in an input VCF. There are several ways that it could proceed with a record/VCF that doesn't have AF, but the way it currently does (silently skip all of those records) is not a good one. I had zero indication as to why the output was empty.; FWIW, I would suggest doing the following:; 1. Warn the user the first time it encounters a record with no AF that such a record will be ignored.; 2. If no records contain AF then throw a User error at the end of the traversal.; 3. Provide an option to the tool that tells it to assume that the AF is less than 0.2 if not present. We should probably fix this before the release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3955:775,release,release,775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3955,1,['release'],['release']
Deployability,"After discussion in #3084, I offer myself to port the indel realignment pipeline. After exploring the GATK3 implementation, I will split the port in the following independent tasks:. - [ ] Port `RealignerTargetCreator` (require test data after run with GATK3); - [ ] Port `ConstrainedMateFixingManager`; - [ ] Port `NWaySAMFileWriter` (requires some change in the engine to get the ID for the inputs). The previous port will be integrated in the `IndelRealigner` tool implementation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104:72,pipeline,pipeline,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104,2,"['integrat', 'pipeline']","['integrated', 'pipeline']"
Deployability,"After spending some time to resolve this users issue, https://gatkforums.broadinstitute.org/gatk/discussion/24134/gatk4-rmsmappingquality-results-differ-between-v4-0-0-0-and-v4-1-1-0/p1, it became clear that the issue was that the user simply mismatched her versions of gatk, which seems to have caused their MQ annotations to tank. The user didn't notice the warnings of this fact until we had already nearly found the issue by debugging. I propose that we upgrade the warning to an exception with explicit override to make it harder for this issue to slip past people in the future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6060:458,upgrade,upgrade,458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6060,1,['upgrade'],['upgrade']
Deployability,"After the merger with gatk-protected, the docker integration tests are intermittently (but frequently) hitting the hard travis time limit of 50 minutes. We need to fix this ASAP!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2808:49,integrat,integration,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2808,1,['integrat'],['integration']
Deployability,"After the updates to the python environment made in #8561, the CNN tools are no longer functional and were deprecated. However, we may want to provide a separate environment in which some of these tools can still be used, perhaps in conjunction with NVScoreVariants. If so, we should also re-enable the relevant integration and WDL tests and provide appropriate documentation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8907:10,update,updates,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8907,2,"['integrat', 'update']","['integration', 'updates']"
Deployability,Ah allelesubsetting updated,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6277:20,update,updated,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6277,1,['update'],['updated']
Deployability,Ah-update ingest for AoU production,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7126:3,update,update,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7126,1,['update'],['update']
Deployability,All hands on deck: tool doc updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853:28,update,updates,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853,1,['update'],['updates']
Deployability,"All of the Locatable collections associated with a sample and backed by a TSV file (e.g., read-count files, copy-ratio files, segment files, allelic-count files) extend SampleLocatableCollection in the new CNV pipeline. SampleRecordCollection is even more generic, in that the records are not required to be Locatables; this will be used to output posterior summaries for modeling results, for example. Eventually we will want to expand the sample metadata to include a sequence dictionary when appropriate. This will be used to define the ordering in SampleLocatableCollection. For now, we keep lexicographical ordering to be consistent with the old read-count collection, but we should switch this over as soon as possible. @asmirnov239 we will fit your new read-count code into this framework when it's in. @droazen would appreciate any thoughts you might have on whether it's worth using the Tribble framework rather than TableReader/TableWriter for this sort of thing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3716:210,pipeline,pipeline,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3716,1,['pipeline'],['pipeline']
Deployability,All that was required was pointing to the latest barclay release and; creating a new build target in build.gradle. fixes #1454,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3424:57,release,release,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3424,1,['release'],['release']
Deployability,"Allele-specific annotations are going into the production pipeline when we kick off exome reprocessing in a month or so. Lots of projects subset samples (and I think we may even do it in production for people we really like), which will lead to subsetting alleles, which will lead to invalid AS-annotations since they're not going to be split at all. Expected behavior is that any time alleles are subset (e.g. `SelectVariants -sn` or `VariantsToTable --split-mutli-allelic`), AS* annotations are also subset. (GenotypeGVCFs does this already when alleles are subset by QUAL score.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4795:58,pipeline,pipeline,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4795,1,['pipeline'],['pipeline']
Deployability,Allow VQSR Classic Memory overrides to be passed from GvsJointVariantCalling.wdl to GvsCreateFilterSet.wdl.; Increase memory overhead on a couple of tasks in GvsVQSRClassic.wdl. @RoriCremer is running the integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8a8b5553-d9d4-47f5-80fb-ec5992172143).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8452:205,integrat,integration,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8452,1,['integrat'],['integration']
Deployability,Allow updates to an invalid sequence dictionary in a VCF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6140:6,update,updates,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6140,1,['update'],['updates']
Deployability,Allowing our pipeline to function with a sample size of one,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8055:13,pipeline,pipeline,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8055,1,['pipeline'],['pipeline']
Deployability,"Also fixed some minor style issues in argument variable names and the WDL. This should help recover some deletions and might possibly clear up some issues with MAF estimation when the number of hets is small. @LeeTL1220 can you run on some test cases to check the effect? (Note that the changes to fix estimation of the posterior widths, which will in turn affect similar-segment smoothing, are in another branch; we should test those changes as well.). Note that the default threshold of zero for the tumor in matched-normal mode should ensure that the sites genotyped as het should always match in the tumor and the normal. (This will ultimately make multisample segmentation, as enabled by #5524, more straightforward.) There was previously a check for this condition in the integration test; however, it wasn't actually activated by the test data. I could modify the test data to add a proper regression test, but since these test files are generated by running another tool on a test BAM in the repo, this could be misleading. I'm OK with punting in this case. @jonn-smith do you mind reviewing, since this resulted from your turn as liaison? Should be super quick. Thanks again for raising the issue!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5556:778,integrat,integration,778,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5556,1,['integrat'],['integration']
Deployability,Also reduced the overall number of tests to execute by eliminating non-docker unit and integration tests from the travis.yml. I would suggest checking that the docker tests in this travis execution actually uploaded to where it says they do on gcloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3353:87,integrat,integration,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3353,1,['integrat'],['integration']
Deployability,Also remove imports and update comments. Metrics are saved using NIO so we don't need authHolder. This work is part of #2402.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2785:24,update,update,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2785,1,['update'],['update']
Deployability,Also uncomment getAuthenticatedGcs test and update; code to new way of setting retry params.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2581:44,update,update,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2581,1,['update'],['update']
Deployability,"Also updated GATK docker to latest. Latest run with missing columns [here](https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/d9e5ec21-da22-42de-944c-a43ccf83e220) to show what ""failing"" looks like.; Quickstart integration failed because of https://broadworkbench.atlassian.net/browse/VS-1010.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8422:5,update,updated,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8422,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,"Also updated GKL version to 0.5.2, since it adds some output with information about num threads",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2838:5,update,updated,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2838,1,['update'],['updated']
Deployability,"Also updated code and documentation to indicate how to properly run this; test. Sadly it has to be done manually because I don't know of a; reasonable way to disable default credentials. Nevertheless it's good that the test is there even for automated runs,; so we can check that loading the explicit credentials does not break; anything.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879:5,update,updated,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879,1,['update'],['updated']
Deployability,"Also, update code to use Hellbender's IOUtils instead of htsjdk's IOUtil; for these checks. We have both, presumably there's a reason Hellbender has their own and we should use them (for example, we can only add the hinting in our own). Sample error now:. A USER ERROR has occurred: Couldn't read file gs://foo/sam/m54113_160913_184949.scraps.beginning.sam. Error was: Error 403: jp-testing@redacted.iam.gserviceaccount.com does not have storage.objects.get access to foo/sam/m54113_160913_184949.scraps.beginning.sam. Potential cause: incorrect Google Cloud configuration; see instructions in the README. Fixes: #5468",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5477:6,update,update,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5477,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,"Also, update the last paragraph of `gcnvkernel/README.txt` to mention that python CLI scripts are located at `gatk/src/main/resources/org/broadinstitute/hellbender/tools/copynumber/`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4054:6,update,update,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4054,1,['update'],['update']
Deployability,Also:; - Cleaned up headers in some test resources.; - Made sequence-dictionary checking more uniform across all CNV tools.; - Fixed an NPE bug in PlotModeledSegments input validation.; - Improved documentation regarding sex chromosomes in the ModelSegments pipeline.; - Miscellaneous boy-scout activities. Closes #3916.; Closes #3951.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4268:258,pipeline,pipeline,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4268,1,['pipeline'],['pipeline']
Deployability,"An issue has come up on the GATK forum with a user running FilterFuncotations. They have two transcripts for the same gene and are getting a duplicate key error. Ted Brookings identified how to solve this issue:; -The exact problem is in AlleleFrequencyUtils.java, line 30.; -The solution is to skip collecting as a map, have getMaxMinorAlleleFreq take a stream and return an optional float, then return false if the float is missing, otherwise value <= maxMaf. Don't ever call allFrequenciesFiltered. This request was created from a contribution made by Azza Ahmed on October 14, 2021 10:53 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df  broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)).. ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7504:939,pipeline,pipeline,939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504,1,['pipeline'],['pipeline']
Deployability,AoU Echo Precision and Sensitivity Updates [VS-1093],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8578:35,Update,Updates,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8578,1,['Update'],['Updates']
Deployability,Apparently I broke the inclusion of VAT schema JSONs in our Docker image when I was redoing the Dockerfile recently. Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/b270ddee-1bc8-4488-bc60-79d0cb82797e).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8072:117,Integrat,Integration,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8072,1,['Integrat'],['Integration']
Deployability,"Apple is going to turn on Software Signing with OSX Catalina very soon (sometime this fall or so). While signing GATK will be fine, theoretically we have to sign all of the dynamic libraries that we leverage. OpenJDK did a release a while ago with these security features on and it was a major fiasco. We need to research what the signing requirements are and how they will affect the GATK release process.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6756:223,release,release,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6756,2,['release'],['release']
Deployability,ApplyBQSR into Skeleton pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/815:24,pipeline,pipeline,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/815,1,['pipeline'],['pipeline']
Deployability,"Arguments may now define `sensitive`. Sensitive arguments will be printed as ***\* instead of as their value.; Removed output of raw command line from IntegrationTestSpec. The censored command line is output by the command line program itself. These changes are far from bulletproof. Sensitive arguments may be leaked stack traces or errors, but it should fix the worst offenders.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/852:151,Integrat,IntegrationTestSpec,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/852,1,['Integrat'],['IntegrationTestSpec']
Deployability,ArgumentsBuilder in Mutect2 pipeline tool tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6219:28,pipeline,pipeline,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6219,1,['pipeline'],['pipeline']
Deployability,"As an example, the following test is failing in [`ExampleFeatureWalkerIntegrationTest`](https://github.com/broadinstitute/gatk/blob/master/src/test/java/org/broadinstitute/hellbender/tools/examples/ExampleFeatureWalkerIntegrationTest.java):. ```java; ...; @Test; public void testExampleFeatureWalkerWithIntervals() throws IOException {; IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -F "" + TEST_DATA_DIRECTORY + ""example_features.bed"" +; "" -L 1 "" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"",; Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt""); );; testSpec.executeTest(""testExampleIntervalWalker"", this);; }; ```. Where `expected_ExampleFeatureWalkerIntegrationTestWithIntervals_output.txt` is just the first feature from the [`expected_ExampleFeatureWalkerIntegrationTest_output.txt`](https://github.com/broadinstitute/gatk/blob/master/src/test/resources/org/broadinstitute/hellbender/tools/examples/expected_ExampleFeatureWalkerIntegrationTest_output.txt):. ```; Current feature: htsjdk.tribble.bed.FullBEDFeature:1:11-300; 	Overlapping reference bases: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN. 	Overlapping read at 1:200-275; 	Overlapping read at 1:205-280; 	Overlapping read at 1:210-285. 	Overlapping variant at 1:100-100. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:199-200. Ref: GG* Alt(s): [G]; 	Overlapping variant at 1:200-200. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:203-206. Ref: GGGG* Alt(s): [G]; 	Overlapping variant at 1:280-280. Ref: G* Alt(s): [A]; 	Overlapping variant at 1:284-286. Ref: GGG* Alt(s): [G];",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2310:337,Integrat,IntegrationTestSpec,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2310,2,['Integrat'],['IntegrationTestSpec']
Deployability,"As discussed at the GATK Office Hours meeting, the --genotype-germline-sites should no longer be labeled as experimental. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077847071-Mutect-genotype-germline-sites-status). \--. If not an error, choose a category for your question(REQUIRED): ; ; e) Will Mutect2's \[--genotype-germline-sites\](/hc/en-us/articles/360037593851-Mutect2#--genotype-germline-sites) be in future releases?. I notice that this flag is still set to `EXPERIMENTAL` in the Mutect2 docs. Is there a way I can track the status of this feature? It is important for the functionality of the pipeline I have built (i.e. we want to be able to see all of the germline variants that Mutect wants to call -- doesn't have to perfect per s, but those calls are important), so we want to ensure it stays in the picture because it seems to be working for our needs (even after a lot of pressure testing).<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/149087'>Zendesk ticket #149087</a>)<br>gz#149087</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7318:549,release,releases,549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7318,2,"['pipeline', 'release']","['pipeline', 'releases']"
Deployability,"As discussed briefly on Slack, this would encourage stability of these dependencies and would cut down on Travis build time. (However, we might still want to test the build regularly.). One possible approach would be to use a base yml containing the non-GATK python packages to establish the conda enviroment in the base Docker, and then separately add the lines to pip install the GATK python package for users that might want to use the resulting yml outside of Docker. Note that we may want to clean up dependencies for e.g. VCF processing and perhaps consolidate on a common ML framework at some point in the near future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6535:370,install,install,370,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6535,1,['install'],['install']
Deployability,"As discussed here is the information regarding the OutOfBoundsException. Please find below the user report:. I am running the Mutect2 pipeline on canine tumor samples in Terra, using WDL version 2.5 and GATK version 4.1.2.0. I was able to run the pipeline successfully without entering a germline resource file or VCF of common variants for contamination, however, when I did add these files in, I got the following error:. ```; java.lang.IndexOutOfBoundsException: Index: 5, Size: 5; 	at java.util.ArrayList.rangeCheck(ArrayList.java:657); 	at java.util.ArrayList.get(ArrayList.java:433); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$31(SomaticGenotypingEngine.java:350); 	at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:352); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:335); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:250); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(Assembl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:134,pipeline,pipeline,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,2,['pipeline'],['pipeline']
Deployability,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5740:360,Release,Release,360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740,4,"['Release', 'configurat', 'install']","['Release', 'configuration', 'install']"
Deployability,"As discussed in https://github.com/broadinstitute/picard/issues/786#issuecomment-293999711 and approved by @vdauwera , for the 4.0 release of GATK4 we are going to standardize all of our long argument names to follow the `--lowercase-with-dashes-as-separators` style, as opposed to the current inconsistent mix of camel case, underscores, and dashes. As part of this, we should have an opt-in enforcement check in barclay to ensure that this convention is obeyed. This change will be made more palatable for our users via https://github.com/broadinstitute/gatk/issues/1454, which is now targeted for 4.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2596:131,release,release,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596,1,['release'],['release']
Deployability,"As discussed internally, we should publish our docker image to GCR in addition to dockerhub, so that dockerhub won't get clobbered by our production users. The GCR repo to use will be provided by @hjfbynara. Once we have it, we should patch `build_docker.sh -p` to push there as well, and update our ""How to release GATK"" wiki article appropriately.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3793:235,patch,patch,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3793,3,"['patch', 'release', 'update']","['patch', 'release', 'update']"
Deployability,"As explain in the documentation. for logging GATK is using `org.apache.logging.log4j.Logger`. Nevertheless, because I would like to use GATK4 as a framework, I think that API users could benefit from using [SLF4J](http://www.slf4j.org/) as a plugin system for allow users to decide which system use. Because it exists a wrapper for log4j, I think that this could be done easily without changing any behaviour or configuration. In addition, I believe that the usage of SLF4J is similar as the one used in log4j.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176:412,configurat,configuration,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176,1,['configurat'],['configuration']
Deployability,"As more and more branches are cleaned up, trunk needs to be updated. This reflects updates to the tree described in #2703",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3752:60,update,updated,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3752,2,['update'],"['updated', 'updates']"
Deployability,"As noted in #1752, only the first sample with coverage is returned in the `AlignmentContext`. This is a simple patch to make the `LoscusIteratorByState` returns an `AlignmentContext` with all the information in the provided iterator.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1757:111,patch,patch,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1757,1,['patch'],['patch']
Deployability,As part of #8083 we have added yet another heterogenous way of spitting out detailed debugging logs from yet another part of the HaplotypeCaller (namely in this case a toggle that prints to Stdout). This is in contrast to the existing (mostly assembly region position information) `--verbosity DEBUG` debug arguments and the various debug output stream arguments like `--debug-assembly-region-state`. These debug modes have proliferated and it has become difficult/confusing to know which ones are relevant if you are developing the code. . At some point it might be worth creating some sort of static debug manager class that manages the various specific output streams and saves us from having to pass debugger state to all of the various subclasses/utility methods of the HaplotypeCallerEngine. Importantly using `DEBUG` to stderr is not entirely useful as it jumbles all of the various debugger outputs into one output which quickly becomes large and difficult to read. We must support optionally splitting the output streams by functionality to files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8246:168,toggle,toggle,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8246,1,['toggle'],['toggle']
Deployability,"As part of my work in the Pipeline Dev team, I created 2 GATK images to address issue discussed [here](https://github.com/broadinstitute/gatk/issues/8684) (ie. having too many docker layers, we hit ACR limits very quickly). The images are in terrapublic, a premium-tier ACR and is publicly accessible. I made two images, one is squashed to just 1 layer, the other is reduced to just 12 layers (from the original 45). With these changes and the fact that terrapublic is on [premium](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling) tier, the maximum docker pulls per minute becomes 833 (ie. 10k readOps / 12 layers) for the reduced-layers image and 10,000 for the squashed one. We have yet to test these in our pipelines but I anticipate the squashed version to be slower since it wont be able to take advantage of any parallel pulls or caching, hence the two versions to allow pipeline devs to decide which one is better for their use-case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808:26,Pipeline,Pipeline,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808,3,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline', 'pipelines']"
Deployability,"As per internal discussions, we're removing the illumina package from hellbender's scope (for now at least) so I deleted all those classes. Also, for the following tools I wasn't able to get enough userbase/usecases so they are going away too. ```; ScatterIntervalsByNs ; BaitDesigner; BamIndexStats; CheckTerminatorBlock; ConvertSequencingArtifactToOxoG; ExtractSequences; FifoBuffer; MarkDuplicatesWithMateCigar; UpdateVcfSequenceDictionary; VcfFormatConverter; ViewSam; ```. @lbergelson please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/470:415,Update,UpdateVcfSequenceDictionary,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/470,1,['Update'],['UpdateVcfSequenceDictionary']
Deployability,As per the discussion on #6780 our temp file code calls out to `createTempFileInDirectory()` which itself has a list of hard-coded index extensions that it appends to the file. This seems brittle and could leak tmp files onto disk if we don't diligently update that method with all new side outputs. . We should change this method to create a tmp directory that is in its entirety marked as `deleteOnExit()` so we don't have to worry about hard coded side outputs being insufficient.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7017:254,update,update,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7017,1,['update'],['update']
Deployability,"As reported in https://github.com/broadinstitute/gatk/issues/4133 piped commands that work in Picard, don't work when running with GATK. The immediate culprit is the line:; ```; 15:29:29.595 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.8.2.jar!/com/intel/gkl/native/libgkl_compression.dylib; ```; Which is emitted to stdout in GATK, a similar warning is instead output to STDERR in picard. . There are likely other problematic logging lines.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4135:300,install,install,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4135,1,['install'],['install']
Deployability,"As requested in the forum by SL. ----; @esalinas The segment means in the .seg file should be non log_2 (this is the output of the GATK CNV tool, as indicated in the javadoc), but the tangent normalized coverages in the tn.tsv should be log_2. @shlee Perhaps it's worth making this more explicit in the javadoc. Future versions of the CNV pipeline will only deal with raw (integer) read-count coverage and only output non log_2 copy-ratio estimates, which will hopefully prevent such sources of confusion. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/39744#Comment_39744",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3156:339,pipeline,pipeline,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3156,1,['pipeline'],['pipeline']
Deployability,"As stated in the title. I tried the new gatk version 4.2.1.0 to update the GENCODE data for Funcotator. Log:; /home/robby/Tools/NGS/gatk-4.2.1.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gencode/hg19/gencode.v38lift37.annotation.REORDERED.gtf; 14:34:51.448 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.1.0/gatk-package-4.2.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 02, 2021 2:34:51 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:34:51.566 INFO IndexFeatureFile - ------------------------------------------------------------; 14:34:51.566 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.1.0; 14:34:51.566 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:34:51.572 INFO IndexFeatureFile - Initializing engine; 14:34:51.572 INFO IndexFeatureFile - Done initializing engine; 14:34:51.674 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38 (Ensembl 104), mapped to GRCh37 with gencode-backmap Continuing, but errors may occur.; 14:34:51.676 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 38): ##description: evidence-based annotation of the human genome (GRCh38), version 38",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7385:64,update,update,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7385,1,['update'],['update']
Deployability,"As the title suggests, this PR includes a quick update to our README to include documentation on what tools are included in our docker images.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8745:48,update,update,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8745,1,['update'],['update']
Deployability,"As we have finished implementing the updated logic for how variants are interpreted and location inferred by studying local assembly contig alignment signatures, it is time to clean up the corresponding package in the pipeline and make the switch to the updated implementation, which now outputs not only insertion, deletion, small tandem duplication, and inversions, but also novel adjacencies (BND records whose meanings cannot be fully resolved solely from assembly alignment signatures) as well as complex variants that theoretically could be arbitrarily complex (`<CPX>`, as long as we have assembled across the full event). . ## Planed organization. the `discovery` package could be divided roughly now into. ### interface. `SvDiscoveryDataBundle`, `SvDiscoverFromLocalAssemblyContigAlignmentsSpark`, `SvType`, `AnnotatedVariantProducer`. ### alignment prep (sub package). `AlignmentInterval`, `AlignedContig` (refactor `AssemblyContigWithFineTunedAlignments` into `AlignedContig`), `AlignedContigGenerator`, `AlignedAssembly`, `ContigAlignmentsModifier` (refactor `AlnModType` into it), `GappedAlignmentSplitter`, `StrandSwitch`, `FilterLongReadAlignmentsSAMSpark` (factor out the major methods in the new alignment filter by score into a 1st level class). ### type & location inference (sub package). * imprecise: refactor out methods from to-be-deprecated `DiscoverVariantsFromContigAlignmentsSAMSpark`. * alignment classification: `ChimericAlignment` and `NovelAdjacencyReferenceLocations` (very tricky to decouple the functionalities because both have over 50 uses), `AssemblyContigAlignmentSignatureClassifier`, `VariantDetectorFromLocalAssemblyContigAlignments`. * simple: `SimpleSVType`, `SvTypeInference`, `InsDelVariantDetector`, `BreakpointComplications` (rename to `BreakpointComplicationsForSimpleTypes`). * complex: `BreakEndVariantType`, `SuspectedTransLocDetector`, `SimpleStrandSwitchVariantDetector`. ### deprecated. `DiscoverVariantsFromContigAlignmentsSAMSpark` . It currentl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111:37,update,updated,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111,3,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,AssertionError: The optimization step for ELBO update returned a NaN while running DetermineGermlineContigPloidy,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6573:47,update,update,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6573,1,['update'],['update']
Deployability,Assign Ids to samples.; Update the sample info table.; Update import wdl to remove the generation of the sample_info.tsv files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7355:24,Update,Update,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7355,2,['Update'],['Update']
Deployability,"At least -I and -O, to be consistent with ModelSegments pipeline. I think our general rule for the CNV tools is that we do not introduce any CNV-specific short names, but we can use the standard ones already available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4403:56,pipeline,pipeline,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4403,1,['pipeline'],['pipeline']
Deployability,"At the moment, all tests share a global Spark context. Once the test context is created, subsequent calls to `getTestSparkContext(Map<String, String> overridingProperties)` or `getSparkContext(...)` return the existing context and `overridingProperties` is ignored. This results in failure of integration tests of tools that need to override certain Spark configs (e.g. to register custom serializers). To make life a bit easier for gatk-protected devs, it is (at least) desirable to take a global set of overriding Spark config key-value pairs from within the gradle build script, and considering them when instantiating the global test Spark context. In particular, I would like to add a few comma-separated extra registrators to `spark.kryo.registrator`. Perhaps this feature is already present?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337:293,integrat,integration,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337,1,['integrat'],['integration']
Deployability,BQSR Spark integration tests use incorrect list of variants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1017:11,integrat,integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1017,1,['integrat'],['integration']
Deployability,BQSR pipeline without saving + fixed tests for BQSR on spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1031:5,pipeline,pipeline,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1031,1,['pipeline'],['pipeline']
Deployability,"Barclay defines the ExperimentalFeature annotation; displays it in command line program usage output, and also propagates an ""experimental"" property to the freemarker template used for doc. To fully manifest these in GATK we need to:. - update the custom doc templates used by GATK to recognize the ""experimental"" property; - update the command line output from --list/--help, which is generated directly by GATK, to look for the ExperimentalFeature annotation and display accordingly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2744:237,update,update,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2744,2,['update'],['update']
Deployability,"Based on a GATK test run using a locally built htsjdk, the following are prerequisites to upgrading to the next htsjdk release (post-2.19.0):. - https://github.com/broadinstitute/gatk/issues/5839; - Fix CountReads and CountReadsSpark tests. These are failing because they specify `src/test/resources/org/broadinstitute/hellbender/tools/chr7_1_20.interval_list` as an interval argument. Since there is now a tribble codec that handles that extension, the file is treated as a picard interval list and parsed by the codec, but the file is not in picard interval list format.; - A release of Disq upgraded to the new htsjdk version (due to CRAM changes - Container.offset renamed to Container.byteOffset); - Fix failing SortSamSparkIntegration.testSortBAMsSharded tests. Under investigation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5877:119,release,release,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5877,3,"['release', 'upgrade']","['release', 'upgraded']"
Deployability,"Because I'm implementing a program in GATK that comes for a pipeline using `samtools mpileup` and post-processing the file, I need a way to check if the internal `ReadPileup` generated by the tool is indeed the same as the one obtained for samtools. Thus, I ported the `CheckPileup` tool from GATK3 in this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1862:60,pipeline,pipeline,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862,1,['pipeline'],['pipeline']
Deployability,"Before Sam's awesome AGBT talk extolling the virtues of the new GATK-CNN filtering tool we want to be able to make a GATK release with the following:; [ ] A cool name!; [ ] Model training script (in Python, eventually in Java); [ ] Pretrained model for WGS; [ ] Pretrained model for WEx; [ ] Model inference and VCF annotation (in Java); [ ] Solution for applying filters based on CNN score cutoff; [ ] Alternate joint calling WDL? Or for re-filtering? (ideally with a $$$ estimate); [ ] Performance optimizations?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225:122,release,release,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225,1,['release'],['release']
Deployability,"Below are the contents of my reference folder. The index is there, but I don't know why the tool can't recognize it. Please help, thanks!; kh3@rgcaahauva08091 ~/Resources/genome_b37 $> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171:760,install,install,760,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171,2,['install'],['install']
Deployability,Benchmark bam conversion vs. reading directly from crams in single-sample pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5208:74,pipeline,pipeline,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5208,1,['pipeline'],['pipeline']
Deployability,Better GATK-wide support for HDFS and GCS (via Java NIO Path and htsjdk-level patches),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1648:78,patch,patches,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1648,1,['patch'],['patches']
Deployability,"Both Travis and Github actions are failing all of the conda tests at the moment. [Example Log](https://api.travis-ci.com/v3/job/567253185/log.txt) . There seems to be a problem with the conda environment.; ```; java.lang.RuntimeException: A required Python package (""gcnvkernel"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; ```; ```; java.lang.AssertionError: The installed version of sklearn does not match the 0.23.1 version that was requested. Check the build log to see the actual version that was resolved by conda.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7800:559,install,installed,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7800,1,['install'],['installed']
Deployability,"Broad production is finding that after running HG38 GVCFs through `GenomicsDBImport`, `GenomicsDB`, and then `GenotypeGVCFs`, the sort order of the contigs in the header is scrambled. Specifically, the contig names appear to be sorted by ASCII value, so, eg., ""chr10"" comes before ""chr9"". . This is a problem, since there are tools that use the contig ordering in the header to generate contig indices used for comparison of relative contig ordering. `GatherVcfs` in Picard is one example. We need to identify which of the three possible suspects above (`GenomicsDBImport`, `GenomicsDB`, or `GenotypeGVCFs`) is clobbering the contig ordering, and patch it quickly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2753:647,patch,patch,647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2753,1,['patch'],['patch']
Deployability,BucketUtils does not make use of FileSystemProviders.installedProviders(),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3569:53,install,installedProviders,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3569,1,['install'],['installedProviders']
Deployability,Bug fix for Mitochondria Pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5861:25,Pipeline,Pipeline,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5861,1,['Pipeline'],['Pipeline']
Deployability,Bug fix release that fixes #5919 and https://github.com/disq-bio/disq/pull/101 (the latter caused BAM count errors).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5981:8,release,release,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5981,1,['release'],['release']
Deployability,Bug on 4.0.0.0 reported on forum -- please advise how to proceed @droazen @lbergelson. ---. Ok now things are getting more interesting. Disabling intelflaters seems to solve the problem. But why?. Same flaters work with standalone picard as well and they don't have any issues. Compression level does not change anything. Is this a memory allocation error? I don't think so because it does not segfault. The error message says filename not found. . I am wondering if this has anything to do with meltdown patches. Both of my systems (Ubuntu and macOS) are patched against meltdown bug. And Intel flaters are causing this issue. But this won't explain why standalone picard is still working fine with them. The only explanation that I have is the difference between HTSJDK and GKL versions between standalone picard 2.17.2 and embedded picard 2.17.2 in gatk. The latest HTSJDK update for standalone picard mentions 2.13.1 version but the one in gatk 4.0 is 2.13.2 I believe. I don't have the info about GKL version used in them. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/44985#Comment_44985,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4133:505,patch,patches,505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4133,3,"['patch', 'update']","['patched', 'patches', 'update']"
Deployability,Build automatic evaluation of gCNV pipeline and establish best practices.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123:35,pipeline,pipeline,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123,1,['pipeline'],['pipeline']
Deployability,Build automatic evaluation of somatic CNV pipeline and establish best practices.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122:42,pipeline,pipeline,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122,1,['pipeline'],['pipeline']
Deployability,Build command is: . ./gradlew gatkDoc -PphpDoc. Will update this issue with the location for pushing the docs to the website shortly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4425:53,update,update,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4425,1,['update'],['update']
Deployability,Bump to ADAM 0.23.0 release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4044:20,release,release,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4044,1,['release'],['release']
Deployability,"Bumps commons-io:commons-io from 2.7 to 2.14.0. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=commons-io:commons-io&package-manager=gradle&previous-version=2.7&new-version=2.14.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9003:347,update,updates,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9003,1,['update'],['updates']
Deployability,BwaSpark needs good integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2523:20,integrat,integration,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2523,1,['integrat'],['integration']
Deployability,CNN checklist for pre-AGBT release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225:27,release,release,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225,1,['release'],['release']
Deployability,"CNNVariant Update models, validate scores, cleanup training",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175:11,Update,Update,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175,1,['Update'],['Update']
Deployability,"CNNVariantScore has a 2d integration test but the vcf and bam inputs have no overlapping genomic territory, so no reads data is ever transmitted to the python code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4536:25,integrat,integration,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4536,1,['integrat'],['integration']
Deployability,CNV Pipeline outputs NaN and errors at segmentation with heavily downsampled input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2944:4,Pipeline,Pipeline,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944,1,['Pipeline'],['Pipeline']
Deployability,CNV TODOs before release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826:17,release,release,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826,1,['release'],['release']
Deployability,CRAM container offsets should be updated when merging parts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2201:33,update,updated,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2201,1,['update'],['updated']
Deployability,CRAM integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/784:5,integrat,integration,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/784,1,['integrat'],['integration']
Deployability,"CTX events have CTX_PP/QQ or CTX_PQ/QP as CPX_TYPE values in GATK-SV VCFs. However, slashes are not allowed as part of enum values, so these are represented in the ComplexVariantSubtype enum as CTX_PP_QQ and CTX_PQ_QP. This caused the CTX subtypes to not be recognized and to be dropped during SVConcordance (https://github.com/broadinstitute/gatk-sv/issues/664). This representation may merit further discussion, as it's not ideal that the ComplexVariantSubtype values are not a 1:1 match with the VCF values. But as one possible solution without altering the VCF, I've implemented a workaround that replaces the slash with an underscore during getComplexSubtype. I've also updated SVAnnotate to use this method, in the hopes that we can use it across all of the SV tools for more consistent behavior. I added a unit test for getComplexSubtype with one regular CPX_TYPE, one CTX subtype, and one non-CPX event. The test suites for SVCallRecord, SVCallRecordUtils, SVAnnotate, and SVConcordance all ran successfully on my branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8885:675,update,updated,675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8885,1,['update'],['updated']
Deployability,CalculateGenotypePostiors minor updates to javadoc and logger type,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5601:32,update,updates,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601,1,['update'],['updates']
Deployability,"Caller - Start Date/Time: May 28, 2018 9:54:28 PM UTC; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.625 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Version: 2.14.3; 21:54:28.626 INFO GermlineCNVCaller - Picard Version: 2.18.2; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:54:28.626 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:54:28.627 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:54:28.627 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:54:28.627 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:54:28.627 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:54:28.627 WARN GermlineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:54:28.627 INFO GermlineCNVCaller - Initializing engine; 21:54:31.994 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:54:33.457 INFO GermlineCNVCaller - Intervals specified...; 21:54:34.113 INFO IntervalArgumentCollection - Processing 10999816 bp from intervals; 21:54:34.145 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4826:1785,patch,patch,1785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826,1,['patch'],['patch']
Deployability,Can we simplify updates to the GENCODE version?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4786:16,update,updates,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4786,2,['update'],['updates']
Deployability,Cannot access pull request for Owner configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3945:37,configurat,configuration,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3945,1,['configurat'],['configuration']
Deployability,"CentOS Linux release 7.8.2003 (Core); JAVA: openjdk/14.0.1; GATK: 4.1.8.1. ---. I was running this following command:. intervals=$(echo ""$(seq 1 22) X Y"" | tr "" "" ""\n"" | sed 's/^/-L /' | xargs); ref_fasta=""/data1/GenomicDatabases/Human/GATK/b37/human_g1k_v37_decoy.fasta""; gvcfs=$(find hapcall -maxdepth 1 -name ""*_hapcall.g.vcf.gz"" -type f | xargs ls | sed 's/^/-V /' | xargs); /data1/software/gatk/4.1.8.1/gatk --java-options ""-XX:ParallelGCThreads=30 -Xms100g -Xmx100g -Djava.io.tmpdir=tmp"" CombineGVCFs -R ${ref_fasta} -O combine/human_combine.g.vcf.gz ${gvcfs} ${intervals} -G StandardAnnotation -G AS_StandardAnnotation --create-output-variant-index true > combine/human_combine.log 2>&1. ---. Here the log:; [human_combine.log](https://github.com/broadinstitute/gatk/files/5165640/human_combine.log). 09:10:26.647 INFO IntervalArgumentCollection - Processing 3095677412 bp from intervals; 09:10:26.694 INFO CombineGVCFs - Done initializing engine; 09:10:26.713 INFO ProgressMeter - Starting traversal; 09:10:26.714 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:10:30.685 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location 1:13021 the annotation MLEAC=[1, 0] was not a numerical value and was ignored; 09:10:39.543 INFO ProgressMeter - 1:232994 0.2 1000 4676.9; 09:10:51.253 INFO ProgressMeter - 1:688469 0.4 2000 4890.4; 09:11:01.889 INFO ProgressMeter - 1:809005 0.6 3000 5117.3; 09:11:13.838 INFO ProgressMeter - 1:818424 0.8 5000 6366.2; 09:11:16.811 INFO CombineGVCFs - Shutting down engine; [September 3, 2020 at 9:11:16 AM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 1.20 minutes.; Runtime.totalMemory()=107374182400; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.encode(StrandBiasUtils.java:52); 	at java.base/java.util.stream.ReferencePipeline$3$1.accep",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6790:13,release,release,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6790,1,['release'],['release']
Deployability,Change UpdateVCFSequenceDictionary to use the specified dictionary uniformly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5093:7,Update,UpdateVCFSequenceDictionary,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5093,1,['Update'],['UpdateVCFSequenceDictionary']
Deployability,Change conda environment creation to work with release distribution.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4233:47,release,release,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4233,1,['release'],['release']
Deployability,Change extract so that when we filter at the genotype level (with FT) the VCF header has the filter definition in the comment/unstructured fields of the VCF Header.; Also minor renaming of ExtractCohort argument. Passing Integration Test (all chromosomes) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8e240d47-a75e-46b6-88f4-e95e7c1cf4e8); Passing Integration Test (chr20 and friends) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/17ce6f1d-932d-4f4f-a2c8-f5f044bb1a67),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8773:221,Integrat,Integration,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8773,2,['Integrat'],['Integration']
Deployability,Changed PreprocessIntervals behavior to not merge intervals that overlap after padding and updated docs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4027:91,update,updated,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4027,1,['update'],['updated']
Deployability,"Changes are; - fixes to `add_max_as_vqs_score.py` to correspond to changes from https://github.com/broadinstitute/gatk/pull/8412 that changes ""AS_VQS_SENS"" to ""CALIBRATION_SENSITIVITY""; - documentation updates. Run to create filter set: https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/cd08d3fa-26a8-4e00-bbeb-0b458dff11ac; Run to extract control VCFs: https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/b351b3d4-4fb0-45be-a7fb-138f2ad0191e; Run to calculate Precision and Sensitivity: https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/34be5132-7f17-42e8-a44f-8c8ed37745e3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8459:202,update,updates,202,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8459,1,['update'],['updates']
Deployability,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1813:1218,install,installed,1218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813,1,['install'],['installed']
Deployability,"Charlotte recently stepped on a bug in GATK: It interprets -L argument that ends in the regex ':[0-9]*' as indicating a single site in the contig that precedes it and then barfs if it cannot find that contig in the dictionary. In hg38 we have contig names like 'HLA:01:01:01' and when used on the command-line (as in CreateRealignerTargets) it barfs as in the following workflow: https://picard.broadinstitute.org/pipeline/workflows/viewWorkflow/8536444. given that the SAM spec allows any printed character ! through ~ in the ending of contig names (yikes!!) https://github.com/samtools/hts-specs/issues/124 it seems that some more ""smarts"" needs to be put into the parsing of this argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1438:414,pipeline,pipeline,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1438,1,['pipeline'],['pipeline']
Deployability,Check in an installer script (ideally with Java frontend) for funcotator datasources,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4549:12,install,installer,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4549,1,['install'],['installer']
Deployability,Check whether outstanding GKL tickets are still valid with the latest GKL release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7093:74,release,release,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7093,1,['release'],['release']
Deployability,Choose library to use for GATK configuration storage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078:31,configurat,configuration,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078,1,['configurat'],['configuration']
Deployability,Clarify Gnarly pipeline inputs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7231:15,pipeline,pipeline,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7231,1,['pipeline'],['pipeline']
Deployability,Clean up and update CNV methods documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3005:13,update,update,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3005,1,['update'],['update']
Deployability,ClipReads plus integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/160:15,integrat,integration,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/160,1,['integrat'],['integration']
Deployability,"Closes #4519. Note that we should still explore and evaluate other collection strategies in the future. However, I think we can go ahead and use this strategy as the default for now. @asmirnov239 @mbabadi Let's get this merged quick and run the evaluations for the AACR poster using this collection strategy. We should also run some subset of the evaluations with CollectFragmentCounts and check that results improve. @LeeTL1220 @MartonKN We should rerun the somatic evaluations as necessary to make sure they still look good. We may also want to show any interested parties in CGA the relevant results. @sooheelee Note that some tutorial material, slides, etc. should be updated at the appropriate time (and may need to be updated again as we continue to tweak the strategy).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4564:672,update,updated,672,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4564,2,['update'],['updated']
Deployability,Closes #4550 . @jonn-smith You seem to be the godfather of this ticket. All the integration test files changed are just the consequence of renaming; the only substance is the changes to ArgumentsBuilder.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6474:80,integrat,integration,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6474,1,['integrat'],['integration']
Deployability,"Closes #4782 ; Closes #5959. We should discuss a few issues, and modify/cleanup if necessary, before this goes in (if it does at all). In particular, I'd like to understand the original intent behind using the /root directory (e.g., in all of our WDLs) and make sure we don't break typical downstream uses by instead using /gatk; we could even consider ""deprecating"" this over a few releases. I think it's also worth discussing whether we want to continue to release a rootful image, but perhaps parameterize the Docker build script to easily allow the building of an image with a non-root user. I must admit that I don't have good visibility on the various use cases of our Dockers (outside of our typical use with Terra/Cromwell), so if any users would like to chime in, that would certainly be appreciated. In particular, users may still have to do some work on their end to remap user namespaces. In any case, this is at least a proof-of-principle that mounting resources is possible within our test framework with the option for a non-root user. So unless we have other good reasons for requiring a root user, it seems worthwhile to at least allow this option, even if we don't make that the new default. (EDIT: Just to clarify, at some point I was told by another developer that the need to mount testing resources within Travis was at least one reason why we needed a root user---turns out this isn't the case.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6525:383,release,releases,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6525,2,['release'],"['release', 'releases']"
Deployability,"Closes #4824 (at least for underflow of overdispersion); Closes #6226 ; Closes #6227. Along with some other minor fixes. I've done some manual testing and this change is most likely harmless, but ideally we'd have some better automated testing to cover this sort of minor model change... Extremely conscientious users might want to rebuild models just to be safe, which we can mention in the release notes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6245:392,release,release,392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6245,1,['release'],['release']
Deployability,Closes #5060. @meganshand This fixes your bug. Do you have time to review before Monday's release?. @takutosato @LeeTL1220 It improves sensitivity and specificity. @ldgauthier This probably affects HaplotypeCaller as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5057:90,release,release,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5057,1,['release'],['release']
Deployability,"Closes #5391. @takutosato this will make several users happy: https://gatkforums.broadinstitute.org/gatk/discussion/13467/mutect2-pipeline-fails-for-some-inputs#latest, https://gatkforums.broadinstitute.org/gatk/discussion/12618/run-mutect2-in-gatk-4-0-4-0-with-an-error-java-lang-numberformatexception-for-input-string. Looping in @bhanugandham",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5442:130,pipeline,pipeline-fails-for-some-inputs,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5442,1,['pipeline'],['pipeline-fails-for-some-inputs']
Deployability,"Closes #5885. @ldgauthier Fortunately the change in output is tiny, as you can see in the changed integration test VCFs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6401:98,integrat,integration,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6401,1,['integrat'],['integration']
Deployability,"Closes #6030. @takutosato This a complete rewrite of the realignment filter, so I would review `FilterAlignmentArtifacts` and its engine from scratch, not from the diff. There's still a bit of tuning to be done, but it's already far superior to the old version and I want to be using the release jar as much as possible for MC3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6143:288,release,release,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6143,1,['release'],['release']
Deployability,"Closes #6391.; Closes #5748. Note that the *.denoisedLimit4.png output has been removed from the PlotDenoisedCopyRatios tool and the corresponding WDL task. The default behavior has changed slightly, in that the remaining *.denoised.png output is now delimited to maximum copy ratio = 4.0 (instead of covering the entire range in the data). @fleharty @droazen we may want to mention this in the release notes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6482:395,release,release,395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6482,1,['release'],['release']
Deployability,"Closes #6586. @droazen . `AlleleLikelihoods` caches the evidence-to-index `Map`. The previous implementation tried to update this map on the fly whenever evidence was removed. The new approach is to simply invalidate the cache and allow the existing code to generate it to run later. I don't expect this to cause performance problems for a few reasons:. 1. It only applies when we're doing contamination downsampling.; 2. It may save time whenever evidence is removed and we don't need the evidence-to-index map later.; 3. Regenerating the cache is O(N), but so is updating on-the-fly even when only one read is removed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6593:118,update,update,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6593,1,['update'],['update']
Deployability,"Closes #7884. The bug was caused by two position-only checks that should have been contig+position checks. I added a minimal regression test that uses a 2-record snippet of the test data discussed in the first post of the issue. Also discussed there is expected behavior that this fix does not induce. It seems the expectation is that a QUAL = 0 variant block should be turned into a GQ0 reference block. However, I wonder if this is not actually representative of the current (pre-fix) behavior of the tool. For example, if the test data is run only over chr13 using master, the position is not dropped (since then the position-only checks are valid). In that case, we do not see a GQ0 reference block; we instead see a GQ40 reference block, since the original record had GQ61. With the fix, we reproduce this reference block. So although we do not induce the expected GQ0 behavior, I would say the bug is fixed. Whether or not we should issue an additional fix to induce the expected GQ0 behavior is another question entirely. I'm not completely sure what the expected behavior should be from the tool or code documentation alone. Someone more familiar with this tool (@droazen perhaps you can suggest?) may have to chime in. They should probably also check that the contig+position checks are all that need to be added to address the original bug; I'm not 100% sure about behavior there either, but at least all other integration tests seem to still pass.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8028:1421,integrat,integration,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8028,1,['integrat'],['integration']
Deployability,"Code cleanup and technical debt payback, plus updated models.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5175:46,update,updated,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5175,1,['update'],['updated']
Deployability,"Code refactoring for better testability as part of the spanning deletions work that is being shelved. . **Refactoring Changes**; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. - don't write to VCF directly, instead have take a Consumer to emit VariantContexts. This lets us provide a different consumer in unit tests to collect our result.; - we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; - made some private methods package-private so we could call them from tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7946:294,integrat,integration,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946,1,['integrat'],['integration']
Deployability,Collect split read and paired end evidence files for GATK-SV pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6356:61,pipeline,pipeline,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6356,1,['pipeline'],['pipeline']
Deployability,"Command:; ```; bash build_docker_bash.sh; ```; Error:; ```; ...; Step 12/13 : RUN Rscript install_R_packages.R; ---> Running in 96b5753b6c04; Installing packages into '/usr/local/lib/R/site-library'; (as 'lib' is unspecified); Error: (converted from warning) dependency 'caTools' is not available; Execution halted; The command '/bin/sh -c Rscript install_R_packages.R' returned a non-zero code: 1; ```; I came up with this temporary workaround in `install_R_packages.R`:; ```; repos <- c(""http://cran.mtu.edu""); install.packages(c(""bitops""), repos = repos, clean = TRUE); InstallPackageFromArchive(""caTools"", ""https://cran.r-project.org/src/contrib/Archive/caTools/caTools_1.17.tar.gz""); dependencies = c(""gplots"",; ""digest"", ""gtable"", ""MASS"", ""plyr"", ""reshape2"", ""scales"", ""tibble"", ""lazyeval"", # for ggplot2; ""tidyselect"", ""BH"", ""plogr"") # for dplyr; install.packages(dependencies, repos = repos, clean = TRUE); ```; which at least builds successfully.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6489:142,Install,Installing,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6489,4,"['Install', 'install']","['InstallPackageFromArchive', 'Installing', 'install']"
Deployability,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302:55,upgrade,upgrades,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302,1,['upgrade'],['upgrades']
Deployability,Common spark integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5723:13,integrat,integration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5723,1,['integrat'],['integration']
Deployability,"CompareSAMs ignores validation stringency. Running this. ```; build/install/hellbender/bin/hellbender CompareSAMs src/test/resources/org/broadinstitute/hellbender/tools/BQSR/NA12878.chr17_69k_70k.dictFix.bam src/test/resources/org/broadinstitute/hellbender/tools/BQSR/NA12878.chr17_69k_70k.dictFix.bam --VALIDATION_STRINGENCY SILENT; ```. results in this. ```; htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 130, Read name 809R9ABXX101220:5:6:17918:145992, Mate Alignment start should be 0 because reference name = *.; at htsjdk.samtools.SAMUtils.processValidationErrors(SAMUtils.java:439); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:643); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:628); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:598); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:544); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:518); at htsjdk.samtools.util.PeekIterator.peek(PeekIterator.java:67); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareCoordinateSortedAlignments(SamComparison.java:111); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:68); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.tools.picard.sam.CompareSAMs.doWork(CompareSAMs.java:34); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:94); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:144); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardComm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419:68,install,install,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419,1,['install'],['install']
Deployability,Completed integration test and added two test files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/857:10,integrat,integration,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/857,1,['integrat'],['integration']
Deployability,Completed integration test. Thanks to @akiezun.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/858:10,integrat,integration,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/858,1,['integrat'],['integration']
Deployability,"Comprises the commits after 7992f64. The only commit with real substance is `Updated metadata and abstract collection classes.`. The rest of the commits simply update calling code, related tests, and test files. These updates were slightly less trivial for the plotting classes, so these are also split off into separate commits. Again, probably could be engineered better (there are two parallel class hierarchies for metadata and collection classes, which is kind of gross), but we can refactor later if needed. @asmirnov239 please review. Again, lower priority than gCNV VCF, but the sooner this is in master the easier it will be to get things into FireCloud. Let's try for early next week. I'll start doc updates concurrently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3914:77,Update,Updated,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3914,4,"['Update', 'update']","['Updated', 'update', 'updates']"
Deployability,"Comprises the last ~~6 commits~~ 7 commits. (Had to fix a test file dependency. From now on, I'd like to request more encapsulation of test resources. Certainly we should have a common pool of general, rarely changed resources, but sharing of specific resources across packages breaks encapsulation.). @lbergelson I removed a few R dependencies. We should update the base Docker image accordingly and make sure I didn't break anything.; @davidbenjamin I had to change one use of HashedListTargetCollection in CalculateContamination. Also note that FilterByOrientationBias is the sole survivor in the exome package, so you may want to move it somewhere else.; @LeeTL1220 @vruano This removes a lot of your code. Please speak up if there are any utility classes, etc. that you'd like to keep. (For example, I kept the HMM code.) I removed the Target codec and associated classes.; @sooheelee I will update the list of tools for doc updates accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3935:356,update,update,356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3935,3,['update'],"['update', 'updates']"
Deployability,Conda environment creation fails when executed from the release distribution,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209:56,release,release,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209,1,['release'],['release']
Deployability,Conda repository changes affecting local installs of earlier versions of GATK (Forum post included),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8504:41,install,installs,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8504,1,['install'],['installs']
Deployability,"ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls to: zorzan; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: Changing modify acls groups to:; 18/04/23 20:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:6350,patch,patch,6350,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['patch'],['patch']
Deployability,"Configuration file overrides system properties, is missing from bundled GATK binary distribution",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4436:0,Configurat,Configuration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436,1,['Configurat'],['Configuration']
Deployability,Configure travis to install/init git lfs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/840:20,install,install,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/840,1,['install'],['install']
Deployability,Continuous snapshots failing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1993:0,Continuous,Continuous,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1993,1,['Continuous'],['Continuous']
Deployability,Copy of mitochondria best practices pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5566:36,pipeline,pipeline,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5566,1,['pipeline'],['pipeline']
Deployability,"Copying over some discussion from Slack, with some slight modifications:. > I took a quick stab at updating the environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:272,update,update,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,5,"['release', 'update']","['release', 'released', 'update', 'updated', 'updates']"
Deployability,"Correct for newly non-optional inputs to `GvsJointVariantCalling`, plus fix an unrelated race condition between creating the dataset and running this workflow. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8797021-f5d4-4003-92be-010001b7f6af).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8833:160,Integrat,Integration,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8833,1,['Integrat'],['Integration']
Deployability,"Could it be possible to set all these properties and configurations in a method annotated with `@BeforeSuite`?. https://github.com/broadinstitute/gatk/blob/3c960c9d7174785a82d272fe9cd33076ae7ed271/src/main/java/org/broadinstitute/hellbender/utils/test/BaseTest.java#L35-L42. Downstream toolkits using the testing framework provided by GATK (and thus, extending `BaseTest`) might benefit for that change - currently they are kept unset if not explicitly defined by the implementation. If it is not possible because it is not correctly handled by TestNG, feel free to close the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5013:53,configurat,configurations,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013,1,['configurat'],['configurations']
Deployability,"Counts - Start Date/Time: May 18, 2021 8:08:44 PM EDT; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - HTSJDK Version: 2.14.1; 20:08:45.223 INFO DenoiseReadCounts - Picard Version: 2.17.2; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:08:45.223 INFO DenoiseReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:08:45.223 INFO DenoiseReadCounts - Deflater: IntelDeflater; 20:08:45.223 INFO DenoiseReadCounts - Inflater: IntelInflater; 20:08:45.223 INFO DenoiseReadCounts - GCS max retries/reopens: 20; 20:08:45.223 INFO DenoiseReadCounts - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:08:45.223 INFO DenoiseReadCounts - Initializing engine; 20:08:45.223 INFO DenoiseReadCounts - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 20:08:45.300 INFO DenoiseReadCounts - Reading read-counts file (BT1813.counts.hdf5)...; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5F.c line 604 in H5Fopen(): unable to open file; major: File accessibilty; minor: Unable to open file; #001: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5Fint.c line 1085 in H5F_open(): unable to read superblock; major: File accessibilty; minor: Read failed; #002: /mnt/scr1/abyrne/HD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7258:3348,patch,patch,3348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258,1,['patch'],['patch']
Deployability,Coveralls github badge should not update until build is complete,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3348:34,update,update,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3348,1,['update'],['update']
Deployability,"Cpx SV PR series, part-6: Update prototyping SV discovery manager tool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3752:26,Update,Update,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3752,1,['Update'],['Update']
Deployability,Create JNI (experimental) for fermi-lite as the assembler in SV pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2072:64,pipeline,pipeline,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2072,1,['pipeline'],['pipeline']
Deployability,Create a plugin for IGV that allows it to display the reads as haplotype caller has assembled them. This would be extremely useful for analysts who are doing manual review and would mostly obviate the need for bamOut. Jim Robinson seems interested in this so we may be able to get help from IGV to do the integration https://github.com/igvteam/igv/issues/428.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3286:305,integrat,integration,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286,1,['integrat'],['integration']
Deployability,"Created ""variantcalling"" test group and split them off from the rest of the integration tests for runtime purposes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4984:76,integrat,integration,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4984,1,['integrat'],['integration']
Deployability,Created NioFileCopier that copies files using nio paths (includes; optional progress indicator and integrity validation).; Updated an error message in funcotator to make it more descriptive. Fixes #4549,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5150:123,Update,Updated,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5150,1,['Update'],['Updated']
Deployability,Created new branch (old one was problematic). Integration test was successful. Let me know if additional parameters should be tested.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/996:46,Integrat,Integration,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/996,1,['Integrat'],['Integration']
Deployability,"Creates a new ""build-base"" Docker image for the expensive and less frequently changed layers of the build image allowing for much improved variantstore image build times. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/ff13e48c-a9dc-48d7-8056-63d4f2028dc0). Other improvements:. * Bumps version of Google Cloud SDK base Docker image to latest `408.0.1-alpine`; * Bumps Arrow library version from 8.0.0 to 10.0.0; * Simplifies Arrow build to use `ninja`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8085:182,integrat,integration,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8085,1,['integrat'],['integration']
Deployability,"Creates integration testing datasets so the tables within them auto-delete at 2 weeks. See the tables in `gvs-internal.quickit_2023_10_24_vs_1049_tables_are_not_forever_265f343_beta` for an example, integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/72e3c830-b4ba-4e52-a264-07acb81a9b5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8563:8,integrat,integration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8563,2,['integrat'],['integration']
Deployability,"Cromwell still struggles with call caching and metadata bloat in our gCNV workflows. Specific improvements to reduce overhead will modify scattered tasks:. 1. `GermlineCNVCallerCohort(Case)Mode` - Replace input `Array[File] read_count_files` with a list of files, i.e. `File read_count_file_paths`. This should be generated using `WritePathList`, rather than using `write_lines()` which does not function in WDL workflow blocks on some Cromwell servers. Replace output `Array[File] gcnv_call_tars` with a single tarball `File gcnv_call_tar` containing all of the calls. It appears there are a number of redundant outputs - kernel version, denoising configs, output file lists, etc. that were added with the [joint calling pipeline](https://github.com/broadinstitute/gatk/commit/31df35bb9204b5551cc1a3ee7468e2b0e577215d). We should rework that to extract/generate those in joint calling workflow when needed and eliminate these outputs.; 2. Add a transpose task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_call_tar` output, and generates a sample-sharded `Array[File] gcnv_calls_by_sample` output of tarballs.; 2. Add a model bundling task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_model_tar` output, extracts the files, and tarballs all of them together to produce a single tarball output. Make this the output of the cohort workflow and input to the case mode workflows, rather than an array of model tars (retain the current `Array[File]` input as an optional type `Array[File]?` that will be used as the default if provided to case mode in order to support users still working with the old paradigm).; 3. `PostprocessGermlineCNVCalls` - replace input `Array[File] gcnv_calls_tars` with `File gcnv_sample_calls`, the sample-sharded output from the aforementioned transpose task. Delete inputs `calling_configs`, `denoising_configs`, `gcnvkernel_version`, `sharded_interval_lists`, as the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7721:722,pipeline,pipeline,722,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7721,1,['pipeline'],['pipeline']
Deployability,"Cromwell v33 released. Supports intelligent file localization, so we should merge forked WDLs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4948:13,release,released,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4948,1,['release'],['released']
Deployability,Currently Funcotator will not check the version of the datasources on startup. This will cause breaking changes to raise stack traces in Funcotator when new datasources are released. Add a check for the version of the data sources so Funcotator will not open incompatible versions.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6712:173,release,released,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6712,1,['release'],['released']
Deployability,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3180:369,integrat,integrate,369,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180,2,['integrat'],['integrate']
Deployability,Currently `Funcotator` uses many command-line options for configuration. This is inefficient and potentially confusing. `Funcotator` should be modified to take in a configuration file (using `Owner`) that can configure all of the CLI options for the tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4581:58,configurat,configuration,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4581,2,['configurat'],['configuration']
Deployability,"Currently every release is a ""snapshot"" unless you manually change a flag in the build script. It should automatically detect ""snapshot status"" from the version.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1791:16,release,release,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1791,1,['release'],['release']
Deployability,"Currently in the inversion calling stage of the SV pipeline, it is assumed that insertion and homology around identified breakpoints don't co-exist. But in theory it is possible. Take an example of an inversion identified by a 5-to-3 contig.; The contig is made from; 1. reference sequence **ref1** `CACTGCCACTGGTCCTTCAACAGATTGATTCTAATTAGCCAATCAAAGACAA`; 2. homology on reference **H** `GGATCCA`; 3. reverse complement of a reference sequence (already RCed) **ref2'**: `CAATGTCAATACAAATGGGGACTAACTATTGTTTTACTTCCCT`; This contig would generate two alignment records: first mapped to 21:27374100 with CIGAR `59M43S` (from compactifying 52M7M43S), second mapped to 21:27374657 with CIGAR `50M52H` (from compactifying 43M7M52H).; Current convention is to give the homologous sequence to the second alignment record to incorporate ambiguity in locating breakpoints. Now if there's an insertion in the sample, say `AAAA`, after the homology. i.e. ```; ref1 + H + I + ref2'; ```. where a prime indicates reverse complement. Then the contig would still be mapped to the same locations on reference, but with CIGARs `59M47S` and `43M4I7M52H` (the aligner would very likely generate a `43M63H`).; The identified breakpoint should still incorporate the ambiguity introduced by the homology. ```; ref2 + I' + H' + ref1'; ```. Our caller currently doesn't handle such cases. Artificially created FASTA records from real reference sequences are attached.; [SV_HomAndIns_Ticket_data.zip](https://github.com/broadinstitute/gatk/files/509267/SV_HomAndIns_Ticket_data.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2196:51,pipeline,pipeline,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2196,1,['pipeline'],['pipeline']
Deployability,"Currently it's getting updated during an active build, so we see things like ""coverage 10%"", then ""coverage 50%"", then ""coverage 80%"", etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3348:23,update,updated,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3348,1,['update'],['updated']
Deployability,"Currently most of the integration tests for BQSR run with BAQ and indel qualities on, which is no longer the default. We should make most of the tests run with the default settings of no BAQ and no indel qualities.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2563:22,integrat,integration,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2563,1,['integrat'],['integration']
Deployability,"Currently our build generates a shell script in build/install/hellbender/bin/hellbender which sets up the Java classpath to point to the various individual jars. We need the ability to package a monolithic jar for distribution, ease of debugging, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/106:54,install,install,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/106,1,['install'],['install']
Deployability,"Currently spark read filters are applied to the initial reads returned from `GATKSparkTool.getReads()` via an overridable `GATKSparkTool.makeReadFilter()`. This is fine for standalone tools, but for multi-tool pipelines it creates a problem, since they must call the underlying transform for each tool instead of invoking the actual `GATKSparkTool`, and so have to handle read filtering manually for each step in the pipeline. We need a Transform abstraction that stores and applies the read filters for each tool, instead of doing this at the `GATKSparkTool` level. . Related to the tool composability ticket https://github.com/broadinstitute/gatk/issues/960",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1165:210,pipeline,pipelines,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1165,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,Currently the `HaplotypeCallerIntegrationTests` cover most of the common use cases for our best practices HaplotypeCaller pipeline but are deficient in covering a few important less-used arguments. A non-exhaustive list of un-tested modes for the HaplotypeCaller that are significant enough to warrant better long term integration tests to ensure they aren't broken in the future are as follows:; - Multisample Calling Mode; - `--emit-all-sites`; - Genotype Given Alleles Mode. To complete this task would mean simply adding some new tests and possibly uploading to our LFS storage some data that appropriately covers the use case to make sure we don't accidentally break these functionalities in embarrassing ways going forwards. Some discretion might be necessary to decide what HC arguments are important enough to warrant new tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7632:122,pipeline,pipeline,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7632,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"Currently the travis CI integration test task is the longest running task, which currently clocks in uncomfortably close to 70 minute hard limit on task runtime. One solution to this would be to split off a second job encompassing some of the longest running integration tests, namely a `VariantCallingIntegrationTests` tag. If we wanted to split the integration tests further then we could also consider splitting off the spark integration tests, which also take some time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4983:24,integrat,integration,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983,4,['integrat'],['integration']
Deployability,"Currently there are number of boolean arguments in GATKTool that are set by default to `true`. Unfortunately, the current syntax for changing that value on the command line is to write ""--argument-name false"" (eg. `--create-output-bam-index false`) which is confusing an counterintuitive. This format made more sense for arguments when the syntax in picard/gatk3 used to be ""ARGUMENT=FALSE"". I propose that these arguments have their statements inverted wherever possible so that the user should always be negating some option that was perviously true (eg. `--disable-output-bam-index-creation` or some other alternative). Given the new argument input format this makes more intuitive sense and helps differentiate between toggle arguments and arguments with inputs on the command line without having to remember what the default value is. . A change like this would affect a number of old/bedrock arguments in the engine like, `--create-output-variant-index`, `--add-output-sam-program-record`, and `--add-output-vcf-command-line` to name a few. Since there are many arguments following this pattern perhaps the overhead from making a change like this isn't worth it. . (Also applies to changes in GATKSparkTool #5574)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5600:723,toggle,toggle,723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5600,1,['toggle'],['toggle']
Deployability,"Currently this implementation could probably use more substantial testing, especially of the core functionality. It is also missing the following features from gatk3:; - Depth of coverage by fragments; - Rtable output format. The following important differences exist from gatk3:; - Fixed numerous bugs with output line ordering, now we necessarily associate the right sample with the right data in the output tables. We also sort the output samples lexicographically consistently so the order of the output columns is deterministic.; - Now gene outputs no longer rely on having exactly matching interval overlaps. Instead they are treated as intervals themselves with coverage information being collected for every exon base covered by the gene. There is an argument to demand that a gene must be completely covered by provided intervals to be in the result table. (Perhaps excising introns from coverage could be toggled?); - Intervals are no longer merged when reported in to the interval tables. . The following (new) features have been pushed into future branches:; - Diagnose targets evaluation field (need to decide what partitioning to attach it to, perhaps sample partitioning info?); - Replacement for `.refseq` format for gene lists. ; - Optional output partitioning based a provided list of read filters. . Depends on #5887 ; Resolves #19",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5913:915,toggle,toggled,915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5913,1,['toggle'],['toggled']
Deployability,"Currently two commits here: Megan's commit that is already on `master` and was approved in #8831, and the other commit with all of my changes for this ticket. . Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/6c033078-f6d3-47c8-926a-07176478823d).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8915:172,integrat,integration,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8915,1,['integrat'],['integration']
Deployability,"Currently we have a dependency on having gcloud and gsutil installed and configured in a certain way, but we don't have any documentation about it. . We're getting authentication partially from gcloud auth login, which is being propagated in a way I don't fully understand through the dataflow pipeline options. . We need to understand exactly what's happening and then write an explanation of what a user needs to do to have it work.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1051:59,install,installed,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1051,2,"['install', 'pipeline']","['installed', 'pipeline']"
Deployability,Currently we have a useless assembleDist task and separate zipBundle task. We should update assembleDist so that it does the right thing.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3059:85,update,update,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3059,1,['update'],['update']
Deployability,"Currently, every single pipeline shows up as ""dataflowcommandlineprogram-jpmartin-(number)"" in the cloud console. It would be helpful if workers could somehow get a name into options.setAppName so it's easier to determine which run is which.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/684:24,pipeline,pipeline,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/684,1,['pipeline'],['pipeline']
Deployability,"Currently, if a tool programmatically adds default read filters (i.e, Mutect2 for one), they don't show up in the gatkDoc as the default value for the `--readFilter` arg. This fixes that. Requires the new Barclay release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6646:213,release,release,213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6646,1,['release'],['release']
Deployability,"Currently, only a public interval list is allowed--this changes that and copies down the SA before the interval list is used. used here:; https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/AoU_DRC_WGS_12-6-21_beta_ingest/job_history/f5ca0b70-1ae2-4e86-b4fc-3d7d19674347 . with interval list: 	gs://prod-drc-broad/beta-release-99k-v3/0000000000-scattered.interval_list",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7743:321,release,release-,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7743,1,['release'],['release-']
Deployability,"Currently, there is an argument to keep all raw annotations, but the flow based pipeline has a use case that needs to keep `RAW_GT_COUNT` without keeping the other raw annotations. This new argument allows the user to keep `RAW_GT_COUNT` while still cleaning up the other raw annotations in GenotypeGVCFs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7996:80,pipeline,pipeline,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7996,1,['pipeline'],['pipeline']
Deployability,"Currently, we need to copy files on S3 to local storage before using; them. This patch enables gatk local and spark modes to access s3a://; files directly to reduce copy overhead and local disk usages. s3a file accesses require additional configuration of core-site.xml; located in CLASSPATH as well as other hadoop applications. Spark; already has hadoop dependencies but local modes need to add hadoop; jars in the classpath. Example core-site.xml:. ```; <configuration>; <property>; <name>fs.s3a.access.key</name>; <value>{Your AWS_ACCESS_KEY_ID}</value>; </property>; <property>; <name>fs.s3a.secret.key</name>; <value>{Your AWS_SECRET_ACCESS_KEY}</value>; </property>; </configuration>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698:81,patch,patch,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698,4,"['configurat', 'patch']","['configuration', 'patch']"
Deployability,"Currently, we recommend the use of gatk-launch, but we're inconsistent about how we display the app name in output, including in command lines embedded in output files. We alternatively use 'gatk', gatk-launch', or '':. > Using GATK wrapper script /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk; > Running:; > /Users/cnorman/projects/gatk/build/install/gatk/bin/gatk CalculateTargetCoverage -I ...; > [July 19, 2017 3:31:55 PM EDT] CalculateTargetCoverage --input ... Note there is no app name listed in the last line above, due to [this](https://github.com/broadinstitute/gatk/blob/33d316f0e8e35572bb60c83a144297c8557bb37d/src/main/java/org/broadinstitute/hellbender/Main.java#L103). This also affects embedded command line output. Also, downstream projects need some way to customize this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3312:283,install,install,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3312,2,['install'],['install']
Deployability,Custom Spark configuration in tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337:13,configurat,configuration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337,1,['configurat'],['configuration']
Deployability,DBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB compl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7598:4224,update,update,4224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598,1,['update'],['update']
Deployability,DRAGEN-GATK release?!!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6912:12,release,release,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6912,1,['release'],['release']
Deployability,"D_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 21:02:08.892 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:02:08.892 INFO PrintReadsSpark - Deflater: IntelDeflater; 21:02:08.892 INFO PrintReadsSpark - Inflater: IntelInflater; 21:02:08.892 INFO PrintReadsSpark - GCS max retries/reopens: 20; 21:02:08.892 INFO PrintReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:02:08.892 WARN PrintReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:6258,configurat,configuration,6258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['configurat'],['configuration']
Deployability,"Dealing with case when two alignment blocks contain each other in their ref span, indicating duplication.; Instead of outputting CIGARs for the duplicated units like we did for duplication records now in master pipeline, we output alt haplotype sequence from the evidence contigs we assembled, since we did an experiment work with inverted duplications. Some performance evaluation based on the logs. ```; master.vcf. 10:28:46.262 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - Discovered 6324 variants.; 10:28:46.277 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - INV: 237; 10:28:46.277 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - DEL: 3680; 10:28:46.277 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - DUP: 1141; 10:28:46.277 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - INS: 1266; 10:28:46.483 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - Shutting down engine; [October 4, 2017 10:28:46 AM EDT] org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark done. Elapsed time: 0.53 minutes.; Runtime.totalMemory()=3954180096. ==============. feature.vcf. 13:51:48.490 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - Discovered 6543 variants.; 13:51:48.502 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - INV: 229; 13:51:48.502 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - DEL: 3679; 13:51:48.502 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - DUP: 1365; 13:51:48.502 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - INS: 1270; 13:51:48.770 INFO DiscoverVariantsFromContigAlignmentsSAMSpark - Shutting down engine; [October 5, 2017 1:51:48 PM EDT] org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=4026531840; ```. No variants that were dropped are simple variants, and they are expected to be brought back with the correct interpretation once complex sv PR series are fully coded. __Two known i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3668:211,pipeline,pipeline,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3668,1,['pipeline'],['pipeline']
Deployability,"Dear *,; I just tested the Docker Container 4.4.0.0 (upgrade from 4.3.0.0) and my pipeline tried to execute BaseRecalibrator and ApplyBQSR. It crashed with a message:; `/usr/bin/env: 'python': No such file or directory`. A simple test with a python script, using ""/usr/bin/python"" in shebang revealed that is truly not available in this path.; A test with the 4.3.0.0 Container worked. I solved the issue by typing ; `ln -s /usr/bin/python3 /usr/bin/python`. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402:53,upgrade,upgrade,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402,2,"['pipeline', 'upgrade']","['pipeline', 'upgrade']"
Deployability,"Dear *,; I just updated the funcotator database for hg19 from 1.6 to 1.7.; Here I observed for EGFR a wrong ENST assignment.; I wanted funcotator to use ENST00000275493, which corresponds to NM_005228.3 ([oncokb.org](https://www.oncokb.org/gene/EGFR)).; I therefore listed the ENST in --transcript-list file, when calling funcotator with --ref-version hg19.; For some reason it works with the 1.6 .database, but not with the 1.7 database.; Here I receive the correct NM_ID in the MAF file, but another ENST number and therefore other protein variants.; The ENST I now got is ENST00000455089. Kind regards,; Daniel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8403:16,update,updated,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8403,1,['update'],['updated']
Deployability,"Dear @sooheelee . I am wondering if I need to perform base recalibration on the normal bam files before I proceed with creating the panel of normals, which will then be used for somatic mutation calling with MuTect2. . The tutorial (https://software.broadinstitute.org/gatk/documentation/article?id=11136) and the documentation (https://software.broadinstitute.org/gatk/documentation/article?id=11127) suggest to me that baseRecalibration is not required, but I wanted to confirm before proceeding with the pipeline. Regards,; Sangjin Lee",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5836:507,pipeline,pipeline,507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5836,1,['pipeline'],['pipeline']
Deployability,"Dear GATK team,. I am getting errors with haplotyper of GATK 4.1.6.0 when i run gatk-variant pipeline from bcbio on one WGS sample. I am getting same error when i run using same version of GATK installed on my machine. ### Error log. [April 15, 2020 8:14:35 AM GMT] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.80 minutes.; Runtime.totalMemory()=4151902208; java.lang.ArrayIndexOutOfBoundsException: 6; at org.broadinstitute.hellbender.utils.GenotypeUtils.computeDiploidGenotypeCounts(GenotypeUtils.java:70); at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.calculateEH(ExcessHet.java:86); at org.broadinstitute.hellbender.tools.walkers.annotator.ExcessHet.annotate(ExcessHet.java:74); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.annotateContext(VariantAnnotatorEngine.java:293); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.makeAnnotatedCall(HaplotypeCallerGenotypingEngine.java:365); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:189); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:608); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6552:93,pipeline,pipeline,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6552,2,"['install', 'pipeline']","['installed', 'pipeline']"
Deployability,"Dear all,. I installed the latest version of gatk as follows;. download the latest version of gatk (https://github.com/broadinstitute/gatk/releases). followed this (https://gatk.broadinstitute.org/hc/en-us/articles/360035889851--How-to-Install-and-use-Conda-for-GATK4) (conda env create -n gatk4 -f gatkcondaenv.yml). installation was completed as below;. Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117482 sha256=5e0f0b2eb6027268eb5814acd8c8b57d265b7aeb371702c736dd4723aa1beee4; Stored in directory: /tmp/pip-ephem-wheel-cache-gyc4oo9g/wheels/86/46/5d/d5d2d327a9cdc718f906fa1d0cd6e18392bd4eea267f327437; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done. when I started to run this command;. gatk CNNScoreVariants -V 21002.HaplotypeCaller.output.g.vcf.gz -R hg19.fa -O annotated.vcf; it gives an error as below;. java.lang.RuntimeException: A required Python package (""gatktool"") could not be imported into the Python environment. This tool requires that the GATK Python environment is properly established and activated. Please refer to GATK README.md file for instructions on setting up the GATK Python environment.; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397:13,install,installed,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397,6,"['Install', 'install', 'release']","['Install-and-use-Conda-for-', 'Installing', 'installation', 'installed', 'releases']"
Deployability,"Dear all,. I'm trying to run GenotypeGVCFs using the my_folder database created with GenomicsDBImport which should contain 222 samples. The database was created by adding progressively 10 samples at a time using the command --genomicsdb-update-workspace-path and the relative .sample_map file containing the path to my g.vcf.gz and g.vcf.gz.tbi files. I tried running the GenomicsDBImport followed by GenotypeGVCFs using only four sampled and it worked appropriately by generating a .vcf.gz file along with the index .vcf.gz.tbi file. However, when I run GenotypeGVCFs with 222 samples I get the error: A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. The GATK version used is gatk-4.4.0.0 and the command used is the following:. python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz. attaches below also the complete program log. and the content of my callset.json file. Any idea about that?. Thank you very much. Stefano. REQUIRED for all errors and issues:; a) GATK version used: gatk-4.4.0.0; b) Exact command used: python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats_fatte_con_GenomicsDBImport.vcf.gz. c) Entire program log:. (base) administrator@srv2-napolioni:/mnt/nas2/Stefano/Cashmere/joint_variant_calling$ python2.7 /home/administrator/tool/gatk-4.4.0.0/gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R /mnt/nas/Stefano/Cashmere/Reference_Genome/GCF_001704415.1_ARS1_genomic.fna -V gendb://my_database -O /mnt/nas2/Stefano/Cashmere/joint_variant_calling/222_goats.vcf.gz; Using GATK jar /home/administrator/tool/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Runnin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8709:237,update,update-workspace-path,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8709,1,['update'],['update-workspace-path']
Deployability,"Dear developer,. I have finished the pipeline of gCNV caller in GATK and get the VCF file for each samples. To make further analysis, are there any functions or codes from GATK which can:; (1) make segmentation and generate plots for results from germline CNV caller, just like for somatic CNV caller? e.g.(https://software.broadinstitute.org/gatk/documentation/article?id=11682)?. (2) detect the LOH, estimate sample purity for germline CNV?. (3) give the exact breakpoints of a CNV result, not just a bin. Best.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6320:37,pipeline,pipeline,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6320,1,['pipeline'],['pipeline']
Deployability,"Dear developer,. We have finished the annotation of gCNV events identified by GATK Germline Copy Number Variation calling pipeline. And we got very exciting results to find several potential susceptibility genes with published literature evidence. But another cooperative team cannot find the same results or similar frequency from the same data, based on the combination algorithm of log R Ratio and B Allele Frequency. It seems that the GATK germline CNV calling pipeline did not use log R Ratio or B Allele Frequency method from SNP data, right? We have discussions about this, and my collogues think that potential CNV events should be supported by the log R Ratio and B Allele Frequency evidence, even though they seem look good in IGV with BAM file. Or they may be not accepted by others. What do you think about this? Dose GATK algorithm have some advantages to ignore LRR or BAF method. Do you have some method or explanation to solve my collogues concerning?. Best regards.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6407:122,pipeline,pipeline,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6407,2,['pipeline'],['pipeline']
Deployability,"Dear developer:; We installed the gatk4.4 and we want to used the GenomicsDBImport. However, we encountered a problem that made me feel very confused and could not be solved. We had not encountered this problem before, because I was relatively familiar with GATK.; Best day!. Code:; /home/test/Software/gatk-4.4.0.0/gatk GenomicsDBImport \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306995.gvcf.gz \; > -V /home/test/Software/gatk-4.4.0.0/test/SRR11306996.gvcf.gz \; > --intervals NC_010443.5 \; > --genomicsdb-workspace-path /home/test/Software/gatk-4.4.0.0/test/02. Erro:; 10:19:39.332 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.334 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0; 10:19:39.335 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:19:39.335 INFO GenomicsDBImport - Executing as test@yang301 on Linux v5.15.0-71-generic amd64; 10:19:39.335 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3+7; 10:19:39.335 INFO GenomicsDBImport - Start Date/Time: ; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.335 INFO GenomicsDBImport - ------------------------------------------------------------; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Version: 3.0.5; 10:19:39.336 INFO GenomicsDBImport - Picard Version: 3.0.0; 10:19:39.336 INFO GenomicsDBImport - Built for Spark Version: 3.3.1; 10:19:39.336 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:19:39.337 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:19:39.337 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:19:39.337 INFO GenomicsDBImport - Inflater: IntelInflater; 10:19:39.337 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8517:20,install,installed,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517,1,['install'],['installed']
Deployability,Decide on similar-segment merging behavior and preliminary values for relevant parameters for ModelSegments release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4032:108,release,release,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4032,1,['release'],['release']
Deployability,Default autosomal coverage and renamed output for mt pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6160:53,pipeline,pipeline,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6160,1,['pipeline'],['pipeline']
Deployability,Defaulting to WGS.; Note that I had to thread the weighted bed file through (and a flag to use it). As this will also be needed to run exomes. Passing (finally!) Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/199f2e66-4b4b-478c-9370-47a760d3fab2).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8429:162,Integrat,Integration,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8429,1,['Integrat'],['Integration']
Deployability,Defaulting to WGS.; Note that I had to thread the weighted bed file through (and a flag to use it). As this will also be needed to run exomes. Successful Joint Calling run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/32d9dfcc-a5c8-4f95-9a5f-f0fbf7294bcf).; Successful Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/65973408-77d4-41a4-9277-6697efdc88fc),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8426:315,Integrat,Integration,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8426,1,['Integrat'],['Integration']
Deployability,Deleted legacy CNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3935:19,pipeline,pipeline,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3935,1,['pipeline'],['pipeline']
Deployability,"Dependent on HTSJDK release after https://github.com/samtools/htsjdk/pull/1544. Fixes #7111 . - Added optional argument `--ignore-non-ref-in-types` to support correct handling of VariantContexts that contain a NON_REF allele; - Default behavior does not change; - Note that this only enables correct handling of GVCF input. The filtered output files are VCF (not GVCF) files, since reference blocks are not extended when a variant is filtered out; - Added integration test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7193:20,release,release,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7193,2,"['integrat', 'release']","['integration', 'release']"
Deployability,Design GATK configuration mechanism,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079:12,configurat,configuration,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079,1,['configurat'],['configuration']
Deployability,"Despite that spark supports equals signs in their configuration values the current code in this class does not as it splits the name=value string in all ""="" present resulting in an bad-argument value exception.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3687:50,configurat,configuration,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3687,1,['configurat'],['configuration']
Deployability,Determine why FTZ gets unset during integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1771:36,integrat,integration,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771,1,['integrat'],['integration']
Deployability,"Devise a dev/release workflow so that release of latest CNV tools will not be blocked by an unstable HaplotypeCaller, and vice versa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:13,release,release,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,2,['release'],['release']
Deployability,"Devise a generic ""ReadWalker""-like dataflow pipeline + tool-facing interface",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/285:44,pipeline,pipeline,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/285,1,['pipeline'],['pipeline']
Deployability,"Different traversal options might optimize some pipelines and give flexibility to the API user to have more efficient tools. Some of the different traversal per-pass that I am interested are:; * Allow different filters applied in each pass. Example usage: collect some metric with all reads in the first pass, and use it for the second pass only on the filtered reads.; * Allow different transformers in each pass. Example usage: in the first pass, untransformed reads are used to collect a metric; based on that metric, the second pass transform the reads to be output.; * Allow different intervals applied in each pass. Example usage: iterate over the user intervals to get some information about the reads, and then in the second pass use that information to iterate over a different set of intervals. I came out with this feature in the `TwoPassReadWalker` to implement a tool for pair-end data. The use case is to iterate in the first pass to the user intervals/filters, collecting where the mates are located (and/or read names), and in the second pass iterate over the reads and its mates independently of where they are located. If giving this flexibility to the `TwoPassReadWalker` is not desired, another option is to implement another walker-type `ReadAndMatesWalker` which implement the first pass in a private method and the second as a `ReadWalker.apply` for the reads and its mates. Which one is the preferred option for the GATK engine team, @droazen?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4849:48,pipeline,pipelines,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849,1,['pipeline'],['pipelines']
Deployability,Disable non-docker unit/integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3294:24,integrat,integration,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3294,1,['integrat'],['integration']
Deployability,"Discussed in office hours. Seems to be a bug with --force-output-intervals in GenotypeGVCFs when using GenomicsDB. ----------. This request was created from a contribution made by Zane Swaydan on June 30, 2022 11:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1](https://gatk.broadinstitute.org/hc/en-us/community/posts/6972994559643-java-lang-IllegalStateException-in-GenotypeGVCFs-after-GenomicsDBImport-GATK-4-2-6-1). \--. I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset... REQUIRED for all errors and issues: ; ; a) GATK version used:. GenomicsDBImport: GATK 4.2.4.0. GenotypeGVCFs: GATK 4.2.6.1. b) Exact command used:. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000\_DataLinks/000\_RefSeq/Cliv2.1\_genomic.fasta --intervals 006\_IntervalsSplit\_DBImport\_VCFref/interval\_9.list --force-output-intervals PigeonBatch4/008\_RawVcfGz/MergeVcf/pigeonBatch1234\_filtered.vcf.gz -V gendb://007\_Database\_DBImport\_VCFref/database\_interval\_9 -O 008\_RawVcfGz\_DBImport\_VCFref/001\_DividedIntervals/interval\_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR.  ; ; c) ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7966:996,update,update,996,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7966,1,['update'],['update']
Deployability,Doc update for GermlineCNVCaller and DetermineGermlineContigPloidy,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4171:4,update,update,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4171,1,['update'],['update']
Deployability,"Doc updates for CollectBaseDistributionByCycleSpark, GetPileupSummaries, Pileup, PileupSpark, CollectInsertSizeMetricsSpark, CollectMultipleMetricsSpark, InsertSizeMetricsCollectorSpark, CompareBaseQualities",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4068:4,update,updates,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4068,1,['update'],['updates']
Deployability,Docker build script --branch and --release options [VS-935],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8354:35,release,release,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8354,1,['release'],['release']
Deployability,"Docker build script should not run ""gradle installAll""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4409:43,install,installAll,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409,1,['install'],['installAll']
Deployability,Docker integration tests intermittently hit hard travis limit of 50 minutes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2808:7,integrat,integration,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2808,1,['integrat'],['integration']
Deployability,Documentation Updates For Sample Subset Deliverables [VS-1448],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8922:14,Update,Updates,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8922,1,['Update'],['Updates']
Deployability,Documentation for Funcotator needs to be updated to be consistent with current functionality.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4654:41,update,updated,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4654,1,['update'],['updated']
Deployability,Documentation update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5658:14,update,update,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5658,1,['update'],['update']
Deployability,Documentation update for the following tools:. BwaMemIndexImageCreator; ClipReads; CompareDuplicatesSpark; ConvertHeaderlessHadoopBamShardToBam; CreateHadoopBamSplittingIndex,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4026:14,update,update,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4026,1,['update'],['update']
Deployability,"Documents steps to clean up GVS assets in the following use case:; - the samples in the callset are going to be joint called once (no sub-cohorts, no re-using the model); - the outputs (VCFs, indexes, interval lists, manifest, sample name list) have all been copied to an independent location (not in the workspace bucket); - no need to maintain provenance; - the workspace where the pipeline was run is only being used for this GVS callset",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7989:384,pipeline,pipeline,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7989,1,['pipeline'],['pipeline']
Deployability,Does this need to be merged into the EchoCallset branch eventually instead?. [Full integration run with the VDS Creation WDL instead of creating a VDS locally in a VM (yes yes also in a WDL but not scaleable)](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/be8cbcbf-1a92-4ec9-97cf-6530fe44ea2c). [VDS Creation run with VQSR Classic](https://job-manager.dsde-prod.broadinstitute.org/jobs/ec917eaf-b1c4-49d5-a51b-b82ddecf47ae). [VDS Creation run with VETS / vqsr LITE](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/112d710d-650e-4ac8-9b54-dc05e194c681),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8554:83,integrat,integration,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8554,1,['integrat'],['integration']
Deployability,"Does:. * utils class cleanups; * removing SGA-related phantom test files (not sure why they were not removed previously); * integration test for new interpretation tool (precursor of #5077 ). Note: ; * to fully resolve #5077, we need to deal with #5111 first.; * the large number of files and lines change is mostly due to removing SGA-related phantom test files. @TedBrookings tagging you as the victim for reviewing.; Please feel free to reassign.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5116:124,integrat,integration,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5116,1,['integrat'],['integration']
Deployability,"Doesn't have to check output, just run the full pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/761:48,pipeline,pipeline,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/761,1,['pipeline'],['pipeline']
Deployability,Dump more evidence info for SV pipeline debugging,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3691:31,pipeline,pipeline,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3691,1,['pipeline'],['pipeline']
Deployability,"EATE_MD5 : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:5721,Configurat,Configuration,5721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Configurat'],['Configuration']
Deployability,"ECIMAL; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; > > 15:10:22.794 INFO PrintReadsSpark - Deflater IntelDeflater; > > 15:10:22.794 INFO PrintReadsSpark - Initializing engine; > > 15:10:22.794 INFO PrintReadsSpark - Done initializing engine; > > 15:10:23.180 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; > > 15:10:25.800 INFO PrintReadsSpark - Shutting down engine; > > [October 18, 2016 3:10:25 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.05 minutes.; > > Runtime.totalMemory()=467140608; > > org.broadinstitute.hellbender.exceptions.GATKException: unable to write bam: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:252); > > at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); > > at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); > > at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); > > at org.broadinstitute.hellbender.Main.main(Main.java:92)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2219:3842,pipeline,pipelines,3842,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219,1,['pipeline'],['pipelines']
Deployability,"EDIT: Parameters are now exposed as individual arguments, so the following quoted text is outdated; see below for more details. > Adds the parameters `--dangling-end-smith-waterman-parameters-table <GATKPath>`, `--haplotype-to-reference-smith-waterman-parameters-table <GATKPath>`, and `--read-to-haplotype-smith-waterman-parameters-table <GATKPath>` to HaplotypeCaller and Mutect2. This allows for input via a TSV containing the column headers `MATCH_VALUE\tMISMATCH_PENALTY\tGAP_OPEN_PENALTY\tGAP_EXTEND_PENALTY` and one row of integers. Enables investigation of #2498 and #5564. Closes #6863 . Just opening this in case anyone wants to play around with it. I'll do some further testing on human and malaria data, but we have already found some cases in the latter for which changing some of the quizzical values to more reasonable ones yields immediate benefits. If anyone has any suggestions for possible evaluations, I'm all ears!. A few notes:. - I still need to add doc strings for the new arguments.; - Per https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705081291, we can wait until after the DRAGEN-GATK dust settles to review/reevaluate/merge.; - At that time, I'll add a few simple integration tests to check that I've properly bubbled up each set of parameters.; - The reviewer might find the diagram at https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707919816 useful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885:1210,integrat,integration,1210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885,1,['integrat'],['integration']
Deployability,"EEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unmerged.bam; ```. Merge unmapped and aligned BAMs:; ```; gatk-4.2.1.0/gatk \",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7398:1737,release,releases,1737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398,1,['release'],['releases']
Deployability,"EEEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEEEEEAEEEEEEEEEAEEEEEEEEEAEEEEEEEEEEEAAAEAAAEEE/A<AEEEEEEEEEEAA"") | \; gzip > R1.fastq.gz. (echo ""@NB500989:333:HKYJNAFX2:1:11101:24447:1024""; echo ""NNNNTTGTATTTTTAATAGAGACGGGGTTTCAACATGTTGGCCAGGCTGGTCTTGAACTCCTGACCTCAGATGATCCACCCGCCTTGGCCTCCCAAAGTGCTAAGATTACAGGTGTGAGCTACTGCACCTGGCCCCCTCTAGTTTCTTTC""; echo ""+""; echo ""####AEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEA<EEEEEEEEE/<EEEEEEE/EEEEEEEEEEE/EEEEEAEEEEEAEAEEEE/EEEEE/EEEEEEEEEEEEEAEEEEEEEEAE/<AAAA/<A<EE<<EA<A/AAE<""; echo ""@NB500989:333:HKYJNAFX2:1:11101:10000:1915""; echo ""TTACAGGATCTGAAGAGAGGGAAAAATAAACATGCACGATTATTTAATTCTTTTGGAAAAACTGCATGTAAGTGAAGTTCTCTTTCACAAGACACAAGCATCGGTAACTTGACAAAAAATGTAAGCTTCAGATTTTTATGAGCCTTTACA""; echo ""+""; echo ""AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE/EEEEEEEEEE6EEEEEEEEEEEEAEEEEEE/EEEAEEEEEEEEEEEEEAEEAE/EEEEEEEEEAEEEEEEA<EEEEEE<AAEEEEEEEEEEAEEEEEAEAAEAE/AAEAEAA<E"") | \; gzip > R2.fastq.gz; ```. Install bwa and GATK resources:; ```; sudo apt install java11-runtime bwa samtools; wget https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip; unzip gatk-4.2.1.0.zip; echo dict fasta fasta.fai fasta.64.alt fasta.64.sa fasta.64.amb fasta.64.bwt fasta.64.ann fasta.64.pac | \; tr ' ' '\n' | xargs -i echo gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.{} | \; gsutil -m cp -I .; ```. Run FastqToSam with the `-SORT_ORDER ""unsorted""` option:; ```; gatk-4.2.1.0/gatk \; FastqToSam \; -FASTQ R1.fastq.gz \; -FASTQ2 R2.fastq.gz \; -OUTPUT unmapped.bam \; -SAMPLE_NAME SM \; -SORT_ORDER ""unsorted""; ```; Notice the option `-SORT_ORDER ""unsorted""` which prevents the tool from resorting the reads which can add both computational and storage requirements. Aligned the data with bwa:; ```; gatk-4.2.1.0/gatk \; SamToFastq \; -INPUT unmapped.bam \; -FASTQ /dev/stdout \; -INTERLEAVE true | \; bwa mem -K 100000000 -p -v 3 -t 16 -Y Homo_sapiens_assembly38.fasta /dev/stdin | \; samtools view -1 - > aligned.unm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7398:1609,Install,Install,1609,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7398,2,"['Install', 'install']","['Install', 'install']"
Deployability,ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3187,configurat,configuration,3187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['configurat'],['configuration']
Deployability,ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2642,configurat,configuration,2642,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['configurat'],['configuration']
Deployability,"E_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 14:40:58.312 INFO ProgressMeter - chr22:10659064 0.2 35790 208384.3; 14:41:09.992 INFO ProgressMeter - chr22:10687910 0.4 35990 98217.0. ```. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:8456,install,install,8456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,1,['install'],['install']
Deployability,Echo Scale Test VDS Updates [VS-1095] [VS-1112],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8602:20,Update,Updates,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8602,1,['Update'],['Updates']
Deployability,Enables integration with GenomicsDB.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2603:8,integrat,integration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2603,1,['integrat'],['integration']
Deployability,"Environment:. * GATK 4.0.3.0; * Python 2.7.14; * Red Hat Enterprise Linux Server release 7.4 (Maipo). I'm using GATK CNNScoreVariants 1D model(CPU mode) for hundreds of VCF files, but the following error occurred in some of them. 00:13:26.470 INFO ProgressMeter - Traversal complete. Processed 4924627 total variants in 79.6 minutes.; 00:14:14.422 INFO CNNScoreVariants - Shutting down engine; [2018/05/02 13:10:44 JST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 79.75 minutes.; Runtime.totalMemory()=1126694912; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line1, in <module>; File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File "".../lib/python2.7/site-packages/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError('Error! Unknown code:', '\x00'); >>>; at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:385); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ; I tried again, but same message are shown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4727:81,release,release,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727,1,['release'],['release']
Deployability,"Error: A JNI error has occurred, please check your installation and try again; Exception in thread ""main"" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes; 	at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:314); 	at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:268); 	at java.util.jar.JarVerifier.processEntry(JarVerifier.java:316); 	at java.util.jar.JarVerifier.update(JarVerifier.java:228); 	at java.util.jar.JarFile.initializeVerifier(JarFile.java:383); 	at java.util.jar.JarFile.getInputStream(JarFile.java:450); 	at sun.misc.JarIndex.getJarIndex(JarIndex.java:137); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:839); 	at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:831); 	at java.security.AccessController.doPrivileged(Native Method); 	at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:830); 	at sun.misc.URLClassPath$JarLoader.<init>(URLClassPath.java:803); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:530); 	at sun.misc.URLClassPath$3.run(URLClassPath.java:520). This error seems to be resulting from signed jars and there are recommendations on the web such as exclude 'META-INF/*.RSA', 'META-INF/*.SF','META-INF/*.DSA'; I will greatly appreciate if someone could tell me where to add the aforementioned line in 'build.gradle' file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2676:51,install,installation,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2676,2,"['install', 'update']","['installation', 'update']"
Deployability,Establish uniform rules for applying filters in Spark pipelines and multiple collectors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2150:54,pipeline,pipelines,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2150,1,['pipeline'],['pipelines']
Deployability,Evaluate commons-configuration for use as an overridable master GATK configuration mechanism,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2297:17,configurat,configuration,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297,2,['configurat'],['configuration']
Deployability,Evaluate whether we can update our jgrapht dependency to a modern version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6201:24,update,update,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6201,1,['update'],['update']
Deployability,"Even if you know that only some tools are enabled for Spark, its not obvious how to find them. And the tool list has more than one Spark program group, which I didn't notice at first:. Spark Validation tools: Tools written in Spark to compare aspects of two different files; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); Spark tools: Tools that use Apache Spark for scaling out (experimental)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1291:281,pipeline,pipelines,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1291,2,"['Pipeline', 'pipeline']","['Pipelines', 'pipelines']"
Deployability,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into EchoCallset; Integration tests (run on the branch into ah_var_store) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ffed3910-761e-487f-98b3-c0582acc13e8); Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8734:92,Integrat,Integration,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8734,2,"['Integrat', 'update']","['Integration', 'update']"
Deployability,"Exclude the field AS_YNG from VCFs extracted from GVS.; This is the merge into ah_var_store to match the one into EchoCallset; Integration tests [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5b513630-203f-4e4c-a14c-7423f300645e) - one failure for unrelated reasons.; Note that I had to update the truth data (since the contents of the VCFs had changed, and that there is one failure, but it's due to a slight cost difference - which is presumably in the noise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8738:127,Integrat,Integration,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8738,2,"['Integrat', 'update']","['Integration', 'update']"
Deployability,Exome integration test. Passing integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/894d12a1-35b8-4a9b-850e-a11dca4b4257),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8448:6,integrat,integration,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8448,2,['integrat'],['integration']
Deployability,Expand integration test cases in StructuralVariationDiscoveryPipelineSparkIntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5077:7,integrat,integration,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5077,1,['integrat'],['integration']
Deployability,"Experiment with different algorithms and settings for compression of shuffle files (ie not only algorithm but buffer sizes, file sizes etc). Use MarkDuplicatesSpark as the pipeline and run on broad cluster and cloud cluster. Remember to try ""no compression"" too.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1809:172,pipeline,pipeline,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1809,1,['pipeline'],['pipeline']
Deployability,Experimental patch to ReferenceConfidenceVariantContextMerger for the GVS team,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8288:13,patch,patch,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8288,1,['patch'],['patch']
Deployability,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3122:288,patch,patch,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122,1,['patch'],['patch']
Deployability,"F.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr --files file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa.img,file:///restricted/projectnb/casa/ref/GRCh38_ignored_kmers.txt --conf spark.yarn.executor.memoryOverhead=5000 --conf spark.network.timeout=600 --conf spark.executor.heartbeatInterval=120 /share/pkg/gatk/4.1.0.0/install/bin/gatk-package-4.1.0.0-spark.jar StructuralVariationDiscoveryPipelineSpark -R file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img --kmers-to-ignore GRCh38_ignored_kmers.txt --contig-sam-file hdfs:///project/casa/gcad/adsp.cc/sv//A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file -I hdfs:///project/casa/gcad/adsp.cc/cram/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.cram -O hdfs:///project/casa/gcad/adsp.cc/sv/A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:1876,deploy,deploy-mode,1876,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['deploy'],['deploy-mode']
Deployability,F1R2 and F2R2 annotations not updated by GenotypeGvcfs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5704:30,update,updated,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5704,1,['update'],['updated']
Deployability,FASTQ file format for Paired-FASTQ-to-unmapped-BAM pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6509:51,pipeline,pipeline,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6509,1,['pipeline'],['pipeline']
Deployability,Feature Request: Make Mitochondria Pipeline more portable with string inputs for tool paths,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6258:35,Pipeline,Pipeline,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6258,1,['Pipeline'],['Pipeline']
Deployability,Feature Request: Mitochondria pipeline should tag variants as heteroplasmic or homoplasmic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6257:30,pipeline,pipeline,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6257,1,['pipeline'],['pipeline']
Deployability,Figure out a way to keep spark and non spark integration tests in synch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/468:45,integrat,integration,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/468,1,['integrat'],['integration']
Deployability,Figure out how to set and update the version numbers. Git-describe with manually set tags seems like a good start.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/184:26,update,update,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/184,1,['update'],['update']
Deployability,Figure out why the travis tests are running slower in the new configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4989:62,configurat,configuration,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989,1,['configurat'],['configuration']
Deployability,FileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:1759,deploy,deploy,1759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,2,['deploy'],['deploy']
Deployability,Filter out SRA readers in SplitReads tests in prep for htsjdk upgrade.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1241:62,upgrade,upgrade,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1241,1,['upgrade'],['upgrade']
Deployability,FilterReads integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/876:12,integrat,integration,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/876,1,['integrat'],['integration']
Deployability,Final Funcotator Documentation Updates for 4.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5611:31,Update,Updates,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5611,1,['Update'],['Updates']
Deployability,"Find out if there's a way to disable this awful behavior via `SamReaderFactory` -- if not, let's patch htsjdk itself to turn off reference auto-downloading by default.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/677:97,patch,patch,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/677,1,['patch'],['patch']
Deployability,"First commit:; -Added CreateReadCountPanelOfNormals tool. This is an update of CreatePanelOfNormals. Related code is written from scratch.; -Added DenoiseReadCounts tool. This is an update of NormalizeSomaticReadCounts. Related code is written from scratch.; -Added AnnotateIntervals tool. This is an update of AnnotateTargets. Related code (e.g., GCBiasCorrector) is mostly ported and does not have to be closely re-reviewed. I naively introduced RecordCollection and LocatableCollection classes that are analogous to SampleRecordCollection and SampleLocatableCollection, respectively, for collections that are not tied to a sample (e.g., GC-content annotations); we can go back and refactor these classes later.; -SVDDenoisingUtils contains many package-private helper methods for filtering and denoising without unit tests. This is intentional. I have verified that this code exactly reproduces the old PoN results down to the 1E-16 level (with the discrepancy coming from the removal of redundant pseudoinverse operations). Rather than writing or porting unit tests for this code, I think it is best if we simply do not reuse this code or make non-trivial changes to it going forward. We can add unit tests later if we have extra time on our hands...; -SparkGenomeReadCounts now outputs TSV and HDF5.; -Added some tests for SimpleCountCollection, HDF5SimpleCountCollection, and some disabled tests for HDF5Utils.; -Miscellaneous cleanup and boy scout activities. Second commit:; -Updated coverage collection in germline and legacy somatic CNV WDLs to use only integer read counts and account for changes to SparkGenomeReadCounts.; -Added tasks for PreprocessIntevals, AnnotateIntervals, and CollectFragmentCounts.; -Renamed and moved some files. Closes #3570.; Closes #3356.; Closes #3349.; Closes #3246.; Closes #3153.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3820:69,update,update,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3820,4,"['Update', 'update']","['Updated', 'update']"
Deployability,First cut of BQSR. No integration tests yet because of the file size issue. Will add another ticked. The pull req is mostly for the utils etc.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/119:22,integrat,integration,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/119,1,['integrat'],['integration']
Deployability,"First of all, thank you so much for your hard work! I am excited to explore GATK 4.0.0.0.; I would like to report minor issues in setting up optional dependencies for GATK4. First, `gatkcondaenv.yml` defines `gatkPythonPackageArchive.zip` location as; `build/gatkPythonPackageArchive.zip`; The path doesn't seem to work seamlessly with the release archive, though the path makes sense in the root directory of source repo. Unzipping the package file locates both files in the same directory and, to me, it is natural to create the conda environment in the directory. Second, it would be more convenient to include `install_R_packages.R` in the release like gatkcondaenv.yml for python dependencies. Also, I had an issue running `install_R_packages.R` for fresh compiled R-3.2.5. The way `install_R_packages.R` install `optparse` prevents its dependency (the getopt package) from being automatically installed, because (I guess) repos argument is set to NULL. I think the helper script needs to install the getopt package as well.; ```; optparseUrl=""http://cran.r-project.org/src/contrib/Archive/optparse/optparse_1.3.2.tar.gz""; if (!(""optparse"" %in% rownames(installed.packages()))) {; install.packages(optparseUrl, repos=NULL, type=""source""); }; ```. Finally, it would be convenient to depend on a R version available in conda, so that R dependency can be a part of the gatk conda environment. I wasn't able to get the recommended R-3.2.5 from conda.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209:340,release,release,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209,7,"['install', 'release']","['install', 'installed', 'release']"
Deployability,"First version of Funcotator, a GATK Oncotator analog. ; Fixes #3283. Includes functionality to create annotations from GENCODE for HG19 and; HG38 for MNPs and INDELs in all regions of the genome. NOTE: There are still some INDEL issues, and some INDEL tests are still; commented out. This is very much a work in progress and should still be considered a; BETA tool. Added required test files for Funcotator unit and integration tests to; git lfs. Rebased on master, now tests are GATKBaseTests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3779:416,integrat,integration,416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3779,1,['integrat'],['integration']
Deployability,Fix a bug where ExtractCohortToPgen does not write to the cost_observability table. - Successful run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/df2420f5-0da3-4801-baa7-d651ce3d6910); - Successful integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/13865dd1-f561-49a2-8581-87343bf79069),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8958:245,integrat,integration,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8958,1,['integrat'],['integration']
Deployability,"Fix a particular ""bug"" observed that caused us dozens of variants:. When an assembly contig has a unique region covered by one or more MQ 0 alignments, the current configuration picker classifies such contig as ""ambiguous"". ; This PR fix this problem by implementing a tie breaker saying that for such contigs, we prefer the unique configuration with no MQ 0 alignments; that is, this tie breaker only saves contigs have one unique configuration with all non-MQ-0 mappings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326:164,configurat,configuration,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326,3,['configurat'],['configuration']
Deployability,Fix bug and update images,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7912:12,update,update,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7912,1,['update'],['update']
Deployability,Fix erroneous warning about GCS test configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5987:37,configurat,configuration,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5987,1,['configurat'],['configuration']
Deployability,Fix integration expectations for fixed AD [VS-689],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8066:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8066,1,['integrat'],['integration']
Deployability,Fix integration tests [VS-1374],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8833:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8833,1,['integrat'],['integration']
Deployability,Fix jexl logging and update VariantFiltration doc.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5422:21,update,update,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5422,1,['update'],['update']
Deployability,Fix maven release and perform manual release of 4.0.9.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5212:10,release,release,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212,2,['release'],['release']
Deployability,Fix new gradle warnings since update.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7625:30,update,update,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625,1,['update'],['update']
Deployability,Fix potential bug in VCF Integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8316:25,Integrat,Integration,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8316,1,['Integrat'],['Integration']
Deployability,Fix requester pays access by updating the google-cloud-nio library to the latest release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7700:81,release,release,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7700,1,['release'],['release']
Deployability,Fixed an incompatibility with the HTSJDK update branch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3869:41,update,update,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3869,1,['update'],['update']
Deployability,Fixed gCNV WDL broken by Cromwell update on FireCloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5407:34,update,update,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5407,1,['update'],['update']
Deployability,Fixed logging in Funcotator.java; Added a method to the VcfOutputRenderer to fix indel boundaries.; Now VCF and MAF files are compared in an integration test.; Fixed a bug in how data from multiple alleles are parsed into funcotation maps. Fixes #4378,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5131:141,integrat,integration,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5131,1,['integrat'],['integration']
Deployability,"Fixes #1027 . This follows google java style guide. Not sure why `pf_read_only` and `aligned_reads_only` are not; camel-cased, but I'm all for that style of casing. ---. The following test passes:. > gradle test --tests org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest. It seems that there is no need for a unit test here, but please let me; know if you would prefer one. I have a skeleton test class to verify; that GatkReadFilter blocks secondary alignment reads,; blocks supplementary alignment reads, can restrict to passing filter; reads only, and can restrict to aligned reads only.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1053:262,pipeline,pipelines,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1053,1,['pipeline'],['pipelines']
Deployability,"Fixes #1731 . **Summary of changes**; - SelectVariants.java; - createVCFHeaderLineList() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - setFilteredGenotypeToNocall() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - VariantFiltration.java; - initializeVcfWriter() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - makeGenotypes() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - GATKVariantContextUtils.java; - incrementChromosomeCountsInfo() - count the total and alternate alleles for a genotype; - updateChromosomeCountsInfo() - update AC, AN and AF with the computed count of total and alternate alleles; - selectVariantsInfoField.vcf\* and variantFiltrationInfoField.vcf*; - User data from https://github.com/broadinstitute/gsa-unstable/issues/1325; - VariantFiltrationIntegrationTest.java; - Tests that show the fix for https://github.com/broadinstitute/gsa-unstable/issues/1325; - SelectVariantsIntegrationTest.java; - MD5 changed in testSetFilteredGtoNocall() because AC, AN and AF are always in the VCF if --setFilteredGtToNocall; - GATKVariantContextUtilsUnitTest.java; - Test new GATKVariantContextUtils methods",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1871:739,update,updateChromosomeCountsInfo,739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871,2,['update'],"['update', 'updateChromosomeCountsInfo']"
Deployability,"Fixes #2738, and is based on #3106 (which should be merged first). Also, requires a Hadoop-BAM release. @lbergelson you mentioned something about changes you made to `HaplotypeCallerEngine` for GVCF - is that relevant to this PR do you think?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3450:95,release,release,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3450,1,['release'],['release']
Deployability,Fixes #3956. Now gencode data sources have names preserved from config files.; Updated MafOutputRenderer to put a space and delimiter between the date and first funcotation factory information.; Updated some test cases to be correct with the new Gencode name preservation and MAF renderer update.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4823:79,Update,Updated,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4823,3,"['Update', 'update']","['Updated', 'update']"
Deployability,"Fixes #4274 and #4303. Relies on https://github.com/HadoopGenomics/Hadoop-BAM/pull/194, so this won't pass (and shouldn't be merged) until a new release of Hadoop-BAM is available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4463:145,release,release,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4463,1,['release'],['release']
Deployability,Fixes #4357. Need to release new copy of data sources along with this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4593:21,release,release,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4593,1,['release'],['release']
Deployability,Fixes #4490 . Release notes:; http://owner.aeonbits.org/news/2018/03/01/owner-1-0-10-released,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4638:14,Release,Release,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4638,2,"['Release', 'release']","['Release', 'released']"
Deployability,Fixes #4581. Had to change a configuration method to allow Funcotator tests to work. Currently funcotator will not use the config cache - it will always; create a new configuration on startup. This should be fine for now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960:29,configurat,configuration,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960,2,['configurat'],['configuration']
Deployability,Fixes #4586. Released new version of datasources to go with this release (1.4.20180615).; This was necessary because the data sources needed to be made; consistent with hg19 (before they were a mix of hg19 and b37; contig names). Now Funcotator assumes all data sources for the hg19 reference are; compliant with hg19 contig names. Updated the minimum data source version to the new release (1.4.20180615). Simplified `Funcotator::enqueueAndHandleVariant`. Not clear that the `--allow-hg19-gencode-b37-contig-matching-override`; flag does anything anymore. Updated the `getDbSNP.sh` and `createSqliteCosmicDb.sh` data source; scripts to preprocess those data sources to be have hg19-compliant; contigs names. New speeds are ~20k variants/minute for hg19 and ~200k variants/minute for hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4927:13,Release,Released,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4927,5,"['Release', 'Update', 'release']","['Released', 'Updated', 'release']"
Deployability,Fixes #5421. Picard updates are in https://github.com/broadinstitute/picard/pull/1259.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5551:20,update,updates,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5551,1,['update'],['updates']
Deployability,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5819:337,Integrat,Integration,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819,1,['Integrat'],['Integration']
Deployability,"Fixes #7068 . - When adding AC, AF, AN, DP header lines, SelectVariants now checks if these lines are in the original header already and if so, overwrites these lines with the respective standard lines; - Without this check, an issue in htsjdk causes duplicate header lines with the same ID if the description differs. This should be fixed there but this fix provides a downstream workaround; - Modified the integration test validation files, which have been invalid VCF files with duplicate header lines; - Removed addition of AC, AF, AN if `--set-filtered-gt-to-nocall` is set, because these lines will be added later anyway",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7069:408,integrat,integration,408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7069,1,['integrat'],['integration']
Deployability,"Fixes #https://github.com/broadinstitute/gatk/issues/4995 for the `StreamingScriptExecutor`. Still need to enable this for `PythonScriptExecutor`. Also includes an opportunistic fix for the localDevCondaEnv gradle task, which sometimes fails to update the Python package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5081:245,update,update,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5081,1,['update'],['update']
Deployability,"Fixes a bug that I inadvertently introduced long ago in #5408. A method there originally assumed that the intervals to be filtered coincide with the annotated intervals when filtering on annotations, but this assumption can be violated when e.g. using `-XL` to further exclude intervals. This breaks the annotation-based filtering and will result in incorrectly retained/dropped intervals. Unfortunately, the integration test cases were not quite complete enough to catch this. Fortunately, for typical data and parameters, the number of affected intervals is typically a very small percentage, especially in human (e.g., the default GC filters of [0.1, 0.9] affect <0.1% of 250bp bins); I only caught this when running on malaria data, since GC filtering has more impact there. I would also expect count-based filters to mitigate some of the effects (e.g., a bin with extreme GC that was erroneously retained when filtering on annotations might later be filtered because it has poor coverage). Downstream results are also all correct---they're just given for a slightly different set of intervals than would be expected. This bug would affect users that made use of the `blacklist_intervals_for_filter_intervals` option in the gCNV WDLs, but my feeling is this functionality is not used that frequently. @droazen I think this is a relatively minor bug, but it might be good to mention it in the release notes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7046:409,integrat,integration,409,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7046,2,"['integrat', 'release']","['integration', 'release']"
Deployability,"Fixes broken docker image pull that throws an error:; ```; docker pull broadinstitute/genomes-in-the-cloud:2.1.1; 2.1.1: Pulling from broadinstitute/genomes-in-the-cloud; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/genomes-in-the-cloud:2.1.1 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8891:327,release,release,327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8891,2,"['release', 'upgrade']","['release', 'upgrade']"
Deployability,"Fixes https://github.com/broadinstitute/gatk/issues/1226. We should resolve whether or not we want to invest in fixing the issues with IntegrationTestSpec (see https://github.com/broadinstitute/gatk/issues/1562) before we do any more of these, since now that we can write CRAMs on SPARK we can also add CRAM tests to ApplyBQSRSpark as well, we just need to decide which way to write the tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1564:135,Integrat,IntegrationTestSpec,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1564,1,['Integrat'],['IntegrationTestSpec']
Deployability,Fixes https://github.com/broadinstitute/gatk/issues/2744. Depends on Barclay upgrade.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2791:77,upgrade,upgrade,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2791,1,['upgrade'],['upgrade']
Deployability,Fixes https://github.com/broadinstitute/gatk/issues/5674. All the tests are written using `IntegrationTestSpec` which creates the output files before running the test. Verified manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5681:91,Integrat,IntegrationTestSpec,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5681,1,['Integrat'],['IntegrationTestSpec']
Deployability,"Fixes https://github.com/broadinstitute/gatk/issues/589 (Walkers and Spark tools only, not integrated with Picard tools).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1900:91,integrat,integrated,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900,1,['integrat'],['integrated']
Deployability,"Fixes https://github.com/broadinstitute/gatk/issues/611. Uses ""validationStringency"" as the argument; PicardCommandLineProgram currently uses ""VALIDATION_STRINGENCY""; should we align all of these to use the same name?. I've done the same work for ReadSparkSource and GATKSparkTool but it requires a Hadoop-BAM upgrade so its in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1439:310,upgrade,upgrade,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1439,1,['upgrade'],['upgrade']
Deployability,Fixes https://github.com/broadinstitute/gatk/issues/6161. Requires an htsjdk update.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6246:77,update,update,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6246,1,['update'],['update']
Deployability,Fixes https://github.com/broadinstitute/gatk/issues/6480. Note that xbyak issue is unchanged and still exists with this upgrade. This also contains a test for some of the key python packages to verify that they are resolved by Conda to the version of the package requested in the .yml file.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6494:120,upgrade,upgrade,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6494,1,['upgrade'],['upgrade']
Deployability,Fixes the Python part of https://github.com/broadinstitute/gatk/issues/4209. Also updated the README with more specific instructions based on previous feedback from another user. This was tested manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4233:82,update,updated,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4233,1,['update'],['updated']
Deployability,Fixes the gatk doc part of https://github.com/broadinstitute/gatk/issues/5509 now that the public doc has been updated.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5525:111,update,updated,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5525,1,['update'],['updated']
Deployability,"Folks occasionally but consistently ask for this information on the forum and our current answer is that we provide these files as is. For the new GATK4 documentation, which we plan on releasing on January 9th alongside the new GATK4, I think we should aim to be more transparent. . The doc team aims to have select documentation ready by December 13, 2017, in preparation for the release. ### Those involved in the creation of the GRCh38 resource files, could you kindly provide READMEs to place alongside these files? . For example, what were the processing steps used to generate each, what is the original source (version) of the resource used, etc. Thank you. . The files are as follows:; ```; gs://genomics-public-data/resources/broad/hg38/v0/1000G.phase3.integrated.sites_only.no_MATCHED_REV.hg38.vcf; gs://genomics-public-data/resources/broad/hg38/v0/1000G.phase3.integrated.sites_only.no_MATCHED_REV.hg38.vcf.idx; gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz; gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz.tbi; gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz; gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi; gs://genomics-public-data/resources/broad/hg38/v0/Axiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz; gs://genomics-public-data/resources/broad/hg38/v0/Axiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz.tbi; gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf; gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.idx; gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dict; gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta; gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta.64.alt; gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3768:381,release,release,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3768,3,"['integrat', 'release']","['integrated', 'release']"
Deployability,Follow up on CNN deprecation done in the update to python 3.10.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8907:41,update,update,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8907,1,['update'],['update']
Deployability,"Follow up to https://github.com/broadinstitute/gatk/pull/2010.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. Remove corresponding test `IntervalUtilsUnitTest.testIntervalFileToListNegativeOneLength`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2089:218,release,released,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2089,1,['release'],['released']
Deployability,Follow up to https://github.com/broadinstitute/gatk/pull/4097.; This implements 2D CNN model inference. Update to using semantics json file for model serialization.; Still very slow on CPU. Needs more tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4245:104,Update,Update,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4245,1,['Update'],['Update']
Deployability,"Following our recommended IntelliJ setup instructions in our README leads to an IntelliJ project that does not respect the user's PATH environment variable when, eg., building via gradle. We instead end up with a default PATH that includes only a few directories such as `/usr/bin/`. . We need to find a way to get IntelliJ to respect the user's PATH when building, and update our README accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/337:370,update,update,370,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/337,1,['update'],['update']
Deployability,"Following the instructions to build from the main readme, `./gradlew bundle` and `./gradlew clean` ends with these error messages:; ```; ...; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 105. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.; ```. The build is being attempted in a docker container in a 64-bit ubuntu VM on a Win10 host. What am I doing wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6019:603,install,installed,603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6019,1,['install'],['installed']
Deployability,"For @droazen on his request. Thank you for looking into this. . [shlee_ref200.zip](https://github.com/broadinstitute/gatk/files/1609980/shlee_ref200.zip). Observations and commands are in file **shlee_ref200_commands.txt** and should be copy-pastable if you run them in the **shlee_ref200** folder. Folder contains everything you need (reference set, BAMs for normal, tumorA, tumorB, and ref (this last has no variants)) plus the results. The reference is 200bp long and has a fair amount of low complexity regions. I interspersed some different nucleotides here and there such that pretty much 99% of simulated reads (2x50 to the reference) map unambiguously. If you are wondering of sequence context biases for site 121, then I point out that GATK3.7 HaplotypeCaller has no problems calling all of the designed variants in the three samples. It is GATK4.beta.6 and GATK4.beta.6+ (master branch of 1/6 Saturday morning) that seem to be blind to the G allele there (no problems with C allele in same location). I think perhaps something is not at parity in terms of the parameters between the two sets and any insight would be appreciated. Used gatk4.beta.6 master branch of Saturday 1/6/2017 (gatk-4.beta.6-144-g54af6b4-SNAPSHOT).; - Results show same oddness with stable GATK4.beta.6 release.; - Results are similarly odd with GATK4 Mutect2.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4073:1286,release,release,1286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4073,1,['release'],['release']
Deployability,"For GVS Feature Extract, ~Cohort Extract~ and Prepare Callset we should add a bq labels to indicate the query and tool being. gvs_tool_name (e.g. feature-extract); gvs_query_name (e.g. read-sample-table). Python Prepare Callset:. <img width=""334"" alt=""Screen Shot 2021-05-19 at 9 31 04 AM"" src=""https://user-images.githubusercontent.com/6863459/118821262-1193eb80-b885-11eb-8456-71225b40192b.png"">. Java GVS Feature Extract:; <img width=""346"" alt=""Screen Shot 2021-05-19 at 9 31 25 AM"" src=""https://user-images.githubusercontent.com/6863459/118821247-0e006480-b885-11eb-9d95-99441c6dbebd.png"">. for Feature Extract; update the wdl to take in a query_labels optional string; update the GATK tool to take in a query_labels param; update the GATK tool to validate labels; update the GATK tool to add constant kv labels: ""gvs_tool_name"", ""extract-features"" and ""gvs_query_name"", ""extract-features"" (is there a way to get more explicit in the queries? isn't it just one query?); test that this works with and without a label param passed in. for Prepare Callset; update the wdl to take in a query_labels optional string; update the python script to take in a query_labels param; update the python scrip to validate passed in labels; update the python script to add constant kv labels for ever single query individually and as a default; test that this works with and without a label param passed in. <img width=""717"" alt=""Screen Shot 2021-05-05 at 2 40 30 PM"" src=""https://user-images.githubusercontent.com/6863459/117192554-dd61fa80-adaf-11eb-8996-be1dc266dcc2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7233:616,update,update,616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7233,8,['update'],['update']
Deployability,"For INDELs, ReferenceContext is rendered with too many bases. This is because of how the VCF format stores INDELs (vs the MAF format). The logic for creating the ReferenceContext base string needs to be updated to account for the extra bases in the VCF format.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4407:203,update,updated,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4407,1,['update'],['updated']
Deployability,For TAG team after release. This will mostly involve checking that cost-optimizations don't break. Should add corresponding WDL tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3940:19,release,release,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3940,1,['release'],['release']
Deployability,"For allele-specific ranksums, an allele may not have a rank sum value if it didn't have any hets. Right now this is output as ""nul"", but should use the VCF . for missing. . This will a pretty noticeable change with respect to older versions, though we never condone using different versions together in the same pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5940:312,pipeline,pipeline,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5940,1,['pipeline'],['pipeline']
Deployability,"For discussion with @davidbenjamin. Conversation will benefit Mutect2 workflow documentation. The Mutect2 WDL pipeline at <https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl> uses the following:. 1. SplitIntervals in default mode (clarified in #3032 that default INTERVAL_SUBDIVISION mode can cut into an interval in the intervals list); ```; java -jar $GATK_JAR SplitIntervals -R ${ref_fasta} ${""-L "" + intervals} -scatter ${scatter_count} -O interval-files; cp interval-files/*.intervals .; ```; 2. Scatter Mutect2 over files of intervals; ```; scatter (subintervals in SplitIntervals.interval_files ) {; call M2 {; ```; 3. MergeVCFs to collate the resulting VCF callsets; ```; java -Xmx2g -jar $GATK_JAR MergeVcfs -I ${sep=' -I ' input_vcfs} -O ${output_vcf_name}.vcf; ```. Assuming Mutect2 handles active regions in the same manner as HaplotypeCaller, which will expand/pad active regions, then a consequence of the current workflow is potential _duplicate calls_ (that can also differ slightly from each other e.g. due to rounding) for the same genomic locus that result from expansion of the active region into the edges of the intervals being split. MergeVCFs as well as GatherVCFs, allows duplicate calls in the final VCF without checks. GatherVCFs does not allow for out-of-genomic-order-inputs and will give an error. However, I'm noticing something interesting (see below). I would recommend creating intervals without splitting, i.e. with the BALANCING_WITHOUT_INTERVAL_SUBDIVISION option and setting the default of the tool to such to ward against accidental misuse.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061:110,pipeline,pipeline,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061,1,['pipeline'],['pipeline']
Deployability,"For each hellbender ReadWalker, write at least 1 integration test that uses CRAM input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/676:49,integrat,integration,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/676,1,['integrat'],['integration']
Deployability,"For some tools that I'm developing, I want to keep as much as possible the nomenclature for some common parameters in Picard/GATK to integrate my custom tools into a framework that is consistent regarding the argument names. Because of that, I'm having several issues with the `SplitReads`arguments while using splitting by these parameters in my toolkit:. - The current `SplitReads` argument names are in capital, but the rest of the framework use a camel case formatting.; - The short names clash with the `AddOrReplaceReadGroups` from Picard. Because I have a tool that adds read groups with the possibility of splitting in a similar way as `SplitReads` I would like to integrate these parameters in the same tool and I can't with the current implementation. I propose to change the `SplitReads` names to a camel case format different from Picard and pointing out that they are related with splitting.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2383:133,integrat,integrate,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2383,2,['integrat'],['integrate']
Deployability,"For the automated regression pipelines, I would like to add `bc` and possibly other programs to the docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5494:29,pipeline,pipelines,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5494,1,['pipeline'],['pipelines']
Deployability,For the tests to work we must define:; HELLBENDER_TEST_PROJECT : Google Project to use; HELLBENDER_JSON_SERVICE_ACCOUNT_KEY : path to a JSON file with service; account credentials. (I've updated the README accordingly). (We've used both before so they should already be configured). This fixes issue #3125,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3159:187,update,updated,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3159,1,['update'],['updated']
Deployability,"For two reasons, ; 1. I guess that it belongs there until non exome CNV calling pipelines will make use of it.; 2. Would make it easier to share code between elements of the exome CNV pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/530:80,pipeline,pipelines,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/530,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,Formalize release process across public and protected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2475:10,release,release,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2475,1,['release'],['release']
Deployability,Forum user question on Spark configurations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3822:29,configurat,configurations,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3822,1,['configurat'],['configurations']
Deployability,Found updates for broken links in the tool docs and replaced them or removed them.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7309:6,update,updates,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7309,1,['update'],['updates']
Deployability,Fragment-count collection for ModelSegments pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3775:44,pipeline,pipeline,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3775,1,['pipeline'],['pipeline']
Deployability,"Frequently we find our pipeline detecting STR expansions whose size >50, i.e. in the SV domain, but we cannot fully assemble the expanded allele, as judged by PacBio calls.; We need a strategy on how to reliably report ; * what is found and; * how likely it is that we have assembled the full allele, or only part of the expansion (lower bound estimate).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4388:23,pipeline,pipeline,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4388,1,['pipeline'],['pipeline']
Deployability,"From @tomwhite . ""Isilon exposes a Hadoop filesystem interface which makes it possible; to use it as a source or sink for Spark. (There are some notes here:; http://www.cloudera.com/documentation/enterprise/latest/topics/cm_mc_isilon_service.html). Note however that you lose the benefits of locality, so it won't be as; fast as HDFS. Definitely worth a try. Also, for a pipeline with; multiple steps, you could use HDFS to store intermediate data, only; reading from Isilon for the source files and writing to Isilon with; the final result.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1508:371,pipeline,pipeline,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1508,1,['pipeline'],['pipeline']
Deployability,"From a researcher in the field. Their data processing would be much simpler if GenomicsDB accepted non-diploid and mixed-ploidy cases. Currently, researcher is encountering challenges to a workaround that uses CombineGVCFs (a GATK3 tool). ---. As an update, looks like `GenomicsDBImport` only supports diploid data, so we cannot use it. Would really appreciate your help on this. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/53201#Comment_53201",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5383:250,update,update,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5383,1,['update'],['update']
Deployability,Funcotator - Merged updates for many issues,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3952:20,update,updates,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3952,1,['update'],['updates']
Deployability,Funcotator - Need Performance Upgrades,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4586:30,Upgrade,Upgrades,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4586,1,['Upgrade'],['Upgrades']
Deployability,Funcotator - Need to update M2 WDL to allow for all options to be passed in for Funcotator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4587:21,update,update,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4587,1,['update'],['update']
Deployability,Funcotator - Update documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4654:13,Update,Update,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4654,1,['Update'],['Update']
Deployability,Funcotator - integrate WDL into M2 WDL & automate testing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4088:13,integrat,integrate,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4088,1,['integrat'],['integrate']
Deployability,Funcotator - review and finalize data sources release numbers / versioning,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4582:46,release,release,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4582,1,['release'],['release']
Deployability,Funcotator - update WDL to include exclude annotations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5141:13,update,update,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5141,1,['update'],['update']
Deployability,Funcotator / Clinical Pipeline should move from ExAC to gnomAD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5259:22,Pipeline,Pipeline,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5259,1,['Pipeline'],['Pipeline']
Deployability,Funcotator Update for Datasource Release V1.8,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8512:11,Update,Update,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8512,2,"['Release', 'Update']","['Release', 'Update']"
Deployability,Funcotator currently ignores transcript version numbers when doing internal comparisons. . There should be a flag to enable transcript ID version checking (but it should remain off by default). This will involve updates to:; - Funcotator.java; - GencodeFuncotationFactory.java; - SimpleXsvFuncotationFactory.java. And possibly other classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5558:212,update,updates,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5558,1,['update'],['updates']
Deployability,"Funcotator doc updates: produces `TCGA MAF` files, not `GDC MAF` files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7424:15,update,updates,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7424,1,['update'],['updates']
Deployability,Funcotator hg38 data source not working; configuration files contains reference to files not present,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521:41,configurat,configuration,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521,1,['configurat'],['configuration']
Deployability,Funcotator needs more complete integration tests. These tests should better verify that the produced results are correct.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4344:31,integrat,integration,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4344,1,['integrat'],['integration']
Deployability,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4771:262,update,update,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771,2,['update'],['update']
Deployability,"Funcotator produces MAF files in accordance with `TCGA MAF Spec v2.4.1` ([TCGA_MAF_SPEC_2_4_1.tar.gz](https://github.com/broadinstitute/gatk/files/7008573/TCGA_MAF_SPEC_2_4_1.tar.gz)). The current public MAF format is [`GDC MAF v1.0.0`](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/). . All our documentation must be updated to reflect this. Worse, the TCGA MAF 2.4.1 Spec is now locked behind a login screen and may not be available at all. When I sent an email about this, I was told it would change soon. That was over 2 years ago.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7424:329,update,updated,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7424,1,['update'],['updated']
Deployability,Funcotator should be able to toggle between using and ignoring transcript ID version numbers.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5558:29,toggle,toggle,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5558,1,['toggle'],['toggle']
Deployability,"GATK SV pipeline typically run sucessfully in 1 hour 20 minutes for a 30x WGS crams. However, there will be some crams that get stuck at stage 7 with little progress. These never finish and need to be killed. Any suggestions for this? . Below is some stage info.... Stage 7	. collect at FindBreakpointEvidenceSpark.java:738 +details. org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getKmerIntervals(FindBreakpointEvidenceSpark.java:738); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getKmerAndIntervalsSet(FindBreakpointEvidenceSpark.java:532); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.addAssemblyQNames(FindBreakpointEvidenceSpark.java:489); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.gatherEvidenceAndWriteContigSamFile(FindBreakpointEvidenceSpark.java:174); org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark.runTool(StructuralVariationDiscoveryPipelineSpark.java:147); org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); org.broadinstitute.hellbender.Main.main(Main.java:288); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635:8,pipeline,pipeline,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635,1,['pipeline'],['pipeline']
Deployability,"GATK Version 4.0.9.0. What is the issue:; When using the -OBI flag in e.g. ApplyBQSR, GATK uses an inconsistent pattern for adding index extensions. When using BAM format as output:; my_file.bam; my_file.bai. When using CRAM format as output:; my_file.cram; my_file.cram.bai. Since e.g. samtools uses the second pattern for both BAM and CRAM (well, actually they have .crai, but that is a different discssion), I think it would be sensible to adopt that schema. It's not a huge deal, but I tripped over it when writing a pipeline where the user could specify the desired output format - and noticed that there is this odd difference.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5299:521,pipeline,pipeline,521,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5299,1,['pipeline'],['pipeline']
Deployability,GATK public doesn't have any example of an `AssemblyRegionWalker` and no integration tests. This makes it easy to accidentally break the haplotype caller without realizing it. We should have an ExampleAssemblyRegionWalker and integration tests for it to prevent this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2172:73,integrat,integration,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2172,2,['integrat'],['integration']
Deployability,GATK upgrade from 4.0.11.0 to 4.1.0.0 seems to be breaking CalculateContamination,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5880:5,upgrade,upgrade,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880,1,['upgrade'],['upgrade']
Deployability,"GATK4 depends on a beta release of Protocol Buffer 3.0.0, specifically ""3.0.0-beta-1"" because other versions break the tests depending on hadoop.dfs.minicluster.utils. Eventually, we must move up to a stable version of protocol buffers i.e. either .3.0.0 or higher.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2437:24,release,release,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2437,1,['release'],['release']
Deployability,"GATK4 needs to work on cram files. We need to add tests to check that we can process CRAM files and get results equivalent to BAM. At the minimum, this needs to cover adding tests for:; - [x] MarkDuplicates; - [x] BQSR; - [x] PrintReads; - [x] CountReads ; - [x] CountBases; - [x] MeanQualityByCycle; - [x] QualityScoreDistribution; - [ ] CalculateHsMetrics; - [ ] CollectGCBiasMetrics; - [x] CollectBaseDistributionByCycle; - [x] CollectQualityYieldMetrics; - [x] CollectInsertSizeMetrics; - [x] CollectAlignmentSummaryMetrics; - [ ] all picard tools in the sam package. Ideally also unit/integration tests for :; - [ ] HaplotypeCaller; - [ ] CNV coverage collection. (Make sure to use non-lossy settings when converting our BAMs to CRAM)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/590:590,integrat,integration,590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/590,1,['integrat'],['integration']
Deployability,GATKPathSpecifier URI class and update FeatureInput.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5526:32,update,update,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5526,1,['update'],['update']
Deployability,"GATKSparkTool.java:362); > 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); > 	at org.broadinstitute.hellbender.Main.main(Main.java:239); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:733); > 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); > 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); > 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); > 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); > Caused by: java.nio.file.ProviderNotFoundException: Provider ""maprfs"" not found; > 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:341); > 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); > 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:143); > 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:226); > 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:178); > 	at org.apache.spark.rdd.NewHadoopRDD$$a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:3573,deploy,deploy,3573,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['deploy'],['deploy']
Deployability,GATKTool pre/post ReadTransformers methods + ReadWalker/LocusWalker integration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085:68,integrat,integration,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085,1,['integrat'],['integration']
Deployability,GKL v0.5.2 is known to hang / fail to load on CentOS - should upgrade to 0.5.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3389:62,upgrade,upgrade,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3389,1,['upgrade'],['upgrade']
Deployability,GL-548 - Update CreateVat code to handle samples that do not contain all population groups.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7965:9,Update,Update,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7965,1,['Update'],['Update']
Deployability,"GRCh37/hs37d5.fa \; --batch-size 400 \; --reader-threads 5; </pre>. It seems that GenomicsDBImport crash after finishing 1 batch for large chromosomes. For example, here I simultaneously run for chr1-12 (not finished yet). For chr5-12, file size of 1 batch is less than 40GB and they successfully finished import batch 1 and running for batch 2 or 3. Thus, the file size for chr5-12 are 59GB now. However, for chr1-4, they just crash in batch 1 for very long time without any error. I have check the memory usage and there is still >35GB free memory for the compute node of each chromosome. Please see the followings for detail:. File size for all chromosomes, the GenomicsDB for chr1-4 is smaller:; <pre>[hcaoad@login-0 GenomicsDB]> du -h --max-depth=1; 59G ./chr10; 59G ./chr6; 50G ./chr2; 59G ./chr12; 59G ./chr9; 59G ./chr5; 59G ./chr7; 48G ./chr1; 59G ./chr11; 59G ./chr8; 40G ./chr4; 41G ./chr3; 647G .; </pre>. Files in GenomicsDB of chr1 batch 1. As you can see, no update for the database since Apr 20 13:34, while current time is Apr 21.; <pre>[hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> pwd; /home/hcaoad/scratch/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/1$1$249250621/.__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> date; Wed Apr 21 11:09:46 HKT 2021; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> ll -h; total 48G; -rwx------ 1 hcaoad boip 260M Apr 20 13:34 AD.tdb; -rwx------ 1 hcaoad boip 203M Apr 20 13:34 AD_var.tdb; -rwx------ 1 hcaoad boip 304M Apr 20 13:34 ALT.tdb; -rwx------ 1 hcaoad boip 146M Apr 20 13:34 ALT_var.tdb; -rwx------ 1 hcaoad boip 353M Apr 20 13:34 BaseQRankSum.tdb; -rwx------ 1 hcaoad boip 7.3G Apr 20 13:34 __coords.tdb; -rwx------ 1 hcaoad boip 132M Apr 20 13:34 DB.tdb; -rwx------ 1 hcaoad boip 3.4G Apr 20 13:34 DP_FORMAT.tdb; -rwx------ 1 hcaoad boip 295M Apr 20 13:34 DP.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7218:1405,update,update,1405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218,1,['update'],['update']
Deployability,GVS / Hail VDS integration test [VS-639],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8086:15,integrat,integration,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086,1,['integrat'],['integration']
Deployability,"GatherBAMFiles and some other tools use BamFileIoUtils.gatherWithBlockCopying to do block transfer of BAM file blocks, but degenerate to decoding individual records for CRAM. A similar optimization should be able to be done for CRAM containers and then integrated with those tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1163:253,integrat,integrated,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1163,1,['integrat'],['integrated']
Deployability,Gatk3 allowed inputs like `-L some_intervals.vcf`. This functionality should be integrated into the `IntervalArgumentCollection` so that it is available to all tools.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/605:80,integrat,integrated,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/605,1,['integrat'],['integrated']
Deployability,Gauss-Legendre integration error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3317:15,integrat,integration,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3317,1,['integrat'],['integration']
Deployability,"Generally, it is easier on users (and people doing deployment) to have multiple files that list the bam and index files in corresponding order. . So two files, rather than one:; ```; sample1.bam; sample2.bam; ....; ```; and; ```; sample1.bai; sample2.bai; ....; ```. For example, changing the input parameters to:. ```; workflow CNVSomaticPanelWorkflow {; # Workflow input files; ....snip....; # The next two parameters are files that list (in corresponding order) the bam files and bam index files, respectively.; File normal_bams; File normal_bam_idxs; ; # Create (bam, bai) pairs for iterating over scatter loop.; Array[Pair[File,File]] normal_pairs = zip(read_lines(normal_bams), read_lines(normal_bam_idxs)); ....snip....; ```. and further down in that file:. ```; ....snip....; scatter (normal_pair in normal_pairs) {; call CollectCoverage {; input:; padded_targets = select_first([PadTargets.padded_targets, """"]),; bam = normal_pair.left,; bam_idx = normal_pair.right,; ref_fasta = ref_fasta,; ref_fasta_fai = ref_fasta_fai,; ref_fasta_dict = ref_fasta_dict,; gatk_jar = gatk_jar,; gatk_docker = gatk_docker; }; }; ....snip....; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3460:51,deploy,deployment,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3460,1,['deploy'],['deployment']
Deployability,"Generate multi-release jars, to automatically select the implementation version when running under Java 9 or newer. This fixes https://github.com/broadinstitute/gatk/issues/7338",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7339:15,release,release,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7339,1,['release'],['release']
Deployability,Generating AllelicCNV like output from new GATK CNV pipeline (WGS),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6685:52,pipeline,pipeline,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6685,1,['pipeline'],['pipeline']
Deployability,"GenomicsDB 1.1.2 is missing the Mac version of the GDB native library (`libtiledbgenomicsdb.dylib`). This PR reverts the two recent GDB PRs (https://github.com/broadinstitute/gatk/pull/6190 and https://github.com/broadinstitute/gatk/pull/6188), and takes us back to the previous release of GenomicsDB, which didn't have this problem. I've asked @nalinigans to submit a fresh PR once there's a GenomicsDB release that fixes this issue, and also to add a GATK test that asserts the `.dylib` is present on the classpath.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6204:279,release,release,279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6204,2,['release'],['release']
Deployability,GenomicsDB upgrade fixes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7257:11,upgrade,upgrade,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7257,1,['upgrade'],['upgrade']
Deployability,"GenomicsDB, update-workspace and the backup recommendation?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6558:12,update,update-workspace,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6558,1,['update'],['update-workspace']
Deployability,GenomicsDB: add support for incremental updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4773:40,update,updates,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4773,1,['update'],['updates']
Deployability,"GenomicsDBImport - Start Date/Time: July 20, 2018 11:49:24 AM EDT; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.135 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 11:49:25.135 INFO GenomicsDBImport - Picard Version: 2.18.7; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:49:25.136 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:49:25.136 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:49:25.136 INFO GenomicsDBImport - Inflater: IntelInflater; 11:49:25.136 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:49:25.136 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:49:25.137 INFO GenomicsDBImport - Initializing engine; 11:49:25.926 INFO IntervalArgumentCollection - Processing 249250621 bp from intervals; 11:49:25.931 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/log/CDL-164-04P-1_0_249250621_genomicsdb; 11:49:26.339 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 11:49:26.339 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 11:49:26.339 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 11:49:26.339 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 11:49:26.339 INFO ProgressMeter - Starting traversal;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:3423,patch,patch,3423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['patch'],['patch']
Deployability,"GenomicsDBImport - Start Date/Time: July 23, 2018 10:24:55 AM EDT; 10:24:56.136 INFO GenomicsDBImport - ------------------------------------------------------------; 10:24:56.136 INFO GenomicsDBImport - ------------------------------------------------------------; 10:24:56.137 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 10:24:56.138 INFO GenomicsDBImport - Picard Version: 2.18.7; 10:24:56.138 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:56.138 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:56.138 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:56.138 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:56.138 INFO GenomicsDBImport - Deflater: IntelDeflater; 10:24:56.138 INFO GenomicsDBImport - Inflater: IntelInflater; 10:24:56.139 INFO GenomicsDBImport - GCS max retries/reopens: 20; 10:24:56.139 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:24:56.139 INFO GenomicsDBImport - Initializing engine; 10:24:57.198 INFO IntervalArgumentCollection - Processing 249250621 bp from intervals; 10:24:57.205 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/CDL-164-04P-1_0_249250621_genomicsdb; 10:24:57.553 INFO GenomicsDBImport - Vid Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/vidmap.json; 10:24:57.554 INFO GenomicsDBImport - Callset Map JSON file will be written to CDL-164-04P-1_0_249250621_genomicsdb/callset.json; 10:24:57.554 INFO GenomicsDBImport - Complete VCF Header will be written to CDL-164-04P-1_0_249250621_genomicsdb/vcfheader.vcf; 10:24:57.554 INFO GenomicsDBImport - Importing to array - CDL-164-04P-1_0_249250621_genomicsdb/genomicsdb_array; 10:24:57.554 INFO ProgressMeter - Starting traversal; 10:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:9295,patch,patch,9295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['patch'],['patch']
Deployability,"GenomicsDBImport lets users writes variants to GenomicsDB. The inputs are a loader JSON configuration file, callsets JSON file containing sample names and corresponding stream names and a stream JSON file containing files names of the streams. Note: This code uses GenomicsDB v0.4.0. Please check whether Maven central has the updated version first. @kgururaj , please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389:88,configurat,configuration,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"GenomicsDBImport; Latest public release version [4.2.0.0]. I am running GenomicsDBImport on 5X WGS of ~1000 human samples, which is parallelized for each chromosome with batch size = 400. The code I use is as following:. <pre>gatk --java-options ""-Xmx80G -Xms80G"" GenomicsDBImport \; --genomicsdb-workspace-path ""${outpath}/chr${region}"" \; -L $region \; --sample-name-map $sample_map \; -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa \; --batch-size 400 \; --reader-threads 5; </pre>. It seems that GenomicsDBImport crash after finishing 1 batch for large chromosomes. For example, here I simultaneously run for chr1-12 (not finished yet). For chr5-12, file size of 1 batch is less than 40GB and they successfully finished import batch 1 and running for batch 2 or 3. Thus, the file size for chr5-12 are 59GB now. However, for chr1-4, they just crash in batch 1 for very long time without any error. I have check the memory usage and there is still >35GB free memory for the compute node of each chromosome. Please see the followings for detail:. File size for all chromosomes, the GenomicsDB for chr1-4 is smaller:; <pre>[hcaoad@login-0 GenomicsDB]> du -h --max-depth=1; 59G ./chr10; 59G ./chr6; 50G ./chr2; 59G ./chr12; 59G ./chr9; 59G ./chr5; 59G ./chr7; 48G ./chr1; 59G ./chr11; 59G ./chr8; 40G ./chr4; 41G ./chr3; 647G .; </pre>. Files in GenomicsDB of chr1 batch 1. As you can see, no update for the database since Apr 20 13:34, while current time is Apr 21.; <pre>[hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> pwd; /home/hcaoad/scratch/Han/WGS/HK_WGS_5X/GenomicsDB/chr1/1$1$249250621/.__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> date; Wed Apr 21 11:09:46 HKT 2021; [hcaoad@login-0 .__6f46d0f3-86b3-4e38-92be-9a912291df0c139544093255424_1618555702611]> ll -h; total 48G; -rwx------ 1 hcaoad boip 260M Apr 20 13:34 AD.tdb; -rwx--",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7218:32,release,release,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218,1,['release'],['release']
Deployability,Genotyping code for the Gnarly Pipeline (gnomAD v3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947:31,Pipeline,Pipeline,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947,1,['Pipeline'],['Pipeline']
Deployability,Get runtime statistics from Broad production on the MarkDupes + BQSR portion of the pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/765:84,pipeline,pipeline,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/765,1,['pipeline'],['pipeline']
Deployability,GetPileupSummaries tool is adjusted to use the standard MappingQualityReadFilter as other walkers. Default value is set to 50 as in the original one. Old parameter set is removed and documentation is updated.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8781:200,update,updated,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8781,1,['update'],['updated']
Deployability,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715:725,release,released,725,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715,1,['release'],['released']
Deployability,"Going ahead and making this change so TAG team can start trying out the ModelSegments pipeline. It's possible that we could use NIO for other files (reference, interval lists), but this will not have as much impact as using it for BAMs (and in some cases, decreases performance, see comments in #4806). Closes #4806.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5015:86,pipeline,pipeline,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015,1,['pipeline'],['pipeline']
Deployability,"Google is deprecating and removing their implementation of the old style GA4GH read and reference API's. . > ; > Reads API functionality is now replaced by the htsget protocol ; > ; > This year, the GA4GH team introduced the htsget protocol to allow users to download read data for subsections of the genome in which they are interested. This is a richer and more flexible approach to working with reads data. It allows you to keep your genomics data in a common BAM file format on Google Cloud Storage and work with it efficiently from your computation pipelines, using standard bioinformatics tools. We have already launched our own open source implementation of this protocol, which you can use to access your reads data. Many popular tools such as samtools and htslib have been updated by the community to support htsget. Documentation is provided here. The Reads API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month by those receiving this notice, whichever comes first. ; > ; > Variants API is now replaced by htsget and Variant Transforms ; > ; > The GA4GH team also plans to extend the htsget protocol to cover variant data, and we will extend our implementation of htsget to cover this use case. ; > ; > After analyzing usage of the Variants API, we found that users primarily used it to import variant data and then export it to BigQuery. To save time and effort, we created Variant Transforms, an open source tool for directly importing VCF data into BigQuery. Variant Transforms and its documentation are published here. Variant Transforms is more scalable than the legacy Variants API, and it has a robust roadmap with a dedicated team. We also welcome collaborators on this project as it advances. ; > ; > The Variants API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month, whichever comes first. ; > ; > We are excited to move in step with the global ge",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166:554,pipeline,pipelines,554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,"Google made an incompatible change to the dataproc api which is causing all builds to fail. We're seeing errors like this:; ```; ERROR: (gcloud.beta.dataproc.clusters.create) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; ```. It's mentioned in gcloud release notes here:; ```; 260.0.0 (2019-08-27); Breaking Changes; (Cloud Dataproc) Modified --region flag to be mandatory.; To use Cloud Dataproc commands, pass the --region flag on every invocation, or set the dataproc/region configuration variable via gcloud config set dataproc/region.; For gcloud beta dataproc commands, this flag/config value is required.; For gcloud dataproc commands, the default will remain global until January 2020.; ```. I'm going to set the environment variable in our travis config right now, and then open a separate PR to specify region in all the commands.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6129:532,release,release,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6129,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"Gradle 2.12 just released which includes some improvements we've been waiting for. It includes a ""compileOnly"" scope which should make some of our spark configuration unnecessary. We should investigate if we can simplify the sparkJar setup using the new scope, and possible improve things for gatk-protected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1578:17,release,released,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1578,2,"['configurat', 'release']","['configuration', 'released']"
Deployability,"Greetings,. I would like to be able to run spark reads pipeline from the jar file without the gatk wrapper script. My reason for this is that I already have existing infra for spark jobs that I would like to use. Specifically, I would like to be able to run gatk jobs with the kubernetes [spark operator](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator). This requires being able to give the SparkApplication CRD a main class location and a jar file. Is this feasible with gatk?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6198:55,pipeline,pipeline,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6198,1,['pipeline'],['pipeline']
Deployability,GroundTruthScorer doc update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8597:22,update,update,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8597,1,['update'],['update']
Deployability,"GvsCreateFilterSet.wdl failed recently for Morgan because of this bug. When run in a brand new project, filter model creation fails because we expect the project to have a hard coded dataset named ""temp_tables"" which is likely does not have. The workaround is simply to manually create one. This ticket removes the need for this dataset altogether. This is removed, and instead, the default dataset is used (that the many other tables created in this pipeline use as the default). able to reproduce with a dummy dataset name:; <img width=""1278"" alt=""Screen Shot 2022-03-03 at 10 44 39 PM"" src=""https://user-images.githubusercontent.com/6863459/156822409-a99d7068-169c-48a2-83ff-5bcc81cdbd2e.png"">. tested here:; https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_ingest/job_history/1dd27d90-82c4-44e6-8172-15c10c8a9c7f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7704:451,pipeline,pipeline,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7704,1,['pipeline'],['pipeline']
Deployability,HDF5 integration.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/498:5,integrat,integration,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/498,1,['integrat'],['integration']
Deployability,"HI @sooheelee, could you take a look at the updated documentation?; Thank you, Marton",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3989:44,update,updated,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3989,1,['update'],['updated']
Deployability,"Hadoop-BAM 7.5.0 has not been released yet, so this will fail.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1817:30,release,released,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1817,1,['release'],['released']
Deployability,"Hadoop-BAM and htsjdk upgrades, Spark tool changes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1469:22,upgrade,upgrades,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1469,1,['upgrade'],['upgrades']
Deployability,HaplotypeCaller: add a good integration test for the --max-alternate-alleles argument,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4497:28,integrat,integration,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4497,1,['integrat'],['integration']
Deployability,HaplotypeCaller: omnibus updates and fixes for 4.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3519:25,update,updates,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3519,1,['update'],['updates']
Deployability,HaplotypeCallerIntegrationTest: add a boolean toggle to update the expected outputs for all exact-match-based tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5324:46,toggle,toggle,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5324,2,"['toggle', 'update']","['toggle', 'update']"
Deployability,Has anyone reported the new release 4.0.12.0 CalculateContamination's BUG?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5536:28,release,release,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5536,1,['release'],['release']
Deployability,Has updated optical duplicate finding code for MarkDuplicatesSpark.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5597:4,update,updated,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5597,1,['update'],['updated']
Deployability,"Have been having the following issue running StructuralVariationDiscoveryPipeline on previous and the newest release of GATK(4.1.0.0). Currently attempting to use on a computing cluster without spark enabled. . Command line used:; gatk/gatk-4.1.0.0/gatk StructuralVariationDiscoveryPipelineSpark \; -I $CRAM \; -R $Hg38 \; --aligner-index-image reference.fasta.Hg38.img \; --kmers-to-ignore kmers_to_ignore_hg38.txt \; --contig-sam-file aligned_contigs.sam \; -O ${base}_GATK_SV_output.vcf . **Error Log**:; 19/02/01 21:28:27 INFO TaskSetManager: Starting task 700.0 in stage 5.0 (TID 4405, localhost, executor driver, partition 700, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:27 INFO Executor: Running task 700.0 in stage 5.0 (TID 4405); 19/02/01 21:28:27 INFO TaskSetManager: Finished task 668.0 in stage 5.0 (TID 4373) in 37331 ms on localhost (executor driver) (669/741); 19/02/01 21:28:27 INFO BlockManagerInfo: Removed taskresult_4373 on 10.120.16.54:34926 in memory (size: 1645.1 KB, free: 15.8 GB); 19/02/01 21:28:27 INFO NewHadoopRDD: Input split: file: /cram8/1-00004__CG0000-1789.GMKF2.cram:23488102400+33554432; 19/02/01 21:28:28 ERROR Executor: Exception in task 698.0 in stage 5.0 (TID 4403); **java.lang.IllegalArgumentException: provided start is negative: -24**; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.ha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5647:109,release,release,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647,1,['release'],['release']
Deployability,Have updated the gatk override jar with a fresh build (ah_var_store on 7/8/2022); And updated the docker similarly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7934:5,update,updated,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7934,2,['update'],['updated']
Deployability,Heap Space error in PopulateFilterSetInfo. Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50d867c5-3f6f-405b-8e7b-a714ab7e806f).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8575:51,Integrat,Integration,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8575,1,['Integrat'],['Integration']
Deployability,Hellbender should export unit/integration testing utils as a maven artifact.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/525:30,integrat,integration,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/525,1,['integrat'],['integration']
Deployability,"Hello GATK team!. ## Bug Report. ### Affected tool(s) or class(es). mutect2. ### Affected version(s); - Latest public release version: 4.2.6.1. ### Description . getting fail for all scatter task with the argument ""--emit-ref-confidence GVCF"", no fails without it. error:; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar GetSampleName -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -O tumor_name.txt -encode --gcs-project-for-requester-pays broad-firecloud-ccle; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.b3fd1830; 14:13:40.205 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:13:40.275 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.276 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.2.6.1; 14:13:40.277 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:13:40.277 INFO Mutect2 - Executing as root@0b46ce3a6ba5 on Linux v5.10.107+ amd64; 14:13:40.277 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:13:40.278 INFO Mutect2 - Start Date/Time: May 13, 2022 2:13:40 PM GMT; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.278 INFO Mutect2 - ------------------------------------------------------------; 14:13:40.279 INFO Mutect2 - HTSJDK Version: 2.24.1; 14:13:40.280 INFO Mutect2 - Picard Version: 2.27.1; 14:13:40.284 INFO Mutect2 - Built for Spark Version: 2.4.5; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:13:40.284 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:13:40.285 INFO Mutect2 - H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:118,release,release,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['release'],['release']
Deployability,"Hello again, I have not been able to solve the problem with the launch ReadsPipelineSparkMulticore.wdl. :(; ![qoKs8pEt_-0](https://user-images.githubusercontent.com/55628707/161521914-49b7b10f-3264-43d5-8f5c-739d924656a1.jpg); I use this command:; `java -jar ../cromwell-77.jar run ReadsPipelineSparkMulticore.wdl -i exome/ReadsPiplineSpark_exome.json`; This is my json file:; ![qo2S-PSVs_xNXb5ZysWT8g](https://user-images.githubusercontent.com/55628707/164080985-ae983a19-104a-4742-9401-82ea938ef69e.jpeg); Here is my configuration (CPU and RAM):; ![PEP_IbcPaXyZtql0oefE_A](https://user-images.githubusercontent.com/55628707/164081131-58958f86-5aaf-450f-b985-d7885d4a8de4.jpeg); ![FDmF-6wr4RelT11qqByfGA](https://user-images.githubusercontent.com/55628707/164081137-2d07fe83-845e-463e-ab17-7a5cecb415e0.jpeg). Found this error in my stderr file:; 22/04/06 15:36:17 ERROR Executor: Exception in task 10.0 in stage 11.0 (TID 1596); java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateFractionalErrorArray(BaseRecalibrationEngine.java:440); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:141); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$null$0(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$705/136574652.accept(Unknown Source); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at org.broadinstitute.hellbender.utils.iterators.CloseAtEndIterator.forEachRemaining(CloseAtEndIterator.java:47); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796:519,configurat,configuration,519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796,1,['configurat'],['configuration']
Deployability,"Hello there,. ## Documentation request. It would be awesome if each major/minor release of GATK contained benchmarking results run against a truth set. For calling short germline variants, you could evaluate precision and recall using one of the NIST samples (i.e. HG002) and for calling short somatic variants you could use the SEQC2 tumor-normal pair. . I would like to see how new releases of GATK and how your recommendations/best practices impact precision and recall against a known truth set. This transparency would help anyone using your tools decide if it's worth upgrading to a new release of GATK, or if their implementation of your recommended set of best practices is performing as expected. . If this information could be added to the best practices pages, a relevant tutorial, or a new section of the documentation (containing hap.py/som.py benchmarking information and run times for a set of tested tools and/or workflows), that would be great!. Please let me know what you think, and if you have any questions. Best regards,; @skchronicles",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/9019:80,release,release,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9019,3,['release'],"['release', 'releases']"
Deployability,"Hello, i'm trying to run GATK4 BaseRecalibratorSpark in local in a nextflow pipeline, but I got every time the same error with this command :. **gatk-launch BaseRecalibratorSpark --spark-runner LOCAL --spark-master local[23] -R $indexbit -I ${input_bam} --known-sites ${params.dbsnpAll} -L ${params.targetBed} -O ${output_name}-recalibration_report.grp**. Here is the full error output, basically, the failure seems to be linked to an absence of activity of the BaseRecalibratorSpark during 12s which kill the spark heartbeat and then the processus :. ```; 18/03/09 09:22:08 WARN Executor: Issue communicating with driver in heartbeater; java.lang.NullPointerException; at org.apache.spark.storage.memory.MemoryStore.getSize(MemoryStore.scala:127); at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(BlockManager.scala:387); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:218); at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:76,pipeline,pipeline,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['pipeline'],['pipeline']
Deployability,"Hello, thanks for great software.; After GATK 4.1.8.0 release we updated our internal Docker containers (from GATK v4.1.7.0) and noticed changes in Haplotype Caller results:. [check_against_37.woRandomLine.vcf.txt](https://github.com/broadinstitute/gatk/files/4864731/check_against_37.woRandomLine.vcf.txt); [test_v37.haplotypecaller.woRandomLine.vcf.txt](https://github.com/broadinstitute/gatk/files/4864733/test_v37.haplotypecaller.woRandomLine.vcf.txt). Here is the difference:; ```; 17	7571487	rs17880560	A	AGCCGTG	166.10	.	AC=2;AF=1.00;AN=2;DB;DP=4;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=28.73;SOR=0.693	GT:AD:DP:GQ:PL	1/1:0,4:4:12:180,12,0; 17	7571487	rs17880560;rs79948390	A	AGCCGTG	166.10	.	AC=2;AF=1.00;AN=2;DB;DP=4;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=28.73;SOR=0.693	GT:AD:DP:GQ:PL	1/1:0,4:4:12:180,12,0; ```; ```; 17	7578711	rs141204613	CTTT	C	232.93	.	AC=2;AF=1.00;AN=2;DB;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.83;QD=30.97;SOR=1.329	GT:AD:DP:GQ:PL	1/1:0,6:6:18:247,18,0; 17	7578711	rs141204613;rs5819162	CTTT	C	232.93	.	AC=2;AF=1.00;AN=2;DB;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.83;QD=30.97;SOR=1.329	GT:AD:DP:GQ:PL	1/1:0,6:6:18:247,18,0; ```; ```; 17	7579643	rs150200764	CCCCCAGCCCTCCAGGT	C	1834.03	.	AC=2;AF=1.00;AN=2;DB;DP=52;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=61.03;QD=27.24;SOR=0.843	GT:AD:DP:GQ:PL	1/1:0,41:41:99:1848,125,0; 17	7579643	rs150200764;rs146534833;rs59758982	CCCCCAGCCCTCCAGGT	C	1834.03	.	AC=2;AF=1.00;AN=2;DB;DP=52;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=61.03;QD=27.24;SOR=0.843	GT:AD:DP:GQ:PL	1/1:0,41:41:99:1848,125,0; ```; New file (test_v37) contains multiple `rsID` in `ID` field. Is that expected behavior or not? ; I can't find any info about in in changelog.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6690:54,release,release,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6690,2,"['release', 'update']","['release', 'updated']"
Deployability,"Hello,. Do you have an estimate on when you might be releasing the next minor GATK release? I'm hoping to pick up the GenomicsDB update (https://github.com/broadinstitute/gatk/commit/8796404cab594b716d43755f61fd405c92208141). I realize we could build our own, but for one of our projects we release a public dataset with documentation on the tools, and it's nicer to use official releases. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7322:83,release,release,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7322,4,"['release', 'update']","['release', 'releases', 'update']"
Deployability,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:137,install,install,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,2,['install'],"['install', 'installed']"
Deployability,"Hello,. I am trying to use gatk/4.1.4.1 and picard/2.22.0 to do joint variant calling of PacBio HiFi reads and Illumina short-reads. ; My pipeline is basically the recommended one (without base recalibration) where after HaplotypeCaller, I use GenomicsDBImport and GenotypeGVCFs and end up with the vcf file. The HiFI reads have normal hifi phred scores up to 93 and the illumina are encoded with phred33 quality scores. The output of the joint variant calling is strange and it causes issues when I try to use it with tools like plink and it makes me wonder if the joint variant calling went badly as well. This is even after variant filtration where I filter for QUAL<30 and QD<2. Here is an example of what the first few lines of my vcf calls look like. Notice the QUAL column that looks like Num,Num:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Illumina_sample0 sample1 sample2 sample_3 sample4 sample5 sample6 hifi_sample; Chr1 10608 . GCA G 174,34 PASS AC=2;AF=0.143;AN=14;BaseQRankSum=0.00;DP=55;ExcessHet=3.3579;FS=7.404;MLEAC=2;MLEAF=0.143;MQ=57.97;MQRankSum=0.00;QD=15.85;ReadPosRankSum=1.00;SOR=3.611 GT:AD:DP:GQ:PL ./.:0,0:0:.:0,0,0 0/1:3,3:6:99:117,0,117 0/0:5,0:5:15:0,15,141 0/1:3,2:5:71:71,0,120 0/0:3,0:3:9:0,9,113 0/0:7,0:7:21:0,21,190 0/0:5,0:5:15:0,15,175 0/0:20,0:20:60:0,60,900; Chr1 10616 . G A 903,42 PASS AC=8;AF=0.571;AN=14;BaseQRankSum=0.00;DP=66;ExcessHet=0.7136;FS=9.277;MLEAC=9;MLEAF=0.643;MQ=59.80;MQRankSum=0.00;QD=20.08;ReadPosRankSum=1.00;SOR=0.251 GT:AD:DP:GQ:PL ./.:0,0:0:.:0,0,0 0/0:10,0:10:30:0,30,367 1/1:0,5:5:15:141,15,0 0/1:2,3:5:75:110,0,75 0/0:3,0:3:9:0,9,113 1/1:0,8:8:24:232,24,0 1/1:0,7:7:21:224,21,0 0/1:14,6:20:99:210,0,570. ```. I believe this can be reproduced with any illumina+hifi samples. Are you aware of this problem? Could the snp calling be wrong because of the different phred score encoding? What do you suggest to do in these cases?. I have read your response in this issue: ; https://gatk.broadinstitute.org/hc/en-us/community/pos",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8372:138,pipeline,pipeline,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8372,1,['pipeline'],['pipeline']
Deployability,"Hello,. I'm trying to use the CNVGermline Pipeline with Nextflow and a GATK Singularity image pulled from your Docker image. . As you can see, I'm trying to run the DetermineGermlineContigPloidy with 57 samples, and I got a permission error within my Singularity container. This error is directly related to Singularity permissions to create a directory ('/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'), because it's running fine with Singularity when I'm root and Docker (without root). So it's not really a GATK4 problem but more a singularity-GATK4 related problem. Maybe your GATK4 Germline CNV calling pipeline is not designed to work as a Singularity container (which is quite understandable), because for tools like HaplotypeCaller, MarkDuplicates ... It's working fine. I didn't know where to post this error, singularity or GATK github, so why not both ! . Do you have any idea make it run properly ? Change a bit the design of your CNV pipeline to make it compatible with Singularity ?. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.0.4.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. GATK4 : ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/build/libs/gatk-package-4.0.4.0-local.jar DetermineGermlineContigPloidy --input 2044098202-8046_S5_sample.counts.hdf5 --input 2045946179-9076_S2_sample.counts.hdf5 --input 2045946166-9075_S1_sample.counts.hdf5 --input 2048220927-11022_S4_sample.counts.hdf5 --input 2045599261-9046ci_S1_sample.counts.hdf5 --input 2046745668-1007_S5_sample.counts.hdf5 --input 2044098101-8043_S2_sample.counts.hdf5 --input 2044098168-8044_S3_sample.counts.hdf5 --input 2046746598-1012_S4_sample.counts.hdf5 --input 2044395763-8064ci_S4_sample.counts.hdf5 --input 2044395647-8061ci_S1_sample.counts.hdf5 --input 70-20-CI_S3_sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782:42,Pipeline,Pipeline,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782,3,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,"Hello,. I'm using the GATK4 docker image to evaluate the efficiency of my targeted experiment. After googling around, I realize that CollectHsMetrics is the tool to use from now on (correct me if i'm wrong). The old DepthOfCoverage, CalculateTargetCoverage, CalculateHsMetrics are not available anymore, so I'm trying to implement this utility in our pipeline. Given the fact I'm providing the same file as the BAITS_INTERVALS and TARGETS_INTERVALS, I'd expect the PCT_OFF_BAIT and PCT_EXC_OFF_TARGET to be the same, but then I [read here](https://gatkforums.broadinstitute.org/gatk/discussion/7442/collecthsmetrics) that the ON_BAIT_BASES and ON_TARGET_BASES may actually differ due to the fact one relies on the NEAR_DISTANCE argument (that only applies on the ON_BAIT_BASES) and other depends on some read filters (MINIMUM_BASE_QUALITY and MINIMUM_MAPPING_QUALITY that may apply to ON_TARGET_BASES, didn't find any documention on that). Indeed, I observed the described differences in the ON_BAIT_BASES/ON_TARGET_BASES metrics:. > ON_BAIT_BASES 1733719583; ON_TARGET_BASES 1274990601. which was reflected in the PCT_SELECTED_BASES (Fraction of bases from reads passing the filters located on or near a baited region):; > PCT_SELECTED_BASES 0.690911. Then, for the PCT_EXC_OFF_TARGET (Fraction of aligned bases that were filtered out because they did not align over a target base.) I got:; >PCT_EXC_OFF_TARGET 0.731878. which doesn't make sense to be so high. Shouldn't this value be close to the result of (`1-PCT_SELECTED_BASES`) , given that the BAITS and TARGETS intervals file is the same ?. Sending this example attached.; [H2106.1_HS_metrics.txt](https://github.com/broadinstitute/gatk/files/1643326/H2106.1_HS_metrics.txt). Best,; Pedro Barbosa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4200:351,pipeline,pipeline,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4200,1,['pipeline'],['pipeline']
Deployability,"Hello,. I'm using the latest docker image to test GATK4 pipeline (v 4.0.1.2) and I'm struggling to get it done. Apparently it has to do with the task manager service, please find attached the full log of one of my runs.; [65670_gatk_hc.log](https://github.com/broadinstitute/gatk/files/1717363/65670_gatk_hc.log). Best,; Pedro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4393:56,pipeline,pipeline,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4393,1,['pipeline'],['pipeline']
Deployability,"Hello,. It seems the parameter `--sequence-dictionary` does not change the dictionary looked by **HaplotypeCaller**. ```; averdier@bioinfo:~/test/dna-seq-pipeline$ ./dna-seq-pipeline.pl -1 CACTTCGA-ACACGACC_S156_L003_R1_001.fastq.gz -2 CACTTCGA-ACACGACC_S156_L003_R2_001.fastq.gz -r Triticum_aestivum_Claire_EIv1.1.fa.gz -s ClaireTest --nb_threads 30; --mem_limit 100; Mapping; Mark Duplicates; Variants Calling; 09:54:54.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 11, 2020 9:54:54 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:54:54.730 INFO HaplotypeCaller - ------------------------------------------------------------; 09:54:54.731 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 09:54:54.731 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:54:54.731 INFO HaplotypeCaller - Executing as averdier@bioinfo on Linux v4.4.0-178-generic amd64; 09:54:54.731 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.04-b01; 09:54:54.731 INFO HaplotypeCaller - Start Date/Time: September 11, 2020 9:54:54 AM CEST; 09:54:54.731 INFO HaplotypeCaller - ------------------------------------------------------------; 09:54:54.731 INFO HaplotypeCaller - ------------------------------------------------------------; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Version: 2.21.2; 09:54:54.732 INFO HaplotypeCaller - Picard Version: 2.21.9; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:54:54.732 INFO HaplotypeCaller - HTSJDK Defaults.USE_A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6808:154,pipeline,pipeline,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6808,2,['pipeline'],['pipeline']
Deployability,"Hello,. There was at least one prior conversation about migrating or not migrating GATK3 CombineVariants to GATK4. My understanding is that there was a decision in GATK not to migrate CombineVariants, and instead push people to use Picard MergeVcfs. As you know, Picard MergeVcfs is somewhat similar; however, it doesnt merge genotypes. That is a pretty big difference in function. . CombineVariants is one of the few GATK3 tools my lab is still using. I'd like to move us off GATK3 in the coming months. Given GATK has already decided not to migrate it, I would first like to propose that we could port and take it over in my lab's DISCVRseq project (https://github.com/bimberlab/discvrseq). I'm happy to give attribution to GATK, etc. I would likely rename it MergeVcfsAndGenotypes (this is more intuitive to me), but I would otherwise not change functionality much. I'd prefer to do this instead of porting to GATK4 because porting to GATK is going to throw up a lot more obstacles and probably require that I modernize/update a good amount of that tool's code. I appreciate why this is required, but it takes a lot more work from us. If you did not like this, I'm open to considering porting to GATK4. In my initial review, it looked like CombineVariants was fairly self-contained and that most of the accessory code (merging genotypes is the most complex thing) was already migrated to GATK4. Some of you may have already done a more thorough review of it. . What do you think?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7038:1023,update,update,1023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7038,1,['update'],['update']
Deployability,"Hello,. To our thinking, one of the compelling reasons to use GenomicsDB over Combined gVCFs is that ability to incrementally append samples, and this is supported through --genomicsdb-update-workspace-path. However, you docs state: ""It is recommended that users backup existing genomicsdb workspaces before adding new samples using --genomicsdb-update-workspace-path. If the tool fails during incremental import for any reason, the workspace may be in an inconsistent/corrupted state"". These files are not small. Given that recommendation, wouldnt it make just as much sense to let the user specify the workspace-to-update, the input gVCFs, and specify an alternate output path where the merged data is written? To us, the key feature is incremental sample append, not update-in-place. Is the user wanted to update the original copy, after successful completion of GenomicsDB they could certainly delete the original and move the new copy into that location? Not to mention, in theory one could have some job trying to read the original workspace, which might get hosed if some other job is trying to edit that workspace in place. Thanks for any thoughts or comments. -Ben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6558:185,update,update-workspace-path,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6558,5,['update'],"['update', 'update-in-place', 'update-workspace-path']"
Deployability,"Hello,. when running the cnn I have this error : ; File ""<stdin>"", line 1, in <module>; AttributeError: 'module' object has no attribute 'get_metric_dict'. As I know you are working on another version, I am not totally sure if it is appropriate to report this issue for the current version. You can ignore it I can wait for the other version release. . Thanks a lot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4591:342,release,release,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4591,1,['release'],['release']
Deployability,"Hello,; I am missing the SparkGenomeReadCounts tool in the new release which was included in the beta. Will it be included again soon? What was the reason for removing it? Is there an alternative tool in the new release?. Best; ST",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4185:63,release,release,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4185,2,['release'],['release']
Deployability,"Hello.; I am not sure if this is a bug or simply a tool version problem but when running the tests with `./gradlew test` I get 18 failures with the `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest` class. I wanted to make sure GATK was working right when compiled it from source to have a working base a I intend to try out some experimental modifications to the code. ## Bug Report. ### Affected tool(s) or class(es); `org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest`. ### Affected version(s); - [ ] Latest public release version [version?]; - [x] Latest master branch as of [11.12.18]. ### Description; The following commands were used to build and test GATK; ```; git pull; ./gradlew clean; ./gradlew bundle; ./gradlew test; ```; The tests resulted in 18 failed as can be seen in the attached file. ; [Test results - Class org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip](https://github.com/broadinstitute/gatk/files/2667609/Test.results.-.Class.org.broadinstitute.hellbender.tools.walkers.variantutils.LeftAlignAndTrimVariantsUnitTest.html.zip). If this is normal (expected) when building from the last commit on master you can close this issue. For experimental development should I use the most recent release or can I work from the most recent commit on master ?. #### Steps to reproduce; see commands above description. The problem could be from a library or java version maybe. I run a Ubuntu 16.04 LTS desktop.; ```; uname -a; Linux A13PC04 4.4.0-140-generic #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. javac -version; javac 1.8.0_102. java -version; java version ""1.8.0_102""; Java(TM) SE Runtime Environment (build 1.8.0_102-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode); ```. #### Expected behavior; I was expecting the tests to pass. #### Actual behavior; 18 tests failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5511:599,release,release,599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5511,2,['release'],['release']
Deployability,Here are the changes needed to get the full pipeline running on WGS BAMs. This contains commits from other PRs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3106:44,pipeline,pipeline,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3106,1,['pipeline'],['pipeline']
Deployability,"Here is an issue ticket for adding the diagnosetarget feature to DepthofCoverage. This request was created from a contribution made by Benoit Dewitte on February 09, 2022 13:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-). \--. Hi! ; ; I'm currentlly refactoring ours pipeline which use depthofcoverage and diagnosetarget from GATK 3.8. Despite google researchs I can not found the equivalent of diagnosetarget for gatk 4 and the depthofcoverage tool that i found is still in beta. I found this [github post](https://github.com/broadinstitute/gatk/pull/5913) which talk about pushing the diagnosetarget feature in depthofcoverage but I was not able to find any more informations. Should I stay on the gatk 3.8 version or upgrade our pipe to gatk 4?. I appreciate very much if someone could enlighten me.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270769'>Zendesk ticket #270769</a>)<br> gz#270769</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7702:511,pipeline,pipeline,511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7702,2,"['pipeline', 'upgrade']","['pipeline', 'upgrade']"
Deployability,"Here it is. An overview of what's been added:; - metrics package; - a few general metrics classes (e.g. MultiLevelMetrics); - may want to push these down into HTSJDK later; - added some utils; - utils.gene: gene annotation; - utils.illumina: general Illumina-related utils (adapters, etc); - utils.text.parsers: text parsing; - utils.variant: added dbSNP stuff; - MathUtils: added a few basic things (mean, stddev, etc) with unit tests; - tools; - three major packages:; - analysis: metrics + analyses (including necessary Rscripts); - illumina: Illumina parsing + validation; - vcf: VCF manipulation + GenotypeConcordance; - also two smaller packages, fastq and intervals, containing a few tools each; - tests; - all existing tests were ported; still, overall test coverage goes down by ~6%; - all CLP integration tests have been ported to the new argument system; - test data has also been carried over, and is neatly organized; - there are no huge files, and very few above 100KB (just a few VCFs I think); - however, the Illumina test data is pretty big - ~6MB spread over ~1700 files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/347:803,integrat,integration,803,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/347,1,['integrat'],['integration']
Deployability,"Here the log:. 11:08:24.240 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/results/SOD1/.snakemake/conda/93139e1d/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 19, 2020 11:08:25 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:08:25.663 INFO SplitNCigarReads - ------------------------------------------------------------ ; ; 11:08:25.663 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.8.1 ; ; 11:08:25.663 INFO SplitNCigarReads - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:08:25.664 INFO SplitNCigarReads - Executing as giulia@### on Linux v2.6.32-754.31.1.el6.x86\_64 amd64 ; ; 11:08:25.664 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 ; ; 11:08:25.664 INFO SplitNCigarReads - Start Date/Time: August 19, 2020 11:08:24 AM CEST ; ; 11:08:25.664 INFO SplitNCigarReads - ------------------------------------------------------------ ; ; 11:08:25.664 INFO SplitNCigarReads - ------------------------------------------------------------ ; ; 11:08:25.668 INFO SplitNCigarReads - HTSJDK Version: 2.23.0 ; ; 11:08:25.668 INFO SplitNCigarReads - Picard Version: 2.22.8 ; ; 11:08:25.668 INFO SplitNCigarReads - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 11:08:25.668 INFO SplitNCigarReads - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 11:08:25.668 INFO SplitNCigarReads - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 11:08:25.668 INFO SplitNCigarReads - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 11:08:25.668 INFO SplitNCigarReads - Deflater: IntelDeflater ; ; 11:08:25.669 INFO SplitNCigarReads - Inflater: IntelInflater ; ; 11:08:25.669 INFO SplitNCigarReads - GCS max retries/re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6776:2172,release,release-,2172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6776,1,['release'],['release-']
Deployability,"Hey @mbabadi, I've answered this user question but I think my answer needs your review. Also, DR mentioned perhaps this option could be set automatically for users? Thanks. ---; Thanks @shlee for the update,; I used the latest jar from the gatk4 repo as recommended. And managed to derive the read count input file and sex genotype table. I just wanted to confirm whether Nd4j also needed to be installed if not using Spark. #script run; ./gatk-launch GermlineCNVCaller --contigAnnotationsTable ../gatk4_Hellbender/grch37_contig_annotations.tsv --copyNumberTransitionPriorTable ../gatk4_Hellbender/grch37_germline_CN_priors.tsv --jobType LEARN_AND_CALL --outputPath ./TS1 --input ../gatk4_Hellbender/target_cov.tsv --targets ../gatk4_Hellbender/targets.txt --disableSpark true --sexGenotypeTable ../gatk4_Hellbender/TS1_genotype --rddCheckpointing false --biasCovariateSolverType LOCAL. #I am getting the following error which seems to be linked with Nd4j:. Using GATK jar ~/localwork/playground/programs/gatk-protected/build/libs/gatk-protected-package-b4390fb-SNAPSHOT-local.jar; 102-b14; Version: 4.alpha.2-1136-gc18e780-SNAPSHOT; 16:55:21.931 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:21.932 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:21.932 INFO GermlineCNVCaller - Deflater: IntelDeflater; 16:55:21.932 INFO GermlineCNVCaller - Inflater: IntelInflater; 16:55:21.932 INFO GermlineCNVCaller - Initializing engine; 16:55:21.932 INFO GermlineCNVCaller - Done initializing engine; 16:55:21.933 INFO GermlineCNVCaller - Spark disabled. sparkMaster option (local[*]) ignored.; 16:55:23.448 INFO GermlineCNVCaller - Parsing the read counts table...; 16:55:24.876 INFO GermlineCNVCaller - Parsing the sample sex genotypes table...; 16:55:24.896",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3098:200,update,update,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3098,2,"['install', 'update']","['installed', 'update']"
Deployability,"Hey folks,. I have a test dataset that interestingly core-dumps or JVM errors with `--smith-waterman FASTEST_AVAILABLE` but not with `--smith-waterman JAVA`. The only thing I can think of is somehow Intel's HMM has a length limitation, as I am using `--assembly-region-padding 1000` to GATK to call 100-1000bp indels (and it works!). I cannot share the test BAM unfortunately. What can I do to help debug further?. I'm using `gatk4-4.1.8.1-0` from `conda create -n debug-gatk4 -c defaults -c conda-forge -c bioconda gatk4`. ```; $gatk ... -version; The Genome Analysis Toolkit (GATK) v4.1.8.1; HTSJDK Version: 2.23.0; Picard Version: 2.22.8; $ java -version; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode); ```. First error motif:; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010efa9dc2, pid=23946, tid=0x000000000000a503; #; # JRE version: OpenJDK Runtime Environment (8.0_152-b12) (build 1.8.0_152-release-1056-b12); # Java VM: OpenJDK 64-Bit Server VM (25.152-b12 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # V [libjvm.dylib+0x3a9dc2] PhaseIdealLoop::set_ctrl(Node*, Node*)+0x10; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; #; # Compiler replay data is saved as:; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Second error motif:; ```; java(24057,0x7000035bd000) malloc: Incorrect checksum for freed object 0x7fd8a8193600: probably modified after being freed.; Corrupt value: 0x2e4630002e47e; java(24057,0x7000035bd000) malloc: *** set a breakpoint in malloc_error_break to debug; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733:686,release,release,686,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733,3,['release'],"['release', 'release-']"
Deployability,Hi ; We have a forum post asking help for getting GATK 4.1.0.0 conda environment installed using the yml file. ; [https://gatk.broadinstitute.org/hc/en-us/community/posts/18332470602523-Install-GATK-version-4-1-0-0-using-Conda-](url). Looks like restructuring of the default repository under conda took out some of these packages and they are no longer directly accessible. They can be accessed from the forge repo with certain flags. This issue seems to deprecate some of the older but still usable versions of GATK (due to various reasons). Directing people to use the docker version or upgrading to the latest GATK version seems to be the only solution left for now. Any other ideas of how we should pursue this issue? @lbergelson @droazen ?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8504:81,install,installed,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8504,2,"['Install', 'install']","['Install-GATK-version-', 'installed']"
Deployability,"Hi GATK team!; I would like a minimum fragment length filter during calling CNVs (the CountReads step) if possible. @LeeTL1220 helped me to find an option to set a maximum fragment length with `--read_filter FragmentLength --maxFragmentLength <value>` ([link to the GATK forum page](https://gatkforums.broadinstitute.org/gatk/discussion/2338/how-can-i-invoke-read-filters-and-their-arguments)). However, it appears there is no minimum fragment length filter implemented. Would it be possible to include the min fragment filter option in the future release or as a pre-release version?. The reason of asking this filter is:. there are shorter fragments in a FFPE tumor PCR-free WGS (top) compared to the PCR-plus WGS (bottom) from the same individual:; ![image](https://user-images.githubusercontent.com/5141643/56065278-611c9a80-5d42-11e9-8b60-b82ea1d69e87.png); and I observed higher MAD values in the PCR-free samples. I would like to see whether the results improve after discarding the shorter fragments in the FFPE PCR-free WGS.; ![image](https://user-images.githubusercontent.com/5141643/56065600-50b8ef80-5d43-11e9-9160-fb0beb5c11de.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5884:548,release,release,548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5884,2,['release'],['release']
Deployability,"Hi GATK team,. I tried running your PathSeq pipeline (broadinstitute/gatk:4.1.8.0) on my cohort and almost half of the samples failed the scoring step with this error message:; `20/07/17 09:38:35 INFO NewHadoopRDD: Input split: file:/cromwell_root/fc-6e61d4b2-bdc8-4abd-bb94-18d8fa11d9b6/7c1b0faa-e956-4289-9e2d-4fb8b9eff6ff/PathSeqPipeline/0ca5578f-70d3-498e-b7cc-23590f0ab31f/call-PathSeqAlign/MMRF_2072_2_BM.microbe_aligned.paired.bam:33554432+33554432 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5) java.util.NoSuchElementException: next on empty iterator at scala.collection.Iterator$$anon$2.next(Iterator.scala:39) at scala.collection.Iterator$$anon$2.next(Iterator.scala:37) at scala.collection.Iterator$$anon$13.next(Iterator.scala:469) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155) at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709:44,pipeline,pipeline,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709,1,['pipeline'],['pipeline']
Deployability,"Hi all,. Below error occurs trying to access Funcotator data source directory installed on lustre file system. We have a non-lustre mounted fs for cases like this, but I thought it was worth bringing up. ```; org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: null; 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotations(CosmicFuncotationFactory.java:244); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:404); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:316); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413:78,install,installed,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413,1,['install'],['installed']
Deployability,"Hi all.; This looks like an issue presented by a forum post . [Haplotypecaller FORMAT:DP is affected by the interval in WGS](https://gatk.broadinstitute.org/hc/en-us/community/posts/27992393649051-Haplotypecaller-FORMAT-DP-is-affected-by-the-interval-in-WGS). User uploaded a toy data for us to test and I was able to recreate this issue under GATK 4.6.0.0. I have not tested it with any other versions. When whole contig is given as interval all variant sites in the multisample VCF is reported with DP value much less than what it is supposed to be in samples where no variation occur. Samples with variants are shown as expected DP. Setting ploidy 2 for this analysis restores the expected DP value for samples with HOMREF sites no matter what interval is used. Numbers can be seen in the figure as well as those variant contexts. . This is what user and I got with the whole contig given as interval. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	DA10_DA10	TDY1754_TDY1754; CP022325.1	69348	.	T	A	2920.63	.	AC=1;AF=0.500;AN=2;DP=85;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;QD=29.56;SOR=0.824	GT:AD:DP:GQ:PL	0:4,0:4:99:0,119	1:0,79:79:99:2931,0; ```; This is what comes when -L is set to `CP022325.1:69347-69349`. This is the same DP reported when ploidy is set to 2 no matter what interval is used. This is also the expected DP value. . ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	DA10_DA10	TDY1754_TDY1754; CP022325.1	69348	.	T	A	2920.63	.	AC=1;AF=0.500;AN=2;DP=98;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;QD=25.36;SOR=0.824	GT:AD:DP:GQ:PL	0:17,0:17:99:0,685	1:0,79:79:99:2931,0; ```. ![image](https://github.com/user-attachments/assets/269169f8-7de2-415c-ba60-8356937da561). User data is in the incoming folder with name `cmateusiak_20240805.tar.gz`. The reference genome is a fungal one from the below link. [Fungi reference C.NeoformansKN99](https://fungidb.org/common/downloads/release-68/CneoformansKN99/fasta/data/FungiDB-68_CneoformansKN99_Genome.fasta)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8943:1893,release,release-,1893,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8943,1,['release'],['release-']
Deployability,"Hi all;; When validation runs on the GATK 4.0.0 release (congrats!) we're running into segfault issues on some `GenomicsDBImport` runs which look to be due to the length of the database path:; ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f99a7642c5b, pid=7446, tid=0x00007f99fbfa0700; #; # JRE version: OpenJDK Runtime Environment (8.0_121-b15) (build 1.8.0_121-b15); # Java VM: OpenJDK 64-Bit Server VM (25.121-b15 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libtiledbgenomicsdb8843204539247232071.so+0x4fdc5b] std::string::compare(char const*) const+0x1b; ```; Here is a self-contained test case that reproduces the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk/gatk4_genomicsdb_length.tar.gz. A standard small name and longer name of 105 characters work fine:; ```; gatk-launch --java-options '-Xms1g -Xmx2g' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path short_genomicsdb -L chr22:15069-15500 --variant Test1.vcf.gz --variant Test2.vcf.gz; gatk-launch --java-options '-Xms1g -Xmx2g' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path long_aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_genomicsdb/works_aaaaaaaaaaaaaaaaaaaaaaaaaa -L chr22:15069-15500 --variant Test1.vcf.gz --variant Test2.vcf.gz; ```; But when you add an additional character, you trigger the segfault:; ```; gatk-launch --java-options '-Xms1g -Xmx2g' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path long_aaaaaa; aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_genomicsdb/fails_aaaaaaaaaaaaaaaaaaaaaaaaaaa -L chr22:15069-15500 -; -variant Test1.vcf.gz --variant Test2.vcf.gz; ```; Thank you for looking at this and please let me know if I can provide any other information to help debug.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4160:48,release,release,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4160,1,['release'],['release']
Deployability,"Hi everyone,. I am going to use GATK4 to carry out population-based snp-calling for around 500 samples with the ploidy level from 2 to 6.; The sequence depth varied from 2x to 80x. Since there are many samples with low sequence depth (<5x), I was advised to use the population-based snp-calling pipeline. ; Recently, I met some problems in the flowchart of this analysis. Firstly, I found the pipelines that detailedly explained the flowchart of BQSR and variant calling for single-sample snp-calling. But I have no idea about the pipeline of the BQSR in population-based snp-calling. Should I apply the BQSR sample by sample following the flowchart menthioned above (the command line below), and then used these corrected bam files to identify the variants based on population-based method? Or there is another correct pipeline for this analysis. **Command lines:**. ```; for i in sample1.bam sample2.bam sample3.bam; do; java -jar picard.jar MarkDuplicates I=$i O=$i_bam CREATE_INDEX=true M=$i_metrics; gatk HaplotypeCaller -R $reference -I $i.sorted.dedup.bam -O ""$i"".g.vcf.gz --tmp-dir ./tmp -ERC GVCF; #SNP; gatk GenotypeGVCFs -R $reference -V ""$i"".g.vcf.gz -O ""$i"".vcf.gz; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type SNP -O ""$i"".snp.vcf.gz; gatk VariantFiltration -R $reference -V $path_BQSR/bqsr1.snp.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 60.0"" --filter-name ""FS60"" -filter ""MQ < 40.0"" --filter-name ""MQ40"" -filter ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" -filter ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8.0"" -O $""i"".filtered.snp.vcf.gz; gatk SelectVariants -R $reference -V $""i"".filtered.snp.vcf.gz --exclude-filtered -O ""$i"".select.filtered.snp.vcf.gz; #INDEL; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type INDEL -O ""$i"".indel.vcf.gz; gatk VariantFiltration -R $reference -V ""$i"".indel.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 200.0"" --filter-name ""FS200"" -filter ""ReadPosRankSum < -20.0"" --fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8414:295,pipeline,pipeline,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8414,4,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi folks. @chandrans and I have laid out some plans towards updating GATK4 docs for the January 9 release. Our approach is to prioritize documentation around stable Best Practice Workflows. On the docket currently is the single stable workflow--germline SNP and indel calling from DNA data. We will of course update tool docs (excluding Spark and BWA tools) and supporting tutorials. Even for tools we are unfamiliar with, we aim to have at the least a basic description and an example command. Thanks for the documentation you have already done and the help you give us in updating these. If you are certain your workflow will be ready for the release, then please let us know immediately so we can plan accordingly. If your workflow will be ready later, then can you still give us an estimate for your release so we can plan ahead? Thanks. - @davidbenjamin, did I hear you correctly that you think somatic SNV and indel calling will be ready for the Jan 9 release?; - @samuelklee, I know major changes are currently afoot for somatic CNV. Will you make the Jan 9 release for the targeted exomes use-case? What about WGS?; - @mbabadi, is March, 2018 still the plan?; - @jonn-smith, what is the status on the Tool-That-Must-Not-Be-Named?; - @cwhelan @tedsharpe @SHuang-Broad, is SV on for next year or thereafter?. It would be most helpful to users if we also have validation of our workflows as applied to real data. Are there plans to make benchmarking stats available for each of your workflows?. Sheila and I have 30-man days we can give between us towards updating documentation by December 14. Besides Geraldine, we will rely on some of you to review further refinements to documentation from now to December 14. Thanks again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3769:98,release,release,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3769,6,"['release', 'update']","['release', 'update']"
Deployability,"Hi gatk team,. I'm working with generating CNV for somatic with wdl, using this command:. `java -Xmx75G -Dconfig.file=gatk.conf -jar cromwell-46.1.jar run cnv_somatic_panel_workflow.wdl -i parameters.json `. But I got this error in which I don't know the exact reason for it:. ```; [2019-10-01 02:52:52,49] [info] Running with database db.url = jdbc:hsqldb:mem:e98d186c-96db-46ae-92e5-c326e7aa05d9;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,19] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-10-01 02:53:01,20] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-10-01 02:53:01,31] [info] Running with database db.url = jdbc:hsqldb:mem:c4b3296a-4b73-4053-b6bf-d4eeb71c8956;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,85] [info] Slf4jLogger started; [2019-10-01 02:53:02,22] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-876ccf5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-10-01 02:53:02,28] [info] Metadata summary refreshing every 1 second.; [2019-10-01 02:53:02,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,31] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:901,configurat,configuration,901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['configurat'],['configuration']
Deployability,"Hi!. I am using the gatk tool AnalyzeSaturationMutagenesis to analyze some data produced with the MITE-seq technology. It works perfectly for my purpose, so I decided to include it as a step in my pipeline written in Nextflow. ; Strangely enough, when I try to run the SAME EXACT command line inside a Nextflow module, it gives a generic error for most of the samples (sometimes all of them, sometimes some of them). ; It looks like a random issue, because if I run the same code outside of Nextflow, it works perfectly on every sample. . I would really appreciate if someone may give me some hints on why this is occurring and, eventually, how to fix it. ## Bug Report. ### Affected tool(s) or class(es); AnalyzeSaturationMutagenesis . ### Affected version(s); gatk4-4.3.0.0. ### Description ; Here it follows the output from Nextflow that appears on screen:. ```; Error executing process > 'gatk_count (gatk)'. Caused by:; Process `gatk_count (gatk)` terminated with an error exit status (247). Command executed:. gatk AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//i ndex/genome.fa --orf 1-5610 -O ./MITE6_P1. Command exit status:; 247. Command output:; (empty). Command error:; WARNING: Not mounting requested bind point (already mounted in container): /home/tigem/f.panariello; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=f alse -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar AnalyzeSaturationMutagenesis -I MITE6_P1_out.sam -R /home/tigem/f.panariello/Scratch/Cacchiarelli/MITE/QC_1804//index/genome.fa --orf 1-5610 -O ./MIT E6_P1; 09:36:03.173 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package -4.3.0.0-local.jar!/com/intel/gkl/native/libgk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8357:197,pipeline,pipeline,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8357,1,['pipeline'],['pipeline']
Deployability,"Hi, . I am testing the GATK beta 4.6 at the moment. It's a great improvement in time used to analyse data. Similarly I am interested in calling germline CNV events. I tried out the workflow with only two samples, just to find the right tools and see how it behaves. ; First I used `gatk-launch CalculateTargetCoverage` and `gatk-launch TargetCoverageSexGenotyper` and used the resulting files as input for `gatk-launch GermlineCNVCaller`. Unfortunatelly I got the following error when calling GermlineCNVCaller:. `A USER ERROR has occurred: Couldn't read file /media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsvx. Error was: Could not read NDArray tsv file`. I found out that this error has something to do with Numpy. I have installed Numpy 1.13.1. ; Have you seen some error like this before?. Thanks in advance; Stefan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3996:738,install,installed,738,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3996,1,['install'],['installed']
Deployability,"Hi, ; I am experimenting with submitting a PrintReadsSpark job to a yarn spark cluster in AWS. I run the job with the following command. ```; spark-submit --deploy-mode cluster --class org.broadinstitute.hellbender.Main --deploy-mode cluster --master yarn gatk-package-4.alpha.2-248-gcd449bf-SNAPSHOT-spark.jar PrintReadsSpark -I hdfs://chr1.bam -O hdfs://output.bam; ```. I can see from the output files that the job finished successfully, however the cluster tells me that it failed. It shows the following error message:. ```; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); ```. I believe this may be due to the `System.exit(0)` statement at line 144 in hellbender.Main, though I am not sure. . Here is a more complete snippet from the stderr log. . ```; 17/05/05 06:03:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1; 17/05/05 06:03:52 WARN DFSClient: Caught exception ; java.lang.InterruptedException; 	at java.lang.Object.wait(Native Method); 	at java.lang.Thread.join(Thread.java:1249); 	at java.lang.Thread.join(Thread.java:1323); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546); 17/05/05 06:03:52 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000001_0' to hdfs://ip-172-30-0-86.ec2.internal:8020/output.bam.parts/_temporary/0/task_20170505060336_0011_r_000001; 17/05/05 06:03:52 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 1921 bytes result sent to driver; 17/05/05 06:03:52 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 10260 ms on localhost (executor driver) (1/4); 17/05/05 06:03:53 INFO FileOutputCommitter: Saved output of task 'attempt_20170505060336_0011_r_000000_0' to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2666:157,deploy,deploy-mode,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666,2,['deploy'],['deploy-mode']
Deployability,"Hi, ; I have a suggestion that the updated verisons of Mutect2 method [pdfs](https://github.com/broadinstitute/gatk/blob/master/docs/mutect/mutect.pdf) should be kept. ; Of course we can look up from the git history log for what have changed in each edition, but it will be much more straightforward to obtain information from pdf file. Such as, [this picture](https://us.v-cdn.net/5019796/uploads/editor/8d/1rt7qtu6ohp2.png) ; in (this post)[https://gatkforums.broadinstitute.org/gatk/discussion/comment/56644#Comment_56644].; I know it comes from the Mutect2 method old version, but it was no longer here. Xiucz.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6001:35,update,updated,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6001,1,['update'],['updated']
Deployability,"Hi, ; This is what i got when i run the command gatk --help ; (base) ameni@ameni-Aspire-A315-55G:~/Documents/pharmacogenomics$ gatk --help. Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8280:506,Configurat,Configuration,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280,1,['Configurat'],['Configuration']
Deployability,"Hi, ; when i test gatk4 MarkDuplicatesSpark command on yarn cluster, i encountered an issue ""non zero exit code 13"". How can i fix it ? . Here is my command and what i received from the terminal :; ****; gatk MarkDuplicatesSpark -I hdfs://192.168.0.104:9000/user/jacky/NA12878.mapped.illumina.mosaik.CEU.exome.20110411.bam -O hdfs://192.168.0.104:9000/user/jacky/marked_dup.bam -M hdfs://192.168.0.104:9000/user/jacky/marked_dup_metrics.txt -- --spark-runner SPARK --deploy-mode cluster --spark-master yarn; Using GATK jar /home/jacky/Exec/gatk/build/libs/gatk-spark.jar; Running:; /home/jacky/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.executor.memoryOverhead=600 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --deploy-mode cluster /home/jacky/Exec/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I hdfs://192.168.0.104:9000/user/jacky/NA12878.mapped.illumina.mosaik.CEU.exome.20110411.bam -O hdfs://192.168.0.104:9000/user/jacky/marked_dup.bam -M hdfs://192.168.0.104:9000/user/jacky/marked_dup_metrics.txt --spark-master yarn; 20/10/22 12:02:26 INFO client.RMProxy: Connecting to ResourceManager at /192.168.0.104:8032; 20/10/22 12:02:26 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers; 20/10/22 12:02:26 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1536 MB per container); 20/10/22 12:02:26 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:467,deploy,deploy-mode,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,1,['deploy'],['deploy-mode']
Deployability,"Hi, GATK team! I'm working on GATK WGS somatic CNV calling pipeline. . When I tried gatk --java-options ""-Xmx2800g"" ModelSegments --denoised-copy-ratios ${tumor}.denoisedCR.tsv --allelic-counts ${tumor}.allelicCounts.tsv --normal-allelic-counts ${normal}.allelicCounts.tsv --output-prefix ${tumor} -O ${outdir}, I got this type of error:. 10:00:18.408 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/acct-medliuyb/medliuyb-user1/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 10, 2022 10:00:18 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:00:18.544 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.544 INFO ModelSegments - The Genome Analysis Toolkit (GATK) v4.2.0.0; 10:00:18.544 INFO ModelSegments - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:00:18.544 INFO ModelSegments - Executing as medliuyb-user1@huge2.pi.sjtu.edu.cn on Linux v3.10.0-1062.el7.x86_64 amd64; 10:00:18.545 INFO ModelSegments - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 10:00:18.545 INFO ModelSegments - Start Date/Time: January 10, 2022 at 10:00:18 AM CST; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - ------------------------------------------------------------; 10:00:18.545 INFO ModelSegments - HTSJDK Version: 2.24.0; 10:00:18.545 INFO ModelSegments - Picard Version: 2.25.0; 10:00:18.545 INFO ModelSegments - Built for Spark Version: 2.4.5; 10:00:18.545 INFO ModelSegments - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:00:18.546 INFO ModelSegments - HTSJDK Defaults",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7633:59,pipeline,pipeline,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633,1,['pipeline'],['pipeline']
Deployability,"Hi, a small question (and maybe issue here). Does BwaSpark require the input bam to be queryname sorted ?. I have a pipeline doing `.fastq -> FastqToSam -> .bam -> BwaSpark -> Some other stuffs` and FastqToSam take time (40min) relative to the entire pipeline (2h).; I tried to speed-up it by creating a unsorted bam using -SO unsorted. It's faster but BwaSpark crashes. There is no indication in BwaSpark documentation and help it requires a name sorted bam. The only information about it is in the help message of `BwaAndMarkDuplicatesPipelineSpark`: ; > Takes name-sorted file and runs BWA and MarkDuplicates.; > Version:4.0.1.2. Is it a missing information ?. Additional question : could it be possible to do the conversion on Spark (FastqToSamSpark) ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4612:116,pipeline,pipeline,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612,2,['pipeline'],['pipeline']
Deployability,"Hi, i can't install gatk via conda/mamba. couldyou pls help; pls see steps that i took. ```; $conda config --add channels conda-forge; $conda config --add channels bioconda; $conda config --add channels defaults; $conda config --set channel_priority strict; ```. install command; ```; bash:iscxf001:/data1/greenbab/users/ahunos/apps/gatk-4.5.0.0 1023 $ conda env create -n gatk -f gatkcondaenv.yml; ```. ```; Channels:; - conda-forge; - defaults; - bioconda; Platform: linux-64; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - conda-forge::typing_extensions==4.1.1; - conda-forge::theano==1.0.4; - pkgs/main::tensorflow==1.15.0; - conda-forge::scipy==1.0.0; - conda-forge::scikit-learn==0.23.1; - conda-forge::python==3.6.10; - bioconda::pysam==0.15.3; - conda-forge::pymc3==3.1; - conda-forge::pip==21.3.1; - conda-forge::pandas==1.0.3; - conda-forge::numpy==1.17.5; - conda-forge::mkl-service==2.3.0; - conda-forge::mkl==2019.5; - conda-forge::matplotlib==3.2.1; - conda-forge::keras==2.2.4; - conda-forge::joblib==1.1.1; - pkgs/main::intel-openmp==2019.4; - conda-forge::h5py==2.10.0; - conda-forge::dill==0.3.4. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/r/linux-64; - https://conda.anaconda.org/bioconda/linux-64; - https://conda.anaconda.org/bioconda; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https:/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8838:12,install,install,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8838,2,['install'],['install']
Deployability,"Hi, there:. I realize that GATK team released plenty of wonderful tutorials, best practice guidances, WDL scripts, etc. However, for users like me, I still prefer some simple and straight-forward BASH scripts that I could easily embed into existing pipelines and fire up. . Below is what I got from Chat-GPT. I tested it and it actually worked magically, processing my fasta.gz files into VCF. Can someone please kindly take a look at this, and let me know if there is some issue with this script?. Thank you very much & best regards,; Jie. ![image](https://github.com/broadinstitute/gatk/assets/26947455/12e2c577-2633-4189-a02c-ec45c677aa50)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8905:37,release,released,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8905,2,"['pipeline', 'release']","['pipelines', 'released']"
Deployability,"Hi,. I am on GATK v4.3.0.0 (CentOS) and would like to implement GermlineCNVCaller in my work. In an attempt to set up the conda environment (gcnvkernel) from zip using the command `conda env create -f gatkcondaenv.yml`, I got an error like this: `Found conflicts! Looking for incompatible packages.` . I also tried installing with tar, but I couldn't find gatkPythonPackageArchive.zip as required in the yml. Any help on this issue is much appreciated!. Java version: 1.8.0_201",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8091:315,install,installing,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8091,1,['install'],['installing']
Deployability,"Hi,. I am trying to call common and rare germline copy number variants with GATK 4, for more than 100 human samples based on human genome reference: hg19. For this project, I have 500 GB for memory, 10 TB for storage and 300 cpu cores. The program version is as below:. GATK Version: 4.1.2.0; Openjdk Version: 1.8.0_232; Python Version: 3.6.8. I didn't use the WDL way. I just follow the document of Notebook#11684 and build a local pipeline. I split the my project based on Chromosome, including (chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chrMT). After finish the pipeline, I am testing it with 6 samples. When I separately submit my script for each chromosome, every sub-project goes well: through my Input BAM Files, I can get the corresponding VCF Files (10 cores and 10 GB for each single project). That is to say, the environment of our GATK and Python for germline copy number variants calling should be OK. However, When I submit all the 25 sub-projects (12 cores and 12 GB for each single project) at the same time, I' m **randomly** suffering the two following PythonScriptExecutorException for some of the **random** sub-projects: . .............................................................(BUG 001).......................................................... Traceback (most recent call last):; File ""/tmp/cohort_determine_ploidy_and_depth.3351404099122294482.py"", line 8, in <module>; import gcnvkernel; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import times",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235:433,pipeline,pipeline,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235,2,['pipeline'],['pipeline']
Deployability,"Hi,. I am trying to test the pathseq tutorial following the tutorial on [this]( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ""this"") link. I ran the following commands. bioinfo@bioinfo$ conda activate gatk; (gatk) bioinfo@bioinfo$ gatk PathSeqPipelineSpark \; > --input test_sample.bam \; > --filter-bwa-image hg19mini.fasta.img \; > --kmer-file hg19mini.hss \; > --min-clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:163,pipeline,pipeline,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,3,"['Install', 'pipeline']","['Installers', 'pipeline']"
Deployability,"Hi,. I am using GATK `version gatk4-4.0.6.0-0` as part of the bcbio-nextgen pipeline for RNA-seq variant calling. There is one step in the pipeline i.e. `gatk GenomicsDBImport` that's been failing consistently no matter how less or many resources in terms of memory and cores I provide. I have tried to run the command as part of the pipeline and in stand-alone mode (like below) and both produce the same error:. ```; [rathik@reslnrefo01 log]$ gatk --java-options '-Xms454m -Xmx3181m -XX:+UseSerialGC' GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:76,pipeline,pipeline,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,3,['pipeline'],['pipeline']
Deployability,"Hi,. I report a bug here https://gatkforums.broadinstitute.org/gatk/discussion/23236/has-anyone-reported-the-new-release-4-0-12-0-calculatecontaminations-bug/p1?new=1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5536:113,release,release-,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5536,1,['release'],['release-']
Deployability,"Hi,. I tried to use your new released GATK4 package to do CNV analysis. I know it's a beta tool... I followed the tools documentation and did ""CollectFragmentCounts"" first to get hdf5 files. After that I tried to run ""DetermineGermlineContigPloidy"" in cohort mode with these files. I created a ploidy_priors.tsv file like described in the documentation and used 8 hdf5-files. A few seconds after starting the tool I get the following error:. ```; [11. Januar 2018 16:42:24 MEZ]; org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=2294808576; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python /tmp/die9s/cohort_determine_ploidy_and_depth.9149389425697869853.py --sample_coverage_metadata=/tmp/die9s/samples-by-coverage-per-contig814612566493224652.tsv --output_calls_path=/media/Berechnungen/CNV_analysis/GATK4/normal_cohort-calls --mapping_error_rate=1.000000e-02 --psi_s_scale=1.000000e-04 --mean_bias_sd=1.000000e-02 --psi_j_scale=1.000000e-03 --learning_rate=5.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.990000e-01 --log_emission_samples_per_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel_error=5.000000e-04 --max_advi_iter_first_epoch=1000 --max_advi_iter_subsequent_epochs=1000 --min_training_epochs=20 --max_training_epochs=100 --initial_temperature=2.000000e+00 --num_thermal_epochs=20 --convergence_snr_averaging_window=5000 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=1 --caller_update_convergence_threshold=1.000000e-03 --caller_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false --interval_list=/tmp/die9s/intervals671187352630642175.tsv --contig_ploidy_prior_table=/media/Berechnungen/CNV_analysis/GATK4/ploidy_priors.tsv --output_model_path=/media/Berechnungen/CNV_analysis/GATK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125:29,release,released,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125,1,['release'],['released']
Deployability,"Hi,. I'm working on the GATK pipeline with aim to find the best pipeline before start my project.; I never do VQSR before. I need your suggestion according to my result.; I using the NA12878 Exome data from SureSelect7 version with 4 difference main pipelines. All pipeline had follow the best practice option.; 1. BWA7.15-GATK3.7; 2. BWA7.15-GATK4.4; 3. DRAMAP1.2.1-GATK4.4; 4. DRAMAP1.2.1-GATK4.4-dragen. I have questions about the number of variant output and VQSR plot from each pipeline; BWA-GATK3.7 pipeline gave me 100694 variants; BWA-GATK4.4 pipeline gave me 45861 variants; DRAMAP-GATK4.4 gave me 44835 variants; DRAMAP-GATK4.4-dragen gave me 42969 variants. For the VQSR tranchs plot; 1. BWA-GATK3.7; ![image](https://github.com/broadinstitute/gatk/assets/15682256/7d158c0b-d0af-4e5e-a605-993fad38cc0b). 2. BWA-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/1898f5f0-cfa4-476c-8637-9a4ea3333f49). 3. DRAMAP-GATK4.4; ![image](https://github.com/broadinstitute/gatk/assets/15682256/52f6e56c-7dc5-4047-9086-2a9fb37f478f). 4. DRAMAP-GATK4.4-dragen; ![image](https://github.com/broadinstitute/gatk/assets/15682256/20bfc543-ad76-4e3d-a2ba-693cbed46a92). From what I see it far from Ti/Tv ratio that suitable for whole exome sequencing as ~ 2.8. Please give me some suggestion. ; Which pipeline I should selected as the best pipeline for my project. Thanks,; GM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8435:29,pipeline,pipeline,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8435,9,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi,. I've encountered an issue with the R script generated by `VariantRecalibrator` during my analysis. The generated R script uses color functions (`scale_fill_gradient`) where the RGB color space is still being employed to calculate the gradient. However, in newer versions of R, the ggplot2 and scales libraries have deprecated the RGB color space, and the library now requires the ""Lab"" color space to calculate the gradient. This issue causes the R script to fail unless it is later modified by the user to use `scale_fill_gradient(..., space = ""Lab"")`. Updating the script generation in `VariantRecalibrator` would prevent this problem and make the R script compatible with newer versions of R, and ggplot2. an scales libraries. Here is the suggested change:. update : scale_fill_gradient(..., space = ""rgb""); to : scale_fill_gradient(..., space = ""Lab""). Versions:. The Genome Analysis Toolkit (GATK) v4.6.0.0; HTSJDK Version: 4.1.1; Picard Version: 3.2.0. RStudio 2024.04.2+764 ""Chocolate Cosmos"" Release (e4392fc9ddc21961fd1d0efd47484b43f07a4177, 2024-06-05) for Ubuntu Jammy; Library scales 1.3.0; library ggplot2 3.51",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8992:766,update,update,766,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8992,2,"['Release', 'update']","['Release', 'update']"
Deployability,"Hi,. When I try to run Funcotator, using the below command:. gatk Funcotator \; --variant /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz \; --reference /rsrch5/home/tdccct/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \; --ref-version hg38 \; --data-sources-path /rsrch5/home/tdccct/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s \; --output /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf \; --output-file-format VCF; ; I get the following error:; ; Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar Funcotator --variant /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz --reference /home/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s --output /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf --output-file-format VCF; 16:36:22.352 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:36:22.392 INFO Funcotator - ------------------------------------------------------------; 16:36:22.396 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:499,pipeline,pipelines,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,1,['pipeline'],['pipelines']
Deployability,"Hi,; Currently, using spark tools, we can set the runner and master using --sparkRunner and sparkMaster.; However, there is not similar parameter to set the deploy-mode so we have to manually set it using --conf.; For example , the following parameters are currently used in the command-line to run on a yarn+cluster spark environment : ; `--sparkRunner SPARK --sparkMaster yarn --conf 'spark.submit.deployMode=cluster'`; It's not very user-friendly, a sparkDeployMode parameter could be usefull :; `--sparkRunner SPARK --sparkMaster yarn --sparkDeployMode cluster`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933:157,deploy,deploy-mode,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933,2,['deploy'],"['deploy-mode', 'deployMode']"
Deployability,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:117,install,installed,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,3,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"Hi,; I am trying to generate vcf using GATK pipeline from bam file, but everytime, I am getting the following exception:; 01:13:15.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/ngs/programs/gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:13:16.075 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.075 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0; 01:13:16.075 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:13:16.076 INFO HaplotypeCaller - Executing as shashank@grande on Linux v3.13.0-79-generic amd64; 01:13:16.076 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_72-internal-b15; 01:13:16.076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2; 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater; 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater; 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6384:44,pipeline,pipeline,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6384,1,['pipeline'],['pipeline']
Deployability,"Hi,; I recently started getting error messages when running a Nextflow pipeline for WGS analysis. I am using GATK 4.0.1.2 and was wondering whether:. /bin/env python - too many levels of symbolic links. may have to do with a broken conda environment (which GATK seems to use)? This happens for tools such as GenomicsDBImport. If I run the job in question outside of Nextflow, it seems to start just fine. But as far as I know Nextflow does not use python, so doesn't look like the obvious culprit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4459:71,pipeline,pipeline,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4459,1,['pipeline'],['pipeline']
Deployability,"Hi,; I recently used GATK4 Spark local version for somatic variant call. The machine has 40 cores and 160g mem. I tried 20 and 10 cores for each tumor/normal pair in the BQSR step (BaseRecalibratorSpark) and the two samples are processed at the same time. However, the pipeline frequently failes (errors like outofmemory, cannot allocate a page) unless I use 4 cores for each sample. I think the problem should be solved by tuning Spark and JAVA parameters. I considered options like `--conf spark.driver.memory=10g`, `-XX:ParallelGCThreads=10` but had no luck. Can someone suggest the parameter options that I should look at? . Thanks,. -Han",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3465:269,pipeline,pipeline,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3465,1,['pipeline'],['pipeline']
Deployability,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:52,deploy,deploy,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,11,"['configurat', 'deploy', 'install', 'release', 'update']","['configuration', 'deploy', 'install', 'releases', 'update']"
Deployability,"Hi,; Our spark installation use a mapr filesystem ( hdfs compatible ).; GATK spark tools does not seems to recognize it.; When running the following command:; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input maprfs://spark-ics/user/axverdier/data/710-PE-G1.bam --output maprfs://spark-ics/user/axverdier/testOutGATK_CountReadsSpark --sparkRunner SPARK --sparkMaster yarn --javaOptions -Dmapr.library.flatclass; I got the following error!. > Driver stacktrace:; > 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1436); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1424); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); > 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); > 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1423); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at scala.Option.foreach(Option.scala:257); > 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1651); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1606); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1595); > 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); > 	at org.apache.spark.SparkCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:15,install,installation,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['install'],['installation']
Deployability,"Hi,; The HaplotypeCaller_GATK4_VCF task in the gatk4-exome-analysis-pipeline doesn't seem to add any interval padding. Shouldn't there be interval padding?. Unless the configured Broad intervals already have padding added, but it is not clear why that would be, since that same file is used for calculating HsMetrics, which should not have padding. The question is, for my own implementation of this pipeline, should I add on interval padding to the interval list file? And if so, what size padding? Or should I add the interval padding option to the HaplotypeCaller itself in the wdl script. Thanks for any advice on this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6071:68,pipeline,pipeline,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6071,2,['pipeline'],['pipeline']
Deployability,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:303,integrat,integration-test,303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,2,['integrat'],"['integration-test', 'integration-tests']"
Deployability,"Hi,; first of all, I find it very awkward that after 3.8 release there is 3.8-1. Why the dash instead of a dot, as usual? It only complicates automated package downloads which in general work with numbers separated by dots. You just mix together two schemes. Is that really necessary?. Anyway, the pom.xml is broken:. ```; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelPar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:57,release,release,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,1,['release'],['release']
Deployability,"Hi,; in the last months for my Master thesis project I've studied your tool, with this pipeline:. ![ngs_pipeline_gatk](https://user-images.githubusercontent.com/10074137/47147968-ae5a0b00-d2cf-11e8-9fd6-15cd23fbcdcf.png); in spark version, yes, I know is still in beta but Ive found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5323:87,pipeline,pipeline,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323,1,['pipeline'],['pipeline']
Deployability,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:1001,Configurat,ConfigurationContainerInternal,1001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['Configurat'],['ConfigurationContainerInternal']
Deployability,Hook arguments from SelectVariants/GenotypeGVCFs/GnarlyGenotyper to GenomicsDB Export Configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6456:86,Configurat,Configuration,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6456,1,['Configurat'],['Configuration']
Deployability,How does GATK4 Accept Sharded Data and SNP&Indels calling Pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3141:58,Pipeline,Pipeline,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3141,1,['Pipeline'],['Pipeline']
Deployability,How to run the entire pipeline (using even Spark tools) from Java?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3878:22,pipeline,pipeline,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3878,1,['pipeline'],['pipeline']
Deployability,How to use pipeline to call SNP and INDEL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6592:11,pipeline,pipeline,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6592,1,['pipeline'],['pipeline']
Deployability,HtsgetReader integration tests are failing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6803:13,integrat,integration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6803,1,['integrat'],['integration']
Deployability,"I _believe_ that the user error text didn't get updated when the `--sequence-dictionary` argument got added. It would be great to mention that argument in GATKTool::initializeIntervals(), which currently says `""We require a sequence dictionary from a reference, a source of reads, or a source of variants to process intervals. "" +; ""Since reference and reads files generally contain sequence dictionaries, this error most commonly occurs "" +; ""for VariantWalkers that do not require a reference or reads. You can fix the problem by passing a reference file with a sequence dictionary "" +; ""via the -R argument or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`. I would change the last sentence to:; `You can fix the problem by passing a reference file with a sequence dictionary via the -R argument, a *.dict dictionary file via the --sequence-dictionary argument, or you can run the tool UpdateVCFSequenceDictionary on your vcf.""`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4507:48,update,updated,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4507,3,"['Update', 'update']","['UpdateVCFSequenceDictionary', 'updated']"
Deployability,I also need to add documentation still. - document places where --ignore-above-gq-threshold is used and why / what it means; - if we went back to GQ60 what would need to get updated?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7584:174,update,updated,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7584,1,['update'],['updated']
Deployability,I also updated the gatkbase Docker image to 1.2.2. See comments in #4209.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4246:7,update,updated,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4246,1,['update'],['updated']
Deployability,"I am looking at using GATK and first checked at the docker image using **_docker pull broadinstitute/gatk_**. this container image has 1460 vulnerabilities and a lot of them are critical. ; <img width=""1737"" alt=""Screenshot 2023-02-21 212830"" src=""https://user-images.githubusercontent.com/4427764/220508376-aeead13b-999b-4cfd-a7d6-295241df532a.png"">. Then I decided not to use this image and instead create my own image and just deploy the released version 4.2.6.1 from here (https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip). Even this has many vulnerabilities include things stemming from log4j 1.2.17. These have been fixed by log4j team years back in version 2.17.1 onwards. I am really stunned that a popular library like gatk is not keeping up with basic security fixes. <img width=""854"" alt=""Screenshot 2023-02-21 212751"" src=""https://user-images.githubusercontent.com/4427764/220508300-7bfe331d-8286-4950-a6dc-e1f5f97c65d0.png"">. the latest version of docker desktop has integrated image scanning and can very easily highlight the issues listed above. Can we start addressing these issues sooner than later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215:430,deploy,deploy,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215,4,"['deploy', 'integrat', 'release']","['deploy', 'integrated', 'released', 'releases']"
Deployability,I am looking to call single nucleotide polymorphism (SNP) and INDEL from my genome mapping results generated from HISAT pipeline. Please suggest process and command for that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6592:120,pipeline,pipeline,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6592,1,['pipeline'],['pipeline']
Deployability,"I am running `gatk GenotypeGVCFs` and want to get output for all genomic sites. I was expecting the output to include parameters like QUAL and mapping quality (MQ) for invariant sites. This is based on a previous study that used an earlier release of GATK, v2.8-1 ([reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4617969/)) that used QUAL and MQ values from that sites (although I am aware that these values are computed differently as for variant sites). However, the version I am using, v4.2.1.0, seems to not produce this output, and I cannot find a relevant option to include it. Am I missing something? is there anyway to get this information?. GATK is run as:. gatk HaplotypeCaller -I sample1.bam -O sample1.vcf -R reference.fa -ploidy 1 -ERC BP_RESOLUTION -stand-call-conf 10.0; ...; gatk CombineGVCFs -R reference.fa -O all_samples.g.vcf --variant sample1.vcf --variant sample2.vcf ...; gatk GenotypeGVCFs -R reference.fa -V all_samples.g.vcf -O all_samples.vcf -ploidy 1 -all-sites. Thank you",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7459:240,release,release,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7459,1,['release'],['release']
Deployability,"I am starting a Maven Project in which I would like to import your library; so I added this [dependency](http://search.maven.org/#artifactdetails%7Corg.broadinstitute%7Cgatk%7C4.beta.2%7C) to my pom.xml; ```; <dependency>; <groupId>org.broadinstitute</groupId>; <artifactId>gatk</artifactId>; <version>4.beta.2</version>; </dependency>; ```; When I execute `mvn clear install` in my folder project, I receive this error: ; ```; [ERROR] Failed to execute goal on project GATKpipe: ; Could not resolve dependencies for project uk.ac.ncl:GATKpipe:jar:0.0.1-SNAPSHOT: ; Could not find artifact com.github.fommil.netlib:all:jar:1.1.2 in ; all (https://mvnrepository.com/artifact/com.github.fommil.netlib/all) -> [Help 1]; ```; and it seems that the problem is the dependency by com.github.fommil.netlib/all, indeed according to the output of `mvn clear install`, it attempt to download all-1.1.2.jar:; `Downloading: https://repo.maven.apache.org/maven2/com/github/fommil/netlib/all/1.1.2/all-1.1.2.jar`; but this jar is not available in the repository. I noticed that even in other [projects](https://github.com/amplab/ml-matrix/issues/11) have the same issue. How is possible to resolve this issue? . Thanks for your time,; Nicholas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3724:380,install,install,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3724,2,['install'],['install']
Deployability,"I am trying to understand the calculation of the [`StrandOddsRatio`](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_annotator_StrandOddsRatio.php). The online documentation and javadoc for the [`StrandOddsRatio` class](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java#L40) shows it as:; ```latex; $$ refRatio = \frac{max(X[0][0], X[0][1])}{min(X[0][0], X[0][1} $$; ```. Nonetheless, my reading of [the code](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java#L102) shows it as; ```latex; $$ refRatio = \frac{min(X[0][0], X[0][1])}{max(X[0][0], X[0][1} $$; ```; The code is:; ```java; final double refRatio = min(t00, t01)/ max(t00, t01);; ```. The docs say its max/min while the code does min/max. The same is true for the docs and implementation of `altRatio`. It looks like either a bug, or the docs need to be updated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5700:1062,update,updated,1062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5700,1,['update'],['updated']
Deployability,"I am using GATK 4.4.0.0 via the official docker release to reheader output from SVABA with an appropriate sequence dictionary. I am using `UpdateVCFSequenceDictionary` for this purpose with the following command: . ```; singularity exec -B ""$PWD"" broadinstitute-gatk-4.4.0.0.img gatk UpdateVCFSequenceDictionary --source-dictionary Mus_musculus.GRCm39.dna.primary_assembly.dict -V svaba.somatic.indel.vcf --replace true -O svaba.somatic.indel.vcf.reheaded.vcf; ```. I have encountered a curious behavior, where by the tool is not simply adjusting the sequence dictionary, but is also modifying a FORMAT field. . Original VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=String,Description=""Genotype quality (currently not supported. Always 0)"">; ```. Updated VCF header: . ```; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ```. From what I can see, the updated text is used frequently in your GATK VCF files, but I can't dig out the specific code where it is being set via `UpdateVCFSequenceDictionary`. I am wondering if there is a collision where `UpdateVCFSequenceDictionary` detects GQ and prints a stock header field to match expectation, rather than leaving it alone. I would expect the tool to simply replace the dictionary portion of the VCF without modifying the FORMAT/INFO fields. This is causing issues with downstream analysis because SVABA QC values are float/string not integer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8629:48,release,release,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629,7,"['Update', 'release', 'update']","['UpdateVCFSequenceDictionary', 'Updated', 'release', 'updated']"
Deployability,"I asked about a status update on the travis image space issue in the travis ticket, and based on feedback I tried moving to the new image. It seems to work now. Fixes https://github.com/broadinstitute/gatk/issues/3559.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3622:23,update,update,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3622,1,['update'],['update']
Deployability,"I believe that the problem is that HaplotypeCallerGenotypingEngine doesn't use the version of StandardCallerArgumentCollection.getSampleContamination() with the sampleID, so the sampleContamination variable is never initialized and we get a NPE. The -contamination argument is standard in our production GATK pipeline... and we're about to start telling everyone to use it! This is an important one to fix. Here's a stacktrace:. java.lang.NullPointerException; 	at java.util.Collections$UnmodifiableMap.<init>(Collections.java:1446); 	at java.util.Collections.unmodifiableMap(Collections.java:1433); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.StandardCallerArgumentCollection.getSampleContamination(StandardCallerArgumentCollection.java:89); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:566); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:218); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:295)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4312:309,pipeline,pipeline,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4312,1,['pipeline'],['pipeline']
Deployability,I broke external forks pull requests when the dataflow tests were turned on. They need to be updated so that tests that don't require keys will run without them.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/810:93,update,updated,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/810,1,['update'],['updated']
Deployability,"I bumped into an error in a PR of mine due to a recent update in master. While I should make the code of the PR more robust I think that the approach take to compose approximate likehoods in ```VariantAnnotator.makeLikelihoods``` can and should be improved. Currently uses -Infility as ""unlikely"" lk (I would say rather ""impossible"" lk) and 0 as ""likely"" based on whether the read pileup does not match the allele or it does match the allele. . IMO the ""unlikely"" lk should never be less than the mapping quality of the read. And it can be further reduced by the base quality in case of an snp or the indel error probrability; by default is 45 Phred yet as part of the integration with Illumina/DRAGEN Dragstr, at least in germline, we can come out with indel penalties that are tailred to the reference, read context.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7312:55,update,update,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7312,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"I came across some notes I made 20170901, while developing the Helsinki Mutect2 tutorial, to report a particular bug, that I am just now getting around to. Here is the command I ended up using to solve the problem:; ```; gatk-launch PrintReads -I hcc1143_N_clean.bam -O hcc1143_N_chr17.bam -L chr17 -L chr11:915890-1133890 -L chr6:29941013-29946495 -L chr11_KI270927v1_alt -L HLA-A*24:03:01:1+; ```. Notice the protection tag `:1+` that I add to the last interval. If I do not add this, I get the following error:; ```; ***********************************************************************. A USER ERROR has occurred: Badly formed genome unclippedLoc: Contig 'HLA-A*24:03' does not match any contig in the GATK sequence dictionary derived from the reference; are you sure you are using the correct reference fasta file?. ***********************************************************************; ```; This error persists even if I provide a dictionary/ref to the command. . The solution comes from the pipelines team, in their [PESS workflow that I documented](https://gatkforums.broadinstitute.org/gatk/discussion/7899/reference-implementation-pairedendsinglesamplewf-pipeline), so credit goes to them. Perhaps our code can do this internally for HLA contigs. Sorry for the late notice. I've been extremely busy. I will assign @droazen since we touched upon this briefly last week.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3807:1002,pipeline,pipelines,1002,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3807,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:235,install,installDist,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,2,"['configurat', 'install']","['configuration', 'installDist']"
Deployability,"I created a minimal branch to clean up the way we were passing around credentials. We create a GCSOptions class instead of a DataflowPipelineOptions when we create the pipeline and pass in secrets in at the point instead at the ReadSources level. ReadSources now takes a pipeline instead of the secrets file location. This isn't a long term solution. We should switch the code to get rid of the GenomicsSecret and instead use the more general secret. I think much of the secets factory junk can go away now (they dated from a time when the Dataflow API wasn't built out much. All tests passed locally. Oddly, I now am sometimes getting a dialog about DSDE needing access to basic information about my Google account, not sure source of the issue (maybe the secret I grabbed?), if it's repeatable, or blocking. I recommend the reviewer patch my branch and test locally.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/513:168,pipeline,pipeline,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/513,3,"['patch', 'pipeline']","['patch', 'pipeline']"
Deployability,"I didnt see an example, but I could have easily missed it: I am writing an integration test for a walker where the user supplies an output prefix, and the tool created three output files based on that prefix. I want to write an integration test, ideally using IntegrationTestSpec in an intended form. Are there any existing integration tests of tools that use this style argument, and/or is there an intended way to make that work using IntegrationTestSpec? . Most examples I see require the test to supply ""-O %s"", and the test creates a temp file for the output. It then expects to compare that path to the expected file you supply. I see how to work around it (dont use %s and manually check outputs), but I thought I'd check to see if there was an official solution. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5446:75,integrat,integration,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446,5,"['Integrat', 'integrat']","['IntegrationTestSpec', 'integration']"
Deployability,"I discovered some strange behavior in our test suite. This test in `PrintReadsSparkIntegrationTest` passes:. ```; @Test; public void testReadFiltering() throws IOException {; final File samWithOneMalformedRead = new File(getTestDataDir(), ""print_reads_one_malformed_read.sam"");; final File outBam = createTempFile(""print_reads_testReadFiltering"", "".bam"");. ArgumentsBuilder args = new ArgumentsBuilder();; args.add(""--"" + StandardArgumentDefinitions.INPUT_LONG_NAME);; args.add(samWithOneMalformedRead.getCanonicalPath());; args.add(""--"" + StandardArgumentDefinitions.OUTPUT_LONG_NAME);; args.add(outBam.getCanonicalPath());. runCommandLine(args.getArgsArray());; SamAssertionUtils.assertSamsEqual(outBam, new File(getTestDataDir(), ""expected.print_reads_one_malformed_read.bam""));; }; ```. But if you re-write it to use `IntegrationTestSpec` with the same input, output, and expected file, it fails with `Sort order differs. File 1: nullFile 2: coordinate`:. ```; @Test; public void testReadFiltering() throws IOException {; final File samWithOneMalformedRead = new File(getTestDataDir(), ""print_reads_one_malformed_read.sam"");; final File outBam = createTempFile(""print_reads_testReadFiltering"", "".bam"");. final IntegrationTestSpec spec = new IntegrationTestSpec(; "" --"" + StandardArgumentDefinitions.INPUT_LONG_NAME + "" "" + samWithOneMalformedRead.getCanonicalPath() +; "" --"" + StandardArgumentDefinitions.OUTPUT_LONG_NAME + "" "" + outBam.getCanonicalPath(),; Arrays.asList(new File(getTestDataDir(), ""expected.print_reads_one_malformed_read.bam"").getCanonicalPath()); );. spec.executeTest(""PrintReadsSpark_testReadFiltering"", this);; }; ```. Possibly this is indicative of a bug in `IntegrationTestSpec`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1164:822,Integrat,IntegrationTestSpec,822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1164,4,['Integrat'],['IntegrationTestSpec']
Deployability,"I don't think this is true anymore. We've been using it successfully with gatk4's `HaplotypeCaller`. I'd love some confirmation, and then the docs could be updated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5943:156,update,updated,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5943,1,['update'],['updated']
Deployability,I download 4.1.8.1 release tar.gz file but can't unzip.; ```; 63800K .......... .......... .......... .......... .......... 49.2K; 63850K 533G=21m48s. 2020-07-22 09:06:30 (48.8 KB/s) - 4.1.8.1.tar.gz saved [65382686]; ```; Here is Error:; ```; $tar -zxf 4.1.8.1.tar.gz . gzip: stdin: unexpected end of file; tar: Unexpected EOF in archive; tar: Unexpected EOF in archive; tar: Error is not recoverable: exiting now; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6719:19,release,release,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6719,1,['release'],['release']
Deployability,"I found some edge cases when implemented a new walker based on GATKTool that should be considered in the base class to avoid some mistakes by developers. They should either being correctly handled or documented as not-applicable for some walkers:. - [ ] If a tool/walker overrides getPluginDescriptors to remove the `GATKReadFilterPluginDescriptor`, the method `makeReadFilter` will not return a filter with the `getDefaultReadFilters`. This is important for implementing tools where the user shouldn't be able to override the filters. I suggest to either handle the case where the plugin cannot be detected and return a filter with only defaults, or to specify in the documentation that `makeReadFilter` should be overriden in that case. - [ ] Transformer methods for reads (`makePreReadFilterTransformer`and `makePostReadFilterTransformer`) only have effect in `ReadWalker`(and extensions). I think that the `ReadsContext` should have a method to set pre/post transformers, and call this methods to integrate with every extension of `GATKTool`. Otherwise, it should be documented that it has no effect in most of the cases. @droazen - could you give me some way to proceed here? I think that the best way is to implement the proper behavior, but maybe the engine team has a different opinion...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4651:1001,integrat,integrate,1001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651,1,['integrat'],['integrate']
Deployability,"I got 348 samples to analyse their variants. I have read several turorials about how to use gatk to get a population vcf. At the beginning , I tried to use CombineGVCFs to get the Gvcf and use SelectVariants to pick the snps out. . CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8593:739,release,release,739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593,2,['release'],"['release', 'release-']"
Deployability,"I have Java 8 installed, but it's not my _default_ Java version, so `gradle check` gives me this error message:. ```; :compileJava FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileJava'.; > invalid source release: 1.8; ```. `JAVA_HOME=$JAVA8_HOME gradle check` succeeded. . I would prefer an error message like ""Hellbender requires JAVA_HOME to point to a valid Java 8 installation"" to make it immediately obvious what needs to be done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/489:14,install,installed,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/489,3,"['install', 'release']","['installation', 'installed', 'release']"
Deployability,"I have a project that depends on `4.alpha.2-183-ge1e71d7-SNAPSHOT` (I am planning to update it, that's why I look at it), and I discovered that the jfrog repository the folder is empty: https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/broadinstitute/gatk/4.alpha.2-183-ge1e71d7-SNAPSHOT/. When looking at the jfrog repository, it looks like no SNAPSHOT jar file is present. As an example, one after-release SNAPSHOT: https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/org/broadinstitute/gatk/4.0.0.0-1-g9423d25-SNAPSHOT/. Is this in purpose or should it be present? It looks like the current commit https://github.com/broadinstitute/gatk/commit/8463525cc9b523cd00daf2810bcf1c13b69ce0a1 does contain a proper artifact, but I am wondering how long the snapshots are going to be maintained. Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4565:85,update,update,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4565,2,"['release', 'update']","['release', 'update']"
Deployability,"I have been running into an issue with Funcotator where some mutations are causing Funcotator to crash because it attempts to query a segment that extends beyond the boundary of the transcript ( see https://github.com/broadinstitute/gatk/issues/6345 ). This pull request addresses the issue by adding a check for transcript length before executing the query. I looked at the code, and Funcotator currently handles problematic sequence queries in `getFivePrimeUtrSequenceFromTranscriptFasta()` by returning an empty string. I modified `getFivePrimeUtrSequenceFromTranscriptFasta()` to also return an empty string when the segment it is trying to retrieve extends beyond the boundary of the transcript. . I have a small VCF that can be used to reproduce the problem using the current code on `master` and the hg38 data source, and I have verified that this pull request allows Funcotator to process the problematic variant without crashing. I did not add the VCF to the tree, but can provide it if that is preferred. Is there any guidance for how to implement integration tests with funcotator? The Funcotator data source I am using is ~12gb, but I would think the problem could be reproduced with 1 transcript and 1 variant. This is my first pull request to GATK, so please let me know if there is anything you would like me to adjust, I'm happy to address any comments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6546:1058,integrat,integration,1058,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6546,1,['integrat'],['integration']
Deployability,I have been trying to run the tutorial ( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ). When running without the --spark-master the turoail runs smoothly. Bu twhen I try my spark master I get an error. I downloaded SPARK 2.2.0 with hadoop 2.7.3; Java is 1.8.0_131; I set the java classpath (I think correctly); I am aware of this thread: https://github.com/broadinstitute/gatk/issues/3050. But noentheless I cannot get the error to solve. I tried to copy the jar files:; hbase-client-1.4.3.jar; hbase-common-1.4.3.jar; hbase-hadoop2-compat-1.4.3.jar; hbase-protocol-1.4.3.jar; hbase-server-1.4.3.jar; To my spark jar folder. Shall I do smething else? I am also a SPARK newbie. Thank you very much!. ***************** Here is the error log:. ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:124,pipeline,pipeline,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['pipeline'],['pipeline']
Deployability,"I have managed to generate a minimal bam file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6045:333,release,releases,333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045,2,['release'],['releases']
Deployability,"I have noticed that running print reads with a stringent filter which I expect to only return a handful of reads results in the progress meter never printing any progress. This makes it look like the gatk has hung despite the fact it is chugging away and filtering every read it passes over. This should be updated to include an indication of how many reads have been filtered. Additionally, it should be improved to use a second thread to make periodic updates based on execution time in case the tool really has hung in order to make it clearer to the user what is going on.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4641:307,update,updated,307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4641,2,['update'],"['updated', 'updates']"
Deployability,"I have three main reasons to propose to move the arguments in CLP to an argument collection that is configurable by downstream tools/projects:. 1. Support hiding some arguments for downstream projects. For example, I do not want to support a config file by the user, but rather decide the settings for the framework and expose only some configuration.; 1. Set custom defaults for some downstream tools (including GATK). For example, a concrete tool might want to force the temp directory to be specified to avoid failures due to no space (and specify that in the documentation).; 1. Support old-style arguments (not kebab-case) for downstream projects that rely on the current argument definitions. I am specially affected by this one, because updating GATK to the 4.0.0 release of January will be a breaking change that will cause some nightmares for my users - and I don't want to do a major version bump yet (I have to re-work a bit my own framework before it). Thus, the first commit of this PR holds the proposal for the new argument collection. As I know that the team is also trying to normalize arguments and documentation, I included two more commits to help with the task (they can be removed if you think that it is better after the argument collection):; * Use `java.nio.Path` for temp directories (to support temp directories in HDFS, for example); * Change arguments moved to the collection to kebab-case (to help with #3853)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998:337,configurat,configuration,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"I have to deal with this component recently and I found the design rather awkward.... In general between GATK and htsjdk we don't seem to have a proper support for managing and querying Supplementary alignment information from read alignment records:. 1. Querying: implemented in htsjdk consists in forging artificial SAMRecords that contain only the alignment info in the SA tag element... It seems to me that it makes more sense to create class to hold this information alone (e.g. ReadAlignmentInfo or ReadAlignment); SATagBuilder already has defined a private inner class with that in mind ""SARead"" so why not flesh it out and make it public. 2. Writing: currently SATagBuilder gets attached to a read, parsing its current SA attribute content into SARead instances. It provides the possibility adding additional SAM record one by one or clearing the list. ... then it actually updates the SA attribute on the original read when a method (setTag) is explicitly called.; I don't see the need to attach the SATag Builder to a read... it could perfectly be free standing; the same builder could be re-apply to several reads for that matter and I don't see any gain in hiding the read SA tag setting process,... even if typically this builder output would go to the ""SA"" tag, perhaps at some point we would like to also write SA coordinate list somewhere else, some other tag name or perhaps an error message... why impose this single purpose limitation?; I suggest to drop the notion of a builder for a more general custom ReadAlignmentInfo (or whatever name) list. Such list could be making reference to a dictionary to validate its elements, prevent duplicates, keep the primary SA in the first position... etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3324:882,update,updates,882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3324,1,['update'],['updates']
Deployability,"I just pulled to the latest version and was surprised to see `gradlew clean` not work!; ```; $ ./gradlew clean; (...); Could not find org.broadinstitute:barclay:1.0.0-24-g87c3fa2-SNAPSHOT; ```. Reverting to 1.0.0-17-g30db73c-SNAPSHOT didn't work (same error).; Reverting to 1.0.0 made it fail somewhere else, with:; Could not resolve org.broadinstitute:gatk-bwamem-jni:1.0.0-rc1-SNAPSHOT. What's going on? Is there something wrong with my configuration?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579:439,configurat,configuration,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579,1,['configurat'],['configuration']
Deployability,I just ran the pipeline from master on the WGS1 BAM file with the following parameters:. ```; /Users/cwhelan/Documents/code/gatk/gatk StructuralVariationDiscoveryPipelineSpark -I hdfs://cw-test-m:8020/data/G94794.CHMI_CHMI3_WGS1.cram.bam -O hdfs://cw-test-m:8020/output/variants/inv_del_ins.vcf -R hdfs://cw-test-m:8020/reference/Homo_sapiens_assembly38.2bit --aligner-index-image /mnt/1/reference/Homo_sapiens_assembly38.fasta.img --exclusion-intervals hdfs://cw-test-m:8020/reference/Homo_sapiens_assembly38.kill.intervals --kmers-to-ignore hdfs://cw-test-m:8020/reference/Homo_sapiens_assembly38.kill.kmers --cross-contigs-to-ignore hdfs://cw-test-m:8020/reference/Homo_sapiens_assembly38.kill.alts --breakpoint-intervals hdfs://cw-test-m:8020/output/intervals --fastq-dir hdfs://cw-test-m:8020/output/fastq --contig-sam-file hdfs://cw-test-m:8020/output/assemblies.sam --target-link-file hdfs://cw-test-m:8020/output/target_links.bedpe --exp-variants-out-dir hdfs://cw-test-m:8020/output/experimentalVariantInterpretations -- --spark-runner GCS --cluster cw-test --num-executors 20 --driver-memory 30G --executor-memory 30G --conf spark.yarn.executor.memoryOverhead=5000 --conf spark.network.timeout=600 --conf spark.executor.heartbeatInterval=120 --conf spark.driver.userClassPathFirst=false; ```. It failed near the end of the pipeline. Here is the tail of the log:. ```; 20:38:14.368 INFO StructuralVariationDiscoveryPipelineSpark - Used 3549 evidence target links to annotate assembled breakpoints; 20:38:14.462 INFO StructuralVariationDiscoveryPipelineSpark - Called 662 imprecise deletion variants; 20:38:14.492 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 7234 variants.; 20:38:14.506 INFO StructuralVariationDiscoveryPipelineSpark - INV: 184; 20:38:14.506 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 4486; 20:38:14.506 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1170; 20:38:14.506 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1394; 18/01/12 20:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:15,pipeline,pipeline,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['pipeline'],['pipeline']
Deployability,"I made no changes to the copied files. @kcibul @ahaessly let me know if I'm missing anything. I left some of the demo bash scripts thinking that we didn't necessarily need to keep them now that we have WDLs, but let me know if you want me to move anything else. . The readme update here assumes that #6881 will get merged. I also turned off the tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6902:275,update,update,275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6902,1,['update'],['update']
Deployability,"I moved the WDL for importing the array manifest from the variantstore repo and added a test. The test here only checks that the WDL succeeded, it doesn't look a the results (yet). It's ingesting the manifest to a dataset with a 7 day TTL, so the tables eventually get cleaned up. That might be too long for this case, since it adds a table each time the test is run (so on push and PR). . I plan to add more of the ""end-to-end"" pipeline with more testing in the future using a similar scheme, so welcome feedback on the structure.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6860:429,pipeline,pipeline,429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6860,1,['pipeline'],['pipeline']
Deployability,"I noticed a huge amount of ""Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given"" logging in some of our pipeline jobs. This patch fixes two things:. 1) I propose GATK only log this message once, the first time it hits this. 2) I think the logic here is wrong. It lacked parenthesis. This logic was:. ```; if (!jumboGenotypeAnnotations.isEmpty() && !fragmentLikelihoods.isPresent() || !haplotypeLikelihoods.isPresent()) {; ```. In java: ""false && false || true"" gives true, which is not what you want. For example, if jumboGenotypeAnnotations was empty, this would still log if either fragmentLikelihoods or haplotypeLikelihoods tests true. That doesnt seem like what you want. In contrast, false && (false || true) still tests false. Adding the parentheses fixes the logic.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7612:154,pipeline,pipeline,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612,2,"['patch', 'pipeline']","['patch', 'pipeline']"
Deployability,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6037:131,release,releases,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037,2,['release'],['releases']
Deployability,"I obtain this reproducible issue with gatk 4.1.3.0:. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/78rz5wrhu9zkfzh/AACW9ZPhl4WnD-wmAkKcdHT3a?dl=0. Then setup a GATK working environment:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.3.0/gatk-4.1.3.0.zip; unzip gatk-4.1.3.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405\; .15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict; ```. Now if I run Mutect2:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. This will generate a VCF file with one variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:6,21:0.778:27:4,8:0,11:2,4,12,9; ```; With an allelic depth of six supporting the reference. However, there are only four fragments supporting the reference. If I remove those for fragments from the BAM file:; ```; samtools view -h input.bam | \; grep -v "":6112\|:10233\|:18618\|:20229"" | \; samtools view -Sb -o input2.bam && \; samtools index input2.bam; ```. And I run Mutect2 again:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input2.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. It will generate a VCF with the same variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:0,20:0.954:20:0,7:0,11:0,0,11,9; ```; With an allelic depth of zero supporting the reference. The same problem exists with the HaplotypeCaller. I believe this was not the intended ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6096:302,release,releases,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096,2,['release'],['releases']
Deployability,"I recently noticed a series of what were evidently memory failures when running HaplotypeCaller on some standard test WGS data when using the exact task used in the warp pipeline here: https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/variant_calling/VariantCalling.wdl. I found that running that wdl with otherwise default inputs except for `haplotype_scatter_count` being set to 10 (so each node doing approximately 5x as much work as when the default, 50, is set) I would get repeated HaplotypeCaller job failures after a few hours that had the pattern of memory failures. The errors tend to involve HaplotypeCaller abruptly ending without any sort of error message or exception at all (which could indicate the vm is dying):; ```; 03:22:15.993 INFO ProgressMeter - chr13:18173014 378.6 1419490 3749.0; 03:22:26.338 INFO ProgressMeter - chr13:18177988 378.8 1419530 3747.4; 03:22:36.801 INFO ProgressMeter - chr13:18203610 379.0 1419700 3746.1; (END); ```; Or alternatively it seems to end without the end-of-run messages being output:; ```; 23:05:30.662 INFO ProgressMeter - chr2:47207099 428.8 1372310 3200.4; 23:05:40.859 INFO ProgressMeter - chr2:47323745 429.0 1372960 3200.7; 23:05:50.896 INFO ProgressMeter - chr2:47476709 429.1 1373720 3201.2; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6933m -Xms6933m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller [INPUTS]; 2022/02/10 23:06:52 Starting delocalization.; 2022/02/10 23:06:53 Delocalization script execution started...; ```. These failures appear to be reproducible and happen at about the same point in every run. The fact that increasing the memory or decreasing the interval per shard seems to remove the issue it makes me suspect there might be an issue where Hapl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7693:170,pipeline,pipeline,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7693,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,I run `build/install/hellbender/bin/hellbender BaseRecalibrator`; and I get this scary message (notice the three colons (`:`) on that line and lots of UPPERCASE). ```; ***********************************************************************. A USER ERROR has occurred: Invalid command line: Argument RECAL_TABLE_FILE was missing: Argument 'RECAL_TABLE_FILE' is required. ***********************************************************************; ```. I think it should say something like this:. ```; ***********************************************************************. Invalid command line for BaseRecalibrator: Required argument RECAL_TABLE_FILE was missing. Run BaseRecalibrator -h to see all arguments. ***********************************************************************; ```. @vdauwera please weigh in on what's most useful,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/418:13,install,install,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/418,1,['install'],['install']
Deployability,"I saw this message recently when running gcloud:. ```; WARNING: `gcloud auth login` no longer writes application default credentials.; If you need to use ADC, see:; gcloud auth application-default --help; ```. It looks like we may have to update our travis script / our instructions for running on cloud files at some point in the near future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2660:239,update,update,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2660,1,['update'],['update']
Deployability,"I tested this by:; - running the `GvsAssignIds` workflow; - manually updated the `sample_ids` to span from 1 to 15535; - manually created the `vet_002`, `vet_003`, `vet_004`, `ref_ranges_002`, `ref_ranges_003`, `ref_ranged_004` tables; - running the `GvsImportGenomes` workflow; - running the `GvsPopulateAltAllele` workflow with `max_alt_allele_shards` to 3 so that it would divide the vet tables into (at most) 3 files; see https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/b1b319bc-a2aa-44cb-ad9a-079b7c1c33de for `GvsPopulateAltAllele` run",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7998:69,update,updated,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7998,1,['update'],['updated']
Deployability,"I think hdfview might be broken for e.g. some Ubuntu distributions. Just need to change some tool docs, but might also want to update tutorials. See context in #6924.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6927:127,update,update,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6927,1,['update'],['update']
Deployability,"I think this makes sense, but is low priority and can definitely wait until after release.; Add tests for theano HMM + example use case for Hybrid ADVI.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4043:82,release,release,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4043,1,['release'],['release']
Deployability,I think we should make an effort to keep the project at 0 warnings. We could do this during the bugfixing freeze every release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/118:119,release,release,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/118,1,['release'],['release']
Deployability,"I tried running MarkDuplicatesSpark with multiple inputs like it is run in production and got this user error. ```; A USER ERROR has occurred: Sorry, we only support a single reads input for spark tools for now.; ```. For this to go into production it would need to have the ability to take in multiple inputs (I'm currently trying to make a ""fast"" version of the production germline pipeline and it would be great to have this tool included in that pipeline). @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5398:384,pipeline,pipeline,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5398,2,['pipeline'],['pipeline']
Deployability,"I tried to process data with BQSRPipelineSpark( the latest released gatk4 beta version), but the job always failed at middle of processing.; I use ERR000589, the bam file size is 1.3G.; knownSites uses dbsnp_138.hg19.vcf, and the data size is 10G.; reference is ucsc.hg19.2bit, data size is 0.8G .; it was running on spark2.0, and there are 4 worker in total. Each node has 16 physical cores and 64G data memory.; Below is my command.; ./gatk-launch BQSRPipelineSpark -I hdfs:///user/xxx/ERR000589.bwa.mark.bam -O hdfs:///user/xxx/ERR000589.bwa.marked.bqsr.bam -R hdfs:///user/liucheng/refs/ucsc.hg19.2bit --knownSites hdfs:///user/liucheng/dbsnp/dbsnp_138.hg19.vcf -- --sparkRunner SPARK --sparkMaster spark://cu11:7077 --total-executor-cores 48 --executor-cores 6 --executor-memory 25G --driver-memory 30G. The log is attached as follow:; [July 19, 2017 2:39:55 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BQSRPipelineSpark done. Elapsed time: 3.24 minutes.; Runtime.totalMemory()=23515365376; com.esotericsoftware.kryo.KryoException: java.lang.NegativeArraySizeException; Serialization trace:; vs (org.broadinstitute.hellbender.utils.collections.IntervalsSkipListOneContig); intervals (org.broadinstitute.hellbender.utils.collections.IntervalsSkipList); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:109); 	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3303:59,release,released,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303,2,"['pipeline', 'release']","['pipelines', 'released']"
Deployability,"I tried to run VariantRecalibrator using the args echoed from an integration test and found that the resource files weren't listed properly. The command in the test was ` "" --resource known,known=true,prior=10.0:"" + getLargeVQSRTestDataDir() + ""dbsnp_132_b37.leftAligned.20.1M-10M.vcf""` and what came out of the engine was `--resource known:/Users/gauthier/workspaces/gatk/src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf`, so it lost the known=true and the prior which makes the command line unrunnable. Probably affects #2269 too. This behavior can be replicated by running any of the VariantRecalibration integration tests and checking the console output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3247:65,integrat,integration,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3247,2,['integrat'],['integration']
Deployability,"I tried to run the whole test suite with `./gradlew clean test`, and several tests are failing with `org.broadinstitute.hellbender.exceptions.UserException$HardwareFeatureException: Machine does not support AVX PairHMM`:. * `HaplotypeCallerIntegrationTest`; * `HaplotypeCallerSparkIntegrationTest`; * `ReadsPipelineSparkIntegrationTest`. Because there are already several tests skipped if the support is not present (e.g., `VectorPairHMMUnitTest`), I expect that the tests do not fail and are skipped instead. I understand that maybe it is important to keep them failing with the GKL implementation for integration, but maybe a setting a flag to force them to do not be skipped would be enough in the travis build to check that nothing is broken, and still do not be scare if lots of tests fail after an unrelated change in a non-AVX machine...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3732:603,integrat,integration,603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3732,1,['integrat'],['integration']
Deployability,"I updated from 4.0.4.0 to 4.0.6.0 and noticed either a memory bug or spike in GenotypeGVCF. . Based on https://github.com/EvanTheB/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.simple.wdl:. ```; ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenomicsDBImport \; --genomicsdb-workspace-path ""$genomicsdb"" \; --batch-size ""50"" \; -L ""chr18:1-80373285"" \; --sample-name-map ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/-321562876/sample_map"" \; --reader-threads 5 \; -ip 500. tmp_vcf=""$TMPDIR""/tmp.vcf.gz. ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenotypeGVCFs \; -R ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.fasta"" \; -O ""$tmp_vcf"" \; -D ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf"" \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; --use-new-qual-calculator \; -V gendb://""$genomicsdb"" \; -L ""chr18:1-80373285"". ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; VariantFiltration \; --filter-expression ""ExcessHet > 54.69"" \; --filter-name ExcessHet \; -O ""output.vcf.gz"" \; -V ""$tmp_vcf""; ```. And a SGE hard memory limit of 40G (GenotypeGVCFs has -Xmx8g).; On gatk 4.0.4.0 I see peak memory usage of 15.7G, while with gatk 4.0.6.0 I get:. ```; ...; 19:06:23.757 INFO GenotypeGVCFs - Initializing engine; 19:06:24.785 INFO FeatureManager - Using codec VCFCodec to read file file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e9bf8c5e-3e70-476a-9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024:2,update,updated,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024,1,['update'],['updated']
Deployability,I used the very last version of the WDL using the last master commit for the docker and the name of the output files seem to be a random permutation of the actual sample names. The name inside the file (i.e. the one listed in the #CHROM line) seems to be correct. A previous run using a earlier version of the WDL (probably the one just before the last update) and the latest official gatk docker didn't have this issue.; .,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5217:353,update,update,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5217,1,['update'],['update']
Deployability,I was testing the latest beta.6 release and ran into an issue that HaplotypeCallerSpark no longer outputs bgzipped VCF outputs. Specifying `--output outfile.vcf.gz` produces a plain unzipped VCF. This worked in the beta.5 release so appears to be due to recent changes. Here is a small self-contained test case that reproduces the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk/gatk4_spark_output.tar.gz. Please let me know if I can provide anything else that would help. Thanks as always for all the improvements and work on GATK.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3725:32,release,release,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3725,2,['release'],['release']
Deployability,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2260:198,patch,patch,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260,1,['patch'],['patch']
Deployability,"I would like to start a new project to extend the engine of GATK (mostly walker types, e.g. https://github.com/broadinstitute/gatk/issues/1198 and common utilities), and thus I require to have an idea of how the versioning scheme is related to the public API if at all. This will allow me to say that the extensions works with GATK between 4.0.0.0 and 4.1.0.0, for example, and to know when some work is required to move to the next released version. Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4603:433,release,released,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603,1,['release'],['released']
Deployability,I'm continuously sad that my shell can't autocomplete GATK commands. It would be great if we could generate completion files for bash and zsh automatically. This could piggyback on the upcoming documentation generation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1454:4,continuous,continuously,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1454,1,['continuous'],['continuously']
Deployability,"I'm missing the possibility of tuning the logging to produce TRACE level log messages. As it is stands the users only can choose down to DEBUG. . It seems that this is due to the integration of several logging systems from old GATK, htjsdk and picard where DEBUG the lowest common level. . It would be great to have the ability to produce TRACE level log messages allowing the user to have control on whether these are output or not. . In this case DEBUG log messages that are going to be produced in big numbers should be TRACE whereas unfrequent (one very 5 second or more) would stay as DEBUG.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6378:179,integrat,integration,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6378,1,['integrat'],['integration']
Deployability,"I'm told there will be a new gcloud release any day now (https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2110) with @jean-philippe-martin 's NIO retry fixes. We should update as soon as it's out, and confirm that it resolves https://github.com/broadinstitute/gatk/issues/2749, https://github.com/broadinstitute/gatk/issues/2685, and (possibly) https://github.com/broadinstitute/gatk/issues/2686",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2822:36,release,release,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2822,2,"['release', 'update']","['release', 'update']"
Deployability,"I'm trying to figure out the best way to replicate GATK3 behavior in GATK4. The GATK4 VariantWalker iterates all variants from VCF(s), calling apply() once per variant. If the input VCFs has duplicates at a given location, apply() is called multiple times for the same locus. In GATK3, VariantEval iterates each locus, and generates a list of variants at that site. I'm trying to figure out the most efficient way to do this in GATK4. One solution is to override traverse(), and add some kind of groupingBy step, for example:. StreamSupport.stream(getSpliteratorForDrivingVariants(), false); .filter(variantfilter); .collect(Collectors.groupingBy(x -> new SimpleInterval(x))); .values(); .forEach(variantList -> {; final SimpleInterval variantInterval = new SimpleInterval(variantList.get(0));; apply(variantList,; new ReadsContext(reads, variantInterval, readFilter),; new ReferenceContext(reference, variantInterval),; new FeatureContext(features, variantInterval));. progressMeter.update(variantInterval);; });. This will get me the right end result (like of variants per site); however, it's not clear to me if this is the most efficient route, and I'm not sure if it's aware of the sorted input. Because the input data are sorted, I could iterate, track the previous location and maintain a list of track variants per site. Each time we hit a new location I call apply() with that list. Are the places in GATK4 that already do this type of per-locus grouping?. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4456:984,update,update,984,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4456,1,['update'],['update']
Deployability,"I'm trying to genotype 2388 whole genome samples of a wild species. This species has a large genome and lots of diversity. I've created my genomicsDB for my combined set and genotypeGVCF gets stuck when trying to make the VCF. ; I've been giving it 120G of ram, 32 cores and 30 minutes and it only prints out the first variable 12 sites (corresponding to about 800bp of the genome) to the VCF . I understand that is certainly going to be slow, and I'm prepared to heavily parallelize it, but this is currently unusable to me. Is there any way to speed it up?. Here's my command:. ```; /gatk/gatk-launch --java-options ""-Xmx120G"" GenotypeGVCFs \; -R /home/user/bin/ref/reference.fa \; --intervals $contig \; -V gendb://${chr}_$pos \; -O /scratch/wild_gwas/$genomicsdb/${chr}_$pos.tmp.vcf.gz \; --seconds-between-progress-updates 5 --verbosity DEBUG; ```; Here's standard out:. > 21:13:04.092 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/install/gatk/lib/gkl-0.8.2.jar!/com/intel/gkl/native/libgkl_compression.so; > 21:13:04.108 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/gowens/libgkl_compression3380966567685792416.so; > 21:13:04.218 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:13:04.219 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.0.0; > 21:13:04.219 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; > 21:13:04.219 INFO GenotypeGVCFs - Executing as user@cdr806.int.cedar.computecanada.ca on Linux v3.10.0-693.5.2.el7.x86_64 amd64; > 21:13:04.219 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-2ubuntu1.16.04.3-b11; > 21:13:04.219 INFO GenotypeGVCFs - Start Date/Time: January 15, 2018 9:13:04 PM UTC; > 21:13:04.219 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:13:04.220 INFO GenotypeGVCFs - -------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:820,update,updates,820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,2,"['install', 'update']","['install', 'updates']"
Deployability,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2033:103,integrat,integration,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033,3,"['Integrat', 'integrat']","['IntegrationTestSpec', 'integration', 'integrations']"
Deployability,"I'm using the `ReadLikelihoods` in a test and I'm setting the likelihoods to negative values. Nevertheless, when I'm trying to get the best alleles for each read using the `bestAlleles()` method, it turns out to return the allele where I haven't set any likelihood (by default, 0). I think that the bug is in the private method [`searchBestAllele`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/genotyper/ReadLikelihoods.java#L438), were if the candidate likelihood is __bigger than__ the best likelihood, the best allele is updated. I apologize in advance if this is not a bug, but I would like to know if I should use negative likelihoods as in the removed `PerReadAlleleLikelihoodMap`, or positive ones, as suggest the current implementation for getting the best alleles. Thank you in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2311:580,update,updated,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2311,1,['update'],['updated']
Deployability,"I'm working on an imputation pipeline right now, and the contigs in the returned VCF header don't contain lengths. This fix to UpdateVCFSequenceDictionary allows me to force an update to the VCF's sequence dictionary so I have a valid VCF I can use with the rest of our tools when both --replace and --disable-sequence-dictionary-validation are set to true.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6140:29,pipeline,pipeline,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6140,3,"['Update', 'pipeline', 'update']","['UpdateVCFSequenceDictionary', 'pipeline', 'update']"
Deployability,I'm working on some Plasmodium falciparum callsets in GATK and I have come across a curious error:. ```; Using GATK wrapper script /juffowup/gatk/build/install/gatk/bin/gatk; Running:; /juffowup/gatk/build/install/gatk/bin/gatk HaplotypeCaller -R /juffowup2/malaria/references/PlasmoDB-61_Pfalciparum3D7_Genome.fasta -I /juffowup2/malaria/haplotypecaller_arg_testing/fixed_bam/PG0004-CW.aligned.merged.markDuplicates.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:152,install,install,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,2,['install'],['install']
Deployability,"I've completely rewritten the documentation portion to be more helpful to users (and to reflect the new M2 rather than gatk3's M2), and I have updated the example commands. ---; ### Questions for @davidbenjamin ; - `--af_of_alleles_not_in_resource`: is this allele frequency used only in certain contexts, e.g. with matched normal analyses, or towards tumor sample variant alleles, etc.? I need to add to the doc details how this argument factors into calculations.; - I need a sentence or two describing the new algorithmic improvement on the new Mutect2 integration over uncertainty. ; - The WDLs do not include use of a contamination.table and so I did not include it in the commands. Is this something we want to nudge users to use, i.e. should I put in a sentence in the documentation section about the new tool CalculateContamination?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2816:143,update,updated,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2816,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,"I've had several Travis test failures (on my picard removal branch) that appear to be failures during kryo serialization of a mocked ReferenceMultiSource object (based on the failing class name, (org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource$$EnhancerByMockitoWithCGLIB$$b0dc631f, which looks like the CGLIB names mentioned [here](https://github.com/mockito/mockito/issues/319)). We're on an ancient version of mockito anyway, and newer versions no longer use cglib, so it seemed like a good time to upgrade. To do so I also had to replace usage of the method getArgumentAt, which has been [deprecated](https://github.com/mockito/mockito/pull/373) in favor of getArgument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3581:524,upgrade,upgrade,524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3581,1,['upgrade'],['upgrade']
Deployability,"I've run into an error using a certain BAM file I created for testing. Possibly relevant: I also tried running it through PrintReads - all reads were filtered out by the WellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2219:566,install,install,566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219,3,['install'],['install']
Deployability,I've updated the image and it seems to be working locally. Now for the true test...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8228:5,update,updated,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8228,1,['update'],['updated']
Deployability,"INFO ProgressMeter - chrUn_KI270743v1:125398 9.9 6674000 675662.2; 13:55:00.673 INFO ProgressMeter - chr20_KI270869v1_alt:62679 10.0 6792000 676161.8; 13:55:10.679 INFO ProgressMeter - chr19_GL949752v1_alt:485077 10.2 6910000 676673.7; 13:55:26.149 INFO ProgressMeter - HLA-DRB1*11:01:02:3272 10.5 6938356 662718.7; 13:55:26.149 INFO ProgressMeter - Traversal complete. Processed 6938356 total records in 10.5 minutes.; 13:55:26.149 INFO ComposeSTRTableFile - Shutting down engine; [April 4, 2021 1:55:26 PM EDT] org.broadinstitute.hellbender.tools.dragstr.ComposeSTRTableFile done. Elapsed time: 10.52 minutes.; Runtime.totalMemory()=1128792064; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:55:31.182 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.183 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:55:31.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:11512,install,install,11512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['install'],['install']
Deployability,Ideally using a standard mechanism like Apache `Configuration`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368:48,Configurat,Configuration,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368,1,['Configurat'],['Configuration']
Deployability,Identify inputs and outputs for each tool in the data pre-processing pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/457:69,pipeline,pipeline,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/457,1,['pipeline'],['pipeline']
Deployability,Identify inputs and outputs for tools in the read pre-processing pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/458:65,pipeline,pipeline,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/458,1,['pipeline'],['pipeline']
Deployability,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2081:617,Integrat,IntegrationTestSpec,617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081,2,['Integrat'],['IntegrationTestSpec']
Deployability,If the requested key is missing in an Avro record:. - Avro 1.11 [throws](https://github.com/apache/avro/blob/release-1.11.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L267-L269); - Avro 1.8 [returns null](https://github.com/apache/avro/blob/release-1.8.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L208). Most of the code here was written for Avro 1.8 behavior; these changes adapt for Avro 1.11.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8266:109,release,release-,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8266,2,['release'],['release-']
Deployability,"If there are overlapping (e.g. a long SNV overlapping an INDEL) or multi-allelic germline variants, Mutect2 will check only the AF of the first variant/allele when searching for germline sites to exclude during active region detection. This PR updates the logic to iterate through all germline alleles.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7468:244,update,updates,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7468,1,['update'],['updates']
Deployability,"If you haven't ""built"" the wrapper script, it complains. Strictly speaking, if you're only ever going to use Spark, you don't need the wrapper. The check should only happen if you choose the local walker impl. ```; [ec2-user@ip-10-1-1-71 gatk-4.alpha.rc1]$ ./gatk-launch -h; Missing GATK wrapper script: /home/ec2-user/gatk-4.alpha.rc1/build/install/gatk/bin/gatk; To generate the wrapper run:. /home/ec2-user/gatk-4.alpha.rc1/gradlew installDist; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1331:342,install,install,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1331,2,['install'],"['install', 'installDist']"
Deployability,"If you'd prefer i post this kind of question elsewhere, please let me know. My lab creates a large dataset of macaque variant data. We regularly add new samples to a dataset that currently has ~2300 WGS/WXS datasets. We largely follow the GATK short variant calling pipeline. Our gVCF data are aggregated into a GenomicsDb workspace, followed by GenotypeGVCFs. As is, whenever we get new samples, we append them to this growing GenomicsDb workspace, and then re-call all of the genotypes. These steps are getting slower and slower (even when scatter/gathered on a cluster), and I'm concerned it's going to become untenable. Plus it's just really inefficient to constantly re-call 1000s of datasets at 40m genome-wide sites. My question is: do you have any experience with analogous datasets, where you have a large base of ""static"" datasets with regular additions of new data? It would be quite nice to avoid constantly re-genotyping the existing datasets. We could in theory just run GenotypeGVCFs on the incoming data and do a simple merge with the existing data. Are you aware of anyone running a process that looks more like this?. There are some caveats to this: 1) for the incoming batches of data, we could run GenotypeGVCF where we force it to call genotypes from every site that exists in the current dataset. This would promote consistent calling across a common set of sites, 2) after we genotype the incoming batch, we could compare the sites present in that against the sites in the current data. It's likely there would be a handful of novel sites. We could re-run GenotypeGVCFs on the existing data specifically on those new sites (presumably the existing animals are largely WT at those positions), and merge those new sites with the existing data, 3) we then merge the incoming data with the updated core data, which should each have genotypes called at the identical set of sites. Are there any discussions happening about managing/updating large variant datasets like this? Thanks f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7526:266,pipeline,pipeline,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7526,1,['pipeline'],['pipeline']
Deployability,"Implemented CombineRawData() and GenerateRawData() methods from GATK3 in all Allele Specific annotations and added tests designed to mimic the existing CombineGVCFs integration tests in GATK3 by asserting the combined output matches that of GATK3. This could still use more substantial tests for finalizeRawAnnotations and AnnotateRawData. Additionally, I would like to ask for advice as to how I should go about further implementing tests for the annotation classes. . Fixes #1893; Fixes #3535",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3527:165,integrat,integration,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3527,1,['integrat'],['integration']
Deployability,Implemented VCF ID for VCF data sources. - Now VCF data sources create an ID field for the ID of the variant; used for the annotation. - Updated the regression test suite with a VCF data source to increase; test coverage. Fixes #5186,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5327:137,Update,Updated,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5327,1,['Update'],['Updated']
Deployability,"Implements #1552 . When the StrandAlleleCountsBySample (SAC) annotation is present in VCFs, allele subsetting by SelectVariants will now update this field in the final VCF. **Summary of changes**; - `SelectVariants.subsetRecord()` uses the updated `GATKVariantContextUtils.updatePLsSACsAD()`, which contains the machinery to subset SAC by the used alleles.; - Added unit and integration tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1852:137,update,update,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852,4,"['integrat', 'update']","['integration', 'update', 'updatePLsSACsAD', 'updated']"
Deployability,"Implements a Bwa Spark tool for the PathSeq pipeline. . Tool input:; 1) BAM of paired reads; 2) BAM of unpaired reads; 3) Bwa index image file. Output:; 1) BAM of paired alignments; 2) BAM of unpaired alignments. Notes:; - The tool does not generate secondary/supplementary alignments. ; - Alternate alignments are written to the SA tag. ; - Only the sequences for which there was at least 1 alignment are written to the BAM headers (instead of writing all sequences in the reference, which is 10,000's for the pathogen database and substantially increases the run time of the subsequent scoring tool).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3113:44,pipeline,pipeline,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3113,1,['pipeline'],['pipeline']
Deployability,"Implements https://github.com/broadinstitute/gatk/issues/1382.; Remove diploid assumptions during allele subsetting. **Summary**; - Changed `GenotypeLikelihoods.GenotypeLikelihoodsAllelePair GenotypeLikelihoods.getAllelePair(int PLindex)` to `ArrayList<Integer> GenotypeLikelihoods.getAlleles(int PLindex, int ploidy)` in GATKVariantContextUtils. ; - Added integration and unit tests.tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1845:357,integrat,integration,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1845,1,['integrat'],['integration']
Deployability,"Implements tool for clustering SVs, built on top of the clustering engine code refined recently in #7243. In addition to a few bug fixes, updates also include:. - `PloidyTable` class, which ingests and serves as a simple data class for a tsv of per-sample contig ploidies. This was necessary for inferring genotypes when input vcfs contain non-matching sample and variant records.; - Modified `SVClusterEngine` to render sorted output.; - Improved code for SV record collapsing (see the `CanonicalSVCollapser`), particularly for CNVs. Genotype collapsing now infers allele phasing in certain unambiguous cases, in particular for DUPs and multi-allelic CNVs. Testing for this has been cleaned up and augmented with further cases to validate this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7541:138,update,updates,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7541,1,['update'],['updates']
Deployability,"Implements two new tools and updates some methods for a revamp of the `CombineBatches` cross-batch integration module in [gatk-sv](https://github.com/broadinstitute/gatk-sv). - `SVStratify` - tool for splitting out a VCF by variant class. Users pass in a configuration table (see tool documentation for an example) specifying one or more stratification groups classified by SVTYPE, SVLEN range, and reference context(s). The latter are specified as a set of interval lists using `--context-name` and `--context-intervals` arguments. All variants are matched with their respective group which is annotated in the `STRAT` INFO field. Optionally, the output can be split into multiple VCFs by group, which is a very useful functionality that currently can't be done efficiently with common commands/toolkits.; - `GroupedSVCluster` - a hybrid tool combining functionality from `SVStratify` with `SVCluster` to perform intra-stratum clustering. This tool is critical for fine-tuned clustering of specific variants types within certain reference contexts. For example, small variants in simple repeats tend to have lower breakpoint accuracy and are typically ""reclustered"" during call set refinement with looser clustering criteria.; - `SVStratificationEngine` - new class for performing stratification.; - Updates to breakpoint refinement in `CanonicalSVCollapser` that should improve breakpoint accuracy, particularly in larger call sets. Raw evidence support and variant quality are now considered when choosing a representative breakpoint for a group of clustered SVs.; - Added `FlagFieldLogic` type for customizing how `BOTHSIDE_PASS` and `HIGH_SR_BACKGROUND` INFO flags are collapsed during clustering.; - `RD_CN` is now used as a backup if `CN` is not available when determining carrier status for sample overlap.; - Removed no-sort option in favor of spooled sorting.; - Bug fix: support for empty EVIDENCE info fields; - Bug fix: in one of the JointGermlineCnvDefragmenter tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8990:29,update,updates,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8990,4,"['Update', 'configurat', 'integrat', 'update']","['Updates', 'configuration', 'integration', 'updates']"
Deployability,Improve ChimericAlignment in SV pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3746:32,pipeline,pipeline,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3746,1,['pipeline'],['pipeline']
Deployability,"In GATK Office hours we found a change that contributed to this error message. The issue may be a bug or an issue with the data that is showing up with the more strict filters in the latest version. This request was created from a contribution made by Igor Islanov on July 06, 2020 12:11 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360071204731-FilterVariantTranches-brakes-on-new-version-of-Gatk](https://gatk.broadinstitute.org/hc/en-us/community/posts/360071204731-FilterVariantTranches-brakes-on-new-version-of-Gatk). \--. Good day,. While updating gatk from 4.1.4.0 to 4.1.8.0 and after running pipeline it brakes on FilterVariantTranches step with error:. htsjdk.tribble.TribbleException: The provided reference alleles do not appear to represent the same position, C\* vs. T\*. The command line is ; ; gatk FilterVariantTranches -I ${R1%%\_\*}-recal.bam -V ${R1%%\_\*}-annotated.vcf -R /mnt/d/GenLab/WES/reference/hg19.fasta --create-output-variant-index true --resource /mnt/d/GenLab/WES/db/00-All.vcf.gz --resource /mnt/d/GenLab/WES/db/00-common\_all.vcf.gz --resource /mnt/d/GenLab/WES/reference/1000G\_phase1.indels.hg19.sites.vcf --resource /mnt/d/GenLab/WES/reference/Mills\_and\_1000G\_gold\_standard.indels.hg19.sites.vcf --snp-tranche 99.9 --snp-tranche 99.95 --indel-tranche 99.0 --indel-tranche 99.4 -O ${R1%%\_\*}-filtered.vcf --tmp-dir /mnt/d/GenLab/WES/output/tmp --java-options ""-Xmx24G"". On 4.1.4.0 no problems whatsoever, on 4.1.8.0 not working at all. Double-confirmed by 2 seperate conda envs. The reference file is unchanged during whole running processes, obviously. Full error log: ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx24G -jar /mnt/d/GenLab/WES/software/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar FilterVariantTranches -I D1394-recal.bam -V D1394-annotated.vcf -R /mnt/d/GenLab/WES/refe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701:625,pipeline,pipeline,625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701,1,['pipeline'],['pipeline']
Deployability,"In GATK3, when merging variants, the IDs of all the source VCFs were retained. This code path seems like it intended that, since the variantSources set is generated, but it doesnt get used for anything. This PR will use that set to set the source of the resulting merged VC. Note: i dont think I can kick off the test suite. It is possible this change would result in tests breaking, and those would need updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8750:405,update,updates,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8750,1,['update'],['updates']
Deployability,"In PathSeqPipelineSpark, the reads are repartitioned to ~5k per partition (by default) just prior to the pathogen BWA alignment step (to ensure an even distribution of work). Currently, some samples with a lot of non-host reads cause 10,000's of sharded BAMs to be written at the end of the pipeline. This PR reduces the number of partitions in the read RDD just before writing to disk in the PathSeqPipelineSpark tool. It exposes a command-line option for the number of reads per partition, with a default value that results in a much more reasonable number of sharded BAMs in even the worst cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3545:291,pipeline,pipeline,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3545,1,['pipeline'],['pipeline']
Deployability,"In addition to migrating the remaining tool input and output args, there are few miscellaneous packages, tools and utilities that need additional work:; - [x] Rename `GATKPathSpecifier` to `GATKPath`; - [ ] Add `GATKPath` methods for isDirectory, isReadOnly, resolve sibling, resolve child; - [ ] Fix GenomicsDB URL handling/utils (move isGenomicsDB to GATKPathSpecifier); - [ ] GATKSparkTool still uses `String` in many method signatures; - [ ] Update remaining IOUtils methods; - [ ] Update remaining BucketUtils methods (i.e., remove remaining BucketUtils.isHadoopURL overloads(File, Path, String), etc.); - [ ] Update RevertSAMSpark String/File assumptions and intervconversions; - [ ] Add ReferenceFileSource(`GATKPath`) constructor, remove remaining CachingIndexedFastaSequenceFile/overloads; - [ ] Update tools in the pathseq package (PathSeqBwaSpark, PathSeqScoreSpark) that do directory manipulation. [Edit] Somewhat tangentially, PathSeqBwaSpark currently rejects read inputs specified through `--inputs` and uses separate args to allow the user to identify inputs as paired or unpaired. Once this is using `GATKPathSpecifier` this could be changed to use ""--inputs"" annotated with tags instead. Might be a problem for WDL gen though (which doesn't support tags).; - [ ] Test utilities (createTempFile/Dir, etc. that return GATKPath); - [ ] Add a `toHadoopPath` method to `GATKPath` that returns a `org.apache.hadoop.fs.Path`.; - [ ] Change tools that generate multiple output files using a stem (SplitReads, etc) to use the `resolve` methods listed above once they're available.; - [ ] All usages of `PrintStream` should be replaced with `OutputStreamWriter` (code that requires printf-style formatting can use `write` with `String.format` instead of the `printf` methods). `PrintStream` doesn't propagate IOExceptions and instead requires calls to `checkError`, but almost all usages of `PrintStream` don't call it.; - [ ] Update `org.broadinstitute.hellbender.utils.report` (`GATKReport` ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6610:446,Update,Update,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6610,4,['Update'],['Update']
Deployability,"In an effort to make the Reblocking pipeline more user friendly we'd like to always have the option to remove the PRI annotation (which is incorrectly added by certain versions of dragen) from the GVCF turned on, even when PRI is not present in the input. There was an edge case that threw an exception when there were no FORMAT annotations, so this PR fixes that issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8870:36,pipeline,pipeline,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8870,1,['pipeline'],['pipeline']
Deployability,"In doing some performance evaluation work for some other HaplotypeCaller work I have noticed that there is apparently a performance regression on the order of perhaps 10-20% of runtime. Running locally I find that running over the same section of a WGS chromosome 15 on the current master 78a9ecd3123fdb77acf3dd7a73b0c12bf4602a1c vs the release 4.1.5.0 i get the following results: . Master: ; real	12m19.765s; user	13m49.276s; sys	0m8.571s; 4.1.5.0: ; real	9m50.558s; user	11m11.924s; sys	0m10.193s. Doing some very cursory digging it would appear that the culprit is in the HMM adjacent code being slowed down. (Note the relative runtime of HMM vs SW) ; Master: ; <img width=""822"" alt=""Screen Shot 2020-04-23 at 1 28 52 PM"" src=""https://user-images.githubusercontent.com/16102845/80130392-80115780-8566-11ea-8f2b-a6978ac71d39.png"">. 4.1.5.0: ; <img width=""850"" alt=""Screen Shot 2020-04-23 at 1 28 33 PM"" src=""https://user-images.githubusercontent.com/16102845/80130396-80115780-8566-11ea-9a1e-1e923bef47a5.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6567:337,release,release,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6567,1,['release'],['release']
Deployability,"In helping @bhanugandham figure out why a particular site was failing it became apparent that merging dangling head code was failing to recover deletions in the dangling head. Furthermore there is some code in the dangling end recovery code that asserts a certain high standard of matching (usually 1 but sometimes dangling branch length/kmersize) `getMaxMismatches(final int lengthOfDanglingBranch)`. Both of these facts seem likely to cause dangling heads to be dropped despite their being still potentially informative, particularly the indel code. . I have added the ability for the index recovery code to account for the cigar string when merging dangling ends. Addtionally rather than counting mismatches to reject the branch it simply requires a minimum matching end (which can be changed, I suspect this is where the lionshare of the differences come from). Unfortunately changing the tests is non-trivial (as this happened to change the integration test results for HaplotypeCaller at a few sites) so I wanted to get this branch up to solicit advice a to whether it is worth pursuing this fix. @davidbenjamin @ldgauthier @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6113:946,integrat,integration,946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6113,1,['integrat'],['integration']
Deployability,"In light of the discovery of the (relatively minor) numerical differences caused by changes to non-CNV code outlined in #7649, and because we are still awaiting coverage from pipeline-level/CARROT testing, I decided to go ahead and add these exact-match tests. This essentially freezes current ModelSegments behavior, which has been exactly stable since https://github.com/broadinstitute/gatk/pull/5814; that is, from sometime between 4.1.0.0/4.1.1.0 almost 3 years ago up to 4.2.4.1 today. Note that the original test files were generated from the test BAMs (e.g., src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam), since these BAMs have been used in the past to consistently generate test files for other tools in the ModelSegments and GermlineCNVCaller pipelines. However, these original test files contained insufficient data to activate the changes found in #7649, even had exact-match tests been present. I thus took some old HCC1143T 100% WES data that I had and snippeted it to chr20. I've confirmed that the added tests with these files would've picked up the regression of log10factorial seen in #7649 for all relevant modes (i.e., all those that take in the allele counts as input, since that regression only affected allele-fraction MCMC sampling). Tests take maybe an additional minute to run and there was about ~12MB of additional large resources checked in, but I didn't try too hard to bring either down. I also added some early-fail parameter validation to check that the minimum total allele count in the case sample is zero in matched-normal mode. There are actually some open questions in my mind as to what the best behavior should be here, but given some of the discussion in #6499 and possible plans for using joint segmentation to do filtering of germline events, I think it's best to enforce that all het sites coming out of the genotyping step are the same across all samples. Recall that we added this parameter in #55",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652:175,pipeline,pipeline-level,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652,2,['pipeline'],"['pipeline-level', 'pipelines']"
Deployability,"In light of the newly updated MarkDuplicatesSpark which is finally reaching the point where we trust it, we have evaluated that the work of maintaining both this tool and MarkDuplicatesSpark was too great considering how little code they could directly reuse relative to eachother. . Resolves #4896 #3705",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5166:22,update,updated,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5166,1,['update'],['updated']
Deployability,In light of the recent #7357 and #7358 it has become clear that we are blind changes that cause the logging outputs for GATK to become unusable because we are spitting endless warnings to stdout. I think we should change our integration tests to capture the log output for each of our tests and assert that none of them balloon beyond some reasonable threshold that would capture these problems (perhaps a megabyte but it would take a little bit of sleuthing to be sure). . I would think the best place would be to add a capture into `CommandLineProgramTest.runCommandLine()` that instead of using the current behavior `injectDefaultVerbosity()` we instead leave the logging output as the default and capture it somewhere explicit where we can make assertions about the size of the outputs. Possibly we could create a dummy logging level that just saves and counts the outputs so we can make assertions about the logs. Ideally this should apply to every tool simultaneously since it would be too patchwork to simply add logging output tests for enough of the tools to protect us manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7368:225,integrat,integration,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7368,2,"['integrat', 'patch']","['integration', 'patchwork']"
Deployability,"In order for the variant calling pipeline to be able to scale to process the 68K genomes in the next version of gnomAD and beyond, we need to limit the amount of genotype data we localize. For the filtering portion of the pipeline we could greatly improve performance using a ""sites-only"" query from GenomicsDB. The result could be in BCF format (as I believe it is now) and should include the first 8 fields of the VCF line (CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO) but should not return format-level/genotype data.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688:33,pipeline,pipeline,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688,2,['pipeline'],['pipeline']
Deployability,"In order of priority:. 1) The ability to query and/or stream intervals for locatable collections might reduce the overhead of file localization in the germline workflows---even though we only run GermlineCNVCaller on a subset of intervals in any particular shard, we localize the entire read-count file. This could be enabled in the parent class to benefit all locatable files, but since it will probably require indexing, we should use only when necessary.; 2) Memory requirements for some tools could be reduced by avoiding intermediate creation of an internally held list, streaming it directly instead.; 3) NIO streaming of entire files to/from buckets could be easily added to the relevant CSV/HDF5 read/write classes. Apart from the first issue, I don't think this really adds much, since the largest files are only ~1GB (and most seg files are much smaller) and are typically cheap to localize for single samples. See also #3976, #4004, #4717, and #5715 for context. I think we should first demonstrate if the first issue is really the dominating cost in the germline pipeline. If not, we should first focus on optimizing inference. The other issues are much lower priority.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5716:1075,pipeline,pipeline,1075,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716,1,['pipeline'],['pipeline']
Deployability,"In our Mutect2 workflow, we run a pair of Normal/Tumor through `CalculateContamination` step, the output of which is used in `FilterMutectCalls`. Since upgrading to `4.1.0.0`, `CalculateContamination` is breaking in cases where there're mismatched of N/T samples. . For e.g., `4.0.11.0` generates the following output:; ```; level contamination error; whole_bam 0.5013841326835697 0.0055644124674135865; ```; And `4.1.0.0` gives the following:; ```; sample contamination error; Run06_Pair07_Tumor 1.0 0.03452380752462225; ```. As a result of the above output files, the next step in our pipeline `FilterMutectCall` is failing (issue related to https://github.com/broadinstitute/gatk/issues/5821)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5880:587,pipeline,pipeline,587,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880,1,['pipeline'],['pipeline']
Deployability,"In particular add output GATKTool.getDefaultToolVCFHeaderLines to the VCF header, and rewrite the integration test for GenerateVCFFromPosteriors so that it validates the equivalence of variant context records, instead of file equivalency",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4267:98,integrat,integration,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4267,1,['integrat'],['integration']
Deployability,"In particular, we are a little lax on sequence-dictionary validation in the CNV pipelines. However, it might be that this is a necessary evil---it seems sequence dictionaries are somewhat inconsistent even in datasets such as TCGA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3864:80,pipeline,pipelines,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3864,1,['pipeline'],['pipelines']
Deployability,In releasing bulk ingest I noticed a few places we could add some clarification to the workspace description. This has already been updated in the workspace and can go out in github in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8509:132,update,updated,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8509,2,"['release', 'update']","['release', 'updated']"
Deployability,"In running and re-running GvsPrepareCallset.wdl, one past run did not use compressed references, so that is always used with call caching is turned on (which it is by default), even though the dataset has reingested compressed references since then. This is the exact scenario that GetBQTableLastModifiedDatetime was created for  database-based tasks that we want to be able to call cache accurately. Integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ea2ecb01-f35f-441a-ba08-1e7938da2ebe (single failure is for ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned ""The relative difference between these is 0.0507051, which is greater than the allowed tolerance (0.05)"")",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8667:402,Integrat,Integration,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667,1,['Integrat'],['Integration']
Deployability,"In the ""Related Annotations"" section of the annotations tool docs, the links were broken redirecting to the GATK homepage. We decided to remove these links since there is not a good way to link to the mentioned tool without the link becoming outdated with new releases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7307:260,release,releases,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7307,1,['release'],['releases']
Deployability,"In the news file of a structural variant software I use, I read. >Added FIX_SA and FIX_MISSING_HARD_CLIP; >FIX_SA: rewrites split read SA tags; >corrects GATK indel realignment SA tag data inconsistency; >FIX_MISSING_HARD_CLIP: infers missing hard clipping if split read records have different lengths; >corrects for GATK indel realignment stripping hard clipping when realigning. Could such issues perhaps be resolved in an update to GATK?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6459:425,update,update,425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6459,1,['update'],['update']
Deployability,"In the process of unifying CalculateTargetCoverage / SparkGenomeReadCounts for the rewrite of the CNV pipeline, we decided to experiment with switching over to fragment-based counts due to a request from CGA. For each fragment, CollectFragmentCounts adds a count to *the bin that overlaps with the fragment center*. We filter to properly-paired, first-of-pair reads in order to have well formed fragments and avoid double counting. We also filter out duplicates. In contrast, CalculateTargetCoverage added a count to *all bins that overlapped with a read* and SparkGenomeReadCounts added a count to *the bin that contained the read start*. These tools kept duplicates. However, none of these collection strategies have been rigorously evaluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:102,pipeline,pipeline,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['pipeline'],['pipeline']
Deployability,"In this branch are a number of improvements and changes that form the baseline for the current ongoing evaluation of the DRAGEN/GATK pipeline. This represents the joint work of both msyelf and @vruano. The major improvements in this branch are as follows:; - `EstimateDragstrModelParameters` tool for estimating the per-sample/per-STRType errors for use in the HMM gap open/gap close penalties as well as the necessary changes to the PairHMM loading code in order to adjust the model appropriately.; - Support for using the DragstrParams and flat SNP priors to compute genotype posteriors and the support for using them in the selection of genotypes as well as for computing the QUAL score. ; - Base Quality Dropout (BQD) model which penalizes variants with low average base quality scores among genotyped reads and reads that were otherwise excluded from the genotyper. A number of additional arguments to expose internal behaviors in the readThreadingAssembler and HaplotypeCaller have been made in order to support threading more lowBQ reads through to the genotyper. ; - Foreign Read Detection (FRD) model which uses an adjusted mapping quality score as well as read strandedness information to penalize reads that are likely to have originated from somewhere else on the genome. A number of additional arguments and behaviors have been exposed in order to preserve lower mapping quality reads in the HaplotypeCaller in service.; - Dynamic Read Disqualification, allows for longer/lower base quality reads to be less likely to be rejected by eliminating the hard cap on quality scores and further adjusting the limit based on the average base quality for bases in the read. . Design decisions that I would direct the reviewers attention to as they correspond to potentially dangerous/controversial changes:; - Because FRD/BQD require low quality ends to be included in the models for genotyping, I have added the option to softclipLowQualityEnds (as opposed to their current treatment which involv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6634:133,pipeline,pipeline,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6634,1,['pipeline'],['pipeline']
Deployability,"In this case, the FDR threshold is not honored. The explanation of this is complex, but essentially has to do with the Benjamini-Hochberg procedure not playing well with suppression factor when extended to more than one artifact mode. The definition of ``FilterByOrientationBias`` will have to be changes from ""guaranteeing less than 1% FDR over all mutations"" to ""guaranteeing less than 1% FDR in each specified artifact mode"". This could make the filter more aggressive, so we may have to adjust the FDR threshold. - [x] code fix; - [x] doc updates",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3344:543,update,updates,543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3344,1,['update'],['updates']
Deployability,"Include codec registry, versioning/upgrade chain framework.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5338:35,upgrade,upgrade,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5338,1,['upgrade'],['upgrade']
Deployability,"Includes latest Gencode and an implicit fix for #6564. Had to make some code changes for latest liftover Gencode data(v34 -> hg19). . The associated DS test release correctly annotates data on hg19 and hg38. Left to do:. - [x] Update data sources downloader.; - [x] Update data source version validation code. Code updates:; - Now both hg19 and hg38 have the contig names translated to `chr__`; - Added 'lncRNA' to GeneTranscriptType.; - Added ""TAGENE"" gene tag.; - Added the MANE_SELECT tag to FeatureTag.; - Added the STOP_CODON_READTHROUGH tag to FeatureTag.; - Updated the GTF versions that are parseable.; - Fixed a parsing error with new versions of gencode and the remap; positions (for liftover files).; - Added test for indexing new lifted over gencode GTF.; - Added Gencode_34 entries to MAF output map.; - Minor changes to FuncotatorIntegrationTest.java for code syntax.; - Pointed data source downloader at new data sources URL.; - Minor updates to workflows to point at new data sources. Script updates:; - Updated retrieval scripts for dbSNP and Gencode.; - Added required field to gencode config file generation.; - Now gencode retrieval script enforces double hash comments at; top of gencode GTF files. Bug Fixes:; Removing erroneous trailing tab in MAF file output. - Fixes #6693",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6660:157,release,release,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6660,8,"['Update', 'release', 'update']","['Update', 'Updated', 'release', 'updates']"
Deployability,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4611:13,Configurat,Configuration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611,3,"['Configurat', 'configurat']","['Configuration', 'configuration']"
Deployability,IndexFeatureFile needs integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/235:23,integrat,integration,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/235,1,['integrat'],['integration']
Deployability,Initial GKL integration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1935:12,integrat,integration,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935,1,['integrat'],['integration']
Deployability,Initial implementation of Insertion/Deletion caller in SV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2320:58,pipeline,pipeline,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2320,1,['pipeline'],['pipeline']
Deployability,"Initial port of native PairHMM AVX code from GATK3. (@akiezun #1492). Includes gradle code to build the shared library and package it in the GATK jar file. . Added a new unit test VectorPairHMMUnitTest, which can be integrated into PairHMMUnitTest later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1504:216,integrat,integrated,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504,1,['integrat'],['integrated']
Deployability,Initial updates to bulk ingest docs with 25k run,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8475:8,update,updates,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8475,1,['update'],['updates']
Deployability,Install GATK Python packages.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3964:0,Install,Install,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3964,1,['Install'],['Install']
Deployability,Install python-is-python3 during the GATK docker build,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8497:0,Install,Install,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8497,1,['Install'],['Install']
Deployability,Install the python-is-python3 package in our docker build,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8499:0,Install,Install,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8499,1,['Install'],['Install']
Deployability,"Install, run, and evaluate other SV methods",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1102:0,Install,Install,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1102,1,['Install'],['Install']
Deployability,Integrate BetaFeature annotation with help/doc.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2791:0,Integrat,Integrate,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2791,1,['Integrat'],['Integrate']
Deployability,Integrate CountingVariantFilter.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4954:0,Integrat,Integrate,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4954,1,['Integrat'],['Integrate']
Deployability,Integrate GenomicsDB VariantContextWriter interface,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2088:0,Integrat,Integrate,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2088,1,['Integrat'],['Integrate']
Deployability,Integrate GenomicsDB load VCFs java interface,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2087:0,Integrat,Integrate,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2087,1,['Integrat'],['Integrate']
Deployability,Integrate Picard and GATK report formats,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/251:0,Integrat,Integrate,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/251,1,['Integrat'],['Integrate']
Deployability,Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/a795190c-dcc2-40a7-bfcc-84fa6a4ea0dc); Two failed on ValidateVDS (or rather something upstream). I *don't* think this is an effect of this PR.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8807:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8807,1,['Integrat'],['Integration']
Deployability,"Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:29975,integrat,integration,29975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['integrat'],['integration']
Deployability,"Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8eadce08-04fa-447f-bbbc-934f52e79030), still working on testing this by hand",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8923:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8923,1,['Integrat'],['Integration']
Deployability,Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/5bcd1072-f9c4-4885-805b-a29091e27791).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8066:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8066,1,['Integrat'],['Integration']
Deployability,Integration run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/76c46310-3c0d-43a8-9fce-072ef7750651). As written the task requires `apt-get`. Converting this to Alpine would be non-trivial and not really worthwhile as it might even take longer to build all the extra things into the `alpine` image that we simply download with the `slim` image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8065:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8065,1,['Integrat'],['Integration']
Deployability,"Integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/544fb86b-ffb7-447c-b380-fdefce10be99). Does away with the `STARTED` and `FINISHED` sample load statuses to more explicitly record what work has actually been done for a sample: `REFERENCES_LOADED`, `VARIANTS_LOADED` or `HEADERS_LOADED`. Legacy `FINISHED` and `STARTED` states are recognized and handled appropriately (short circuiting data load and being ignored, respectively).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8674:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8674,1,['Integrat'],['Integration']
Deployability,Integration run going [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/4adcc972-86be-411e-bc95-a27c8a14de3b),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8303:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8303,1,['Integrat'],['Integration']
Deployability,Integration run here (failed because of known issue cost/output checks): https://job-manager.dsde-prod.broadinstitute.org/jobs/e904302b-e338-4bc7-bf7d-3ef1bfcd3fd9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8022:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8022,1,['Integrat'],['Integration']
Deployability,"Integration run here: https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/acb5b878-af45-443d-8139-0f0044cbcb38. The basic problem: https://news.ycombinator.com/item?id=9255830. Repro:. ```; % # make a file shaped like what was failing in ingest; % for i in $(seq 50000); do ; echo foo,${i} >> file.csv; done; % # repro the pipeline that was failing; % set -o pipefail; % cat file.csv | cut -d, -f2 | sort -r -n | head -1; 50000; % echo $?; 141; % # repeat with temp file construct; % head -1 <(cat file.csv | cut -d, -f2 | sort -r -n) ; 50000; % echo $?; 0; %; ```; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8441:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8441,2,"['Integrat', 'pipeline']","['Integration', 'pipeline']"
Deployability,Integration run in progress https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/474c06f0-1c6a-41d0-bc1f-7a22054153fe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8474:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8474,1,['Integrat'],['Integration']
Deployability,"Integration run in progress: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/b0ba98ae-a81f-4be2-bd72-c374359b644c. I actually don't expect this to pass due to VS-1141, but hopefully once this lands VS-1141 will be the *only* thing preventing integration from passing. Stowaway bug fixes:. - Hail version defaulted properly; - VDS tie out now waits for VDS creation to finish",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8586:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8586,2,"['Integrat', 'integrat']","['Integration', 'integration']"
Deployability,Integration run kicked off [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/e706347d-cae4-4138-a5a3-34692b39ae89). Previous versions completed successfully but failed cost comparison because the name of the WDL step changed and was unrecognized by the branches of the comparison code that cut slack.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8262:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8262,1,['Integrat'],['Integration']
Deployability,Integration run with [all the tests here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d6892c6d-0d1e-415b-819b-24a30ed08f0f),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8687:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8687,1,['Integrat'],['Integration']
Deployability,"Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/4de33a53-be6f-43b2-96db-7e8a0fb398f8); Example run of GvsExtractAvroFilesForHail using an Exome data set that has PGT, PID, and PS defined is [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/43a87886-60be-4f60-a2d3-3f4a97ceea5b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8536:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8536,1,['Integrat'],['Integration']
Deployability,Integration test for CollectTargetedPcrMetrics,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/872:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/872,1,['Integrat'],['Integration']
Deployability,Integration test successful https://app.terra.bio/#workspaces/broad-firecloud-dsde/VS-415%20GVS%20Quickstart%20Default%20Extract%20Scatter/job_history/73ac71db-0488-46be-a8e8-7f00e795edec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7888:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7888,1,['Integrat'],['Integration']
Deployability,Integration testing tables are not forever [VS-1049],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8563:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8563,1,['Integrat'],['Integration']
Deployability,"Integration tests are not passing as the gold files have not been updated. Understanding what ""correct"" looks like for spanning deletions beyond just unit tests is very complex, and fraught with bad data (ie multiple overlapping reference blocks, etc). . Putting this work on the shelf until the value is worth the effort",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7945:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7945,2,"['Integrat', 'update']","['Integration', 'updated']"
Deployability,Integration tests have a million different VC/Genotype setup methods,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5709:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5709,1,['Integrat'],['Integration']
Deployability,Integration tests: add a way for intelligent comparison of files of different formats,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/190:0,Integrat,Integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/190,1,['Integrat'],['Integration']
Deployability,IntegrationTestSpec doesn't handle CRAM output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1562:0,Integrat,IntegrationTestSpec,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1562,1,['Integrat'],['IntegrationTestSpec']
Deployability,IntegrationTestSpec is hardwired to text files and bam files but compares them byte-by-byte. We need more digested way of comparing files to remove the brittleness of md5 while retaining the ability to notice failures.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/190:0,Integrat,IntegrationTestSpec,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/190,1,['Integrat'],['IntegrationTestSpec']
Deployability,IntegrationTestSpec should not ignore leading/trailing whitespace by default,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7560:0,Integrat,IntegrationTestSpec,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7560,1,['Integrat'],['IntegrationTestSpec']
Deployability,IntegrationTests for a walker that creates multiple outputs based on a user-supplied prefix?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5446:0,Integrat,IntegrationTests,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5446,1,['Integrat'],['IntegrationTests']
Deployability,Intel Conda environment needs a continuous integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5255:32,continuous,continuous,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5255,2,"['continuous', 'integrat']","['continuous', 'integration']"
Deployability,"Intermittent ""Connection reset by peer"" during pip install on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4194:51,install,install,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4194,1,['install'],['install']
Deployability,"Intermittent failure at https://travis-ci.com/github/broadinstitute/gatk/jobs/297047618. ```; [TileDB::FileSystem] Error: hdfs: Cannot list contents of dir gs://hellbender-test-logs/staging/703469fc-52fe-441d-b6e0-8092a114fe2c//chr20$17960187$17981445/genomicsdb_meta_dir; hdfsBuilderConnect(forceNewInstance=0, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: fs.gs.project.id; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:171); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:168); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:168); 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:448,configurat,configuration,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,3,"['Configurat', 'configurat']","['ConfigurationUtil', 'configuration']"
Deployability,"Introducing the IntervalLocusIterator which will traverse every locus in intervals, regardless of coverage. Minor changes. Removed imports. AlignmentContextLocusIterator first cut. Still needs unit tests. Putting in the walker. Still needs unit tests. Adding tests (and fixes) so that we can get AlignmentContexts. Adding tests (and fixes) so that we can get AlignmentContexts. Working tests. Beginning migration to a LocusWalker change rather than a separate walker. Merging the emit empty loci into locus walker. Still need warnings and validation of parameters. Next step is the LocusWalker testing. Simple test of the new LocusWalker when it emit empty loci. Addressing PR requests and added ShardedIntervalIterator to save RAM on big intervals. Addressing the rest of the PR comments. Rolling back to int from long. Addressing second round of PR comments. Wrapped LIBS in a factory so that we can encapsulate the retrieval of the best alignment context iterator. Spark empty loci traversal being supported. Rebasing based off of the other emit loci branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2731:790,Rolling,Rolling,790,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2731,1,['Rolling'],['Rolling']
Deployability,Investigate normalization in denoising method in the ModelSegments pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4150:67,pipeline,pipeline,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4150,1,['pipeline'],['pipeline']
Deployability,Issue 2457 docker updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2709:18,update,updates,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2709,1,['update'],['updates']
Deployability,"Issue: Integer overflow error caused Mutect2 v4.1.4.0 to generate a stats file with a negative number. Solution is to change the int data type to long. User report:. Hello, I've just adapted my pipeline to the new filtering strategies, while looking at the files I noticed that for a WGS run I obtained a stats file with a negative number:; [egrassi@occam biodiversa]>cat mutect/CRC1307LMO.vcf.gz.stats; statistic value; callable -1.538687311E9. Looking around about the meaning of the number I found https://gatkforums.broadinstitute.org/gatk/discussion/24496/regenerating-mutect2-stats-file, so I'm wondering if I should be worried by having a negative number of callable sites :/; What's more puzzling is that FilterMutectCalls after ran without any error. Before running mutect I used the usual best practices pipeline, then:; ; gatk Mutect2 -tumor CRC1307LMO -R /archive/home/egrassi/bit/task/annotations/dataset/gnomad/GRCh38.d1.vd1.fa -I align/realigned_CRC1307LMO.bam -O mutect/CRC1307LMO.vcf.gz --germline-resource /archive/home/egrassi/bit/task/annotations/dataset/gnomad/af-only-gnomad.hg38.vcf.gz --f1r2-tar-gz mutect/CRC1307LMO_f1r2.tar.gz --independent-mates 2> mutect/CRC1307LMO.vcf.gz.log; ; gatk CalculateContamination -I mutect/CRC1307LMO.pileup.table -O mutect/CRC1307LMO.contamination.table --tumor-segmentation mutect/CRC1307LMO.tum.seg 2> mutect/CRC1307LMO.contamination.table.log; ; gatk LearnReadOrientationModel -I mutect/CRC1307LMO_f1r2.tar.gz -O mutect/CRC1307LMO_read-orientation-model.tar.gz 2> mutect/CRC1307LMO_read-orientation-model.tar.gz.log; ; gatk FilterMutectCalls -V mutect/CRC1307LMO.vcf.gz -O mutect/CRC1307LMO.filtered.vcf.gz -R /archive/home/egrassi/bit/task/annotations/dataset/gnomad/GRCh38.d1.vd1.fa --stats mutect/CRC1307LMO.vcf.gz.stats --contamination-table mutect/CRC1307LMO.contamination.table --tumor-segmentation=mutect/CRC1307LMO.tum.seg --filtering-stats mutect/CRC1307LMO_filtering_stats.tsv --ob-priors mutect/CRC1307LMO_read-orientation-model.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6302:194,pipeline,pipeline,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6302,2,['pipeline'],['pipeline']
Deployability,It is now site-by-site independent (unlike GATK3). All integration tests were ported as is (after ditching the MD5s). The algorithm was changed so that it works on a site-by-site independent basis. Some changes to the engine classes were required to allow queries over intervals in feature data sources. Fixes https://github.com/broadinstitute/hellbender/issues/38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/614:55,integrat,integration,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/614,1,['integrat'],['integration']
Deployability,"It just moved to 1.30, update once it's on maven central.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/329:23,update,update,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/329,1,['update'],['update']
Deployability,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into /home/travis/site-library; (as lib is unspecified); Error: (converted from warning) dependencies rlang, vctrs are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6072:156,Install,Installing,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072,8,"['Install', 'upgrade']","['Installing', 'upgrade']"
Deployability,It looks like the HTSJDK update branch changed a few engine constants and that wasn't pulled into the combineGVCFs branch before merging.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3869:25,update,update,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3869,1,['update'],['update']
Deployability,"It looks like the conda env recently started resolving h5py to v3.1.0, which in turn appears to be incompatible with the keras version we're using, causing the CNNScoreVariants integration tests to fail when keras tries to load the model file (see https://github.com/tensorflow/tensorflow/issues/44467). This PR pins the version to the version used by the last build I could find that succeeded, which is 2.10.0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6955:177,integrat,integration,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6955,1,['integrat'],['integration']
Deployability,"It looks like we copy and execute install_R_packages.R from the repo when building the base Docker image, and then on travis, we use the current repo version for the travis image. We also copy the current version to the final Docker image during the build, but we don't execute it there (at least as far as I can tell). This means the travis tests that use the Docker image run with a different R environment (the one established for the base image) than the non-Docker tests. Also, the Docker image winds up having the updated copy of the script, but its not the one reflected in the actual environment. @jamesemery It looks like you might have added the second copy - can you take a look and verify that we should fix this, or see if there is someting I'm missing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4251:520,update,updated,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4251,1,['update'],['updated']
Deployability,"It needs to be integrated with CRAMIndexer, and tests should be added for both BAM and CRAM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1135:15,integrat,integrated,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1135,1,['integrat'],['integrated']
Deployability,It seems fairly rare that the PL array is truly uninformative and consequently would be removed based on the fact that none of the integration tests seem to have failed as a result of this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7148:131,integrat,integration,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7148,1,['integrat'],['integration']
Deployability,It seems like another R dependency has moved. ; `http://cran.r-project.org/src/contrib/data.table_1.10.4-2.tar.gz` -> `https://cran.r-project.org/src/contrib/Archive/data.table/data.table_1.10.4-2.tar.gz`. We'll need to update our `installRscripts`. We should seriously consider just hosting these files ourselves somewhere.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3772:220,update,update,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3772,2,"['install', 'update']","['installRscripts', 'update']"
Deployability,"It seems that you can find out what FileSystem corresponds to what URI/URL by using the return of FileSystemProviders.installedProviders. Instead BucketUtils has hard-wired scheme names and has many conditional (isCould... isHadoop) for operations that can be done by using the appropriate FileSystem implementation that in turn can be resolved looking at that list. I guess that there are extant utility classes that process that list once for subsequent queries (e.g. scheme-name -> FS), but I have not confirmed that. Otherwise it is easy to write one. Is there is a reason what we need to do our own thing in BucketUtils?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3569:118,install,installedProviders,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3569,1,['install'],['installedProviders']
Deployability,It was noticed while doing #8351 that the `GencodeFunctotation.equals()` method has the following line in it; ``` ; if (geneTranscriptType != that.geneTranscriptType) return false; ; ```. Unfortunately the geneTranscriptType is stored as a Sting and thus this should NOT be expected to succeed in almost any case. As it stands fixing this innocuous oversight seems to break several of the combinatorial funcotator tests and an integration test. Somebody should fix this behavior (easy) and validate that the test changes are within tolerable levels (hard).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8385:427,integrat,integration,427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8385,1,['integrat'],['integration']
Deployability,It would be great if mac users could install GATK easily with homebrew. We should put together a brew formula for it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4164:37,install,install,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4164,1,['install'],['install']
Deployability,"It's a little confusing that we use downsampled tumor BAMs for the gCNV WDL tests, for example. Can wait until after release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4007:117,release,release,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4007,1,['release'],['release']
Deployability,Iterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); ```. I ran the same command again from my computer (not in the cloud) still using the NIO paths and it ran successfully. I've also seen it run successfully when running the same pipeline in the cloud. The only thing I think I've changed is the disk size I'm asking for. I'm in the process of validating the input bam right now.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316:7353,pipeline,pipeline,7353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316,1,['pipeline'],['pipeline']
Deployability,"Its possible to specify CNN inference size argument values that cause the Python process run out of memory, and the failure mode appears to be the java process hangs. Its not clear whether its always possible to recover from this using the global exception handler we currently install on the Python side - we need to explore a bit to see if the handler is being invoked on OOM; whether catching the OOM exception explicitly would help, or if we need an alternative reporting strategy for low-memory conditions. Attached is a log provided by @bhanugandham from a run in a Terra notebook that failed and that exhibited a hang that we assume was due to OOM, and that was resolved by reducing the inference batch size. [gatkStreamingProcessJournal-772629669.txt](https://github.com/broadinstitute/gatk/files/2988819/gatkStreamingProcessJournal-772629669.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5820:278,install,install,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5820,1,['install'],['install']
Deployability,"Java 8 is now old enough that it's not the default on new machines, (java 12 seems to be what comes on a new macbook.) Installing java 8 has become more of a hassle because oracle now requires you to login in order to get it. We should update the readme with information about how to get java 8, probably pointing people to https://adoptopenjdk.net/. I think comms probably needs to update/add some documentation about this as well because it's definitely a friction point. We might also want to advise using jenv for people with multiple java installations to manage them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6024:119,Install,Installing,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6024,4,"['Install', 'install', 'update']","['Installing', 'installations', 'update']"
Deployability,Javadoc update: minor error in the documentation regarding --genotyping-mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5657:8,update,update,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5657,1,['update'],['update']
Deployability,"Just a patch to make the change suggested in the review of broadinstitute/gatk-protected#1001. @asmirnov239 your comment was correct, not sure what I was thinking back then!. Closes #3372.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3378:7,patch,patch,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3378,1,['patch'],['patch']
Deployability,Just checks that the input and output bams are equal (which is still the; case for a bam with no duplicates). Once BQSR is hooked up we'll have; to update the expected output for this test. This is intended as a starting point for the more meaningful tests we; eventually want to have.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/772:148,update,update,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/772,1,['update'],['update']
Deployability,"Just realized CNV WDLs are not using NIO in FireCloud. This is as simple as changing `File` to `String` for supported files. Not sure if these need to live in our repo (I see we have a M2 NIO WDL), I'd be fine with them just living in FireCloud. @bshifaw would you be OK making the changes in FireCloud for the next release? If not, I can add an NIO version to the repo. @LeeTL1220 perhaps something to add to the style guide (if it's not already there)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806:316,release,release,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806,1,['release'],['release']
Deployability,"Justin Rhoades of the blood biopsy team noticed a bug in the Mutect2 pipeline: if the number of scatters was sufficiently high, the last chunk of intervals did not contain any autosomal contigs, and therefore `GetPileupSummaries` was run on an empty interval for that scatter, throwing an error. For the pipeline there is no reason not to allow this empty interval and consequent empty output because it gets merged with other output later in the pipeline. It also seems that this is a generic feature of scattered jobs -- empty intersection of intervals need not imply a user error. Therefore, I added an argument to `IntervalArgumentCollection` to allow empty intervals. Since the change to the Mutect2 pipeline is tiny the primary need in code review is to judge whether the changes to `IntervalArgumentCollection` are acceptable. @lbergelson could you look at this PR?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209:69,pipeline,pipeline,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209,4,['pipeline'],['pipeline']
Deployability,"K/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/ngs/; datanode:; image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:2550,deploy,deploy,2550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['deploy'],['deploy']
Deployability,Kd BGE Doc Updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8926:11,Update,Updates,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8926,1,['Update'],['Updates']
Deployability,Kd GVS GP self service doc updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8886:27,update,updates,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8886,1,['update'],['updates']
Deployability,Kristen requested some small changes for the mitochondria pipeline. This renames the output vcf with the name of the sample and puts a default value for autosomal_median_coverage (meaning you'll now get the NuMT filter even if you don't provide the actual autosomal coverage). Happy to discuss if that's the right thing to do or if we really want to force people to put a value for autosomal coverage.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6160:58,pipeline,pipeline,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6160,1,['pipeline'],['pipeline']
Deployability,"K}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see descrip",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:3119,INSTALL,INSTALLDIRGCC,3119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGCC']
Deployability,Label names are standard from https://docs.google.com/document/d/1eyPrpCSCMyf1CaBkc-aDi39xV7Bq9hthh07ludpgPNw. branch adds labels to:; - `bg query` calls in WDL command blocks; - queries run within python scripts called by WDLs; - datasets created with Utils.BuildGATKJarAndCreateDataset. Quickstart integration run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/15442e08-92af-48bc-ab96-2bdd4f39a801,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7902:300,integrat,integration,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7902,1,['integrat'],['integration']
Deployability,"Last week, spark 2.0.0 is formally released. However, when I tested gatk4 on spark2.0.0, I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2073:35,release,released,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073,1,['release'],['released']
Deployability,Lb nio updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1691:7,update,updates,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1691,1,['update'],['updates']
Deployability,"LearnReadOrientationModel json file does not exist within gatkdoc release subfolder in v.4.1.8.1. We are now utilizing these files to automatically create Galaxy tool wrappers, so it would be awesome to get this in for the next release. Thanks much!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6720:66,release,release,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6720,2,['release'],['release']
Deployability,"Lee, just letting you know I've tagged you in a forum question. ---; The oncotated maf output includes many rejected mutations (using the configuration from the public spaces). This is bad practice. The unfiltered VCF (or preferably a tsv) should have these sites but we should not be annotating them or putting them in final outputs. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11440/m2-gatk4-oncotated-maf-output-includes-rejected-mutations/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4421:138,configurat,configuration,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4421,1,['configurat'],['configuration']
Deployability,"Legacy pipeline (note, the following should only be done after final ModelSegments PR is in):; - [x] Delete prototype tools. (#3887) (SL, PR issued by 12/1); - ~~Add deprecated/legacy tag to legacy pipeline tools. (SL, PR issued by 12/1 EDIT: need further input from @vdauwera )~~; - ~~Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (all, PR issued by 12/15)~~; - ~~(Reach) Collect all legacy code in a new package.~~; - [x] Delete old pipelines. (SL, #3935 awaiting review). ModelSegments pipeline:; - [x] Review and merge denoising PR (#3820).; - [x] Add WDL changes from @LeeTL1220, @meganshand, and @jsotobroad to dev branch. (Note that we exposed PreprocessIntervals.bin_length in these WDLs; I'm assuming that https://github.com/broadinstitute/cromwell/issues/2912 will allow this to be specified via the json, so I reverted this change.); - [x] Make simple improvements to ReCapSeg caller (#3825).; - [x] Review and merge modeling/WDL PR. (#3913 awaiting review. Note that this PR also deletes the old germline WDL.); - ~~Write MultidimensionalKernelSegmenterUnitTest.~~ (SL, punting, filed #3916); - ~~Write ModelSegmentsIntegrationTest.~~ (SL, punting, filed #3916); - [x] Preliminary PCAWG or HCC1143 purity evaluation. (@LeeTL1220) (LL, should be done in time for @vdauwera to present at Broad retreat); - [x] Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (PR #4010 awaiting review.); - [x] Add SM tag and sequence dictionary headers to all appropriate files and sort accordingly. (SL, #3914 awaiting review); - [x] Update tutorial data. (@MartonKN); - [ ] (Reach) Add VCF output.; - [ ] (Reach) Add PG tags to all files.; - [ ] (Reach) Replace ReCapSeg caller with improved version. (@MartonKN). gCNV pipeline:; - [x] Review and merge Python code (#3838). (MB and SL, PR #3925 awaiting review.); - [x] CLI for ploidy determination (cohort). (@samuelklee); - [x] CLI for ploidy determination (case). (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826:7,pipeline,pipeline,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826,5,"['Update', 'pipeline']","['Update', 'pipeline', 'pipelines']"
Deployability,Lessons learned in VDS creation during Echo Scale Testing. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e6aa362-e25b-49d0-83cd-d64e926c6386).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8602:70,integrat,integration,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8602,1,['integrat'],['integration']
Deployability,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126:44,configurat,configuration,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126,2,['configurat'],['configuration']
Deployability,Like #5485 but for tabix. (Build will fail until new Disq release is made.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5574:58,release,release,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5574,1,['release'],['release']
Deployability,Limit the number of parts we run the VAT pipeline on as a test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8780:41,pipeline,pipeline,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8780,1,['pipeline'],['pipeline']
Deployability,Loading the known sites file (e.g. dbsnp_138.hg18.vcf) takes around 6 minutes in the Spark pipeline using `KnownSitesCache`. This ticket is to see if we can improve this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103:91,pipeline,pipeline,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103,1,['pipeline'],['pipeline']
Deployability,Looking into broadcasting reference to all workers for native Spark Reads pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/855:74,pipeline,pipeline,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/855,1,['pipeline'],['pipeline']
Deployability,Looks like a missing set of parentheses caused the logging output for HaplotypeCaller to become unusably flooded with garbage. @droazen we should really get this in before release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7358:172,release,release,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7358,1,['release'],['release']
Deployability,"Looks like this java.lang.NullPointerException is from an environment set up issue. . This request was created from a contribution made by Jordi Maggi on April 25, 2022 09:25 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException](https://gatk.broadinstitute.org/hc/en-us/community/posts/5574426055963-CNNScoreVariants-crashes-with-java-lang-NullPointerException). \--. Hi,. I created a conda environment and installed gatk4 through `conda install -c bioconda gatk4`. I have been using this environment to run all steps of the single sample germline variant calling best practices workflow (both gatk and picard). However, I have never been able to run CNNScoreVariants with this setup, as it always results in a java.lang.NullPointerException error. The only way I am able to run this step is by running it through the docker image you provide. That, however, is not ideal for our setup. Any idea as to what I may try to be able to run it directly?. GATK version:. Using GATK jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar ; ; Running: ; ;   java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar --version ; ; The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; HTSJDK Version: 2.24.1 ; ; Picard Version: 2.25.4. Exact command:. gatk CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ;   java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811:498,install,installed,498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811,2,['install'],"['install', 'installed']"
Deployability,Lots of updates to the Mutect2 wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4201:8,update,updates,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4201,1,['update'],['updates']
Deployability,M2 NIO wdl -- updated for Firecloud pet service accounts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4710:14,update,updated,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4710,1,['update'],['updated']
Deployability,M2 pon wdl works in Firecloud; M2 wdl works in JES; updated oncotator docker in M2 template,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4048:52,update,updated,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4048,1,['update'],['updated']
Deployability,"MASSIVE bug fixes and test updates. (Rebased 61 commits). - Fixed a bug when variants overlap the end of transcripts. - Updated logging in FuncotatorUtils::getAlignedRefAllele. - Fixed a bug in identifying splice sites for intronic regions. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons that caused issues; on reverse stranded variants. - Added in regression test data input files and expected files.; - Updated regression test sets to include only unique variants.; - Added in a 5' flank and a Start Codon insertion to regression test set. - Fixed a bug in FuncotatorUtils::isIndelBetweenCodons. - Finally fixed a bug with indels and start codons:; Now indels in start codons will not have protein renderings, nor will; they have codon change strings. This brings Funcotator closer to; Oncotator functionality (in Oncotator, start codon insertions/deletions; do not have protein change strings or codon change strings). - Fixed a bug in ordering transcripts by appris ranking. - Fixed a minor bug in how other transcripts are generated:.; With RNA/LINCRNA transcripts, the protein change would be null and was; append ed to the end of each `other transcript`. Now the null is no; longer appended. - Fixed a bug in insertions on the - strand:; All insertions on - strand had incorrectly rendered genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; det",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5302:27,update,updates,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302,3,"['Update', 'update']","['Updated', 'updates']"
Deployability,"MPRESSION_LEVEL : 2; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 14:40:58.312 INFO ProgressMeter - chr22:10659064 0.2 35790 208384.3; 14:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:8264,install,install,8264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,1,['install'],['install']
Deployability,"MQ=56.9;MQRankSum=-0.962;QD=2.57;ReadPosRankSum=0.193;SOR=0.712; `. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr6:26914009 [VC chr6.raw.excessHet.vcf.gz @ chr6:26914009 Q276902.75 of type=INDEL alleles=[G*, GTGTA, GTGTATA, GTGTGTA] attr={AC=[4269, 29, 5], AF=[0.620, 4.209e-03, ; #### Steps to reproduce; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL; ```. #### Expected behavior; Create recalibrated vcf file. #### Actual behavior; ```; Caused by:; Process `ApplyRecalibrationIndels` terminated with an error exit status (3). Command executed:. #!/bin/bash; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL. Command exit status:; 3. Command output:; (empty). Command error:; 23:21:52.354 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:22:02.735 INFO ProgressMeter - chr6:1162012 0.2 25000 144494.8; 23:22:12.789 INFO ProgressMeter - chr6:2449556 0.3 53000 155623.0; 23:22:23.019 INFO ProgressMeter - chr6:3663394 0.5 82000 160448.7; 23:22:33.257 INFO ProgressMeter - chr6:4991347 0.7 112000 164291.1; 23:22:43.683 INFO ProgressMeter - chr6:6325045 0.9 141000 164832.0; 23:22:53.824 INFO ProgressMeter - chr6:7646289 1.0 171000 166913.4; 23:23:03.973 INFO ProgressMeter - chr6:902992",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8054:1737,install,install,1737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054,1,['install'],['install']
Deployability,"MT; 22:19:48.307 INFO FixMisencodedBaseQualityReads - ------------------------------------------------------------; 22:19:48.307 INFO FixMisencodedBaseQualityReads - ------------------------------------------------------------; 22:19:48.309 INFO FixMisencodedBaseQualityReads - HTSJDK Version: 2.13.2; 22:19:48.309 INFO FixMisencodedBaseQualityReads - Picard Version: 2.17.2; 22:19:48.310 INFO FixMisencodedBaseQualityReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 22:19:48.314 INFO FixMisencodedBaseQualityReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:19:48.318 INFO FixMisencodedBaseQualityReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:19:48.319 INFO FixMisencodedBaseQualityReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:19:48.325 INFO FixMisencodedBaseQualityReads - Deflater: IntelDeflater; 22:19:48.326 INFO FixMisencodedBaseQualityReads - Inflater: IntelInflater; 22:19:48.330 INFO FixMisencodedBaseQualityReads - GCS max retries/reopens: 20; 22:19:48.330 INFO FixMisencodedBaseQualityReads - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 22:19:48.331 INFO FixMisencodedBaseQualityReads - Initializing engine; 22:19:48.861 INFO FixMisencodedBaseQualityReads - Done initializing engine; 22:19:48.917 INFO ProgressMeter - Starting traversal; 22:19:48.917 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 22:19:49.026 INFO FixMisencodedBaseQualityReads - 196 read(s) filtered by: WellformedReadFilter. 22:19:49.029 INFO ProgressMeter - unmapped 0.0 918 505321.1; 22:19:49.030 INFO ProgressMeter - Traversal complete. Processed 918 total reads in 0.0 minutes.; 22:19:49.079 INFO FixMisencodedBaseQualityReads - Shutting down engine; [January 23, 2018 10:19:49 PM GMT] org.broadinstitute.hellbender.tools.FixMisencodedBaseQualityReads done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=580386816; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4241:2263,patch,patch,2263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4241,1,['patch'],['patch']
Deployability,"M_READER_FACTORY :; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4030,Configurat,Configuration,4030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Configurat'],['Configuration']
Deployability,Made runtime attrs to args in MT pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8417:33,pipeline,pipeline,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8417,1,['pipeline'],['pipeline']
Deployability,MainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.MapReferenceResolver.addWrittenObject(MapReferenceResolver.java:41); 	at com.esotericsoftware.kryo.Kryo.writeReferenceOrNull(Kryo.java:658); 	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:623); 	at c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3303:4716,deploy,deploy,4716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303,1,['deploy'],['deploy']
Deployability,MainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.getFilesMatching(NIOFileUtil.java:91); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:61); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.dat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2793:2373,deploy,deploy,2373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793,1,['deploy'],['deploy']
Deployability,MainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.refreshAccessToken(ComputeEngineCredentials.java:137); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:157); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleCl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591:3544,deploy,deploy,3544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591,1,['deploy'],['deploy']
Deployability,MainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$13.hasNext(Ite,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:12785,deploy,deploy,12785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['deploy'],['deploy']
Deployability,Make building and pushing user docs to the website a step in the release process,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4425:65,release,release,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4425,1,['release'],['release']
Deployability,Make each release in the repo citable with Digital Object Identifiers (doi),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3085:10,release,release,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3085,1,['release'],['release']
Deployability,Make it easier for test authors to make their tests toggle-able between different dataflow backends,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/561:52,toggle,toggle-able,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/561,1,['toggle'],['toggle-able']
Deployability,"Make the logging frequency used by the ProgressLogger available as an input. If not used, sets the default value. Variants team is using a branch of gatk and have made this change there, so pulling this change into master to simplify future merges / branch updates.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8662:257,update,updates,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8662,1,['update'],['updates']
Deployability,"MannWhitneyU was re-written from scratch in 2016 in GATK3,; but these changes never got ported to GATK4. This new version; produces significantly different results from the version; currently in GATK4, resulting in VERY different values for the; RankSumTest annotations in HaplotypeCaller output. @meganshand informs me that the updated GATK3 version has been; validated in R, and has much better tests than the old version. This is a straightforward port of that version with minimal changes:. -Merged ""MWUnitTest"" and ""RankSumUnitTest"" from GATK3 into a single; test class MannWhitneyUUnitTest; -Ported MathUtils.binomialCoefficient() and wrote new test for it; -Updated RankSumTest class and tests as appropriate. I've confirmed that with this change, the RankSum annotations produced; by the GATK4 HaplotypeCaller closely match those produced by the GATK3; HaplotypeCaller. Resolves #2604",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2605:329,update,updated,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605,2,"['Update', 'update']","['Updated', 'updated']"
Deployability,"Many of the pull requests are not using [`CommandLineProgramTest.runCommandLine()`](https://github.com/broadinstitute/hellbender/blob/c6b41e6da8c9ea3f03206a25ce4ad74312b154f0/src/test/java/org/broadinstitute/gatk/CommandLineProgramTest.java). I'm assuming this is because we have not settled on a way to `Assert` that outputs are similar after running a hellbender command line. This issue should resolve with a definition how far one should test before a pull request is accepted. After an arbitrary low level patch to the codebase, I believe the GATK [`MD5DB`](https://github.com/broadgsa/gatk/blob/3b67b448072e24c80779b2e1cbc9dcfcb5dce4cf/public/gatk-tools-public/src/test/java/org/broadinstitute/gatk/utils/MD5DB.java) and [`DiffEngine`](https://github.com/broadgsa/gatk/blob/3b67b448072e24c80779b2e1cbc9dcfcb5dce4cf/public/gatk-tools-public/src/main/java/org/broadinstitute/gatk/engine/walkers/diffengine/DiffEngine.java) are considered too hard to verify-and-update en masse. This limitation would also apply to external framework test utilities, such as TestNG's `FileAssert.assertLength()`. A 2009 discussion of file comparators is archived [here](http://stackoverflow.com/questions/466841/comparing-text-files-w-junit). Ultimately, I believe the biggest pain point with the `MD5DB` is that there does not exist a quick way to a) diagnose what has changed and b) to then update all hundreds of expected outputs. As in `DiffEngine`, we could define a way to regression test that only certain aspects of common file types aren't changing (exact number of reads in BAMs, or exact number of variants in BCF), or that values are falling within a certain range (number of quality scores all above 30 under 60), etc. As for updating results, instead of embedding the expected `MD5DB` outputs in a hundreds of java test files, one could also externalize _all_ of the expected outputs to another file (json, flat text, etc.) such that this singular sorted file for the entire test suite may be updated ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/69:511,patch,patch,511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/69,2,"['patch', 'update']","['patch', 'update']"
Deployability,"Many users do not want mutations filtered in the VCF to show up in the MAF file. Two things:. - Update the default M2 WDL to use `broadinstitute/oncotator:1.9.7.0` docker image. - Add an optional flag to prune filtered mutations to the Oncotator task. If this is set to True, add the following to the oncotator command: `--collapse-filter-cols --prune-filter-cols`. This is unnecessary for the CNV WDL.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4422:96,Update,Update,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4422,1,['Update'],['Update']
Deployability,Master is broken. [E.g.](https://travis-ci.com/broadinstitute/gatk/jobs/227807624). ```; Fetched 217 kB in 1s (163 kB/s); Reading package lists...; W: http://ppa.launchpad.net/couchdb/stable/ubuntu/dists/trusty/Release.gpg: Signature by key 15866BAFD9BCC4F3C1E0DFC7D69548E1C17EAB57 uses weak digest algorithm (SHA1); W: GPG error: https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6B05F25D762E3157; W: The repository 'https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease' is not signed.; W: There is no public key available for the following key IDs:; 6B05F25D762E3157 . ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6116:211,Release,Release,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6116,1,['Release'],['Release']
Deployability,Matched gCNV pipeline arguments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8234:13,pipeline,pipeline,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8234,1,['pipeline'],['pipeline']
Deployability,Matplotlib throws RuntimeError when Python is not installed as a framework,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4743:50,install,installed,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4743,1,['install'],['installed']
Deployability,MeanQualityByCycleSpark integration test fails on ADAM files with setHeaderStrict,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1540:24,integrat,integration,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1540,1,['integrat'],['integration']
Deployability,"Mention in README that ""clean"" should be run before ""installSpark""/""sparkJar"", since gatk-launch cannot currently handle multiple spark jars in build/libs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1317:53,install,installSpark,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1317,1,['install'],['installSpark']
Deployability,Merge changes from master onto our version of gatk Dockerfile. Running integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/35dd363a-b140-47c7-ad69-7e5297a6ff10),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8801:71,integrat,integration,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8801,1,['integrat'],['integration']
Deployability,Merge changes from the EchoCallset branch back into our main branch ('ah_var_store'). Most of these changes are VDS creation related. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f5cb7a2d-b224-4b8e-8daf-2d22939a1d96),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8993:142,Integrat,Integration,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8993,1,['Integrat'],['Integration']
Deployability,"Methods team has updated how we structure our own buckets. `${prefix}/${user}` to `${prefix}-${user}`. Updating scripts to reflect that, and to avoid zombie buckets.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6114:17,update,updated,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6114,1,['update'],['updated']
Deployability,Migrate FuncotateSegments to use Owner for its configuration files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5963:47,configurat,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963,1,['configurat'],['configuration']
Deployability,Migrate GATK engine to new configuration mechanism,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3081:27,configurat,configuration,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3081,1,['configurat'],['configuration']
Deployability,"Minimal GATK port of nvscorevariants from https://github.com/NVIDIA-Genomics-Research/nvscorevariants. The tool runs successfully in both 1D and 2D modes, and a strict integration test passes for the 1D model. However, this PR has a number of outstanding issues that need to be resolved before it can be merged and replace the legacy CNNScoreVariants tool:. - The conda environment in scripts/nvscorevariants_environment.yml needs to be incorporated into the main GATK conda environment. - The integration test for the 2D model does not currently pass, despite using a much higher epsilon than the 1D test. Some of the scores differ by significant amounts vs. the CNNScoreVariants 2D output. We need to investigate why this is. - There is currently no training tool to train a new model, like there is for the legacy CNN tool. @samuelklee and @mwalker174 , could you please comment on what it would take to incorporate the `scripts/nvscorevariants_environment.yml` conda environment into the main GATK conda environment, assuming we are free to remove/retire the CNN tool?. @lbergelson and @zamirai, please do a general code review when you get a chance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8004:168,integrat,integration,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8004,2,['integrat'],['integration']
Deployability,Minor AoU Documentation Update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7999:24,Update,Update,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7999,1,['Update'],['Update']
Deployability,Minor Funcotator WDL updates.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6326:21,update,updates,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6326,1,['update'],['updates']
Deployability,Minor changes for updated VAT reference testing [VS-1054],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8670:18,update,updated,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8670,1,['update'],['updated']
Deployability,Minor update to AoU Documentation; Update to include link to Firecloud support script which can be used to upload sample sets without using the UI.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7994:6,update,update,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7994,2,"['Update', 'update']","['Update', 'update']"
Deployability,Minor updates to picard.sam package to reflect latest version.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/332:6,update,updates,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/332,1,['update'],['updates']
Deployability,Mitochondria Pipeline Tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5310:13,Pipeline,Pipeline,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5310,1,['Pipeline'],['Pipeline']
Deployability,Mock up an example configuration setup using Owner,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126:19,configurat,configuration,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126,1,['configurat'],['configuration']
Deployability,ModelSegments can currently deal with single sample segmentation. This branch contains the backend class (and the corresponding unit tests) that is able to segment based on multiple data samples. The updated version of the front-end class ModelSegments will be addressed in a different branch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5524:200,update,updated,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5524,1,['update'],['updated']
Deployability,ModelSegments integration test failures on newer Java 11 releases,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8107:14,integrat,integration,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8107,2,"['integrat', 'release']","['integration', 'releases']"
Deployability,"Modify CreateVariantIngestFiles to write missing ref intervals with the ZERO state, unless we are dropping that (ZERO) state and none other. Successful Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/0509cd35-50bc-431b-88c2-590e15cd3cc9)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8560:152,Integrat,Integration,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8560,1,['Integrat'],['Integration']
Deployability,"Modify GvsCreateVATFromVDS to take as an optional input the `sites_only_vcf` - if provided, the code bypasses the logic to create it from VDS.; This PR also modifies IndexVcf and SelectVariants to use localization_optional for their inputs. Updated Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/760a8910-77a3-446e-b539-196663bbd90b); Rerun of GvsCreateVATFromVDS using a passed in sites_only VCF [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/9ca7d011-82f3-43a9-9161-1df0f7174fa9).; Rerun of GvsCreateVATFromVDS NOT using a passed in sites_only VCF [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/de8fae29-a904-40d1-849b-15ba84fa0a8f).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8851:241,Update,Updated,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8851,2,"['Integrat', 'Update']","['Integration', 'Updated']"
Deployability,Modify GvsExtractCallset so that you can change the value of 'output_gcs_dir' as an input and it won't cause the extract to be re-run (won't invalidate call caching). Passing run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/a5fd0daf-8816-42e4-ad15-3aa432e2ed80).; Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/3c874e74-4fdd-4cbb-af75-a44bb7e17ea1).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8960:319,Integrat,Integration,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8960,1,['Integrat'],['Integration']
Deployability,More prep work for separating GVS code: remove references to GVS code from non-GVS code. Integration test currently running [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/2af93966-3c84-4aec-bc1e-82cb88478852).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8229:89,Integrat,Integration,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8229,1,['Integrat'],['Integration']
Deployability,More refactoring of Mark duplicates and pipeline hookup,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/770:40,pipeline,pipeline,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/770,1,['pipeline'],['pipeline']
Deployability,"Most of the CRAM tests were added before we could write CRAM on Spark, so this fills out the CRAM write tests for the remaining Spark tools/pipelines (ApplyBQSRSpark, BQSRPipelineSpark and ReadsPipelineSpark). Also includes one totally opportunistic deletion of an unused zero-length fasta file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1667:140,pipeline,pipelines,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1667,1,['pipeline'],['pipelines']
Deployability,"Most of the arguments in RecalibrationArgumentCollection are in the form ""mismatches_context_size"". This probably needs to be updated but it will have the consequence of invalidating existing BaseRecalibrator report tables and require the arguments get changed in all of the report tables checked in as test files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3974:126,update,updated,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3974,1,['update'],['updated']
Deployability,"Most of these changes are to support automated evaluation of GATK CNV. - Updates SimpleAnnotatedGenomicRegion to use the collection framework used in other GATK CNV CLIs. CLIs (both experimental quality): ; - `TagGermlineEvents` is a simple tool that attempts to identify events in a tumor seg file that correspond to a germline events. ; - This is done purely with concordance on the breakpoints of the events (within some padding). ; - Input germline segments must have calls. ; - If a germline call is broken into multiple segments, this tool will handle that appropriately (ditto if there are multiple tumor segments overlapping the germline call); . - `MergeAnnotatedRegions` will merge all overlapping regions and resolve annotation value conflicts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4205:73,Update,Updates,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4205,1,['Update'],['Updates']
Deployability,"Most of these changes are to support automated evaluation of GATK CNV. - Updates `AnnotatedIntervals` (formerly `SimpleAnnotatedGenomicRegion`) to use the tribble framework for reading. Writing is done in a way that should be concordant with a future tribble writing framework, as per discussion with @droazen.; - Changes to `XsvLocatableTableCodec` to support usage of arbitrary config files. This cannot be done when using tribble features in the CLI. Already reviewed with @jonn-smith . Support for SAM File headers and comments is included.; - *Note:* The reading of `AnnotatedIntervals` cannot be done automatically on the command line, unless the config file is a sibling. The tools below do not even attempt this, since the use cases involved will never have a sibling config file.; - Created a default config file in the jar file resources to read tsvs with locatable fields from the CNV collection files. This is much less strict than the framework used by the CNV tools. The reader will accept any columns (or subset of the columns). CLIs (both experimental quality): ; - `TagGermlineEvents` is a simple tool that attempts to identify events in a tumor seg file that correspond to a germline events. ; - This is done purely with concordance on the breakpoints of the events (within some padding). ; - Input germline segments must have calls. ; - If a germline call is broken into multiple segments, this tool will handle that appropriately (ditto if there are multiple tumor segments overlapping the germline call). - `MergeAnnotatedRegions` will merge all overlapping regions and resolve annotation value conflicts. Closes #3995",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276:73,Update,Updates,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276,1,['Update'],['Updates']
Deployability,"Most tools that write vcf outputs call `GATKTool.createVCFWriter(final File outFile)` because they use `File` arguments. Once https://github.com/broadinstitute/gatk/pull/5378 is merged, these calls should be replaced with calls to `GATKTool.createVCFWriter(final Path outFile)`, and then the `File` method can be removed. It would be nice if during the same pass we:. - Change the type of the `File` arguments with use the new `GATKUri` class.; - Integrated the use of `getDefaultToolVCFHeaderLines` use of for all the writers (see https://github.com/broadinstitute/gatk/issues/5493); - Tag each of these outputs as ""gcs-enabled"".",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5512:447,Integrat,Integrated,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5512,1,['Integrat'],['Integrated']
Deployability,Mostly changing `use_classic_VQSR` to `use_VQSR_lite` with a couple of minor bugfixes thrown in. Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/3e60bf43-5cc3-4e5b-97ad-1732b4c543be).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8360:108,integrat,integration,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8360,1,['integrat'],['integration']
Deployability,Move Funcotator integration test expected outputs out of large,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5379:16,integrat,integration,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5379,1,['integrat'],['integration']
Deployability,"Move NativeUtils.loadLibraryFromClasspath() to gatk-native-bindings, publish a new release, and make gatk-bwamem-jni and gatk-fermilite-jni implement NativeLibrary and call load() internally",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2408:83,release,release,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2408,1,['release'],['release']
Deployability,Move ReadFilter plugin integration up to GATKTool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2218:23,integrat,integration,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2218,1,['integrat'],['integration']
Deployability,Move to using [GenomicsDB Release 1.5.0](https://github.com/GenomicsDB/GenomicsDB/releases/v1.5.0). ; Highlights in the release relevant to gatk are; - [readthedocs](https://genomicsdb.readthedocs.io/en/latest/) for GenomicsDB design/usage/functionality - GenomicsDB/GenomicsDB#265.; - GenomicsDB/GenomicsDB#284; - GenomicsDB/GenomicsDB#271; - Exclude spark from genomicsdb core jar GenomicsDB/GenomicsDB#281; - General improved performance/logging.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8358:26,Release,Release,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8358,3,"['Release', 'release']","['Release', 'release', 'releases']"
Deployability,Move to version [GenomicsDB 1.5.4](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.5.4) for fix to PR #8415.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8987:76,release,releases,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8987,1,['release'],['releases']
Deployability,"Moved R dependencies to conda environment, cleaned up R/python dependencies, and updated base Docker/Travis configuration.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026:81,update,updated,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,Moving classes that tests depend on from the test folders into the src folders in the utils.test package. This way they will be available to projects that depend on hellbender. Fixes #525 . Updating to the newest testng release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/527:220,release,release,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/527,1,['release'],['release']
Deployability,"Moving to [GenomicsDB 1.4.1 ](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.1)release will allow for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the `--genomicsdb-use-gcs-hdfs-connector` option. Using the native client with gcs allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc. The defaults are all hardcoded to match what is in gatk at present. It also helps with performance issues with gcs, see #7070. This version also contains fixes for #7089, although it will require additional support from gatk(will be part of a separate PR).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7224:71,release,releases,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7224,2,['release'],"['release', 'releases']"
Deployability,Moving us off of of the weird beta protobuf version to the newest 3.x release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6736:70,release,release,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6736,1,['release'],['release']
Deployability,Multipair WDL for somatic CNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2969:30,pipeline,pipeline,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2969,1,['pipeline'],['pipeline']
Deployability,"Mutect adopted natural logarithms in #5858. In the update, it looks like one base 10 log was missed. This PR updates the missed log.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6884:51,update,update,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6884,2,['update'],"['update', 'updates']"
Deployability,"Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10; 16:46:49.699 INFO Mutect2 - Start Date/Time: November 6, 2019 4:46:49 PM CST; 16:46:49.699 INFO Mutect2 - ------------------------------------------------------------; 16:46:49.699 INFO Mutect2 - ------------------------------------------------------------; 16:46:49.699 INFO Mutect2 - HTSJDK Version: 2.15.1; 16:46:49.699 INFO Mutect2 - Picard Version: 2.18.2; 16:46:49.699 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:46:49.699 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:46:49.700 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:46:49.700 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:46:49.700 INFO Mutect2 - Deflater: IntelDeflater; 16:46:49.700 INFO Mutect2 - Inflater: IntelInflater; 16:46:49.700 INFO Mutect2 - GCS max retries/reopens: 20; 16:46:49.700 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 16:46:49.700 INFO Mutect2 - Initializing engine; 16:46:49.995 INFO FeatureManager - Using codec VCFCodec to read file file:///home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz; 16:46:50.064 INFO Mutect2 - Shutting down engine; [November 6, 2019 4:46:50 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2394947584; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /home/vip/data/Mutect2/af-only-gnomad.raw.sites.hg19.vcf.gz; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:357); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:308); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:255); 	at org.broadinstitute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6248:2499,patch,patch,2499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6248,1,['patch'],['patch']
Deployability,Mutect2 error when running inside a pipeline in parallel with intervals,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7059:36,pipeline,pipeline,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059,1,['pipeline'],['pipeline']
Deployability,Mutect2 method updated versions should be reserved,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6001:15,update,updated,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6001,1,['update'],['updated']
Deployability,Mutect2: Update allele matching during active region detection,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7468:9,Update,Update,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7468,1,['Update'],['Update']
Deployability,"N See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:4956,pipeline,pipelines,4956,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['pipeline'],['pipelines']
Deployability,NFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Version: 2.14.3; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - Picard Version: 2.18.2; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 11:01:49.336 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:01:49.337 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 11:01:49.337 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 11:01:49.337 INFO BwaAndMarkDuplicatesPipelineSpark - GCS max retries/reopens: 20; 11:01:49.337 INFO BwaAndMarkDuplicatesPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:01:49.337 WARN BwaAndMarkDuplicatesPipelineSpark - ; ```; - (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; ```; Using GATK jar /gatk/build/libs/gatk-spark.jar; Running:; /spark//bin/spark-submit --master spark://973f3a3a3407:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.exec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:16807,patch,patch,16807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['patch'],['patch']
Deployability,NFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Version: 2.14.3; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - Picard Version: 2.17.2; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:33:45.590 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:33:45.591 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:33:45.591 INFO BwaAndMarkDuplicatesPipelineSpark - GCS max retries/reopens: 20; 16:33:45.591 INFO BwaAndMarkDuplicatesPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 16:33:45.591 WARN BwaAndMarkDuplicatesPipelineSpark - ; ```; - (GATK) v4.0.4.0. ```; Using GATK jar /gatk/gatk-package-4.0.4.0-spark.jar; Running:; /spark//bin/spark-submit --master spark://926a0516ccf6:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverh,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:13081,patch,patch,13081,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['patch'],['patch']
Deployability,NFO GenomicsDBImport - HTSJDK Defaults.CREATE_MD5 : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4316,Configurat,Configuration,4316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Configurat'],['Configuration']
Deployability,NFO GenomicsDBImport - Start Date/Time: 17 July 2018 17:01:47 BST; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - ------------------------------------------------------------; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 17:01:47.523 INFO GenomicsDBImport - Picard Version: 2.18.2; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:01:47.523 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:01:47.523 INFO GenomicsDBImport - Deflater: IntelDeflater; 17:01:47.523 INFO GenomicsDBImport - Inflater: IntelInflater; 17:01:47.523 INFO GenomicsDBImport - GCS max retries/reopens: 20; 17:01:47.523 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:01:47.524 INFO GenomicsDBImport - Initializing engine; 17:01:47.959 INFO IntervalArgumentCollection - Processing 3362775 bp from intervals; 17:01:47.984 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/chla/outputsb.workspace; 17:01:48.120 INFO GenomicsDBImport - Vid Map JSON file will be written to outputsb.workspace/vidmap.json; 17:01:48.120 INFO GenomicsDBImport - Callset Map JSON file will be written to outputsb.workspace/callset.json; 17:01:48.120 INFO GenomicsDBImport - Complete VCF Header will be written to outputsb.workspace/vcfheader.vcf; 17:01:48.120 INFO GenomicsDBImport - Importing to array - outputsb.workspace/genomicsdb_array; 17:01:48.136 INFO ProgressMeter - Starting traversal; 17:01:48.136 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 17:01:48.250 INFO GenomicsDBImport - Importing batch 1 with ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5064:2625,patch,patch,2625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5064,1,['patch'],['patch']
Deployability,"NIO output support for SelectVariants. Tested like so:. ```; $ ./gatk SelectVariants \; --variant dbsnp_138.b37.excluding_sites_after_129.vcf \; --select-random-fraction 0.01 \; --output gs://mybucket/variants.vcf; $ gsutil ls -lh gs://mybucket/*.vcf; 23.38 MiB 2018-10-30T23:58:12Z gs://mybucket/variants.vcf; ```. Includes the required changes under the hood, plus test updates. This change also gives NIO support to **HaplotypeCaller**, so it can write its VCF to cloud storage. Also, fixes #2128.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378:372,update,updates,372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378,1,['update'],['updates']
Deployability,"NScoreVariants - Start Date/Time: August 13, 2018 11:29:43 AM UTC; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Version: 2.16.0; 11:29:43.469 INFO CNNScoreVariants - Picard Version: 2.18.7; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:29:43.470 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:29:43.470 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:29:43.470 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:29:43.470 INFO CNNScoreVariants - Deflater: IntelDeflater; 11:29:43.470 INFO CNNScoreVariants - Inflater: IntelInflater; 11:29:43.470 INFO CNNScoreVariants - GCS max retries/reopens: 20; 11:29:43.470 INFO CNNScoreVariants - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:29:43.470 WARN CNNScoreVariants -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CNNScoreVariants is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:29:43.470 INFO CNNScoreVariants - Initializing engine; 11:29:44.086 INFO FeatureManager - Using codec VCFCodec to read file file:///restricted/projectnb/casa/wgs.hg38/sv/gatk.cnn/vcf/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; 11:29:44.281 INFO CNNScoreVariants - Done initializing engine; 11:29:52.451 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/farrell/1d_cnn_mix_train_full_bn.3573521081782697200.json and weights:/tmp/farrell/1d_cnn_mix_train_full_bn.1075881893743930029.hd5; 11:29:54.959 INFO ProgressMeter - Starting traversal; 11:29:54.960 INFO ProgressMeter - Current Locus Elapsed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5101:2523,patch,patch,2523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101,1,['patch'],['patch']
Deployability,NVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:3764,Configurat,Configuration,3764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Configurat'],['Configuration']
Deployability,NVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:2826,Configurat,Configuration,2826,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Configurat'],['Configuration']
Deployability,Name Dataflow pipelines,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/684:14,pipeline,pipelines,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/684,1,['pipeline'],['pipelines']
Deployability,Namely MultidimensionalKernelSegmenterUnitTest and ModelSegmentsIntegrationTest. Note that the Cromwell tests already essentially serve as integration tests and that the pipeline has already been through several preliminary evaluations without any issues.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3916:139,integrat,integration,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3916,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,Need additional VQSR integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2290:21,integrat,integration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2290,1,['integrat'],['integration']
Deployability,Need to cleanup the documentation in Funcotator prior to the GATK 4.0 release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4021:70,release,release,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4021,1,['release'],['release']
Deployability,Need to look at how to version the releases of data sources and individual data sources. This is exemplified with the dbSNP `common` vs dbSNP `All` problem.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4582:35,release,releases,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4582,1,['release'],['releases']
Deployability,"New tool aiming to call all types of precise variants detectable by long read alignments (not fully functioning yet in the sense that not all types of variants are detected yet&mdash;to be handled by later PRs in this series).; This new tool splits the input long reads by scanning their alignment characteristics (number of alignments, if strand switch is involved, if mapped to the same chromosome, if have equally good alignment configurations based on the scoring tool, etc), and send them down different code path/logic units for variant type inference and VCF output.; This PR would only deal with simple INSDEL, for long reads having exactly 2 alignments (no other equally good alignment configuration) mapped to the same chromosome without strand switch or order switch (translocation or large tandem duplications), because we already have this type of variant covered in master. __UPDATE__; See updated roadmap in #2703. NEEDS TO WAIT UNTIL PART 1 IS IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3456:432,configurat,configurations,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3456,3,"['configurat', 'update']","['configuration', 'configurations', 'updated']"
Deployability,New: GroundTruthScorer; Update: FlowFeatureMapper,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8579:24,Update,Update,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8579,1,['Update'],['Update']
Deployability,Next minor GATK release?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7322:16,release,release,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7322,1,['release'],['release']
Deployability,"Normally one provides passing workflow runs with a PR. For the integration run [that is here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ab86fb6d-c5d6-48b6-8322-923af691751c). There's also a ""real"" run taking place using this branch [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/db59d5b8-e2ac-4619-9563-aa5631bf053c). However for testing correctness of these changes with respect to the requester pays flag, my pet ""does not have serviceusage.services.use access to the Google Cloud project"". I therefore present instead a [run with my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e712055-f466-4929-b6eb-5306f3cde1a0) that fails in exactly the same way as a [run without my changes](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/185506f5-9dc1-4c02-997d-6fe3f5695259).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8552:63,integrat,integration,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8552,1,['integrat'],['integration']
Deployability,"Not ready for merge as this relies on https://github.com/HadoopGenomics/Hadoop-BAM/pull/136, hence a new Hadoop-BAM release. Addresses https://github.com/broadinstitute/gatk/issues/2572 and https://github.com/broadinstitute/gatk/issues/2571",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3369:116,release,release,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3369,1,['release'],['release']
Deployability,"Note: This branch is still blocked on several changes in Picard (https://github.com/broadinstitute/picard/pull/1236, and possibly https://github.com/broadinstitute/picard/pull/1245), once those are resolved and released then this branch should hopefully get the stamp of approval from @takutosato. * Added optical/library duplicate marking of reads (note: this does not include library tagging); * Added the ability to remove reads from the output bam based on their duplicate marking status. Resolves #4675 ; Resolves #5377",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5377:211,release,released,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5377,1,['release'],['released']
Deployability,"Notebook documenting the _actual_ creation of this VDS here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/analysis/launch/Make%20a%20VDS%20with%20Quickstart.ipynb?mode=edit. This VDS (well I made two, one with VQSR and one with VETS) can then be used for a tie out in the near future with the WDL pipeline for VDS creation. gs://fc-eada2674-7c2b-42a6-8db3-0246872596dc/quickstart-vds-for-wdl-tieout/VQSR-Classic/vds/. gs://fc-eada2674-7c2b-42a6-8db3-0246872596dc/quickstart-vds-for-wdl-tieout/VETS/vds/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8513:313,pipeline,pipeline,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8513,1,['pipeline'],['pipeline']
Deployability,"Notes:. - Classes in hellbender/tools/picard/analysis/artifacts are removed and replaced with Picard versions (except Transition, which is not public in Picard).; - GATK version of GatherVcfs is retained, and the Picard version is masked out - is this what we want ?; - The non-Spark GATK metrics tools have been removed and replaced with the Picard versions. The test data is retained (but moved) since its used by the Spark metrics tool tests. Additional changes we'll want to make separately to minimize the complexity of this PR:; - Eliminate the download of picard.jar from the GATK WDL tests and update the WDL to run Picard tools through GATK.; - Unify and merge the Picard and GATK program groups. These are similar, but not identical, and the combined result has artificial/duplicate groups.; - Normalize the confusing mix of Alpha/Beta/Experimental tags and comments.; - Add unified doc and tab-completion tasks that include Picard.; - Remove and replace SamComparison and Transition classes with the Picard versions.; - Fix GATK CompareBaseQualities (its a PicardCommandLineProgram).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3620:602,update,update,602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3620,1,['update'],['update']
Deployability,Now that ADAM 0.18 has been released. Fixes #957.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1059:28,release,released,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1059,1,['release'],['released']
Deployability,"Now that we're using git lfs to manage our large test resources, we need to configure travis to install/init git lfs before running the test suite.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/840:96,install,install,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/840,1,['install'],['install']
Deployability,Now the Mutect2 WDL has all the hooks to run Funcotator (`Funcotator.wdl` was updated as well). The `Funcotate` task in the M2 WDL is identical to that same task in `Funcotator.wdl`. Fixes #5253,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5735:78,update,updated,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5735,1,['update'],['updated']
Deployability,"O ""$i"".snp.vcf.gz; gatk VariantFiltration -R $reference -V $path_BQSR/bqsr1.snp.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 60.0"" --filter-name ""FS60"" -filter ""MQ < 40.0"" --filter-name ""MQ40"" -filter ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" -filter ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8.0"" -O $""i"".filtered.snp.vcf.gz; gatk SelectVariants -R $reference -V $""i"".filtered.snp.vcf.gz --exclude-filtered -O ""$i"".select.filtered.snp.vcf.gz; #INDEL; gatk SelectVariants -R $reference -V ""$i"".vcf.gz -select-type INDEL -O ""$i"".indel.vcf.gz; gatk VariantFiltration -R $reference -V ""$i"".indel.vcf.gz -filter ""QD < 2.0"" --filter-name ""QD2"" -filter ""FS > 200.0"" --filter-name ""FS200"" -filter ""ReadPosRankSum < -20.0"" --filter-name ""ReadPosRankSum-20"" -O ""$i"".filtered.indel.vcf.gz; gatk SelectVariants -R $reference -V ""$i"".filtered.indel.vcf.gz --exclude-filtered -O ""$i"".selected.filtered.indel.vcf.gz. gatk BaseRecalibrator -R $reference -I $i_bam -O grp1 --use-original-qualities --known-sites ""$i"".select.filtered.snp.vcf.gz --known-sites ""$i"".selected.filtered.indel.vcf.gz; gatk ApplyBQSR -R $reference -I $i_bam -O ""$i"".sorted.dedup.BQSR1.bam -bqsr grp1 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --add-output-sam-program-record --create-output-bam-md5 --use-original-qualities; gatk ValidateSamFile -I ""$i"".sorted.dedup.BQSR.bam -O ""$i""_validateSamFile_of_bqsr_bam_file.out; samtools index ""$i"".sorted.dedup.BQSR.bam; done; gatk --java-options ""-Xmx30G"" HaplotypeCaller -R $reference -I sample1.sorted.dedup.BQSR.bam sample2.sorted.dedup.BQSR.bam sample3.sorted.dedup.BQSR.bam -O sample1_sample2_sample4.g.vcf.gz --tmp-dir tmp -ERC GVCF; ```. Secondly, how do I set the ploidy parameter for different samples in the population-based snp-calling?; Finally, for each taxa, there are some samples with relatively high sequence depth (> 10x). Is there any better choices for the snp-calling pipeline ??. Sincerely.; Jing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8414:3217,pipeline,pipeline,3217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8414,1,['pipeline'],['pipeline']
Deployability,"O NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Aug 30, 2021 11:04:20 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:04:20.983 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Executing as yangyxt@paedyl02 on Linux v3.10.0-1160.11.1.el7.x86\_64 amd64 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Start Date/Time: August 30, 2021 11:04:20 AM HKT ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------ ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 2.24.1 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Picard Version: 2.25.4 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - Built for Spark Version: 2.4.5 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false. 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 11:04:20.984 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 11:04:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:2278,release,release-,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['release'],['release-']
Deployability,"O PedReader - Reading PED file /cromwell\_root/fc-47de7dae-e8e6-429c-b760-b4ba49136eee/resources/1kgp/1kgp\_trios.ped with missing fields: \[\] 19:35:29.854 INFO PedReader - Phenotype is other? true 19:35:32.686 INFO VariantEval - Creating 1881 combinatorial stratification states 19:35:32.742 INFO ProgressMeter - Starting traversal 19:35:32.742 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute 19:36:01.819 INFO VariantEval - Shutting down engine \[May 27, 2021 7:36:01 PM UTC\] org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval done. Elapsed time: 0.54 minutes. Runtime.totalMemory()=4964483072 java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 at java.util.ArrayList.rangeCheck(ArrayList.java:653) at java.util.ArrayList.get(ArrayList.java:429) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.isViolation(MendelianViolation.java:180) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.updateViolations(MendelianViolation.java:122) at org.broadinstitute.hellbender.utils.samples.MendelianViolation.countFamilyViolations(MendelianViolation.java:148) at org.broadinstitute.hellbender.tools.walkers.varianteval.evaluators.MendelianViolationEvaluator.update1(MendelianViolationEvaluator.java:122) at org.broadinstitute.hellbender.tools.walkers.varianteval.util.EvaluationContext.apply(EvaluationContext.java:74) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.processComp(VariantEval.java:596) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval.doApply(VariantEval.java:562) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.callDoApply(VariantEval.java:497) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.addVariant(VariantEval.java:478) at org.broadinstitute.hellbender.tools.walkers.varianteval.VariantEval$PositionAggregator.access$100(VariantEval.java:469) at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7304:4134,update,updateViolations,4134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7304,1,['update'],['updateViolations']
Deployability,"On branch `ll_CollectAllelicCountsSpark`, I have created a CLI called: `CollectAllelicCountsSpark` ... This tool will have the exact same functionality as `CollectAllelicCounts`, to the point where I can re-use the integration tests. However, the integration tests fail. When I dig deeper in `CollectAllelicCountsSpark`, I see that only 8 RDDs (correct amount: 11) are being passed to processAlignments... Consider the following code:. ```; @Override; protected void processAlignments(JavaRDD<LocusWalkerContext> rdd, JavaSparkContext ctx) {; final String sampleName = SampleNameUtils.readSampleName(getHeaderForReads());; final SampleMetadata sampleMetadata = new SimpleSampleMetadata(sampleName);; final Broadcast<SampleMetadata> sampleMetadataBroadcast = ctx.broadcast(sampleMetadata);. final AllelicCountCollector finalAllelicCountCollector =; rdd.mapPartitions(distributedCount(sampleMetadataBroadcast.getValue(), minimumBaseQuality)); .reduce((a1, a2) -> combineAllelicCountCollectors(a1, a2, sampleMetadataBroadcast.getValue()));; final List<LocusWalkerContext> tmp = rdd.collect();; ....snip....; ```. In this case `tmp` will have a size of 8. However, the integration test would indicate a size of 11 is correct, since 11 intervals are being passed in. Note that `emitEmptyLoci()` returns `true`, so 11 is the correct number as seen in `CollectAllelicCountsSparkIntegrationTest` . . Additionally, in (at least) one result, the counts are wrong. `CollectAllelicCounts` (non-spark) passes the integration test. I have tried a couple of tests to gather more information:. - Is `emitEmptyLoci()` causing an issue? ; Does not appear to be causing the issue. I say this because when set to `false`, I get (essentially) the same error.; - The code uses `mapPartition` and not `map`, does this cause the issue? Why are you doing this?; This does not cause the issue. I refactored the code to use `map` and got the exact same issue. I use `mapPartition` in order to instantiate only one instance of `A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3823:215,integrat,integration,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3823,2,['integrat'],['integration']
Deployability,"Once #2457 is merged, the gatkbase docker image will be the basis for GATK4 docker images. However, gatkbase depends on no files in the GATK4 repo. However, we *might* want to move the installation of R libraries into gatkbase. Currently the custom R installation script takes a substantial portion of time in the GATK4 docker image build. The question is whether we expect the R libraries to change much. Once we answer this question, we can decide whether this issue should be addressed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2699:185,install,installation,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2699,2,['install'],['installation']
Deployability,"Once again I've managed to convince David R. to let me merge with some tech debt as follows:; - [ ] Add to GnarlyGenotyper an integration test like testRawAndFinalizedAlleleSpecificAnnotationsThoroughly() for GGVCFs; - [ ] Add a direct unit test for makeReducedAnnotationString() if you exposed it as package-accessible; - [ ] ~Break out finalized key definition, promote getKeyNames and getRawKeyNames to default methods in ReducibleAnnotation interface~; - [ ] One last `ann.getRawKeyNames().get(0)` in GnarlyGenotyperEngine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6203:126,integrat,integration,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6203,1,['integrat'],['integration']
Deployability,"Once https://github.com/broadinstitute/gatk/pull/3620/ is in, we should add/update the gradle tasks for gatkDoc and gatkTabComplete to generate unified (GATK and Picard) doc and tab completion script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3627:76,update,update,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3627,1,['update'],['update']
Deployability,"Once we choose the library to use for GATK configuration, let's have a design meeting to make sure we come up with something that works for Spark, downstream projects, our users, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079:43,configurat,configuration,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079,1,['configurat'],['configuration']
Deployability,"Once we do https://github.com/broadinstitute/gatk/issues/2817, we can disable the non-docker unit and integration tests in travis, saving a huge amount of time and resources. (We should keep the instance of the tests that run on the Oracle JDK, however).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3294:102,integrat,integration,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3294,1,['integrat'],['integration']
Deployability,"One more DepthOfCoverage observation. The GATK4 version let's one control the delimiter in the output, which seems like a useful idea. However:. 1) this isnt that big a deal, but why change the GATK3 default of tab to comma in GATK4?. 2) The enum is named CSV and TABLE. Why not rename 'TABLE' to 'TSV' to make it more clear? A comma-delimited table is still a table. This is a beta tool now, and I assume with the next release changing an argument becomes harder.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6623:420,release,release,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6623,1,['release'],['release']
Deployability,"One of our tests (BaseRecalibratorDataflow, on cloud) started failing. It turns out that the culprit is a Dataflow limitation. This is what I got back from the DF team:. _I examined logs of this pipeline on the service and in this case, metadata.items[3] is the pipelineOptions item, whose biggest part is --filesToStage, built from the classpath: it seems you have too many .jar's in classpath, or the jars have too long (absolute) filenames.; It seems that you are using Gradle and all the absolute filenames point deep inside gradle cache directories.; So, as a work-around, you could consider asking Gradle to build a self-contained distribution of your application, put it in a less deep directory, and run that._. We may run into this problem for other tests as well, so it's good to know about the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/580:195,pipeline,pipeline,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/580,2,['pipeline'],"['pipeline', 'pipelineOptions']"
Deployability,"One of the example commands in CalculateGenotypePosteriors describes the usage of the `-supporting` argument and the `--num-reference-samples-if-no-call` argument at the same time:; ```; gatk --java-options ""-Xmx4g"" CalculateGenotypePosteriors \; -V input.vcf.gz \; -O output.vcf.gz \; -supporting 1000G.phase3.integrated.sites_only.no_MATCHED_REV.hg38.vcf.gz \; --num-reference-samples-if-no-call 2504; ```; Calculate the posterior genotypes of a callset, and impose that a variant *not seen* in the external panel is tantamount to being AC=0, AN=5008 within that panel. We don't have any tests that use both of these arguments at the same time, but it looks like the behavior in that case is wrong. PosteriorProbabilitiesUtils adds numRefSamplesFromMissingResources regardless of whether there was an overlapping variant in the panel or not, effectively diluting the AF of all the variants used as priors and making the number of reference alleles very inconsistent across variants.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4760:311,integrat,integrated,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4760,1,['integrat'],['integrated']
Deployability,"Only performance optimizations were made to the copy-ratio denoising method in the ModelSegments pipeline, which is otherwise identical to that used in GATK CNV. No special care is taken to preserve the normalization of the overall copy-ratio profile during the process. This may become important in downstream tumor-heterogeneity inference; estimates of the ploidy may be otherwise biased. We can investigate using simulated data. This issue could be obviated by #4121 in the near future, but a quick fix might nevertheless be in order.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4150:97,pipeline,pipeline,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4150,1,['pipeline'],['pipeline']
Deployability,"Oops, looks like they just updated the URL last week. Perhaps another reason why we should host these dependencies or have some simple contingencies for testing them other than manually building the base image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3712:27,update,updated,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3712,1,['update'],['updated']
Deployability,"Ops fixed the bad blacklist in broad-references, but this means the old file is no longer there. @ldgauthier or @jsotobroad Can you please do a quick review? @droazen It would be really awesome if this (tiny!) commit could end up in the release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5612:237,release,release,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5612,1,['release'],['release']
Deployability,"Optionally extract VCFs in bgzipped compression format. Integration test - [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/60357098-e6f2-4264-a322-c3088ce36152). [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/2c4dea90-367f-4875-939a-ce0b9ae296e7) is an example extract using bgzip; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/7a8413da-499b-4f93-ba85-554937249bd4) is an example extract not using bgzip (so, just '.gz')",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8809:56,Integrat,Integration,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8809,1,['Integrat'],['Integration']
Deployability,"Our R dependency is primarily for producing plots. It could be possible to create plots using javascript instead. Javascript plots have several potential advantages but also several major downsides. The biggest and most obvious drawback is that we don't have any code to produce them yet, and they are likely harder to generate and experiment with than R scripts. . The advantage would be that we could avoid requiring an R installation to run hellbender scripts, we could potentially also include interactive plotting or other neat tricks to make the plots more useful. I see 2 possible routes to replacing Rscripts with javascript. The first would be for tools that require graphs to perform some html generation and produce html reports with embedded javascript. The user could then open these in their browser and view the plots ( much like how our test suite report and jacoco is done). . A different option would be to use javascript plotting libraries directly within the jvm to generate SVG. Java 8 has a new javascript engine which is supposed to be reasonably fast and offers access to java objects from within it. Unfortunately it doesn't offer a full DOM like a browser does, so most existing javascript libraries will fall over. It seems like it would take a lot of hacking to get something like d3 to run directly on the jvm. (someone has done something of the kind here: http://jazdw.net/content/server-side-svg-generation-using-d3js) . Other options would be to use the javafx web panes to display a browser directly, or to plot directly on a canvas. Either of these options seem like they would be painful and awful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/248:424,install,installation,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/248,1,['install'],['installation']
Deployability,"Our patches to `google-cloud-java` in https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2281 and https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2283 to fix the transient NIO errors have now been merged into master, and will be part of their next release (which will be the release after `0.22.0`). We should update to the next release as soon as it's out, to remove our existing dependency on a SNAPSHOT build of `google-cloud-java`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3500:4,patch,patches,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3500,5,"['patch', 'release', 'update']","['patches', 'release', 'update']"
Deployability,"Our travis gcloud installation scripts relied on piping interactive responses; into the stdin of the installer, which is brittle. This patch changes our script to instead run the installer in non-interactive mode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6974:18,install,installation,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6974,4,"['install', 'patch']","['installation', 'installer', 'patch']"
Deployability,Our wiki has a bunch of outdated things which should be updated: ex: https://github.com/broadinstitute/gatk/wiki/Why-are-there-multiple-GATK-repositories%3F,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2794:56,update,updated,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2794,1,['update'],['updated']
Deployability,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7498:270,update,updated,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498,4,"['integrat', 'pipeline', 'update']","['integration', 'pipeline', 'updated']"
Deployability,PGEN + Docker image updates [VS-1254],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8749:20,update,updates,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8749,1,['update'],['updates']
Deployability,PGEN Updates for AoU [VS-1254],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8739:5,Update,Updates,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8739,1,['Update'],['Updates']
Deployability,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2297:97,configurat,configuration,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297,3,['configurat'],['configuration']
Deployability,Package bash completion file in GATK release zip,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3499:37,release,release,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3499,1,['release'],['release']
Deployability,Parsing the GATK config file currently overrides any command-line specified config options for system-level parameters. Options explicitly specified on the command-line should override what is in the config file. The `GATKConfig.properties` file is missing from the packaged binary release (as created by`gradle bundle`). Gradle must be updated to include it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4436:282,release,release,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436,2,"['release', 'update']","['release', 'updated']"
Deployability,"Part of road map laid out in #4111 . ## Consolidate logic, update variant representation (PR#4663) . ### consolidate logic in the following classes. - [x] `AssemblyContigAlignmentSignatureClassifier` now gone, its inner enum class `RawTypes` is moved to `AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicTypes` and reduced into fewer cases (`Suspicious`, `Simple` and `Complex`). - [x] static method `BreakpointsInference.inferFromSimpleChimera()` now moved to state query method `ChimericAlignment.inferType()`. - [x] `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` merged with `ChimericAlignment.hasIncompletePicture()`. ### update how variants are represented. - [x] change `SVLEN` for `CPX` variants to the difference between _[alt haplotype sequence length]_ and _[affected reference region length]_, which is following the technical definition of `SVLEN` in VCF spec. - [x] change `RPL` output to one of these (note that test coverage is expected); - [x] ins/del, when del/ins bases are < 50 and annotate; when type is determined as ins, the POS will be 1 base before the micro-deleted range and END will be end of the micro-deleted range, where the REF allele will be the corresponding reference bases.; - [x] ins and del when both are >= 50, and link by `EVENT`. - [x] change `SVTYPE=DUP` to`SVYTPE=INS` when the duplicated region is shorter than 50 bp (tests). Note that this will lead to `INS` records with `DUP_REPEAT_UNIT_REF_SPAN` and `DUP_SEQ_CIGARS` (when available). In addition, we are currently treating duplication expansion as insertion. ; The VCF spec doesn't force `DUP` records as such.; If we decide to allow `POS` and `END` to designate the beginning and end of the duplicated reference region, we need to make at least the following change:. - [ ] shift the left breakpoint to the right by 1 base compared to the current implementation, and ; - [ ] `downstreamBreakpointRefPos = complication.getDupSeqRepeatUnitRefSpan().getEnd();`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663:59,update,update,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663,2,['update'],['update']
Deployability,"Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ad05b4d1-7aed-4482-8b5c-ced7b87d2d37).; Verified that GQ0 dropped in 'hail_lite' run and not in 'hail_vcf' run; (queries of count by state from ref ranges table):. **Hail Lite (Hail path, drop state 0):; state count**; 2 2495387; 3 4773472 ; 4 5959290. **Lite VCF (VCF path, drop_state 40):; state count**; 0 2764630; 2 2495387; 3 4773472. Spun up a notebook and ran the vds_validation.py script on the VDS generated by 'hail_lite'. And it passed:. > 2023-10-04 19:08:01.278 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.; > To preserve matrix table column order, first unkey columns with 'key_cols_by()'; > checking that:; > * no reference blocks have GQ=0; > * all ref blocks have END after start; > * all ref blocks are max 1000 bases long; > running densify on 200kb region (0 + 1) / 1]; > took 10.9s to densify 0 rows after interval query; > Hail VDS validation successful======================================(1 + 0) / 1]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8538:8,Integrat,Integration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8538,1,['Integrat'],['Integration']
Deployability,Passing Integration test (using built-in dockers) [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/d2063e26-a22e-412e-ad91-aa2b14fbb7ec).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8566:8,Integrat,Integration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8566,1,['Integrat'],['Integration']
Deployability,Passing integration tests (all chrs) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/be35d4ae-7fd5-48ae-b7e1-a7c74eddd4ab); and chr20/x/y [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f3222e37-13a4-43fc-a341-cd7cdafed777),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8948:8,integrat,integration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8948,1,['integrat'],['integration']
Deployability,Passing run where generated from a VDS [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/12712c01-46c5-4968-b687-0360ac09f8e6); Passing run where used an existing sites-only VCF [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e01f9bf2-2224-4be3-b09e-bf6289126621); Integration Test run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/62087a48-696d-4ce9-aff8-243110c3dce0),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8977:351,Integrat,Integration,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8977,1,['Integrat'],['Integration']
Deployability,Passing workflow here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/38d22351-33cd-4c2c-abf9-feccda71d40a. Mostly passing integration test here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/4a7e6628-6c19-442d-90b8-202da267d8bb; (the failure was a bq time out.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8374:155,integrat,integration,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8374,1,['integrat'],['integration']
Deployability,Patch O(n^2) algorithm in VCFHeader.addMetadataLineLookupEntry(),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3448:0,Patch,Patch,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3448,1,['Patch'],['Patch']
Deployability,Patch build_docker.sh -p option to push to GCR in addition to dockerhub,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3793:0,Patch,Patch,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3793,1,['Patch'],['Patch']
Deployability,Patch gatk-launch to allow it to run with standalone packaged GATK jars,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2090:0,Patch,Patch,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2090,1,['Patch'],['Patch']
Deployability,Patch htsjdk to propagate indices to CRAM readers correctly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/788:0,Patch,Patch,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/788,1,['Patch'],['Patch']
Deployability,PathSeq documentation update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3918:22,update,update,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3918,1,['update'],['update']
Deployability,"PathSeqFilterSpark and PathSeqPipelineSpark clear all the sequences from the input header file, as the Bwa step only accepts unaligned reads. However, the header sequences were being cleared before the reads were loaded, causing WellformedReadFilter to remove any mapped reads (by failing to find the corresponding sequence name in the header). This PR fixes this bug by creating a deep copy of the header. It also refactors this code, which is used in both the Filter and Pipeline tools, into a utility function `checkAndClearHeaderSequences()` in PSUtils. Tests have also been added/updated accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3453:473,Pipeline,Pipeline,473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3453,2,"['Pipeline', 'update']","['Pipeline', 'updated']"
Deployability,Picard integration.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3620:7,integrat,integration,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3620,1,['integrat'],['integration']
Deployability,"Picard style was to have arguments with default collection values be appended to rather than replaced if that argument is also specified on the command line. to remove the defaults you have to specifically pass in `null` and then the new options. this seems like totally bizarre behavior, I thought I had fixed it with my initial patch but it slipped through unchanged. `RevertSam` relies on this functionality, so its interface will need to change slightly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/175:330,patch,patch,330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/175,1,['patch'],['patch']
Deployability,Pip install and pin numpy version.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6413:4,install,install,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6413,1,['install'],['install']
Deployability,Pipeline with multiple datatypes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/281:0,Pipeline,Pipeline,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/281,1,['Pipeline'],['Pipeline']
Deployability,Placed a call to forceJVMLocaleToUSEnglish in Main.runCommandLineProgram so that integration tests can take advantage of this. Fixes #2557,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3064:81,integrat,integration,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3064,1,['integrat'],['integration']
Deployability,Please fix the broken 3.8-1 release with wrong pom.xml,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4685:28,release,release,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685,1,['release'],['release']
Deployability,Please update jbwa with the current github version. A patch for PPC64 is merged with jbwa now. Thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1914:7,update,update,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1914,2,"['patch', 'update']","['patch', 'update']"
Deployability,"Please update the GitHub description to use https://www.broadinstitute.org/gatk/ which saves one redirect, and is more secure with rogue DNS servers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3211:7,update,update,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3211,1,['update'],['update']
Deployability,Please upgrade NIO to a version that has prefetching baked-in.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4986:7,upgrade,upgrade,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4986,1,['upgrade'],['upgrade']
Deployability,"Please, I would like to update my GATK dependency to the latest master. Nevertheless, the Barclay version included contains a bug that is solved in https://github.com/broadinstitute/barclay/commit/87c3fa228fa96d11b7177f8bfdfca4801c83a068, but a change in the usage method of Barclay before the fix to print hidden arguments broke the GATK command line class. Could it be possible to update to the latest Barclay master, please?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2454:24,update,update,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2454,2,['update'],['update']
Deployability,Plot called segments at end of ModelSegments pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4244:45,pipeline,pipeline,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4244,1,['pipeline'],['pipeline']
Deployability,Polish up PathSeq and add pipeline tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3271:26,pipeline,pipeline,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3271,1,['pipeline'],['pipeline']
Deployability,Port IndelRealignment pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104:22,pipeline,pipeline,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104,1,['pipeline'],['pipeline']
Deployability,"Port https://github.com/broadgsa/gatk-protected/pull/24; ```; Without this patch the stream is only closed (thus, flushed) when the; object is garbage collected. This is problematic when subsequent jobs; proceed and expect the output to be available, for example; AnalyzeCovariates. We see failures in approximately 50% of runs due to; this issue and they are confirmed as fixed when applying the patch (on; a busy machine using NFS storage).; ```. The tool `BaseRecalibratorSparkSharded` is affected. The fix will be in `BaseRecalibratorEngineSparkWrapper.saveTextualReport`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3161:75,patch,patch,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3161,2,['patch'],['patch']
Deployability,Port of the GATK3 Version of CombineGVCFs and its associated integration tests. . Fixes #16,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3718:61,integrat,integration,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3718,1,['integrat'],['integration']
Deployability,Ported CallSegments as CallCopyRatioSegments for ModelSegments CNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3731:67,pipeline,pipeline,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3731,1,['pipeline'],['pipeline']
Deployability,Ported Picard tool UpdateVcfSequenceDictionary,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2232:19,Update,UpdateVcfSequenceDictionary,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2232,1,['Update'],['UpdateVcfSequenceDictionary']
Deployability,"PostprocessGermlineCNVCalls performs a check of the denoising/calling hyperparameter configs used to generate the model in GermlineCNVCaller cohort mode against those used to generate the case-mode result passed to PostprocessGermlineCNVCalls. However, although some of these hyperparameters are not exposed in case mode (since they have no effect on the sample-level parameters inferred in case mode, e.g., `psi_t_scale`), their python default values are nevertheless written to the case-mode config. I think that this results in a spurious mismatch between the cohort/case mode configs, which causes PostprocessGermlineCNVCalls to emit the following warnings in case mode when non-default values are used:. ````; WARNING gcnvkernel.postprocess.viterbi_segmentation - Different denoising configuration between model and calls -- proceeding at your own risk!; WARNING gcnvkernel.postprocess.viterbi_segmentation - Different calling configuration between model and calls -- proceeding at your own risk!; ````. I'm pretty sure that inference is actually performed correctly, but we may want to double check and clean up these warnings. We should probably just copy the non-exposed values from the model config on the python side when running GermlineCNVCaller in case mode. Not sure if there's any way to emit sensible warnings on the Java side. These hyperparameters are still exposed to the Java command line in case mode, they just aren't passed on to the python command line. So the user can change their values from their engine defaults without having any effect at all, but this is probably what we want. Perhaps we can document, though.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6994:789,configurat,configuration,789,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6994,2,['configurat'],['configuration']
Deployability,"Preparation for some refactoring related to TileDB integration. Extracted classes are package-protected for now, since they are not intended for direct use outside of the engine package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1929:51,integrat,integration,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1929,1,['integrat'],['integration']
Deployability,Prevents a bug that occurs when a file path contains characters that are illegal in URIs. Specific example was when using `--tmp-dir file:///tmp/workflow#main` GATK would initially correctly interpret this as `/tmp/workflow#main` but then when setting the Java temp directory in `CommandLineProgram.java` line 164 it would send `/tmp/workflow#main` to `IOUtils.getAbsolutePathWithoutFileProtocol` which would then mangle it by turning it into a URI and then removing `file://` resulting in `/tmp/workflow%23main` which later causes issues when things like the Codecs attempt to write configuration files to the temp directory that doesn't exist. CWLTool often creates path names that contain `#` so workflows made by CWLTool and containing GATK can fail because of this bug.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6769:584,configurat,configuration,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6769,1,['configurat'],['configuration']
Deployability,"Previous behavior assumed that the NM tag was the number of substitutions (as would be implied by ""matches or mismatches"" in CIGAR lingo). This patches makes the function correctly account for indels as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3876:144,patch,patches,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3876,1,['patch'],['patches']
Deployability,"Previously we were relying on the gcloud package signing key retrieved; during build of the GATK base image. However, the base image is updated; so infrequently that it's possible that the key it uses has expired. To; address this, we now retrieve an updated key at the start of the Docker; build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7180:136,update,updated,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7180,2,['update'],['updated']
Deployability,"Previously, reads with deletions at the current loci were not being included in; the pileups passed to isActive(), which was a difference vs. the default settings; in GATK3. This patch changes the default to be to include such reads in both; HaplotypeCaller and Mutect2. Resolves #3830",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3831:179,patch,patch,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3831,1,['patch'],['patch']
Deployability,"Previously, the only integration tests involving assembly region traversal were in gatk-protected,; which led to breakage. Resolves #2172",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2179:21,integrat,integration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2179,1,['integrat'],['integration']
Deployability,Primarily the large integration tests check for the non-locatable; funcotation factories producing data. Regenerated the expected output from large tests. Fixes #5773,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5774:20,integrat,integration,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5774,1,['integrat'],['integration']
Deployability,PrintReads --remove-mates or --update-flags option,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6839:31,update,update-flags,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6839,1,['update'],['update-flags']
Deployability,Problem with gcnvkernel installation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8091:24,install,installation,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8091,1,['install'],['installation']
Deployability,"Profiling using JBuilder remotely on gsa5 (with a large load from other programs) seem to show that close to or over 50% of the CPU effort is dedicated to filter ""bad"" reads. <img width=""1006"" alt=""screen shot 2018-09-27 at 2 37 59 pm"" src=""https://user-images.githubusercontent.com/791104/46167159-09fe1f00-c263-11e8-8ea0-02621146659b.png"">. To reproduce you may run (or better make your copy and run on a different profiling port and folder):; ```; cd /dsde/working/valentin/crc-profiling; sh run.sh; ```; ```; #run.sh contents:; java -agentpath:./bin/linux-x64/libjprofilerti.so=port=5006,nowait -jar ./gatk-local.jar \ ; CollectReadCounts \ ; -I /dsde/working/CHM/33remap/msb2.m38.bam \; -R /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.fasta \; -O /tmp/test.tsv -L hg38.interval_list -imr OVERLAPPING_ONLY; ```. The sample was chosen kinda at random , is a CHM pseudo diploid sample but you could use an alternative. Please change profiling port and output file. On the profiling machine (presumably your laptop or desktop) you need to install JProfiler (Broad owns a license for that).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5233:1062,install,install,1062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5233,1,['install'],['install']
Deployability,Proposal: update gradle to 4.6,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4659:10,update,update,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659,1,['update'],['update']
Deployability,Prototype TileDB integration (for CombineGVCFs and GenotypeGVCFs),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1647:17,integrat,integration,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1647,1,['integrat'],['integration']
Deployability,Publish release and nightly GATK jars to the Broad filesystem for internal users,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4208:8,release,release,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4208,1,['release'],['release']
Deployability,Pv sw alignerclass update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3390:19,update,update,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3390,1,['update'],['update']
Deployability,"PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:2422,update,updates,2422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,1,['update'],['updates']
Deployability,Python script executor updates required for NeuralNetInference branch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4218:23,update,updates,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4218,1,['update'],['updates']
Deployability,"Questions: I'm not sure how to integrate the count summary and output from these in hellbender. GATK uses a collection of filters, so it can query each filter individually for a count. Hellbender uses a single filter lambda, which represents a sequence of and'd and or'd filters, so the filter itself needs to report all of the counts based on component filter conditions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/613:31,integrat,integrate,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/613,1,['integrat'],['integrate']
Deployability,Quick update to docs to include an FAQ section with the first common question populated. Already pushed to forum.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5755:6,update,update,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5755,1,['update'],['update']
Deployability,Quickstart based integration test [VS-357],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7812:17,integrat,integration,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7812,1,['integrat'],['integration']
Deployability,"RAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.458 INFO ComposeSTRTableFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:44:55.458 INFO ComposeSTRTableFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:44:55.459 INFO ComposeSTRTableFile - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:44:55.459 INFO ComposeSTRTableFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:44:55.460 INFO ComposeSTRTableFile - Start Date/Time: April 4, 2021 1:44:55 PM EDT; 13:44:55.460 INFO ComposeSTRTableFile - ------------------------------------------------------------; 13:44:55.460 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:4212,install,install,4212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['install'],['install']
Deployability,"README should mention new build targets ""gradle installSpark"" and ""gradle installAll"", remove mention of ""gradle sparkJar""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1302:48,install,installSpark,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1302,2,['install'],"['installAll', 'installSpark']"
Deployability,"README.md instructs developers to create the conda environment with the command:; ```; conda env create -n gatk -f scripts/gatkcondaenv.yml; ```; This currently fails with the following message (at least on MacOS):; ```; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main; status = self.run(options, args); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run; wb.build(autobuilding=True); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build; self.requirement_set.prepare_files(self.finder); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files; ignore_dependencies=self.ignore_dependencies)); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/req/req_set.py"", line 620, in _prepare_file; session=self.session, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 809, in unpack_url; unpack_file_url(link, location, download_dir, hashes=hashes); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/download.py"", line 715, in unpack_file_url; unpack_file(from_path, location, content_type, link); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 599, in unpack_file; flatten=not filename.endswith('.whl'); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pip/utils/__init__.py"", line 482, in unzip_file; zipfp = open(filename, 'rb'); FileNotFoundError: [Errno 2] No such file or directory: '/Users/markw/IdeaProjects/gatk/scripts/build/gatkPythonPackageArchive.zip'; ```; Moving gatkcondaenv.yml to the GATK root solves the issue. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4741:638,install,install,638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741,1,['install'],['install']
Deployability,README: R setup for running tests may need OS X update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5389:48,update,update,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5389,1,['update'],['update']
Deployability,REATE_MD5 : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_pack,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:4801,Configurat,Configuration,4801,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Configurat'],['Configuration']
Deployability,"RITE\_FOR\_SAMTOOLS : true ; ; 00:12:21.143 INFO BaseRecalibrator - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 00:12:21.143 INFO BaseRecalibrator - Deflater: IntelDeflater ; ; 00:12:21.144 INFO BaseRecalibrator - Inflater: IntelInflater ; ; 00:12:21.144 INFO BaseRecalibrator - GCS max retries/reopens: 20 ; ; 00:12:21.144 INFO BaseRecalibrator - Requester pays: disabled ; ; 00:12:21.144 INFO BaseRecalibrator - Initializing engine ; ; 00:12:21.485 INFO FeatureManager - Using codec VCFCodec to read file file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz ; ; 00:12:21.565 INFO FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz ; ; 00:12:21.688 INFO FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz ; ; 00:12:21.797 WARN IndexUtils - Feature file ""file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file ; ; 00:12:21.895 WARN IntelInflater - Zero Bytes Written : 0 ; ; 00:12:21.966 INFO BaseRecalibrator - Done initializing engine ; ; 00:12:21.969 INFO BaseRecalibrationEngine - The covariates being used here: ; ; 00:12:21.969 INFO BaseRecalibrationEngine -   ReadGroupCovariate ; ; 00:12:21.969 INFO BaseRecalibrationEngine -   QualityScoreCovariate ; ; 00:12:21.969 INFO BaseRecalibrationEngine -   ContextCovariate ; ; 00:12:21.969 INFO BaseRecalibrationEngine -   CycleCovariate ; ; 00:12:22.016 INFO ProgressMeter - Starting traversal ; ; 00:12:22.017 INFO ProgressMeter -    Current Locus Elapsed Minutes    Reads Processed   Reads/Minute. **How can I assign a temp directory and won't get the bug?**. I set the gatk environment using conda:. /data/xieduo/WES\_pipe/pipeline/bin/Minicond",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:18267,pipeline,pipeline,18267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"Rather than start off the exome genotyping task with a step to merge the query intervals for better GDB performance, make it an option as for Import in #5540 . This is necessary to get the exome joint calling pipeline onto official GATK release dockers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5741:209,pipeline,pipeline,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5741,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"Rationale: certain evaluators use a pedigree. This PR is a minor change that lets VariantEvalArgCollection supply the PedigreeValidationType. It defaults to the current behavior, which is to always use STRICT. It includes an integration test to cover this feature.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7240:225,integrat,integration,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7240,1,['integrat'],['integration']
Deployability,Re-activate async I/O for the samtools package only (not tribble) after next htsjdk release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1653:84,release,release,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1653,1,['release'],['release']
Deployability,"Re-enable tests for htsget now that the reference server is back to a stable version. * Some tests were disabled due to issues with the htsget reference server, now that it's back to running an older stable version; the tests which work on that version are re-enabled. * Partial fix for #6640 another commit will be needed when the server is upgraded to support fields/tags. The field test had to be disabled because it doesn't seem like the current server version supports the parameter correctly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6668:342,upgrade,upgraded,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6668,1,['upgrade'],['upgraded']
Deployability,Re-packaging SV pipeline classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2698:16,pipeline,pipeline,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2698,1,['pipeline'],['pipeline']
Deployability,ReCapSeg uses HD5F to store matrix data. This issue is about addressing hdf5 library integration in Hellbender. . Time estimate of 5 man days.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/498:85,integrat,integration,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/498,1,['integrat'],['integration']
Deployability,"Read count logging for PathSeq Filter, Score, and Pipeline tools",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611:50,Pipeline,Pipeline,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611,1,['Pipeline'],['Pipeline']
Deployability,"Read counts at different stages of the PathSeq pipeline are now logged using `MetricsFile`. The filter metrics contains the number of reads remaining and number of reads filtered at each step (after filtering pre-aligned reads, low quality/complexity reads, host reads, and duplicates). The score metrics give number of pathogen-mapped and unmapped reads. These metrics are now validated in the PathSeq integration tests, which have also been refactored to use DataProviders instead of separate functions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611:47,pipeline,pipeline,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,ReadSparkSource.getHeader is fragile and needs to be updated,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1346:53,update,updated,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1346,1,['update'],['updated']
Deployability,ReadsPipelineSpark needs an integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1162:28,integrat,integration,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1162,1,['integrat'],['integration']
Deployability,"Reasoning:; - task will not recreate/overwrite table if it exists; - task does not take long, so unnecessary runs are not costly in time or $; - when not volatile, Beta users need to run with call-caching off if they re-run the pipeline. run where tables already existed: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/64782949-33dd-41ef-b3f7-5e88cc5a5dcc. integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9e79ef7f-9e64-46c7-8749-83909a5d423f (it failed the end tests, but the tables were created/populated as expected)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8765:228,pipeline,pipeline,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8765,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"Rebasing on the most recent commit to `ah_var_store` with some fixups, successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%203%20samples/job_history/1156657f-5c31-446a-92e1-5e39ae012ce2). The Docker CI breakages appear to be affecting all Java 8 based branches, not just this one (I currently have no idea what's wrong there).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8434:82,integrat,integration,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8434,1,['integrat'],['integration']
Deployability,Rebuilt the base image so we have the newest ubuntu and library patches.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9005:64,patch,patches,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9005,1,['patch'],['patches']
Deployability,Recent refactoring seems to have introduced a bug in pileup mode that failed to enforce the limit on the number of haplotypes to be considered. With this patch:. - HaplotypeCaller once again respects the limit on haplotypes before genotyping.; - Changed some `HashSet`s to `LinkedHashSets` to preserve determinism.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8489:154,patch,patch,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8489,1,['patch'],['patch']
Deployability,"Recently I was setting up GATK to run in a VM and I had forgotten to install Java8 onto the machine. When I tried to run GATK from the launch script I ran into the following error: ; ```; Using GATK jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar -help; Traceback (most recent call last):; File ""./gatk"", line 479, in <module>; main(sys.argv[1:]); File ""./gatk"", line 152, in main; runGATK(sparkRunner, sparkSubmitCommand, dryRun, gatkArgs, sparkArgs, javaOptions); File ""./gatk"", line 328, in runGATK; runCommand(cmd, dryrun); File ""./gatk"", line 384, in runCommand; check_call(cmd, env=gatk_env); File ""/usr/lib/python2.7/subprocess.py"", line 181, in check_call; retcode = call(*popenargs, **kwargs); File ""/usr/lib/python2.7/subprocess.py"", line 168, in call; return Popen(*popenargs, **kwargs).wait(); File ""/usr/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ```; This should perhaps be made a little bit clearer for users as this isn't particularly helpful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5992:69,install,install,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5992,1,['install'],['install']
Deployability,Reduce the logging a bit.; Probably should make a PR directly into gatk master so that when we next merge gatk master changes we'll get this goodness?. Integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f8c38f97-7945-414f-9432-13b2f12138bb) (note failed one of the subtests for a random docker pull error); Example CreateFilterSet run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/b2e7eb86-e494-4891-885b-5a96cb1056b3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8650:152,Integrat,Integration,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8650,1,['Integrat'],['Integration']
Deployability,Reenable ReferenceTwoBitSourceUnitTest after ADAM 0.18 is released,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/957:58,release,released,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/957,1,['release'],['released']
Deployability,"Refactor Funcotator scripts to take arguments, not internal configurations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5346:60,configurat,configurations,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5346,1,['configurat'],['configurations']
Deployability,"Refactor python code from extract dir into a scripts directory. Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f85602d0-6dc5-49d6-82d1-eb58e9966021); Passing VAT Creation work [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/ddc7fcf9-5fb7-44e2-8117-721389d4f858), [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/0d705f21-3362-4890-b925-5bed2646fe4d), and [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/15cfe125-e700-44c8-b9d0-c3e98d7db4c0)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9017:72,Integrat,Integration,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9017,1,['Integrat'],['Integration']
Deployability,"Reflect the changes from https://github.com/broadinstitute/picard/pull/1782 in the GATK doc build. Note that this requires a new version of Picard that is not yet released. Also, I suppressed deprecation warnings caused by the use of obsolete Java 8 javadoc classes (FieldDoc) for now, since it causes the Java 11 build to fail. A new metrics category shows up in the doc now, with all of the Picard and GATK metrics:. <img width=""1252"" alt=""Screen Shot 2022-10-18 at 4 42 37 PM"" src=""https://user-images.githubusercontent.com/10062863/196540823-a6108b75-c9e7-44c0-a4ba-1d8f927fe5b6.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7749:163,release,released,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7749,1,['release'],['released']
Deployability,"Regression test for #3163. A unit test was added in #3164, but we should add an integration test as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3181:80,integrat,integration,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181,1,['integrat'],['integration']
Deployability,Release GATK 4.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5636:0,Release,Release,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5636,1,['Release'],['Release']
Deployability,Release doc additions [VS-1376],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8927:0,Release,Release,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8927,1,['Release'],['Release']
Deployability,Release docs fixup [VS-1088],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8548:0,Release,Release,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8548,1,['Release'],['Release']
Deployability,Release new version of GVS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8883:0,Release,Release,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8883,1,['Release'],['Release']
Deployability,"Relies on https://github.com/disq-bio/disq/pull/69, which has not yet been merged or released.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5485:85,release,released,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5485,1,['release'],['released']
Deployability,Remove -1 length once a corrected version of the exome interval list is released.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2089:72,release,released,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2089,2,['release'],['released']
Deployability,Remove EXPERIMENTAL_FPGA_LOGLESS_CACHING option from PairHMM after next GKL release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6673:76,release,release,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6673,1,['release'],['release']
Deployability,Remove NuMTs from MT pipeline and updates wdl to GATK4.1.1.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5847:21,pipeline,pipeline,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5847,2,"['pipeline', 'update']","['pipeline', 'updates']"
Deployability,Remove Unneeded Task and Update Docs [VS-817],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8702:25,Update,Update,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8702,1,['Update'],['Update']
Deployability,Remove download of picard.jar from .travis.yml and update Mutect2 WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3625:51,update,update,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3625,1,['update'],['update']
Deployability,"Remove gatk-launch dependency on settings.gradle, update gatkZipDistribution target",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3054:50,update,update,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3054,1,['update'],['update']
Deployability,Remove reference to `PipelineOptions` SV package,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3477:21,Pipeline,PipelineOptions,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3477,1,['Pipeline'],['PipelineOptions']
Deployability,Remove some uses of dataflow.sdk.options.PipelineOptions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2786:41,Pipeline,PipelineOptions,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2786,1,['Pipeline'],['PipelineOptions']
Deployability,Remove travis R install and only run R tests on the Docker.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6454:16,install,install,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6454,1,['install'],['install']
Deployability,Removed GATK3.5 VCFs from HC integration test files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7634:29,integrat,integration,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7634,1,['integrat'],['integration']
Deployability,Removed mapping error rate from estimate of denoised copy ratios output by gCNV and updated sklearn.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7261:84,update,updated,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7261,1,['update'],['updated']
Deployability,Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394:98,update,updated,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394,2,['update'],['updated']
Deployability,Removing the beta tag in advance of the 4.1 release. . Resolves #4675,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5603:44,release,release,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5603,1,['release'],['release']
Deployability,Replace MannWhitneyU with updated version from GATK3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2604:26,update,updated,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2604,2,['update'],['updated']
Deployability,Replace literal arguments with variables in several integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4416:52,integrat,integration,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4416,1,['integrat'],['integration']
Deployability,Replaced bash script in gCNV ScatterIntervals task with updated version of IntervalListTools.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5414:56,update,updated,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5414,1,['update'],['updated']
Deployability,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6088:137,release,release,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088,1,['release'],['release']
Deployability,Request: fine-grained configuration for codec packages for downstream projects,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:22,configurat,configuration,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,1,['configurat'],['configuration']
Deployability,Request: update Barclay dependency,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2454:9,update,update,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2454,1,['update'],['update']
Deployability,Requires a new htsjdk release - draft state until then.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6763:22,release,release,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6763,1,['release'],['release']
Deployability,"Requires upgrades to Hadoop-BAM (not yet released) and htsjdk. Fixes https://github.com/broadinstitute/gatk/issues/1346, https://github.com/broadinstitute/gatk/issues/1261, https://github.com/broadinstitute/gatk/issues/1175, https://github.com/broadinstitute/gatk/issues/1326, https://github.com/broadinstitute/gatk/issues/1259, and much of the underlying code for https://github.com/broadinstitute/gatk/issues/1270, which will be enabled in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1469:9,upgrade,upgrades,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1469,2,"['release', 'upgrade']","['released', 'upgrades']"
Deployability,"Researcher reports error in GenotypeGVCFs that uses a GenomicsDB database using v4.0.5.0. Removing `new qual` param OR using v4.0.4.0 allows the command to run without error. I saw a similar error with v4.0.5.1 when I tried to add `new qual` to a workshop hands-on tutorial GenotypeGVCFs step using a GenomicsDB database. ---; Hi,. I am trying to process locally 260 WES gvcf through joint discovery wdl pipeline. I encountered an error at GenotypeGVCFs below which I am not sure how to proceed. I have used all the default reference libraries and only modified the merge_count in the script to be 8144 so that my server resources won't be maxout fully in the ImportGVCFs step. . [https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl ""https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4-local.wdl""). ```; 23:17:43.992 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:44.064 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 23:17:46.334 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),30.197597194999727,Cpu time(s),28.791204838999864; [June 25, 2018 11:17:46 PM UTC] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 4.01 minutes.; Runtime.totalMemory()=5354029056; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 2.559797571100845E-21,NaN; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotype",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4975:404,pipeline,pipeline,404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975,1,['pipeline'],['pipeline']
Deployability,Restore link in VariantFiltration to point to update online JEXL doc.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5525:46,update,update,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5525,1,['update'],['update']
Deployability,"Revert ""Bump to ADAM 0.23.0 release. (#4044)""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4428:28,release,release,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4428,1,['release'],['release']
Deployability,"Revert ""Upgrade htsjdk to v3.0.0. (#7867)""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7960:8,Upgrade,Upgrade,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7960,1,['Upgrade'],['Upgrade']
Deployability,"Revert ""Upgrade to htsjdk 2.11.0. Make TargetCodec indexable.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3441:8,Upgrade,Upgrade,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3441,1,['Upgrade'],['Upgrade']
Deployability,Revert 403 GCS retry patch once Google fixes the underlying service,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3800:21,patch,patch,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3800,1,['patch'],['patch']
Deployability,Revert explicit GAR references in our Docker build scripts for now. Variants team members are not Methods team members and thus do not have the access required to make Variants GAR repos public in the `broad-dsde-methods` project. Note that Variants images are still ending up in GAR thanks to the magic of DevOps redirects. This PR also retains the Docker image ID-based referencing that was introduced at the same time as the explicit GAR references that are now being backed out. Successful (or at least non-instafailing) [integration run here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/2f00b836-0c2d-41e9-84b1-b8c6a2bea8f6).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8789:526,integrat,integration,526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8789,1,['integrat'],['integration']
Deployability,Revert some phasing changes that were unnecessary for AoU and broke our integration tests. [Integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ba93baa2-9971-4c90-8ce3-635702a81eb6),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8590:72,integrat,integration,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8590,2,"['Integrat', 'integrat']","['Integration', 'integration']"
Deployability,Revert upgrade to GenomicsDB 1.1.2 to fix a regression,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6204:7,upgrade,upgrade,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6204,1,['upgrade'],['upgrade']
Deployability,"Reverting the update to gkl 0.3.1 for now because we've encountered some downstream errors that are making it impossible to update gatk-protected. This reverts commit 9e3c6e3d7370c503d2a57be0c662fb1016d8b764, reversing; changes made to 767974906e91c90079cefa4512b463138ca09f68.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2319:14,update,update,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2319,2,['update'],['update']
Deployability,"Reworks classes used by `JointGermlineCNVSegmentationIntegration` for SV clustering and defragmentation. The design of `SVClusterEngine` has been overhauled to enable the implementation of `CNVDefragmenter` and `BinnedCNVDefragmenter` subclasses. Logic for producing representative records from a collection of clustered SVs has been separated into an `SVCollapser` class, which provides enhanced functionality for handling genotypes for SVs more generally. A number of bugs, particularly with max-clique clustering, have been fixed, as well as a parameter swap bug in `JointGermlineCNVSegmentationIntegration`. This is the first of a series of PRs for an experimental Java-based implementation of some modules in `gatk-sv` pipeline, including SV vcf merging, clustering, evidence aggregation, and genotyping.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7243:724,pipeline,pipeline,724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243,1,['pipeline'],['pipeline']
Deployability,"Right now, in master pipeline we have one `INSERTED_SEQUENCE` AND `INSDERTED_SEQUENCE_MAPPINGS` annotation each, with the 2nd annotation taking possibly multiple values whereas the 1st only 1 value. We need to improve this. ; A possible solution is to have one annotation with paired multiple entries as suggested by @cwhelan . The goal is to make transforming symb alt allele records to BND records easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3647:21,pipeline,pipeline,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3647,1,['pipeline'],['pipeline']
Deployability,Run MD+BQSR+HC pipeline on full genome,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3106:15,pipeline,pipeline,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3106,1,['pipeline'],['pipeline']
Deployability,"Run validation tests continuously in jenkins: ReadsPipelineSpark, BQSR etc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1401:21,continuous,continuously,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1401,1,['continuous'],['continuously']
Deployability,Running `gatk --version` produces a dump of all the available tools and then a user exception.; ```. ... UpdateVcfSequenceDictionary (Picard) Takes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.; VariantAnnotator (BETA Tool) Tool for adding annotations to VCF files; VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. ***********************************************************************. A USER ERROR has occurred: '--version' is not a valid command. ***********************************************************************; ```; It should print the version instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5533:105,Update,UpdateVcfSequenceDictionary,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5533,2,"['Update', 'update']","['UpdateVcfSequenceDictionary', 'updates']"
Deployability,"Running the reads pipeline on latest master on GCP on an exome shows a major performance regression. At the end of last year this took 40 minutes for the whole job, now using master BQSR is taking 1.4 hours, and HC is taking well over an hour (it hasn't finished yet). #4278 is the likely culprit for the HC slowdown. Not sure about BQSR. cc @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376:18,pipeline,pipeline,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376,1,['pipeline'],['pipeline']
Deployability,Running with default arguments locally the runtime (for a WGS full chr15) drops from ~8.9 minutes to ~4.7 minutes after this patch. If I had to peg something else to optimize it would be replacing CSVWriter which seems to be somewhat slow but I can be contented that this tool is reasonably fast when nothing pathological is being triggered.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6740:125,patch,patch,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6740,1,['patch'],['patch']
Deployability,Running without an output file causes an NPR. Needs a null check. ```; ./gatk-launch CountReadsSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_reads_sorted.bam; ```. ```; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.isCloudStorageUrl(BucketUtils.java:44); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.createFile(BucketUtils.java:105); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:310); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1523:454,pipeline,pipelines,454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1523,1,['pipeline'],['pipelines']
Deployability,SSION_LEVEL : 2; 11:35:40.188 INFO Mutect2 - HTSJDK Defaults.CREATE_INDEX : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3302,Configurat,Configuration,3302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Configurat'],['Configuration']
Deployability,SV Pipeline Jobs LongRunning : BAFFromGVCFs_ImportGVCFs and GenotypeVCFs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646:3,Pipeline,Pipeline,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646,1,['Pipeline'],['Pipeline']
Deployability,SV pipeline clean up,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2621:3,pipeline,pipeline,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2621,1,['pipeline'],['pipeline']
Deployability,"SV pipeline fails in experimental variation interpretation with "" Unexpected CIGAR format with deletion neighboring clipping""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4260:3,pipeline,pipeline,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4260,1,['pipeline'],['pipeline']
Deployability,"SV pipeline fails on NA12878 b37 bam with ""observedValue must be non-negative"" in IntHistogram",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:3,pipeline,pipeline,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['pipeline'],['pipeline']
Deployability,"SV pipeline failure on CHM WGS1 with ""two input alignments' overlap on read consumes completely one of them.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:3,pipeline,pipeline,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['pipeline'],['pipeline']
Deployability,SV pipeline run on NA12878 hg19 has a very slow laggard assembly task,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3607:3,pipeline,pipeline,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3607,1,['pipeline'],['pipeline']
Deployability,SV pipeline tries to create illegal interval,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3874:3,pipeline,pipeline,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3874,1,['pipeline'],['pipeline']
Deployability,SV read depth integration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5161:14,integrat,integration,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5161,1,['integrat'],['integration']
Deployability,SV spark pipeline bash script,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2187:9,pipeline,pipeline,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2187,1,['pipeline'],['pipeline']
Deployability,Same integration test fails with IntegrationTestSpec but passes with manual runCommandLine()/assertSamsEqual() calls,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1164:5,integrat,integration,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1164,2,"['Integrat', 'integrat']","['IntegrationTestSpec', 'integration']"
Deployability,"Say hello to Azure SQL Database from `sqlcmd`, Python and Java (via Ammonite) running in a Cromwell on Azure deployment. Since the Azure Batch VMs spun up by Cromwell on Azure appear to have no identity associated with them the workflow currently takes a database access token as a parameter which it passes to the three tasks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8220:109,deploy,deployment,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8220,1,['deploy'],['deployment']
Deployability,Scripts running the whole sv-pipeline as exists now from birth to termination,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435:29,pipeline,pipeline,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435,1,['pipeline'],['pipeline']
Deployability,"See #2488 for context. In short, the internal pathways for authentication changed, breaking some tests. We're pushing forward anyways but need to remember to re-enable the code & tests once we can (should be the next release).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2496:217,release,release,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2496,1,['release'],['release']
Deployability,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5587:299,release,release,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587,2,['release'],['release']
Deployability,"See e.g. SavvyCNV. Should be relatively easy to rework PreprocessIntervals to make this possible, but we should see if the filtering/denoising/segmentation methods in both pipelines play along nicely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6131:172,pipeline,pipelines,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6131,1,['pipeline'],['pipelines']
Deployability,"See https://bismap.hoffmanlab.org/. As of March 2020, some updates have been made to the single-read mappability track. These are probably minor, but we should update the version of the track in our resource files and do appropriate sanity checks. Note that a manual merging of overlapping intervals was performed for that version, but should no longer be necessary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6591:59,update,updates,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6591,2,['update'],"['update', 'updates']"
Deployability,See https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.5.2 for release notes.; Of relevance to gatk are the following ; ```; Support for MacOS universal builds; Catch JNI importer exceptions and propagate them as java IOExceptions; Turn off HDFS support by default; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8710:45,release,releases,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8710,2,['release'],"['release', 'releases']"
Deployability,"See https://github.com/broadinstitute/gatk/issues/4125 (which I suspect is due to the conda env not being established). @mbabadi @samuelklee @vdauwera Unfortunately we didn't add anything to the doc for these tools saying that they require the conda env. Some suggestions:. - Ideally, we could do something along the lines of what @droazen suggested in #4125, where the script executor validates that the environment is established. In a previous discussion though, @vdauwera expressed some concerns around requiring miniconda (as opposed to enumerating the individual requirements and allowing users to install these themselves - which is harder to communicate, and even harder to validate). We should discuss this further.; - Either way, the tools themselves could catch PythonScriptExecutorException and re-throw it with a helpful message saying the conda env is required.; - Update the tool summaries saying that the conda env is required.; - Update the tool javadoc/gatkdoc with more detail.; - Other ? Blog entry/forum post ?. This shouldn't be an issue for Docker users. We did discover a last minute issue that will affect OSX users though, which has a couple of workarounds described in this [PR](https://github.com/broadinstitute/gatk/pull/4087).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4127:604,install,install,604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4127,3,"['Update', 'install']","['Update', 'install']"
Deployability,SelectVariants doesn't update GQ when using --removeUnusedAlternates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3404:23,update,update,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3404,1,['update'],['update']
Deployability,Serialize auth onto pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/868:20,pipeline,pipeline,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/868,1,['pipeline'],['pipeline']
Deployability,Set up continuous tests for the conda environment on MacOS in Travis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6813:7,continuous,continuous,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6813,1,['continuous'],['continuous']
Deployability,"Several changes:. - Fix https://github.com/broadinstitute/gatk/issues/4741, where newer versions of conda appear to treat relative references in the environment yml as being relative to the yml file instead of relative to the cwd (based on observation).; - Add a second conda yml file (`gatkcondaenv.intel.yml`) for environments that use Intel hardware acceleration and Intel Tensorflow package (based on https://github.com/broadinstitute/gatk/pull/4735).; - Add a gradle task (`condaEnvironmentDefinition`) to generate the conda yml files from a single template to ensure that all the environment definitions remain in sync. This task also generates the Python package archive.; - Add a gradle task (`localDevCondaEnv`) to create or update a local (non-Intel) conda environment. This is a shortcut for use during development when you're iteratively changing/testing Python code and want to update the conda env.; - Opportunistically removed the prefix verb ""create"" from the name of the `createPythonPackageArchive` task, which is now called `pythonPackageArchive`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4749:734,update,update,734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4749,2,['update'],['update']
Deployability,Several improvements to SV contig alignment configuration picker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326:44,configurat,configuration,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326,1,['configurat'],['configuration']
Deployability,"Several temporary placeholder program groups were added in https://github.com/broadinstitute/gatk/pull/3924 so they could be used until we get the real ones from Picard. However, since then, the approved list of program groups have changed (i.e., BAMPreprocessing no longer exists). Once the [final groups](https://github.com/broadinstitute/picard/pull/1043) are merged into Picard, we'll need to upgrade to a that Picard, remove the placeholders and replace any references to the placeholders to references to the real thing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4034:397,upgrade,upgrade,397,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4034,1,['upgrade'],['upgrade']
Deployability,Sf cnn wdl updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5251:11,update,updates,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5251,1,['update'],['updates']
Deployability,Sh sv cluster image ver upgrade and custom name,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4137:24,upgrade,upgrade,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4137,1,['upgrade'],['upgrade']
Deployability,Share more code between walker and Spark integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5723:41,integrat,integration,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5723,1,['integrat'],['integration']
Deployability,Shl Update somatic short variants tool docs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4310:4,Update,Update,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4310,1,['Update'],['Update']
Deployability,"Should the CreateHadoopBamSplittingIndex tool also work on a cram? I am getting this error below which suggests not. What are the benefits of a SplittingIndex for a spark job? On average-how long should it take a spark job to get the splits for a 30x bam or cram? . ```; gatk CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar CreateHadoopBamSplittingIndex -I adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 11:47:53.243 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.1.1/install/bin/gatk-package-4.0.1.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - The Genome Analysis Toolkit (GATK) v4.0.1.1; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:53.455 INFO CreateHadoopBamSplittingIndex - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.10.3.el6.x86_64 amd64; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - Start Date/Time: March 7, 2018 11:47:52 AM EST; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506:398,install,install,398,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506,3,['install'],['install']
Deployability,"Should we add this to the gradle build? Unit test require R and various R libraries to be installed, but this isn't mentioned in the documentation or performed by the build script. . I suggest we ; 1. tag all tests that require R in some way so that they can be disabled a system that doesn't have R; 2. add documentation to the readme explaining you need R and a number of r libraries; 3. either document that you must run install_R_packages.R or have gradle do it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/222:90,install,installed,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/222,1,['install'],['installed']
Deployability,Simple copy/paste bug. Closing the header line creator fixes the hanging issues as seen in [this run](https://job-manager.dsde-prod.broadinstitute.org/jobs/21c1ec08-444e-4acd-8490-cc9640d9ea03) (requires PMI ops). Integration run [in progress](https://job-manager.dsde-prod.broadinstitute.org/jobs/3b5129bb-b7fe-47db-abc4-dda5d7f5006a) (regular auth).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8533:214,Integrat,Integration,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8533,1,['Integrat'],['Integration']
Deployability,Simple patch to allow passing specific tool classes to `Main.instanceMain` instead of whole packages to solve #2140. This will allow clients that want their own command line with their tools to include only `IndexFeatureFile` for their own codecs and/or bundle tools like `CreateSequenceDictionary` to pre-process input files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2204:7,patch,patch,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2204,1,['patch'],['patch']
Deployability,Simple patch to improve the `Main` usage in the same direction as previous PRs to finer control by API user:. * Added `handleNonUserException(final Exception exception)` to handle custom exceptions.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2261:7,patch,patch,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2261,1,['patch'],['patch']
Deployability,Simple update to use the correct version of Spark in the scripts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5125:7,update,update,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5125,1,['update'],['update']
Deployability,Simplifying argument in mitochondria pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6904:37,pipeline,pipeline,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6904,1,['pipeline'],['pipeline']
Deployability,"Since the Picard changes in #3620, SamAssertionUtils has been failing silently. See e.g. the Standard error tab for https://storage.googleapis.com/hellbender-test-logs/build_reports/13120.7/tests/test/classes/org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.html:. ```; USAGE: SortSam [arguments]; ...; input is not a recognized option; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3664:251,pipeline,pipelines,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3664,1,['pipeline'],['pipelines']
Deployability,"Since we're using Mutect2 for mitochondrial calling, we want some reference confidence representation for joint calling. I did my best, but further refactoring suggestions appreciated. Tests to follow. @davidbenjamin can you take a look at the LODs in the integration test results? I'm not entirely surprised that at the same depth, the variant LOD is higher than the reference LOD. I'm not sure that the NON_REF LOD at variant sites is coming out right though. Is there an effective negative LOD asymptote?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312:256,integrat,integration,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312,1,['integrat'],['integration']
Deployability,Skeleton of the reads preprocessing pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/655:36,pipeline,pipeline,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/655,1,['pipeline'],['pipeline']
Deployability,Sl dr update docker base image,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8614:6,update,update,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8614,1,['update'],['update']
Deployability,Small updates to GVS Integration WDL [VS-618],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8042:6,update,updates,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8042,2,"['Integrat', 'update']","['Integration', 'updates']"
Deployability,"Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7493:6,update,updates,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7493,1,['update'],['updates']
Deployability,Small updates to JointVcfFiltering WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8027:6,update,updates,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8027,1,['update'],['updates']
Deployability,"Somatic WDLs have them hidden, gCNV WDLs (in sl_gcnv_ploidy_cli) have them exposed. (EDIT: Actually, gCNV WDLs only have them exposed for gCNV-specific tasks. Common tasks such as PreprocessIntervals are also not exposed.). The former makes for cleaner `wdltool inputs` JSONs that contain only the bare minimum inputs, but it is unclear whether FC will allow for task-level parameters to be set. However, this can still be done via JSON, as long as the task is at the main workflow level (although this may change with a C30 hotfix?). The latter makes for messier JSONs and requires more upkeep to make sure everything stays exposed, but should work in FC (unless the workflow is used as a subworkflow?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3980:525,hotfix,hotfix,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3980,1,['hotfix'],['hotfix']
Deployability,"Some (but not all) users get a warning whenever they run a dataflow pipeline that calls `DataflowWorkarounds.registerGenomicsCoders`. Here is an example:. ```; Jul 01, 2015 2:33:36 PM com.google.cloud.genomics.dataflow.utils.DataflowWorkarounds registerGenomicsCoders; INFO: Registering coders for genomics classes; Jul 01, 2015 2:33:36 PM org.reflections.Reflections scan; WARNING: could not create Vfs.Dir from url. ignoring the exception and continuing; org.reflections.ReflectionsException: could not create Vfs.Dir from url, no matching UrlType was found [file:/Library/KeyAccess/KeyAccess.app/Contents/SharedFrameworks/KeyAccess.framework/KeyAccess]; either use fromURL(final URL url, final List<UrlType> urlTypes) or use the static setDefaultURLTypes(final List<UrlType> urlTypes) or addDefaultURLTypes(UrlType urlType) with your specialized UrlType.; at org.reflections.vfs.Vfs.fromURL(Vfs.java:109); at org.reflections.vfs.Vfs.fromURL(Vfs.java:91); at org.reflections.Reflections.scan(Reflections.java:237); at org.reflections.Reflections.scan(Reflections.java:204); at org.reflections.Reflections.<init>(Reflections.java:129); at com.google.cloud.genomics.dataflow.utils.DataflowWorkarounds.registerGenomicsCoders(DataflowWorkarounds.java:90); at org.broadinstitute.hellbender.tools.dataflow.transforms.InsertSizeMetricsTransformUnitTest.testInsertSizeMetricsTransform(InsertSizeMetricsTransformUnitTest.java:49); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:659); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:845); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1153",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/609:68,pipeline,pipeline,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/609,1,['pipeline'],['pipeline']
Deployability,"Some are camel case already, some are python-style underscored arguments, and none are in the new standard format of #2596. Note that this requires carefully changing our wdls!!! @LeeTL1220 and @vdauwera this is easy enough to do, but any considerations on timing relative to releases or other logistical thoughts?. I notice that this inconsistency is shared with HaplotypeCaller. . .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3325:276,release,releases,276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3325,1,['release'],['releases']
Deployability,Some concerns about the Mutect2 WDL pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061:36,pipeline,pipeline,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061,1,['pipeline'],['pipeline']
Deployability,Some issues with the ga4gh htsget reference server have come up while the current htsget integration branch has been in development. A a server update is causing the previously passing tests to begin to fail. We need to re-enable them once the server is back on stable footing.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6640:89,integrat,integration,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6640,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"Some notes on individual commits:. Updated CallCopyRatioSegments and PreprocessIntervals; reorganized copynumber packages.; -For motivation of changes in CallCopyRatioSegments, see #3825.; -I added the ability to turn off binning in PreprocessIntervals by specifying bin_length = 0.; -I removed the separation between coverage and allelic packages to make the package structure a bit simpler.; -@MartonKN should review, since he wrote PreprocessIntervals and is updating the caller. Added segmentation classes and tests for ModelSegments CNV pipeline.; -I added implementations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:35,Update,Updated,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,2,"['Update', 'pipeline']","['Updated', 'pipeline']"
Deployability,"Some of our argument names have grossly inconsistent short vs. long names, like `-writeFullFormat,--never_trim_vcf_format_field`. For the 4.0 release we should do a pass to ensure that all short and long names are consistent with each other.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2597:142,release,release,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2597,1,['release'],['release']
Deployability,"Some of the Docker work from `ah_var_store` needs to be on `EchoCallset` to be able to do the PGEN subsets, particularly the PLINK Docker and GAR changes upon which the PLINK Docker changes depend. I have freshly baked the Variants, PLINK, and Docker images just now for this PR.  . Integration run in progress here https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/2585a1b6-c5da-48f0-a196-b5679e7f40a5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8805:286,Integrat,Integration,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8805,1,['Integrat'],['Integration']
Deployability,"Some preliminary evaluation of the new ModelSegments pipeline on CRSP samples has revealed some weaknesses of the ReCapSeg caller (which is simply ported from the old pipeline) to me. I think there are a lot of confusing things going on:. 1) For determining copy-neutral segments, all segments with log2 mean below some threshold are used (rather than absolute log2). There is a comment that this is done to ""mimic the python code"" but I have no idea why this would be sensible, since it includes all deletions.; 2) There is some confusion arising from inconsistent use of z-score and T-statistic. Standard deviation, rather than standard error, is used for calling; i.e., a ""called segment"" is one that has a mean log2 copy ratio that has a z-score above some threshold with respect to the standard deviation of the log2 copy ratios of intervals that fall within copy-neutral segments (note also that these intervals have already been filtered by z-score to remove outliers). That is, any segment with a mean that falls sufficiently within the fuzziness of the caterpillar is not called.; 3) However, even calling using standard error is probably not what we want. This would simply be asking the question: given a population of copy-neutral intervals with a mean and standard deviation, does any non-copy-neutral segment contain intervals with a mean significantly different than the population? We've already answered this question during segmentation!. I think what we want to do instead is ask questions about the population of segment-level copy-ratio estimates, weighted by length.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3825:53,pipeline,pipeline,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3825,2,['pipeline'],['pipeline']
Deployability,"Some print messages like this:; ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.smithwaterman.SmithWatermanIntelAlignerUnitTest > testSubstringMatchLong[0](359, 7M, SOFTCLIP) STANDARD_ERROR; 03:09:09.419 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils7398935100372553580.so: Shared object ""libm.so.6"" not found, required by ""libgkl_utils7398935100372553580.so""); Test: Test method testSubstringMatchLong[0](359, 7M, SOFTCLIP)(org.broadinstitute.hellbender.utils.smithwaterman.SmithWatermanIntelAlignerUnitTest) produced standard out/err: 03:09:09.419 WARN IntelSmithWaterman - Intel GKL Utils not loaded; ```. libgkl_utils.so is installed in /usr/local/lib/libgkl_utils.so, which is under the standard prefix location /usr/local where all packages are installed. OS: FreeBSD 14.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8939:708,install,installed,708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8939,2,['install'],['installed']
Deployability,"Some read tags get lost when we convert SAM to fastq. This tool allows us to get those tags back once we are done processing the fastqs (some tools e.g. adapter clippers cannot take SAMs as input so the conversion is unavoidable.) So this tool works like Picard MergeBamAlignment, except that we are putting the tags from the unaligned bam to the aligned bam, rather than adding alignment info to the unaligned bam. We will use this in our new TCap RNA pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7739:453,pipeline,pipeline,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7739,1,['pipeline'],['pipeline']
Deployability,Some refactoring of where the main WDLs live. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/009b92ea-9b51-4ebe-8ddd-924c53f28a55).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8970:54,Integrat,Integration,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8970,1,['Integrat'],['Integration']
Deployability,"Some tools work with a large list of intervals. In some case these are quite repetitive and they could specify in a single line but due to the need to enumerate each interval explicitly in the interval lists it might result in a uncessary large file, potentially GB in size. ## Repetitive intervals. For example the SV detection pipeline collects read counts at 100bp intervals. In a 3.2Gbp genome that is roughly 30M entries. Easily a text interval_list in its simplest form would need around 30ch for each interval that bump it up to 900MB . However one could express the same list just like:. `* *:100`. where the first asterisk stands for ""any contig"", the second stands for ""whole contig"" and the 100 means into 100bp adjacent intervals. from 7ch to 900M??? A few more example as to how such a language could look like:. ```; chr1 # the entire chr1; chr1 * # same; chr1,chr2 # both chr1 and chr2, in full.; * # all contigs in full.; * * # same.; chr1 100-200 # sigle interval from 100-200 on chr1.; chr1 { 100-200 } # same; chr1 { # same; 100-200; }; * 100-200 # 100-200 at every contig.; chr1,chr2 100-200 # only on chr1 and chr2; chr1 *200 # from 1-200 i.e. start to 200.; chr1 4000* # from 4000 to the end of chr1.; chr1 4000 # only position 4000; chr1 4M # only position 4 million. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5702:329,pipeline,pipeline,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702,1,['pipeline'],['pipeline']
Deployability,"Somehow Kris managed to generate a VCF with an index that doesn't have a properly sorted sequence dictionary: gs://broad-dsde-methods/kcibul/bug_for_louis I think it was with GATK4 SelectVariants (with a version prior to the commandline being put in the header), but I'm not 100% sure. Generating the index on the fly with GATK3 works fine. I'm not sure if the original tabix index from the GotC pipeline is okay: gs://broad-jg-dev-storage/temp/09C99383.91c5a812-70c5-4526-a3a2-3e99b9cf08fb.g.vcf.gz.tbi. I found this because GATK3 complained about the contig order.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3119:396,pipeline,pipeline,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3119,1,['pipeline'],['pipeline']
Deployability,"Sometimes NON_REF gets a zero and sometimes it's empty. This seems isolated to a much older version of ReblockGVCF, but that was what we were running for production pipeline tests. @droazen I'd like this to go into this week's release",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6442:165,pipeline,pipeline,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6442,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"Somewhere between #835 and now, BaseRecalibrator stopped working. When I try to run testBQSRBucket, I get the error below. This test is currently enabled so regression tests should have caught this. ```; java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:131); at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:104); at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:86); at org.broadinstitute.hellbender.tools.dataflow.pipelines.BaseRecalibratorDataflowIntegrationTest.testBQSRBucket(BaseRecalibratorDataflowIntegrationTest.java:176); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/866:412,Integrat,IntegrationTestSpec,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/866,7,"['Integrat', 'pipeline']","['IntegrationTestSpec', 'pipelines']"
Deployability,Spark SV pipeline example shell scripts currently not working,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6068:9,pipeline,pipeline,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6068,1,['pipeline'],['pipeline']
Deployability,Spark SV pipeline might need property `dfs.client.use.datanode.hostname=true`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6064:9,pipeline,pipeline,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064,1,['pipeline'],['pipeline']
Deployability,Spark Walker base classes need ReadsContext/readFilter integration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2338:55,integrat,integration,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2338,1,['integrat'],['integration']
Deployability,Spark and deploy-mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933:10,deploy,deploy-mode,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933,1,['deploy'],['deploy-mode']
Deployability,"Spark cluster nodes not released, even though spark calculations are complete",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2833:24,release,released,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2833,1,['release'],['released']
Deployability,"Spark local mode. In private Spark tools under development (which travers a WGS BAM and then performing several shuffles) I have seen speedups of up to 40% (~ 46 minutes -> 26 minutes). An initial test of `MarkDuplicatesSpark` using a 30GB bam file gave me a 9% speedup (logs are below). It might be good to investigate making this easier for users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:1109,install,install,1109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,2,['install'],['install']
Deployability,"SparkCommandLineArgumentCollection does not support ""="" in the values of spark configuration variables",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3687:79,configurat,configuration,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3687,1,['configurat'],['configuration']
Deployability,SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:19452,deploy,deploy,19452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['deploy'],['deploy']
Deployability,"SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:7662,deploy,deploy,7662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['deploy'],['deploy']
Deployability,Spawn of VS-1214 which required the ability to run with a wheel. Hopefully we never need to use this but now we would have the ability if we ever need it. Full integration run [in progress](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/6d67fda8-1237-4cd8-bf49-fe582ae7fc13). Runs requiring PMI ops access exercising this new wheel functionality with a Delta-age 0.2.98 wheel:; - [Delta](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/7215bdc8-f951-4b84-b9bf-3aaa80eae0a1); - [Delcho](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20RD/job_history/a336972e-d9f4-4a74-92fe-6ed94d2b5fff),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8692:160,integrat,integration,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8692,1,['integrat'],['integration']
Deployability,Split integration tests into two roughly equal targets,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2818:6,integrat,integration,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2818,1,['integrat'],['integration']
Deployability,"Split more travis integration tests into the ""variant calling"" job",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4990:18,integrat,integration,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990,1,['integrat'],['integration']
Deployability,Split travis integration tests into two jobs to reduce test runtime,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4983:13,integrat,integration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4983,1,['integrat'],['integration']
Deployability,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2026:250,patch,patches,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026,3,"['install', 'patch']","['install', 'patches']"
Deployability,SplitNCigarReadsIntegrationTest and SplitNCigarReadsUnitTest are bizzarely similar. something weird is going on. the 'integration test' is not really an intergration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1209:118,integrat,integration,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1209,1,['integrat'],['integration']
Deployability,Still got to test my Rc vs 923 add validation branch on the integration test now that it's fixed!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8592:60,integrat,integration,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8592,1,['integrat'],['integration']
Deployability,"Still not sure why the tests failed randomly! all XHMM-related tests use their own RNG with fixed seeds and there are no RNG calls in any parallel streams. Therefore, the randomly generated test data must be identical and fully deterministic across all runs. However, it did not appear to be the case! some test runs triggered a bug in HMMPostProcessor (see below) and some runs didn't. I removed a few unnecessary RNGs and the issue is not reproducible anymore. In particular, both XHMMModel and XHMMEmissionProbabilityCalculator had their own RNG but then again, if the tests are run in a deterministic order, it shouldn't matter. The good news is the bug in HMMPostProcessor is fixed; the bad news is, I still don't know why the tests were not deterministic. I bet the failing issue is (magically!) fixed as a result of pulling out the RNG from XHMMModel and XHMMEmissionProbabilityCalculator. If it occurs again, I'll investigate more. - fixed a bug in HMMPostProcessor that required all samples to be queried in the given list of genotyping segments every time (origin of the failing tests: sometimes the randomly generated genotyping segments contained fewer samples than all samples available for genotyping); - got rid of the unnecessary RNG in XHMMModel to make it stateless (sampling requires an external RNG); - also made XHMMEmissionProbabilityCalculator stateless (sampling requires an external RNG); - truncated the target list used in XHMM integration tests (cuts down the test time by a factor of 10)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3140:1455,integrat,integration,1455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3140,1,['integrat'],['integration']
Deployability,Successful Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d43ca844-632b-4737-962e-56369ac91e53),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8696:11,Integrat,Integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8696,1,['Integrat'],['Integration']
Deployability,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/ef747737-4d19-4770-83b7-47715eff8237). tl;dr the only commit really worth looking at is 9ac0befbcc39b9c5a7eb0938dd79a7d5cbd5f297, everything else is a simple merge from master. This is just minor tweaks around recent changes in the JointVariantCalling WDL. I'll need to merge and push this locally to preserve history from master as that option is not available within the GATK GitHub repo.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8537:11,integrat,integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8537,1,['integrat'],['integration']
Deployability,"Successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f7f0131f-96b8-424e-b022-9cb08fd4b39e). Only the ~9 newest commits are actually new, the rest comes from GATK master.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8505:11,integrat,integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8505,1,['integrat'],['integration']
Deployability,Successful integration run https://app.terra.bio/#workspaces/gvs-dev/mlc%20GVS%20Quickstart%20v3/job_history/c711a4cf-ac33-4c93-a4b7-b46b2796f090,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8044:11,integrat,integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8044,1,['integrat'],['integration']
Deployability,"Successful integration run; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/4caeb3e1-6c8f-4547-a334-b3264f2aed95. We initially changed the name of the method (since import_gvs is a bit misleading inside our repo) but because it looks like there are still external changes being made, we decided to keep the name consistent with Tim/Hail's chosen one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8330:11,integrat,integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8330,1,['integrat'],['integration']
Deployability,"Summary of changes:. - Fixed a minor issue in sampling error estimation that could lead to NaN (as a result of division by zero). - Introduced separate _internal_ and _external_ admixing rates. The _internal_ admixing rate is to be used internally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. Whi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:273,update,update,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,2,"['continuous', 'update']","['continuous', 'update']"
Deployability,Support fasta input in dataflow read pre-processing pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/460:52,pipeline,pipeline,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/460,1,['pipeline'],['pipeline']
Deployability,"Support for incrementally adding samples to existing genomicsdb workspaces. I've added these comments to the docs, but just wanted to call out again that we recommend making a backup of the existing workspace before trying to update the workspace. Otherwise, if the incremental update fails the workspace may be in a corrupted/inconsistent state. . If the user chooses not to backup (or can't), there is a (somewhat painful, manual) way to restore the workspace on failure IFF the --consolidate option has not been used. The tool will output a backup callset file (suffixed .inc.backup) and a file suffixed .fragmentlist with a list of all the original fragments. In order to roll back to a consistent workspace, the user must; - replace the callset file in the workspace with the one suffixed .inc.backup. That is, something like:; > mv _workspace_/callset.json.inc.backup _workspace_/callset.json; - delete all the directories in the workspace not named genomicsdb_meta_dir or included the file suffixed .fragmentlist",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970:226,update,update,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970,2,['update'],['update']
Deployability,Support grouping by key of input data for multi-input tools in the dataflow read pre-processing pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/462:96,pipeline,pipeline,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/462,1,['pipeline'],['pipeline']
Deployability,Support interval input in dataflow read pre-processing pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/461:55,pipeline,pipeline,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/461,1,['pipeline'],['pipeline']
Deployability,Switch from ExcessHet back to HWE for array pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6848:44,pipeline,pipeline,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6848,1,['pipeline'],['pipeline']
Deployability,Switch to the updated type & location inference tool in SV pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111:14,update,updated,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,Switch travis gcloud installation to use noninteractive mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6974:21,install,installation,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6974,1,['install'],['installation']
Deployability,Synchronize update of shared genotype likelihood tables.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071:12,update,update,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071,1,['update'],['update']
Deployability,TKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:57,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517:1941,deploy,deploy,1941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517,1,['deploy'],['deploy']
Deployability,TKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.Read,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2503:1219,deploy,deploy,1219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503,1,['deploy'],['deploy']
Deployability,TKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esoteri,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3303:4387,deploy,deploy,4387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303,1,['deploy'],['deploy']
Deployability,TKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2793:2044,deploy,deploy,2044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793,1,['deploy'],['deploy']
Deployability,TKSparkTool.runPipeline(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.refreshAccessToken(ComputeEngineCredentials.java:137); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAda,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591:3215,deploy,deploy,3215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591,1,['deploy'],['deploy']
Deployability,TKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(Assembly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:12456,deploy,deploy,12456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['deploy'],['deploy']
Deployability,TKSparkTool.runPipeline(GATKSparkTool.java:387); at; org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30; ); at; org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at; org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.jav; a:179); at; org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at; org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at; org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at; org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928); at; org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203); at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90); at; org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: scala.Product$class; at java.lang.ClassLoader.findClass(ClassLoader.java:523); at; org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35); at java.lang.ClassLoader.loadClass(ClassLoader.java:418); at; org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40); at; org.apache.spark.util.ChildFirstURLClassLoader.load,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6644:4851,deploy,deploy,4851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6644,1,['deploy'],['deploy']
Deployability,TKSparkTool.runPipeline(GATKSparkTool.java:461); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:19333,deploy,deploy,19333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['deploy'],['deploy']
Deployability,TKSparkTool.runPipeline(GATKSparkTool.java:533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.RuntimeException: Could not serialize lambda; 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:70); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 44 more; Caused by: java.lang.NoSuchMethodException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$94/1029586776.writeReplace(); 	at java.lang.Class.getDeclaredMethod(Class.java:2130); 	at com.twitter.chill.java.ClosureSerializer.write(Clo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6091:3992,deploy,deploy,3992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091,1,['deploy'],['deploy']
Deployability,"TKSparkTool.runPipeline(GATKSparkTool.java:534); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:7543,deploy,deploy,7543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['deploy'],['deploy']
Deployability,Tabix fails in UpdateVCFSequenceDictionary when outputting .vcf.gz file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5087:15,Update,UpdateVCFSequenceDictionary,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087,1,['Update'],['UpdateVCFSequenceDictionary']
Deployability,Takes in a WGS bam or cram and outputs VCF of SNP/Indel calls on the mitochondria. Note this pipeline does not perform any realignment and just uses read-pairs that map to chrM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7414:93,pipeline,pipeline,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7414,1,['pipeline'],['pipeline']
Deployability,Test run with `load_data_scatter_width` set: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/f10f47ab-8b5b-428a-b418-c9dc9f9c3a58; Test run with `load_data_scatter_width` not set: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/5bf5fe73-10d6-4df2-a5df-3f793c25ebde; integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/84f14232-dc62-4ce5-8031-7840f7f2aedc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8985:348,integrat,integration,348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8985,1,['integrat'],['integration']
Deployability,Test will fail until htsjdk is updated. Fixes https://github.com/broadinstitute/gatk/issues/6475.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7066:31,update,updated,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7066,1,['update'],['updated']
Deployability,"Tested using gatk-4.beta.6-151-g1ec409c-SNAPSHOT locally and with dataproc. Observed bug while testing commands for documentation updates in https://github.com/broadinstitute/gatk/pull/4068. . CollectInsertSizeMetricsSpark requires the `--histogramPlotFile` (`-H`, file to write insert size Histogram chart to) and current example commands add the `.pdf` extension to these files. The tool errors without this being specified but then doesn't write the file. In CollectBaseDistributionByCycleSpark, `--chart`(`-C`, A file (with .pdf extension) to write the chart to) is optional. When specified, the tool appears to ignore this option and does not write the file. . Metrics files defined by `-O` are written.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4085:130,update,updates,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4085,1,['update'],['updates']
Deployability,"Tests are ""failing"" with the ""code is too big"" error on the CNN testTrainingReadModel. I had to update my conda yml template to use a newer Tensorflow @cmnbroad found -- should I add that here too?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6330:96,update,update,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6330,1,['update'],['update']
Deployability,"Tests are not passing because I'm now using NIO in the WDL. I'll need to fix that, but the WDL itself should be ready for review. . The changes:; - Updates the pipeline for the new Mutect2 Filtering scheme and pulls filtering after the liftover and recombining of the VCF. ; - Makes the subsetting of the WGS bam fast by using PrintReads over just chrM instead of traversing the whole bam for NuMT mates.; - Moves polymorphic NuMTs based on autosomal coverage to a filter (it was an annotation before); - Adds option to hard filter by VAF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5847:148,Update,Updates,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5847,2,"['Update', 'pipeline']","['Updates', 'pipeline']"
Deployability,Tests that need to access data in a GCS bucket (but not run an actual pipeline); need a PipelineOptions object containing our API key. This new method makes; it for them.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/742:70,pipeline,pipeline,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/742,2,"['Pipeline', 'pipeline']","['PipelineOptions', 'pipeline']"
Deployability,Thank you everyone for your contributions towards this documentation effort. ; Instructions from @vdauwera ~~to follow~~ at [this Google doc](https://docs.google.com/a/broadinstitute.org/document/d/1r1AV4yWP4_vNmniUDR5LojihuggMDI2OnEpfRiYyvdk/edit?usp=sharing); Favorite tool doc examples from @vdauwera NOW in her SOP doc.; Spreadsheet from @sooheelee ~~to be~~ posted [here](https://docs.google.com/a/broadinstitute.org/spreadsheets/d/19SvP6DHyXewm8Cd47WsM3NUku_czP2rkh4L_6fd-Nac/edit?usp=sharing),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853:166,a/b,a/broadinstitute,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853,2,['a/b'],['a/broadinstitute']
Deployability,"The -contamination argument was not hooked up properly in the HaplotypeCaller.; This patch fixes the tool argument, and adds tests on artificially contaminated; data to demonstrate that the feature works as intended. Resolves #4312",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4455:85,patch,patch,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4455,1,['patch'],['patch']
Deployability,"The 1.7 release of the data sources has a lifted over hg19 version of Gencode. In this version contrary to other releases, the individual elements of each transcript seem to be represented in numerical order, rather than the order in which they appear in the transcript at transcription time. For `+` strand transcripts, this doesn't matter, but for `-` strand transcripts, the ordering of the exons/CDS regions in the gencode gtf file is reversed to what is expected. The result is that the coding sequence, protein prediction, and other annotations are incorrect. One user has already run into this issue: https://gatk.broadinstitute.org/hc/en-us/community/posts/360076207992--Repost-Wrong-annotation-with-Funcotator-1-7. One of two fixes is required:. 1. Update the code to always sort the transcript elements by how they appear in the transcribed sequence; 2. When generating the Gencode data, always sort the transcript elements by their transcribed order. We should do both and later roll back the sorting to optimize for speed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7051:8,release,release,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7051,3,"['Update', 'release']","['Update', 'release', 'releases']"
Deployability,"The BCFCodec in the next release of htsjdk (after 2.19) will reject any BCF input that is greater than BCF 2.1 (see https://github.com/broadinstitute/gatk/issues/5838 and https://github.com/samtools/htsjdk/issues/1323). However, GenomicsDB uses htslib, which generates version 2.2 output, to create BCF streams for GATK (with the BCF IDX fields removed). This will no longer work with post-2.19 htsjdk versions. Since GATK bypasses codec discovery and provides the codec directly for GenomicsDB inputs, the proposed solution is to change to the BCF codec in htsjdk to delegate version checking to an overridable method(!), and then provide a subclassed codec in GATK that has relaxed version checking.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5839:25,release,release,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5839,1,['release'],['release']
Deployability,"The BigQuery library upgrade for extract, broke ingest :/ This fixes that. It also rethrows an exception we were eating that I noticed. The error seen during ingest was. ```; java.lang.IllegalArgumentException: JSONObject does not have a bytes field at root.sample_id.; 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.fillField(JsonToProtoMessage.java:306); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessageImpl(JsonToProtoMessage.java:138); 	at com.google.cloud.bigquery.storage.v1beta2.JsonToProtoMessage.convertJsonToProtoMessage(JsonToProtoMessage.java:86); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:110); 	at com.google.cloud.bigquery.storage.v1beta2.JsonStreamWriter.append(JsonStreamWriter.java:90); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.writeLoadStatus(CreateVariantIngestFiles.java:202); 	at org.broadinstitute.hellbender.tools.gvs.ingest.CreateVariantIngestFiles.onTraversalSuccess(CreateVariantIngestFiles.java:369); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7620:21,upgrade,upgrade,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7620,1,['upgrade'],['upgrade']
Deployability,"The CNV WDLs don't use the standard gatk launcher script, which results in important system properties not being set. Assigning to @samuelklee and @LeeTL1220 to fix in time for the 4.0 release. Note that `gatk-launch` is being renamed to `gatk` in https://github.com/broadinstitute/gatk/pull/3961, so you'll want to wait until that PR is merged (should be merged today).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3968:185,release,release,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3968,1,['release'],['release']
Deployability,The Funcotator WDL needs to be integrated into the WDL for M2 and plugged into the automated testing.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4088:31,integrat,integrated,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4088,1,['integrat'],['integrated']
Deployability,"The GATK docker image is on samtools 1.7, which is ancient and has several known issues that users have run into (especially with CRAM files). We shoud update to a modern version.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8460:152,update,update,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8460,1,['update'],['update']
Deployability,"The GATK docker image uses samtools version 0.1.19 instead of the current version 1.9 and can therefore not read `gs://` resources. Samtools is installed in the gatkbase image via apt-get, the recent releases are not available there. Instead, it would have to be built manually (see http://www.htslib.org/download/).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6148:144,install,installed,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6148,2,"['install', 'release']","['installed', 'releases']"
Deployability,"The GCS ReadUtils tests are failing intermittently on the Barclay upgrade branch, probably due to filename collision when tests are running in parallel on Travis because the tests don't use unique temporary filenames.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3810:66,upgrade,upgrade,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3810,1,['upgrade'],['upgrade']
Deployability,"The Genome Analysis Toolkit (GATK) v4.5.0.0; ## Description; Hi,; Here is my situation, I'm testing the feasibility of incremental GenomicsDBI have total 400 samples to joint calling, I have no problem directly using `GenomicsDBImport `and `GenotypeGVCFs `for joint calling of all 400 samples. The configuration used is 4c32g for `GenomicsDBImport `and 2c16g for `GenotypeGVCFs`. But when I first built a GenomicsDB of 200 samples using `GenomicsDBImport `successfully, and then use GenomicsDB `--genomicsdb-update-workspace-path` increment 200 samples into the GenomicsDB , use this incremental imported GenomicsDB to `GenotypeGVCFs`. The error happend and report GENOMICSDB_TIMER,Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; Here are my code; ```; gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenomicsDBImport \; --tmp-dir $PWD \; --genomicsdb-workspace-path ~{workspace_dir_name}~{prefix}.~{index} \; --batch-size 50 \; -L ~{intervals} \; --reader-threads 5 \; --merge-input-intervals \; --consolidate \; -V ~{sep = "" -V "" single_sample_gvcfs}. gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenomicsDBImport \; --tmp-dir $PWD \; --genomicsdb-update-workspace-path ~{workspace_dir_name} \; --batch-size 50 \; --reader-threads 5 \; --merge-input-intervals \; --consolidate \; -V ~{sep = "" -V "" single_sample_gvcfs}. gatk --java-options ""-Xms8000m -Xmx~{max_mem}m"" \; GenotypeGVCFs \; --tmp-dir $PWD \; -R ~{ref} \; -O ~{workspace_dir_name}.vcf.gz \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; -V gendb://~{workspace_dir_name} \; -L ~{intervals} \; --merge-input-intervals \; -all-sites; ```; And I found that before report error the number of threads used by GATK increased, but the memory usage did not exceed the maximum limit of the server.; I also cheched `--max-alternate-alleles` and `--genomicsdb-max-alternate-alleles` to a smaller size but still the same error. I would appreciate some insights in why that is. Thanks,; Yang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8777:1298,update,update-workspace-path,1298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8777,1,['update'],['update-workspace-path']
Deployability,"The Intel-optimized version of TensorFlow 1.9 is now the default for Anaconda users. It now supports all processors with AVX - so everything since Sandy Bridge, which was released in 2011. With that in mind, I was thinking we could dispense with two different conda environments and fold everything into the ```gatk``` environment. @samuelklee , I'm the new guy on the Intel team you've been dealing with.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5142:171,release,released,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142,1,['release'],['released']
Deployability,"The M2 WDLs don't use the standard gatk launcher script, which results in important system properties not being set. Assigning to @davidbenjamin and @LeeTL1220 to fix in time for the 4.0 release. Note that `gatk-launch` is being renamed to `gatk` in https://github.com/broadinstitute/gatk/pull/3961, so you'll want to wait until that PR is merged (should be merged today).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3967:187,release,release,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3967,1,['release'],['release']
Deployability,The README suggests that. >You can use test.single when you just want to run a specific test class:; >`./gradlew test -Dtest.single=SomeSpecificTestClass`. But when I run `./gradlew test -Dtest.single=HaplotypeCallerIntegrationTest` or `./gradlew test -Dtest.single=org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest` gradle runs the entire integration test suite. Running `./gradlew test --tests *HaplotypeCallerIntegrationTest` does produce the desired result of running just `HaplotypeCallerIntegrationTest`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6853:381,integrat,integration,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6853,1,['integrat'],['integration']
Deployability,"The SAMRecord class currently allows the header to be set to null (either at construction time or via `setHeader()`), but may blow up or allow itself to enter an inconsistent state when it lacks a header (eg., the reference name and reference index can get out of sync). We should patch this class (and subclasses such as `BAMRecord`) in https://github.com/samtools/htsjdk/ to behave sensibly in all cases when a header is not present (eg., use a special missing value for reference index when the reference index cannot be looked up), and add unit tests to prove that headerless `SAMRecords` function correctly. This is important for dataflow and spark, where we want to serialize `SAMRecords` without paying the cost of serializing a header for each record.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/903:281,patch,patch,281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/903,1,['patch'],['patch']
Deployability,"The SVConcordance tool is currently too inefficient in terms of memory usage, requiring several 100's of GB of heap space on ~100K samples. This PR aims to reduce memory usage in two ways:. 1. Truth VCF records are stripped of all genotype fields except `GT` and `CN`, which are necessary and sufficient for concordance computations.; 2. A new option `--do-not-sort` is introduced to skip output record sorting. A major source of heap usage is the output buffer in the `ClosestSVFinder` class, which ensures records are emitted in coordinate-sorted order. This buffer quickly fills, however, when there is at least one record being actively clustered that spans a large interval because the buffer cannot be flushed until a variant beyond the maximal clusterable coordinate of that large variant is encountered. This option will allow users to substantially reduce max heap usage on larger call sets (a single SVRecord can consume ~100MB with 100K samples). Includes an integration test to cover the `--do-not-sort` functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8623:970,integrat,integration,970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8623,1,['integrat'],['integration']
Deployability,The SplitReads integration tests will fail once we upgrade htsjdk without this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1241:15,integrat,integration,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1241,2,"['integrat', 'upgrade']","['integration', 'upgrade']"
Deployability,"The VAT pipeline creates 3 tables, 2 are intermediary tables used to create the third. I was tired of deleting them, so I made them temp tables. <img width=""565"" alt=""Screen Shot 2021-09-16 at 10 44 09 AM"" src=""https://user-images.githubusercontent.com/6863459/133633262-a41466a9-2cae-4b69-8c61-28e81d1a8706.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7455:8,pipeline,pipeline,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7455,1,['pipeline'],['pipeline']
Deployability,"The `PS` tag should be type `Integer`, not `String` according to the spec, but no error is reported (for me). ```bash; bcftools view https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz chr1:4001310-4001310 > test.vcf; ```. Related: https://github.com/genome-in-a-bottle/giab_latest_release/issues/15",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6762:177,release,release,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6762,1,['release'],['release']
Deployability,"The `gatktool` Python code installs a system exception handler to catch unhandled Python exceptions, and sends a negative ack to the `StreamingProcessController` when it sees one. The controller then grabs the stdout/stderr contents; writes it to the log/journal file; and throws a java exception. However, there is a shutdown race condition where occasionally the GATK process will get the negative ack and terminate before the Python exception chain is finished processing, and the exception message never appears in the journal. We've seen this happen when the CNN Python inference code runs out of memory. It would be better to have the exception handler write the exception string directly to the ack FIFO, with a message length included, so the controller can deterministically retrieve the message for inclusion in the java exception without having to rely on std in/out.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5100:27,install,installs,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5100,1,['install'],['installs']
Deployability,The acceptance criteria are to replicate the gatk3 functionality and tests. depends on #293 . there's code for some of it at googlegenomics/genomics-pipeline. @jean-philippe-martin can you describe the status of that code?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/424:149,pipeline,pipeline,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/424,1,['pipeline'],['pipeline']
Deployability,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3068:308,integrat,integration,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068,2,['integrat'],['integration']
Deployability,"The build is failing since 1.21.0-SNAPSHOT is no longer available in any Maven repositories. It looks like 1.21.0 was released last week: https://repo1.maven.org/maven2/com/google/http-client/google-http-client/, and changing the build to use that version seems to fix the problem. Related to #650.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1185:118,release,released,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1185,1,['release'],['released']
Deployability,The changes in #5112 introduced several problems that have prevented us from performing a maven release of 4.0.9.0. I believe that the two issues are:. 1. The sources jar is misspelled as `source` jar; 2. The generated pom files are missing project level information. . We should:. - [x] Manually fix the files and peform a release of 4.0.9.0.; - [ ] Patch the build.gradle so that it's correct for future release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5212:96,release,release,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212,4,"['Patch', 'release']","['Patch', 'release']"
Deployability,"The cloud tests are timing out after 10 minutes without emitting any output. It seems like `ApplyBQSRDataflowIntegrationTest.testPR_Cloud` is responsible. It looks like something is crashing in dataflow but the runner is never stopped so it keeps waiting indefinitely (or at least 10 minutes..) See the dataflow log [here](https://console.developers.google.com/project/broad-dsde-dev/dataflow/job/2015-07-24_12_44_26-17415749601435236766). . Executing locally also seems to hang forever, with messages like . ```; Error: (b65a2091061bf0f9): Workflow failed. Causes: (71540087aac21e37): Unable to create VMs. Causes: (71540087aac21994): Error:; Test: Test method testPR_Cloud[0](ApplyBQSR(args=''))(org.broadinstitute.hellbender.tools.walkers.bqsr.ApplyBQSRDataflowIntegrationTest) produced standard out/err: Message: Value for field 'resource.metadata.items[1].value' is too large; ```. Seems like this is possibly a dataflow bug. If the workflow fails in some way the client should be released.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/750:986,release,released,986,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/750,1,['release'],['released']
Deployability,The current SV pipeline is producing invalid VCF files that are missing a header line for these attributes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3224:15,pipeline,pipeline,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3224,1,['pipeline'],['pipeline']
Deployability,"The current docker build script runs `gradle installAll` in addition to running `localJar`. This causes the `gatk` script in our docker image to prefer running with the unpackaged set of jars, instead of the fully packaged jar. This, in turn, can cause us to run out of file handles in certain tools, since we need to open all of the jars for our dependencies individually at once. We should just run something like `gradle clean localJar sparkJar createPythonPackageArchive` in our `Dockerfile`, and avoid `installAll`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4409:45,install,installAll,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409,2,['install'],['installAll']
Deployability,"The current documentation states that the type of the intervals parameter in the GenomicsDBImport ; tool is `List[String]` whereas this tool can only take one interval (as explained in the Caveats section) at https://software.broadinstitute.org/gatk/documentation/tooldocs/4.0.0.0/org_broadinstitute_hellbender_tools_genomicsdb_GenomicsDBImport.php#--intervals. The type should be updated to be just a `string` and maybe a note should be included in the actual argument documentation to say that, in opposition to other tools, only one interval can be provided.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4196:381,update,updated,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4196,1,['update'],['updated']
Deployability,"The current fatJar gradle task does not properly merge resource files, causing an error when you try to run a Spark tool from the resulting jar. This PR replaces the fatJar task by configuring our shadowJar task to properly merge resource files. I've attempted to share configuration with the sparkJar task, which is also of type ShadowJar. Discussed briefly with @lbergelson.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1213:270,configurat,configuration,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213,1,['configurat'],['configuration']
Deployability,"The current implementation of VETS borrowed VQSR's logic for classifying variants into SNPs and indels, for which separate models are trained. We retained this logic to make comparisons with VQSR as straightforward as possible. In this logic, which originates from htsjdk, an alternate allele with len(REF) = len(ALT) is counted as a MNP and classified as a SNP. However, we are now applying VETS to long-read genotyping of SVs, where inversions satisfy the same criterion but are then awkwardly classified as SNPs. We should add a toggle to allow classification of MNPs as indels, probably retaining the old behavior as default to not cause any changes for e.g. GVS. Tagging @koncheto-broad and @fabio-cunial for their visibility.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8733:532,toggle,toggle,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8733,1,['toggle'],['toggle']
Deployability,"The current initialization action for dataproc workers puts the reference image in different places depending on whether or not an SSD is mounted. Preemptible dataproc workers don't have SSDs, so a mixed cluster will have references mounted on different paths depending on the worker. This change symlinks the SSD mount point onto the HDD so that paths can be consistent. . Also increases several cluster configuration parameters relating to retries, which I saw recommended if using preemptible workers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4493:405,configurat,configuration,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4493,1,['configurat'],['configuration']
Deployability,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1732:95,integrat,integration,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732,2,['integrat'],['integration']
Deployability,"The current test report suggests that if we split off the tests in the `exome` package into a separate target, we would be close to a balanced 2-way split. Possibly the right way to do this is to have two new values for `TEST_TYPE` in `build.gradle`: `cnvIntegration` and `nonCNVIntegration`. Specifying just `integration` would run all the integration tests, as before. Specifying `cnvIntegration` would run everything in the `exome`, `copynumber`, and `coveragemodel` packages. Specifying `nonCNVIntegration` would run everything outside of those packages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2818:310,integrat,integration,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2818,2,['integrat'],['integration']
Deployability,"The custom R library installation can be moved into the docker base image (gatkbase), if desired",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2699:21,install,installation,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2699,1,['install'],['installation']
Deployability,The documentation for `LeftAlignAndTrimVariants` indicates that it only works for indels. It should be updated to work for MNPs as well. . This operation would simply remove any common leading bases from all alleles of a `VariantContext` and update the start position by however many bases were removed. It would be implemented in `LeftAlignAndTrimVariants.java:289` replacing the noop for non-indels.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7516:103,update,updated,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7516,2,['update'],"['update', 'updated']"
Deployability,The earlier GKL release did not have all updates merged. Hence submitting another request to include the latest GKL.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4379:16,release,release,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379,2,"['release', 'update']","['release', 'updates']"
Deployability,"The empty alts that results from overtrimming in an alt allele UDF correlate strongly with VAT / alt allele mismatches, but from trial runs with and without this patch they don't seem to actually be causing the mismatches. Nonetheless I think this is a desirable change as it keeps VAT VIDs from getting weird. VIDs have the format `<contig>-<position>-<ref>-<alt>`; if the alt is empty the VID just ends in a dash which just seems like an edge case waiting to break something.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8326:162,patch,patch,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8326,1,['patch'],['patch']
Deployability,"The existing pipelines and multiple collectors are not consistent in how the user's filter requests are merged and propagated to different stages of the pipeline. The BQSR pipeline for instance first retrieves the initial RDD via getReads, which honor's the user's filter requests, but subsequently manually applies the BQSR-specific filter, which might re-enable a filter that was disabled by the user. We should have a more explicit (and user-transparent) set of rules for propagating/applying filters in the cases where we're composing multiple tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2150:13,pipeline,pipelines,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2150,3,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"The first commit has the raw GATK3 files, the second has the ported files. In order to minimize the diffs from GATK3 for the initial port, there are only very minimal style changes. Integration tests will follow in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2094:182,Integrat,Integration,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2094,1,['Integrat'],['Integration']
Deployability,The first step of taking this code from the Hail team is just pasting it into our repo. Successful run:; https://job-manager.dsde-prod.broadinstitute.org/jobs/49d62f48-2dee-417c-aa65-411cbe47be17. GvsQuickstartHailIntegration--we remove the whl from the integration test---sure seems like we wont need one going forward!. Another ticket will be made for these next steps:; Likely this will need to end up in our docker image and the WDL that creates the Avro files can make a version of the input for this scripts instead; Next we will want to remove the Tranches calculations and instead of that value passed in as yet another parameter; Phasing and dropping GQ0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8282:254,integrat,integration,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8282,1,['integrat'],['integration']
Deployability,"The following command generates an error. Other spark programs work when specifying hdfs://scc/user/farrell/adsp/bams/SRR990385.bam as the input. It seems to be having a problem with testing for the presence of the SRR990385.bai file which is present. I tried running the command with hdfs://scc:**8020**/user/farrell/adsp/bams/SRR990385.bam and that works. . `/share/pkg/gatk/4.beta.5/install/bin/gatk-launch SparkGenomeReadCounts -I hdfs://scc/user/farrell/adsp/bams/SRR990385.bam -o SRR990385.ReadCounts -R /restricted/projectnb/genpro/bundle/2.8/b37/human_g1k_v37.fasta --verbosity ERROR -- --sparkRunner SPARK --sparkMaster yarn --num-executors 1 --executor-memory 4G --executor-cores 3`. [December 3, 2017 2:56:35 PM EST] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts done. Elapsed time: 0.57 minutes.; Runtime.totalMemory()=982515712; org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 0.0 failed 4 times, most recent failure: Lost task 12.3 in stage 0.0 (TID 14, scc-q09.scc.bu.edu, executor 1): java.lang.IllegalArgumentException: **Wrong FS: hdfs://scc:8020/user/farrell/adsp/bams/SRR990385.bai, expected: hdfs://scc**; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302); at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766); at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3909:386,install,install,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3909,1,['install'],['install']
Deployability,"The following idiom occurs about 25 times in this repo, mainly in integration tests:; ```; StreamSupport.stream(new FeatureDataSource<VariantContext>(vcf).spliterator(), false). . .; ```; We should extract a method, perhaps `Utils.streamVcf(final File vcf)`, to replace this unwieldy construct.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5006:66,integrat,integration,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5006,1,['integrat'],['integration']
Deployability,The following tools are updated in this PR:; CountBases; CountBasesSpark; CountReads; CountReadsSpark; CheckPileup; EstimateLibraryComplexityGATK; FlagStat; FlagStatSpark; GetSampleName; SplitReads; AnalyzeCovariates. Could you please review @sooheelee? Thank you!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4003:24,update,updated,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4003,1,['update'],['updated']
Deployability,The generated online doc currently doesn't include the names of the default annotations or annotation groups used by various tools. The values are already propagated from the annotation plugin to the freemarker map by Barclay; it should be easy to update the freemarker template to display these similar to the way we display default read filters.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5577:248,update,update,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5577,1,['update'],['update']
Deployability,"The ggplot2 R dependency was not installed correctly in the gatkbase-2.0.0 Docker image. It appears that this resulted from a recent ggplot2 update that has broken dependencies (perhaps for the version of R that we use). This missing ggplot2 dependency was the root cause of #5022. I updated the install_R_packages.R script, which should now fail if any package fails to install, and pushed an updated gatkbase-2.0.1 image. The second commit addresses #5022. This should be considered a temporary fix until #5026 is in. Closes #5022.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040:33,install,installed,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040,5,"['install', 'update']","['install', 'installed', 'update', 'updated']"
Deployability,The goal of this PR is to adjust the ingest in two ways:; 1. To update the ingest to loop through all samples (not just the first 10k); 2. To update the ingest to be far more efficient in a few ways:; - To remove the files that are downloaded to each vm so that they do not carry around the extra weight; - To check that the samples in the fofns have not been ingested already so that additional work doesn't need to be done toward processing those samples. There is still work to do around making the bulk ingest process significantly more user-friendly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8197:64,update,update,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8197,2,['update'],['update']
Deployability,"The google-cloud-java maintainers have merged a fix for the longstanding issue; https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453 that prevented us; from running on a modern version of the library, and forced us to run off of a fork.; This PR updates us to the latest release, which incorporates the fix. Resolves #3591; Resolves #3500; Resolves #4986",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5135:262,update,updates,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5135,2,"['release', 'update']","['release', 'updates']"
Deployability,"The htsget.ga4gh.org appears to be down (tests get 404s, ping fails). This output is from my PR https://github.com/broadinstitute/gatk/pull/6799 that prints out the target URI:. ```; org.broadinstitute.hellbender.exceptions.UserException: Invalid request https://htsget.ga4gh.org/reads/A1-B000168-3_57_F-1-1_R2.mus.Aligned.out.sorted.bam, received error code: 404, error type: NotFound, message: The requested resource could not be associated with a registered data source; at org.broadinstitute.hellbender.tools.HtsgetReader.doWork(HtsgetReader.java:266); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:146); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:187); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:111); at org.broadinstitute.hellbender.tools.HtsgetReaderIntegrationTest.testSuccessfulParameters(HtsgetReaderIntegrationTest.java:85); ```; Jermey (GA4GH dev) says:. > I recently updated the server, but my understanding was that the gatk build was spinning up a local server from an older image; > 11:41; > so htsget.ga4gh.org is using a newer image, while the gatk tests should pull an older image, spin it up locally, and then request from http://localhost. But based on the output above, it looks like we actually target `https://htsget.ga4gh.org/read...`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6803:1481,update,updated,1481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6803,1,['update'],['updated']
Deployability,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3235:162,install,installing,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235,2,"['install', 'integrat']","['installing', 'integration']"
Deployability,"The idea is that this WDL will run all the checks for each release of the VAT table, one call for each validation. The first validation rule (""Validation Check confirms that data is put into the VAT table after completing without an error."") is included as a model for subsequent calls. - workflow should succeed if it's able to try all tests; - workflow output `validation_results` will contain details of each test result in an array of `{""testName"": ""result details""}`:; Example 1  [fail](https://job-manager.dsde-prod.broadinstitute.org/jobs/2728b55b-5344-492a-951a-48fd416e9d0d); `{ ""EnsureVatTableHasVariants"": ""FAIL: The VAT table spec-ops-aou.rsa_gvs_quickstart.rsa_scratch has no variants in it."" }`; Example 2  [pass](https://job-manager.dsde-prod.broadinstitute.org/jobs/83e3bd5a-9144-452e-93d9-9f273055177f); `{ ""EnsureVatTableHasVariants"": ""PASS: The VAT table spec-ops-aou.anvil_100_for_testing.aou_shard_223_vat has 294821 variants in it."" },`; Example 3  [the test wasn't able to run](https://job-manager.dsde-prod.broadinstitute.org/jobs/7179d111-02aa-4bca-a0a0-f55e10e43791); `{ ""EnsureVatTableHasVariants"": ""Something went wrong. The attempt to count the variants returned: Error in query string: Error processing job 'spec-ops- aou:bqjob_r357c4b6fe6b0c6fb_0000017aac301de7_1': Unrecognized name: vid at [1:24]"" }`. Closes https://github.com/broadinstitute/dsp-spec-ops/issues/364",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7352:59,release,release,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7352,1,['release'],['release']
Deployability,"The latest Picard release introduces a dependency on the Google Cloud NIO library that conflicts with GATK's dependency. We are going to have to blacklist the Picard NIO dependency for now. . Longer term, we might want to consider having both projects depend upon a build of htsjdk that comes with the NIO plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4556:18,release,release,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556,1,['release'],['release']
Deployability,"The latest code in htsjdk, which includes https://github.com/samtools/htsjdk/pull/1454 (changes the Allele class into an interface, and uses SimpleAllele as the concrete implementation) causes the `VariantAnnotatorEngineUnitTest.testCombineAnnotations` test to fail because the order of the list returned by `ReducibleAnnotationData.getAlleles` is different with that change than it is without it (presumably due to the different hashCode/equals implementations). `AS_RMSMappingQuality.parseRawData` seems to assume that the order of the Alleles in the list returned by ; `ReducibleAnnotationData.getAlleles` exactly matches the order of the raw data in the String returned by `ReducibleAnnotationData.getRawData`, since it uses indexed access to the list, but I don't see anything that states or ensures/enforces this. Changing the Map maintained by `ReducibleAnnotationData` into a LinkedHashMap fixes the issue for this test, but that just changes the order to be input order - the real issue is that the contract around how the order of the list and the order of the raw data is maintained isn't clear. This will need to be addressed before we can upgrade to the next release of htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7586:1152,upgrade,upgrade,1152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7586,2,"['release', 'upgrade']","['release', 'upgrade']"
Deployability,"The latest version of the funcotator data sources dates to mid-2020. This is almost three years now without any update of the following sources:; *gencode (v34, now 43); *dbsnp; *COSMIC; *clinvar. Therefore, a new bundle should be provided to stay up-to-date again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8296:112,update,update,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8296,1,['update'],['update']
Deployability,"The main purpose of this PR was to output the new estimated bytes read from the Read API for better monitoring and debugging. However in the course of that I discovered that we were using ancient versions of the google APIs. No massive improvements from the release logs, but a lot of nice features (cleaner logging, automatic retries for certain errors, ). bigQuery 1.131.1 -> 2.3.3 [Release log for bigQuery](https://github.com/googleapis/java-bigquery/blob/main/CHANGELOG.md). bigQueryStorage 1.22.3 -> 2.5.0 [Release log for bigQueryStorage](https://github.com/googleapis/java-bigquerystorage/blob/main/CHANGELOG.md)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7601:258,release,release,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601,3,"['Release', 'release']","['Release', 'release']"
Deployability,The new index creation tool `IndexFeatureFile` needs integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/235:53,integrat,integration,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/235,1,['integrat'],['integration']
Deployability,The newest release of GenomicsDB treats spanning deletions (spanning; from earlier positions) as deletions in the min PL value computation.; This behavior now matches the behavior of CombineGVCFs. A more detailed description of the issue is provided in; https://github.com/broadinstitute/gatk/pull/4963. * Deleted a couple of files which are no longer necessary.; * Fixed the index of newMQcalc.combined.g.vcf; * Fixes #5045 (error out with a helpful error message); * Fixes #5300,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5397:11,release,release,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5397,1,['release'],['release']
Deployability,The only tool that is update in this PR is UnmarkDuplicates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4019:22,update,update,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4019,1,['update'],['update']
Deployability,"The output file (`-O`/`--output`) is optional, but if not set you get a NPE:. ```; java.lang.NullPointerException; at java.io.File.<init>(File.java:277); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.runTool(FlagStatSpark.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); ```. This is the command I ran:. ```; ./gatk-launch FlagStatSpark \; --input hdfs:///user/$USER/bam/CEUTrio.HiSeq.WGS.b37.ch1.1m-65m.NA12878.bam \; -bps 134217728 \; -- \; --sparkRunner SUBMIT --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1282:199,pipeline,pipelines,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1282,6,"['deploy', 'pipeline']","['deploy', 'pipelines']"
Deployability,"The package org.broadinstitute.hellbender.utils.commandline contains annotation classes called AdvancedOption and HiddenOption that are duplicates of Advanced and Hidden in org.broadinstitute.hellbender.commandline. All usages of these should be updated and this entire package should be removed. Also, it looks like Hidden is not integrated with the command line parser.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2130:246,update,updated,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2130,2,"['integrat', 'update']","['integrated', 'updated']"
Deployability,"The packages for codecs is a key feature for downstream tools implementing new codecs for other formats or to include overrides of codecs already included. Nevertheless, the current implementation (at version 4.0.0.0) the only way of configuring this is at the package level using the `codec_packages` configuration. I request support for the following fine-grained configuration:. * Add/Remove concrete codec classes; * Exclude single classes from a concrete `codec_package` specified (this can be done by the previous requirement if it uses fully qualified codec names); * Exclude sub-packages from a concrete `codec_package` specified. Representing this in an YML format, I would like to have the ability to configure the codecs as following:. ```yml; - codecs:; - packages:; - htsjdk.variant; - htsjdk.tribble; - exclude_class: bed.BEDCodec; - org.broadinstitute.hellbender.utils.codecs; - exclude_package: gencode; - org.magicdgs.htsjdk.codecs; - classes:; - org.external.htsjdk_codecs.CustomBedCodec; ```. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:302,configurat,configuration,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,2,['configurat'],['configuration']
Deployability,"The palindrome artifact read filter wasn't checking for an edge case. Now it is. @takutosato Could you review? And if you approve and it's Monday or Tuesday, could you merge before the release?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5241:185,release,release,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5241,1,['release'],['release']
Deployability,"The product sheet had old scale numbers and VQSR, updated to VETS and new scale numbers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8929:50,update,updated,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8929,1,['update'],['updated']
Deployability,"The recent branch #8489 has demonstrated that there are some problematic edge cases in the pileup allele merging code that could cause pathological numbers of haplotypes to be handed to the genotyper. In updating the bug in that branch it was observed that it is very common that there are score ties at the 5th haplotype level for the pileupcaller as illustrated by the noise in the updated tests. This algorithm is not a good heuristic and we should replace it with something better, some ideas from that branch that might fix a few of its shortcomings:. 1) Increase/decouple the kmer size used with the reads from the assembly graph kmer size to prevent the filtering step from being redundant with assembly; 2) Normalize the scores to the haplotype lengths to deal with haplotype size bias.; 3) Change the scores to instead reflect the absolute count of unsupported kmers from the graph to also deal with hapotype size bias. ; 4) Iteratively expand the kmer size used for filtering to pare down the number of haplotypes in a more principled fashion.; 5) Utilize the read kmer occurrence counts to construct the scores in order to reduce the risk of spurious reads being sufficient support for a given haplotype. . We have observed that there can be significant changes to the actual genotyping engine output from the pileup engine from even relatively minor changes to the pileupcalling merging code. We should strive to find a more principled solution for merging haplotypes than the one we have currently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8494:384,update,updated,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8494,1,['update'],['updated']
Deployability,The recently-added sequence dictionary validation in `BaseRecalibratorDataflow` does not work when the reference is stored in a bucket -- we should patch it so that it does.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/683:148,patch,patch,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/683,1,['patch'],['patch']
Deployability,The rest of the mitochondria joint calling pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5673:43,pipeline,pipeline,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5673,1,['pipeline'],['pipeline']
Deployability,The return value of picard tools goes to standard output which makes it hard to pipeline these tools nicely.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5790:80,pipeline,pipeline,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5790,1,['pipeline'],['pipeline']
Deployability,The sample name map file accepted by GenomicsDBImport can now optionally contain a third; column giving an explicit path to an index for the corresponding GVCF. It is allowed to; specify an explicit index in some lines of the sample name map and not others. Added comprehensive unit and integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7967:287,integrat,integration,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7967,1,['integrat'],['integration']
Deployability,"The script is no longer located at scripts/install_R_packages.R, so we should update the readme accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3601:78,update,update,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3601,1,['update'],['update']
Deployability,The standard integration/unit tests upload their test report -- the docker tests should as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2817:13,integrat,integration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2817,1,['integrat'],['integration']
Deployability,"The tab completion integration test wasn't actually emitting any output because the classpath contained a list of class names (basenames only, without the "".class"" extension), so no work units were ever created. This PR:. - changes the classpath to use package names that contain CLPs instead of class names; - runs the javadoc in the current JVM (which makes debugging the test so much easier...); - adds an Assert to ensure the javadoc process succeeds. I made the latter change to the doc gen smoke test as well, to make debugging easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6647:19,integrat,integration,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6647,1,['integrat'],['integration']
Deployability,"The test Gencode data sources did not include any genes in flanking distance to the test variants. This update creates a new Gencode data source that includes such regions. Additionally, the Gencode excision script was updated to include a flanking range defaulting to the funcotator 5' flanking defualt (5000 bases). . Fixes #5419",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5423:104,update,update,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5423,2,['update'],"['update', 'updated']"
Deployability,The type of the method changed at some point and the references were never updated.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6478:75,update,updated,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6478,1,['update'],['updated']
Deployability,"The updated jBWA code allows changing scoring parameters for `bwa mem`. ; Specifically, the `BwaMem` class now has a method `updateScoringParameters()`. This ticket is to remind the SV group to update, accordingly, the class `AlignContigsAndCallBreakpointsSpark` and `ContigAligner.java`. Closes #1942 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2001:4,update,updated,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2001,3,['update'],"['update', 'updateScoringParameters', 'updated']"
Deployability,"The version of `conda` we use on the docker uses environment activation commands that are deprecated in newer versions of conda. Update to the latest published version of miniconda, and update the doc to specify the newer commands. Also change the local environment to use `conda env create -f` rather than `conda env update`. Fixes https://github.com/broadinstitute/gatk/issues/5851 and https://github.com/broadinstitute/gatk/issues/5776.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5866:129,Update,Update,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5866,3,"['Update', 'update']","['Update', 'update']"
Deployability,"The version of dbSNP at the research bundle is 138 for hg19 (updated 8.12.2013), but the latest dbSNP version is 150: [https://www.ncbi.nlm.nih.gov/projects/SNP/snp_summary.cgi?view+summary=view+summary&build_id=150](url) . Is it going to affect my results much if I use 138 instead of 150?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3870:61,update,updated,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3870,1,['update'],['updated']
Deployability,"The warp pipeline tests caught some cases that we apparently didn't have in our integration tests, but now we do!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7670:9,pipeline,pipeline,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7670,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"The workflow for mitochondria will include running `AddOriginalAlignmentTags` on a bam, realigning it to the mitochondria contig only, then running `Mutect2` with `--annotation OriginalAlignment` and `--median-autosomal-coverage` to get the appropriate annotations. Then running `FilterMitochondrialMutectCalls`. . I don't think any of these changes should effect the somatic pipeline. @ldgauthier I didn't change the name of `TLOD` or `tumor sample` in the mitochondria vcf. Maybe we can talk next week about how to best do that?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193:376,pipeline,pipeline,376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193,1,['pipeline'],['pipeline']
Deployability,Theano version should be 1.0.4. Doesn't have any effect unless trying to install the package manually with pip.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6227:73,install,install,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6227,1,['install'],['install']
Deployability,Then gatk-protected and gatk docker images can specify this one in ``USING``. See gatk-protected scripts/docker for both a deployment script and a Dockerfile that (subset) can be used for a base image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2457:123,deploy,deployment,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2457,1,['deploy'],['deployment']
Deployability,"There appears to be a memory leak in gCNV coming from Theano 0.9.0, possibly fixed in https://github.com/Theano/Theano/pull/5832. A few possible fixes:. 1) Update Theano to the latest 1.0.4 version. I've tried this and it looks like the leak goes away. Need to confirm reproducibility of results between versions, see also #5730.; 2) Configure Theano 0.9.0 to use MKL, rather than OpenBLAS. It appears the leak is only an issue with the latter. This is a little more complicated, since I now realize that MKL is not actually fully utilized (if at all) in our conda environment. For example, we `pip install numpy`, rather than `conda install` a version from the `default` channel that is compiled against MKL. So we'd need to change a few dependencies in the environment which might have implications for VQSR-CNN. See also #4074. @lucidtronix any thoughts? @jamesemery and @cmnbroad might also be interested, as this could have pretty drastic implications for the size of the python dependencies---if we go with option 1, we might be able to get rid of MKL, etc. Not sure if the memory leak manifests the same across all architectures. Note that I believe this is a separate issue from #5714.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5764:156,Update,Update,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764,3,"['Update', 'install']","['Update', 'install']"
Deployability,"There are 4 separate commits:; - Upgrade to Barclay 2.0.0 and Picard 2.17.2.; - Changes for CommandLinePluginDescriptor updates (required for the Barclay upgrade); - Updates for Experimental tag (dependent on Barclay upgrade).; - Remove placeholder and obsolete program groups - Part 1 (dependent on Picard upgrade).There are still 3 obsolete program groups (ReadProgramGroup, VariantProgramGroup, and SparkProgramGroup) who's tools need to be redistributed to the new program groups. But thats can be a separate PR since it will be a lot of files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4070:33,Upgrade,Upgrade,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4070,6,"['Update', 'Upgrade', 'update', 'upgrade']","['Updates', 'Upgrade', 'updates', 'upgrade']"
Deployability,"There are a few problems with IntegrationTestSpec that surface when adding CRAM tests to the Reads/BQSRSparkPipelines:; - generated output filenames contain no sam extension, so outputs always are treated as .bam; - it doesn't have explicit knowledge of the reference file, which is needed to do proper file comparisons through the SamAssertionUtils assertSamsEqual methods; - there is code that assumes that any expectedFile that doesn't end in "".bam"" should use text comparison. We could fix these, but I'm not sure what the incremental value-added of this class is when we can just use TestNG for expected exceptions, etc.; it might make more sense to just eliminate this style of test completely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1562:30,Integrat,IntegrationTestSpec,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1562,1,['Integrat'],['IntegrationTestSpec']
Deployability,There are a lot of sites where a `ReferenceSequenceFile` is created only to load a sequence dictionary from it. These should be replaced with calls to `ReferenceUtils.loadFastaDictionary()`. There may be updates that can be made to `ReferenceUtils` that make use of new methods in `ReferenceSequenceFileFactory`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5180:204,update,updates,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5180,1,['update'],['updates']
Deployability,"There are a number of GATK code paths that check to ensure that a reference is provided whenever a CRAM input is provided. Since htsjdk now accepts both embedded reference and reference-less (no reference compression) CRAMs, these checks should be removed once we update htsjdk. The CRAM code will defer accessing the reference until one is actually required, and will fail gracefully in the case where it is not provided.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6541:264,update,update,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6541,1,['update'],['update']
Deployability,"There are no Java code changes in this PR. Tests were done manually. As a reminder, the modified files are still considered experimental. Changes:; - combine_tracks.wdl: Fixes bug where string was compared to a float. Closes #5284 ; - combine_tracks.wdl: Converts the processed seg file into a format for GISTIC2. This is a trivial conversion. Closes #5283 ; - Other changes in `aggregate_combine_tracks.wdl` to support the above, including aggregation of individual GISTIC2 seg files into a single GISTIC2 seg file.; - Added gs urls for necessary auxiliary files in the documentation.; - Added multiple output types for the ABSOLUTE skew parameter to support heterogeneous execution configurations. File, Float, and String. All are the same value.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5287:684,configurat,configurations,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5287,1,['configurat'],['configurations']
Deployability,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6018:928,pipeline,pipeline,928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018,2,['pipeline'],['pipeline']
Deployability,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5862:238,update,updated,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862,1,['update'],['updated']
Deployability,"There are too many WDL conventions that work in cromwell 29 and not in 30. Therefore, time to upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4418:94,upgrade,upgrade,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4418,1,['upgrade'],['upgrade']
Deployability,"There are two commits. The first one factores out code that can be shared between the R and Python executors, along with a few opportunistic changes in existing tests that have bad names. The second has a simple PythonScriptExecutor in the spirit of the RScriptExecutor, along with unit tests, and an example tool and integration test. First pass for https://github.com/broadinstitute/gatk/issues/3501.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3536:318,integrat,integration,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3536,1,['integrat'],['integration']
Deployability,There is a new vulnerability released https://nvd.nist.gov/vuln/detail/CVE-2022-42889#vulnCurrentDescriptionTitle and exploitable as documented in the NIST DB. . A quick scan of the GATK jar shows it ships Apache Commons 1.6 (class location in the jar: /org/apache/commons/text/lookup/). Could you please confirm if the GATK is impacted by this issue and a plan to fix this? . Thanks,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8060:29,release,released,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8060,1,['release'],['released']
Deployability,"There is an issue in with gcloud 208.0.0 that prevents the `gcloud dataproc` commands from functioning. We should temporarily pin our version to 207.0.0 instead until the problem can be addressed by google. . ```; Customers affected by this issue are using gcloud version 208.0.0 and may experience an error like ""Problem loading gcloud.dataproc.clusters.create: No module named jsonschema."" when interacting with Google Cloud Dataproc.; Workaround; The workaround is to use gcloud version 207.0.0, a downgrade from 208.0.0 can be done by issuing the command: ""gcloud components update --version 207.0.0"". If installed via apt: sudo apt-get update && sudo apt-get install google-cloud-sdk=207.0.0-0. If installed via yum: sudo yum downgrade google-cloud-sdk-207.0.0; ```. We have to remember to unpin it afterward the problem is fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5003:579,update,update,579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003,5,"['install', 'update']","['install', 'installed', 'update']"
Deployability,"There is now no deprecated code in hellbender. This is probably a good way to start a new project. Closes #162 . I had to update `MergeBamAlignmentIntegrationTest` to remove references to a deprecated parameter, so I also fixed most of intellij's style complaints on it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/228:122,update,update,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/228,1,['update'],['update']
Deployability,There needs to be a validation tool for data sources to ensure that they conform to their formats properly. This tool is envisioned to be run just prior to data source release to fix any silent errors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4380:168,release,release,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4380,1,['release'],['release']
Deployability,"There seems to be no obvious way to read thru the unmapped read pairs in a bam file in Spark. Looking at the code in ```ReadSparkSource#getParallelReads(String, String, List, long)``` it seems that ; perhaps it is possible by setting the appropriate property in Configuration returned by ```ctx.hadoopConfiguration()``` however there is no documentation as to what property that could be. . @droazen I assign it to you initially so that you route it to whoever might be most suited to address this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2572:262,Configurat,Configuration,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2572,1,['Configurat'],['Configuration']
Deployability,"There were a couple of things I needed to do to get the new Spark code running on a cluster:. i. Go back to using Spark's version of Kryo. Using a different version of Kryo is not actually needed (2.21 used by Spark passes the tests), and actually caused errors on the cluster when run with `--conf spark.driver.userClassPathFirst=true` (which is needed to avoid other library conflicts, like with jopt-simple). ii. Exclude Spark from the JAR file to avoid library conflicts. It's normal to exclude Spark and Hadoop from JAR files since they are supplied by `spark-submit`. Since Gradle doesn't have a 'provided' dependency (see https://github.com/broadinstitute/hellbender/issues/836), I had to do a bit of a workaround with the `shadowJar` target, which is now `sparkJar`. . Here's the command I ran:. ``` bash; NAMENODE=...; SPARK_MASTER=yarn-client; HELLBENDER_HOME=...; spark-submit \; --master $SPARK_MASTER \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; build/libs/hellbender-all-*-spark.jar ReadsPipelineSpark \; --input hdfs://$NAMENODE/user/$USER/bam/NA12878.chr17_69k_70k.dictFix.bam \; --output hdfs://$NAMENODE/user/$USER/out/spark-reads-pipeline \; --reference hdfs://$NAMENODE/user/$USER/fasta/human_g1k_v37.chr17_1Mb.fasta \; --baseRecalibrationKnownVariants $HELLBENDER_HOME/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf \; --sparkMaster $SPARK_MASTER ; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/882:1244,pipeline,pipeline,1244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/882,1,['pipeline'],['pipeline']
Deployability,There were a few dead links in the GATK to http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam which is still archived here: https://web.archive.org/web/20160720131152/http://gatkforums.broadinstitute.org/gatk/discussion/58/companion-utilities-reordersam. We should write a new short technical article here: https://gatk.broadinstitute.org/hc/en-us/sections/360007134392-Glossary preserving the knowledge about sort ordering and update the remaining two links in our error messages to be current with that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8272:464,update,update,464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8272,1,['update'],['update']
Deployability,"There's a new [small library for bam reading](https://github.com/hammerlab/spark-bam) from hammer lab that is designed to fix problems in the bam splitting. It sounds promising from they've posted. We've been patching these problems in a very ad-hoc way as we discover them, but they've done a systematic survey to identify them. They also claim it's substantially faster. It's currently missing some important features, like selecting by interval. ; We should investigate if they're performance gains bear out in pratice, and if so if:; 1. We want to use the new library for bam reading; 2. We want to back port their changes to hadoop-bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3993:209,patch,patching,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3993,1,['patch'],['patching']
Deployability,"There's a small set of tools that only outputs their results to stdout, making it difficult to use the output in a pipeline/script. This PR adds a way to output simple results from such tools to an (optional) output file. I Added this option to the following tools:; - CountBases; - CountBasesInReference; - CountReads; - CountVariants; - FlagStat. Other tools that might benefit from this (but it will require an API change, so I didn't do it):; - CompareIntervalLists; - ValidateVariants",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7072:115,pipeline,pipeline,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7072,1,['pipeline'],['pipeline']
Deployability,There's some bad input in the BQSR test; update the input validation test to make sure it can find reads that are malformed in that way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/557:41,update,update,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/557,1,['update'],['update']
Deployability,"These are a bunch of random, mostly just annoying things that I repeatedly encountered during the Java 17 port that we should look into. . **Log Spam Issues:** (these result in lots of error log spam that make the logs super hard to scan when there is a failure):. - The WDL test logs are riddled with localization by hard link failed and ""Docker not found"" failures, which makes it hard to scan them for real failures. Can we eliminate/fix these ?; - The logs have a few gradle task dependency warnings - we should hunt down the cause. ; - We routinely pull ~800 branches every time we run git clone for a CI job. Can we do shallow git clones?; - We're using deprecated gradle features that result in warnings in the logs, these should be updated.; - The test runner seems to serialize (via toString) every argument to every test method. Many of these have *huge* ""toString"" representations (i.e., `org.broadinstitute.hellbender.tools.spark.sv.integration.ExtractOriginalAlignmentRecordsByNameSparkIntegrationTest`) that fill the logs with reams of huge test values. We should codify/unify the test case wrapper class that we use in htsjdk for these cases. . **Other Issues:**. - We should review the shadowJar contents - it includes some surprising stuff (i.e., the publish-picard.sh script we use to publish picard).; - Do we still need the unpacktestjar task in `dockertest.gradle`, to work around testNG inability to find tests in a jar ?; - The test matrix job names all look the same in the github UI because only the first N characters are displayed, and they all have the same prefix. We should rename them so they start with unique prefixes.; - The library it.unimi.dsi:fastutil:7.0.61 appears to not be used[Fix] (reported in IntelliJ/Project Structure/Problems).; - It's non-intuitive that the *Dockerfile* builds the `run_unit_tests.sh` script. Is that necessary - can this not be built on demand ? Also, it should be named to run_tests.sh, since it doesn't run unit tests, but rather ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8087:742,update,updated,742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8087,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,"These are changes we made in order to get the NeuralNetInference branch integration tests to pass, and some example program updates..",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4218:72,integrat,integration,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4218,2,"['integrat', 'update']","['integration', 'updates']"
Deployability,"These changes add arguments specific to the update from GATK 4.5.0.0 -> 4.6.0.0 that special case sites that were flagged previously in the WARP tests. Most of the sites that can now be skipped are based on the no call changes that were expected with this update to JointCalling and ReblockGVCFs. There are also some small changes to HaplotypeCaller at low quality sites that are then dropped by ReblockGVCFs. Additionally there were some expected changes to the Ultima pipelines in HaplotypeCaller and JointCalling which can now be skipped by the VcfComparator tool. Finally if AD is 0 for non-ref reads (which can happen with DRAGEN input), then AS_QD has jitter added which is now accounted for.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8973:44,update,update,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8973,3,"['pipeline', 'update']","['pipelines', 'update']"
Deployability,These need to be updated to use `gatk` instead of `gatk-launch` and kebob case arguments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4076:17,update,updated,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4076,1,['update'],['updated']
Deployability,"This PR Modifies the GvsCreateVDS wdl to no longer store the values for 'yng_status' in the VDS. The field is still used to calculate filtering at the genotype level, but not stored after that. - Example run of GvsCreateVDS [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e4e9905b-c967-4ced-9c02-41a3117eac84); - Passing integratino test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f6929247-1787-4ff7-b4f0-e367b0652ac8)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8861:365,integrat,integratino,365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8861,1,['integrat'],['integratino']
Deployability,"This PR addresses some issues with tracking of sample loading in GVSImportGenomes.; 1) Due to a bug (unresolved as yet) where the status 'STARTED' can be written to sample_load_status multiple times, the method LoadStatus.getSampleLoadState would NEVER return COMPLETE. This PR fixes the logic to allow for the (unexpected) case of multiple 'STARTED' statuses.; 2) I have modified the SetIsLoadedColumn task in GVSImportGenomes to explicitly look for sample_load_status records with both STARTED and FINISHED to use to update sample_info.is_loaded to true.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8052:519,update,update,519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8052,1,['update'],['update']
Deployability,"This PR adds a task to GvsAssignIds to verify that there are no duplicate sample names in the file provided. [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/af6651c5-37e1-48b6-8514-9c0d326dfc6f) is an example run of BulkIngest that replicates the original reported problem. No sample set provided, the sample id column is not sample_id and there's a duplicate in THAT column.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/25dc14df-c6e5-4710-b9b8-67b04906bc78) is an example run where the updated code runs and reports the problem early-ish without creating database tables that need to be cleaned up.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/b29d3eea-8330-4645-88fe-62bbf3b865bf) is a normal run that passes (same basic idea as the initial problem, except that I removed the duplicate row from the samples table. [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/82098882-8b57-4fe6-ad23-69963c3466f6) is a passing integration test.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8818:579,update,updated,579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8818,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,This PR adds an integration test for Exomes.; It also adds an optional input to the test to allow you to use the default dockers (and NOT build the gatk override jar) if you so desire. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/e16edc16-92a7-4a52-834a-1b45e1a2f92c).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8433:16,integrat,integration,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8433,2,"['Integrat', 'integrat']","['Integration', 'integration']"
Deployability,"This PR adds segments VCF writing to `PostprocessGermlineCNVCalls`. Segmentation (Viterbi) and segment quality calculation are performed by `gcnvkernel`. This PR introduces the following additional features:; - Calls and model shards are not required to be provided in sorted order anymore; - The user can specify the ref copy-number state for autosomal contigs, as well as allosomal contigs; - For both intervals and segments VCF output: now we use either `<DUP>` or `<DEL>` alleles (in place of `CN_x` alleles), depending on whether the most likely copy-number call is below or above the ; contig baseline. The contig baseline state is whatever the user has specified for autosomal contigs, and the contig ploidy state on sex chromosomes (from the output of `DetermineGermlineContigPloidy`).; - Fail-fast validations and better test coverage; - Updated cohort and case WDL scripts and WDL tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4396:847,Update,Updated,847,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4396,1,['Update'],['Updated']
Deployability,"This PR adds the 'SCORE' field as an output in the VQSR-Lite derived VCFs; Score is the value from which the `CALIBRATION_SENSITIVITY` is derived. The latter is what we use for filtering based on sensitivity, but Sam and Laura also want the SCORE stored in the VCF. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/721bc470-a968-4fe4-9be3-a1ddddc9a792)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8423:274,Integrat,Integration,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8423,1,['Integrat'],['Integration']
Deployability,"This PR allows the extract process to read ploidy information from an optional table and use it when writing out reference data. This code does NOT create that table. In the absence of such data, it will do nothing and behave like before (assuming a ploidy of 2 at all sites and expanding the reference data accordingly). Quickstart extract WITHOUT ploidy table specified:https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/fcc47f3f-080c-41f3-9847-0dd1487ef39c. Quickstart extract WITH ploidy table specified: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/2d608711-758f-47d2-ab54-ae825293e4a9. Successful integration run for verifying backwards compatibility: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d30c9db9-bdeb-4ff7-a236-3d3078258d06. The ploidy table used was based on quickstart data, but had data for samples 5, 9, and 10 manually updated to haploid. This will produce a INCORRECT vcf, inasmuch as it will reflect a mismatch in ploidy between the variant and ref data. But it allows us to see that, when the table is specified, it does in fact use it for writing out the ref data. As expected, shards 0-21 are identical with the only changes being on shards 22 and 23, and with diffs of this form:. ```25106c25106; < chrX	2800975	.	C	CA	.	.	AC=2;AF=0.250;AN=8;AS_QUALapprox=0|108;CALIBRATION_SENSITIVITY=0.9621;QUALapprox=81;SCORE=-0.5449	GT:AD:GQ:RGQ	./.	./.	./.	./.	0/0:.:30	./.	0/0:.:30	0/1:8,3:27:27	./.	0/1:6,5:80:81; ---; > chrX	2800975	.	C	CA	.	.	AC=2;AF=0.286;AN=7;AS_QUALapprox=0|108;CALIBRATION_SENSITIVITY=0.9621;QUALapprox=81;SCORE=-0.5449	GT:AD:GQ:RGQ	./.	./.	./.	./.	0/0:.:30	./.	0:.:30	0/1:8,3:27:27	./.	0/1:6,5:80:81; 25122c25122; < chrX	2805509	.	C	T	.	.	AC=1;AF=0.100;AN=10;AS_QUALapprox=0|360;CALIBRATION_SENSITIVITY=0.8769;QUALapprox=360;SCORE=-0.4865	GT:AD:GQ:RGQ	0/0:.:30	./.	./.	./.	./.	0/0:.:20	0/0:.:30	0/1:14,13:99:360	0/0:.:30	./.; ---; > chrX	2805509	.	C	T	",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8857:683,integrat,integration,683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8857,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,This PR builds off @lbergelson's `lb_add_header_line_to_genomicsdbimport` branch and fixes:. - [3677](https://github.com/broadinstitute/gatk/issues/3677); - Increments GenomicsDB release to 0.8.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3994:179,release,release,179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3994,1,['release'],['release']
Deployability,"This PR changes GvsExtractCallset to optionally pad the output VCFs with a leading zero. They are named in a similar fashion to the (also optionally renamed) interval list shards. If the input `zero_pad_output_vcf_filenames` is set to **true** (the default) the VCFs and interval files will be named as such:; - 0000000000-gg_filterset.vcf.gz; - 0000000000-gg_filterset.vcf.gz.tbi; - 0000000000-gg_filterset.vcf.gz.interval_list; - 0000000001-gg_filterset.vcf.gz; - 0000000001-gg_filterset.vcf.gz.tbi; - 0000000001-gg_filterset.vcf.gz.interval_list; - ... An example workflow is [here](https://app.terra.bio/#workspaces/warp-pipelines/ggrant%20-%20GVS%20Quickstart%20V2%20copy/job_history/3d3fc1e4-7f83-40d7-8f8c-115d4a4de158). If the input `zero_pad_output_vcf_filenames` is set to **false**, the workflow will name things as it has in the past:; - gg_filterset_0.vcf.gz; - gg_filterset_0.vcf.gz.tbi; - 0000000000-scattered.interval_list; - gg_filterset_1.vcf.gz; - gg_filterset_1.vcf.gz.tbi; - 0000000001-scattered.interval_list; - ... An example workflow is [here](https://app.terra.bio/#workspaces/warp-pipelines/ggrant%20-%20GVS%20Quickstart%20V2%20copy/job_history/27bd9b06-4400-4db1-89b4-4bdb5856aaff)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7783:625,pipeline,pipelines,625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7783,2,['pipeline'],['pipelines']
Deployability,This PR contains the GATK code necessary to enable some features present in the recent GenomicsDB update:; - Fixes https://github.com/broadinstitute/gatk/issues/7222; - Adds tests for https://github.com/broadinstitute/gatk/issues/7089; - Fixed an issue identified by @kcibul where the combine operation for certain fields needs to take care to not remap missing fields to NON_REF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7257:98,update,update,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7257,1,['update'],['update']
Deployability,"This PR converts the Mutect2Filtering engine to be allele specific. This required changes to SomaticClusteringModel and ThresholdCalculator as well as ErrorProbabilities and of course the filters themselves. There are some filters which have not yet been converted, but I am prioritizing the ones in this PR for Sarah Calvo and the mitochondria pipeline. This provides the implementation for dsp-spec-ops tickets 166, 168, 169",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6399:345,pipeline,pipeline,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6399,1,['pipeline'],['pipeline']
Deployability,"This PR creates a tool for generating split read and paired end SV evidence files from an input WGS CRAM or BAM file for use in the GATK-SV pipeline. This tool emulates the behavior of `svtk collect-pesr`, which is the tool used in the current version of the pipeline. Briefly, it creates two tab-delimited, tabix-able output files. The first stores information about discordant read pairs -- the positions and orientations of a read and its mate, for each read pair marked ""not properly paired"" in the input file. Records are reported only for the upstream read in the pair. The second file contains the locations of all soft clips in the input file, including the coordinate and ""direction"" (right or left clipping) and the count of the number of reads clipped at that position and direction. The integration test expected results file was generated using `svtk collect-pesr` to help ensure that the results are identical. We hope to eventually replace this component of the SV pipeline with this GATK tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6356:140,pipeline,pipeline,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6356,4,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to ; * the same chromosome with reference order switch but without strand switch, or; * different chromosomes. This brings us (unfiltered) ~6000 mated BND records, half of which are on canonical chromosomes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3571:91,configurat,configuration,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3571,1,['configurat'],['configuration']
Deployability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to the same chromosome with strand switch, but NOT significantly overlapping each other. We used to call inversions from such alignments, but it is more appropriate to emit BND records because a lot of times such signal is actually generated from inverted segmental duplications, or simply inverted mobile element insertions. To confidently interpret and distinguish between such events, we need other types of evidence, and is better to be dealt with downstream logic units. Inverted duplications are NOT dealt with in this PR and is going to be in the next. NEEDS TO WAIT UNTIL PART 1 & 2 ARE IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3457:91,configurat,configuration,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3457,1,['configurat'],['configuration']
Deployability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to the same chromosome with strand switch, significantly overlapping each other on their reference spans. We used to call inversions from such alignments when feasible, but it is more appropriate to emit inverted duplication records. NEEDS TO WAIT UNTIL PARTS 1, 2 AND 3 ARE IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3464:91,configurat,configuration,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3464,1,['configurat'],['configuration']
Deployability,"This PR deals with the test failures that were occurring when we ran ALL chromosomes through the integration test, rather than just chr20 and X and Y (the default). It adds another truth set for all chromosomes.; Also two small changes.; - Skip the cost/table size check for the Hail integration, to allow it to get to the hail part if there are spurious test failures in cost.; - Change the name of the files used for table size and cost checking. Makes it easier to install new test data. Passing integration test on all chromosomes [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/d8837252-26fa-4d40-bdf1-e42ff8932fd1); Passing integration test on chr20/x/y [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f552e7a3-d245-492d-b5e1-a35ba323fae8).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8787:97,integrat,integration,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8787,5,"['install', 'integrat']","['install', 'integration']"
Deployability,This PR expands the GermlineCNVCaller integration test suite and addresses #6893 and #4375. The tests that were added are: . - Numerical accuracy test that checks for changes of gCNV model posterior values as compared to a previously computed model. This test is meant to detect Python library updates that affect gCNV results and unintentional consequences of minor gCNV model changes.; - A test that runs gCNV in the COHORT mode with a pre-trained model as a starting point.; - A test that runs gCNV with an annotated intervals file that contain GC content column. As @samuelklee suggested we should consider adding functionality to the GermlineCNVCallerIntegrationTest to regenerate test files when there is a discrepancy in gCNV model outputs and we are okay with that discrepancy. See example of it in the HaplotypeCallerSparkIntegrationTest class -- specifically note UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS flag. @mwalker174 let me know what you think.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7889:38,integrat,integration,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7889,2,"['integrat', 'update']","['integration', 'updates']"
Deployability,"This PR fixes a bug I found in testing. I was extracting all the samples for an Exome run and it was widely scattered. So there occurred a situation where there no VET entries in one of the shards and a NPE happened.; This PR fixes that and makes it tool generate an empty (well, has a header) VCF, which the GVS workflow can handle. Failing workflow showing the problem [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/9f821329-f2bd-487c-a9af-4a81d0716072). Passing workflow (after the fix) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Data%2049k/job_history/52ecbbaa-199d-413b-95fe-2a3285462b43); Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/a2f67baa-9613-4c4e-be3b-85a1b25a3b3b)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8388:662,Integrat,Integration,662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8388,1,['Integrat'],['Integration']
Deployability,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8113:920,patch,patched,920,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113,1,['patch'],['patched']
Deployability,"This PR fixes two problems I noticed relative to how we are treating cross-contig evidence that comes from the alt contigs:. - The cross-contig exclusion rule in `ReadClassifier` was incorrect. Due to the structure of the `if`-`else if` logic in `checkDiscordantPair()`, `SameStrandPair` or `OutiesPair` evidence was still being created for cross-contig pairs based on the strand configuration of the two reads (even though they were aligned to different contigs).; - The `runWholePipeline` script we've been using was not actually setting the cross-contig kill list parameter, meaning no cross-contig evidence was being filtered out. The change makes the number of intervals for CMHMIX WGS1 drop from 31962 to 27645.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3262:380,configurat,configuration,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3262,1,['configurat'],['configuration']
Deployability,"This PR includes two changes:; 1. Provide a command line argument to toggle the overlapping base quality correction (i.e. min(bq, 20)) before reassembly, which happens in FragmentUtils. I've found, however, that by the time SomaticGenotypingEngine runs, those the quality of these bases get bumped up to what they used to be, so this may be a no-op. I included it in case I missed something, and to be consistent with the branch @fleharty and @madduran have been using.; 2. Provide a command line argument to count the two reads in an overlapping pair separately in StrandArtfiact and StrandBiasBySample. This feature is only available in Mutect i.e. it won't affect other tools that use StrandBiasBySample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5286:69,toggle,toggle,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5286,1,['toggle'],['toggle']
Deployability,This PR increases the memory for IndexVCF; It also updates SelectVariants to use symlinks so that the VCF and its index file can be in different paths.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8151:51,update,updates,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8151,1,['update'],['updates']
Deployability,This PR is against ah_var_store. Need to make another against EchoCallset. The new reference disk is installed.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/ba18dc3d-0548-48ac-a67b-cec55250de8a) is a passing run of GvsCreateVatFromVDS using quickstart against the new reference.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8746:101,install,installed,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8746,1,['install'],['installed']
Deployability,"This PR is intended to introduce several new tools related to the CleanVcf workflow in GATK-SV, which the use of these tools being documented in https://github.com/broadinstitute/gatk-sv/pull/733. These tools are intended to introduce several enhancements over the existing implementation, including but not limited to:; - Introduce various unit and integration tests into the workflow.; - Create more robust and generalizable tools that can be used independent of _CleanVcf_.; - Improve runtime and execution speed by leveraging Java.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8996:350,integrat,integration,350,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8996,1,['integrat'],['integration']
Deployability,"This PR is the initial stage of implementing the calling of IMPRECISE variants in the SV pipeline. It introduces the concept of an evidence-target link, which joins an evidence interval to its distal target. This is an extension of the 'coherent' evidence concept previously used in determining evidence thresholds for assembly. The code in this PR contains the following changes:. - Evidence intervals and distal targets now are treated as stranded, and evidence-target link clustering depends on overlaps between both intervals and strands.; - Evidence target interval and distal target interval calculations have been modified to make sure that evidence supporting the same event clusters together (has overlapping intervals). This includes several changes such as extending the 'rest-of-fragment-size' calculation to try to capture almost all non-outlier fragment sizes in the library; increasing the split read location uncertainty a little; and being more precise about the boundaries of distal target intervals by taking advantage of information in the MD and MC tags if available.; - Evidence target links are gathered for every piece of evidence supporting a high-quality distal target. ; - Evidence target links are clustered together and store the amount of split-read and read-pair evidence that went into each cluster.; - All evidence target link clusters that are composed of at least 1 split read or at least 2 read pairs are collected in the driver and emitted in a BEDPE formatted file specified in the command line parameters.; - A `PairedStrandedIntervalTree` data structure is introduced to allow `SVIntervalTree`-style lookups for paired intervals. To finish this work, future PRs will 1) use the collected evidence target links to annotate our assembly called-variants with the number of split reads and read pairs observed in the original mappings and 2) create IMPRECISE VCF records for events that have enough evidence-target-link support, first for deletions and then possibl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469:89,pipeline,pipeline,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469,1,['pipeline'],['pipeline']
Deployability,"This PR makes four changes to the SV pipeline management scripts: First it uses the new glob argument for copying data into HDFS to only copy the requested BAM file and its index, rather than everything in the GCS bucket directory (this is useful when the bucket directory contains multiple samples, as is the case with our HGSV trios). Second, the scripts now respect the project argument at each phase of the pipeline. Third, the output directory will include the name of the cluster so that if multiple samples are being processed by the same branch in parallel it's easier to figure out which results directory contains which sample and the results won't collide if two runs happen to start at the same timestamp. Finally, if the user requests not to copy the FASTQ files from the main management script, the script will not direct the SV discovery tool to write them to disk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4646:37,pipeline,pipeline,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4646,2,['pipeline'],['pipeline']
Deployability,"This PR makes two changes to Mutect2's filtering. 1. The first change updates `Math.min` to `Math.max` in `applyFiltersAndAccumulateOutputStats()`, which is probably the intended behavior. Unfortunately, this update breaks some of the integration tests at `org.broadinstitute.hellbender.tools.walkers.readorientation.LearnReadOrientationModelIntegrationTest.testOnRealBam`. I'm not quite sure how the dev team would prefer to handle the failed tests, so I thought I'd raise the issue here. 2. In `StrictStrandBiasFilter`, the argument `minReadsOnEachStrand` is not used in the `areAllelesArtifacts()` function. The second update turns on the `minReadsOnEachStrand` argument rather than using the default of 0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6903:70,update,updates,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6903,4,"['integrat', 'update']","['integration', 'update', 'updates']"
Deployability,"This PR modifies the VDS->VAT pipeline to scatter. We take the input sites only VCF, and scatter it.; Successful run at: https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20WGS%2010K%20Callset/job_history/468ab890-a49e-4487-ad12-7813296d9f04",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8122:30,pipeline,pipeline,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8122,1,['pipeline'],['pipeline']
Deployability,This PR modifies the behavior of GvsExtractToPgen to no-call any filtered genotypes; It also allows one to run GvsExtractCallset so that VCFs generated by it also have no-called GTs.; I also took the liberty of renaming 'VQSR Classic' to 'VQSR' and 'VQSR Lite' to 'VETS' in much of the Java code. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/1e1ed014-47cf-4c95-96f4-5c1284fc4616); Run of tie out pgen to VCF with no-called GTs [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/9e78f44a-f531-450b-acd8-db66cc6454be).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8793:305,Integrat,Integration,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8793,1,['Integrat'],['Integration']
Deployability,"This PR reimplements the overlap detector used in WeighedSplitIntervals in a much faster form for our particular use case. It also involved preprocessing the weighted bed input file in a new way, so the previous weights files will no longer work. As such, there's a new weights file uploaded and referred to as part of this pr. I pulled down the documentation and rationale for the original process from the git issue to a markdown file that can live in our repo, and made python scripts out of the necessary bits of python logic there (as well as a new one to do the further preprocessing step that I added). The motivation for this was the inability of the previous WeightedSplitIntervals task to complete when run against an exome interval list. This new one does, and it does so quickly. The link referenced below is not a ""successful"" run in the Terra sense because it was 190k exomes and that was simply too much for Terra to handle, but it DOES show a successful WeightedSplitIntervals run before the real extract started and I believe that is sufficient to merge. Delaying while ticket VS-189 gets figured out will create an unnecessary delay. Successful integration run: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/294fd6a8-15ed-4722-a63e-bdf089c1c52a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8507:1163,integrat,integration,1163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8507,1,['integrat'],['integration']
Deployability,"This PR removes the use of hadoop distcp to copy reference and sample data into HDFS, and replaces it with the new ParallelCopyGCSDirectoryIntoHDFSSpark tool. Based on initial tests, cluster creation with reference and a 110GB sample BAM file now takes ~8 minutes with SGA installation on the worker nodes and ~5 minutes without SGA installation. @SHuang-Broad can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2576:273,install,installation,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2576,2,['install'],['installation']
Deployability,This PR resolves all the pre-release M2 stuff except for the launch script. @LeeTL1220 @ruchim,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4048:29,release,release,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4048,1,['release'],['release']
Deployability,"This PR updates GvsWithdrawSamples to:; 1) Use a ""true"" temporary table (uniquely named, goes away after 24 hours); 2) Check if there are any samples in the uploaded list of samples to withdraw that are NOT in the existing sample_info table. Fail and print out the list of samples if so.; 3) Added a boolean flag to allow the user to pass the workflow if condition 2 is true. A) Example [Run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/26482e64-cc22-4191-b88f-f7765b173450) with 0 samples withdrawn and 0 new samples (samples in the withdrawn file that aren't in sample_info); B) Example [Run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/00f17100-18c4-4522-8dee-24630ef291b1) with 1 sample withdrawn and 0 new samples (samples in the withdrawn file that aren't in sample_info); C) Example [Run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/c7dfc547-305c-4b31-aec8-685494b92221) with 1 sample withdrawn and 1 new sample (sample in the withdrawn file that wasn't in sample_info). This run was run with the override flag allowing it to pass; D) Example [Run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/c2d2c897-ec1b-47ad-9927-70c6d7eb7e9b) with 1 sample withdrawn and 1 new sample (sample in the withdrawn file that wasn't in sample_info). This run failed (as intended)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8599:8,update,updates,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8599,1,['update'],['updates']
Deployability,"This PR updates the Freemarker templates so that the resulting pages will work with the current state of the website code. Most of the changes have to do with functionality I put in to enable hosting multiple doc versions and easy switching between them via a dropdown menu. . I had already done some retrofitting on older tooldocs so the versioned tool docs go back to 3.5, and we can add beta versions of 4 without changing the ""latest supported version"". . The only remaining problem is that I couldn't figure out how to output php instead of html. To test the web integration, I just renamed all *.html to *.php with `for f in *.html; do mv -- ""$f"" ""${f%.html}.php""; done` but that doesn't take care of internal links, which are of course broken as a result. @cmnbroad please let me know if I missed something obvious on this front ^^. . That being said this PR is fully functional as far as I'm concerned.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3165:8,update,updates,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3165,2,"['integrat', 'update']","['integration', 'updates']"
Deployability,"This PR updates the `--use-posteriors-to-calculate-qual` mode to properly treat the star allele as a non-variant (for that site) allele. With this change, if a spanning deletion is present, it is treated as a non-variant allele for that site, and the posterior of no variant allele being present becomes the sum of the posteriors of all genotypes composed of combinations of the reference allele and the star allele.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6856:8,update,updates,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6856,1,['update'],['updates']
Deployability,"This PR:. * fixes a problem observed for a complex event with tandem duplicated sequence and insertion and deletion, which causes the discovery phase of SV pipeline to throw exceptions;; * the fix works by changing how tandem duplications are annotated:; * duplicated sequence is no longer provided, instead the corresponding duplication unit reference span is provided in `DUP_REPET_UNIT_REF_SPAN`,; * the duplicated units on the assembled contigs' CIGAR when aligned to reference is provided in `DUP_ASM_CTG_CIGARS`;; * adds annotation `DUP_ANNOT_FROM_OPT` when tandem repeat annotations were generated by a simple approximation procedure, which should be viewed with care;; * logs the total number of variants and different types; * updated and added tests to reflect these changes. The PR was tested to be runnable based on output from scanning the CHM-mix bam with PR #2444, which discovered the exception.; The number of variants discovered are:. For CHM-mix; ```; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 204 INVs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2775 DELs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 954 DUPs; 20:43:36.213 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 977 INSs; ```; And for NA12878_PCR-_30X; ```; 22:14:15.653 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - Discovered 4686 variants.; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 228 INV's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 2719 DEL's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 835 DUP's; 22:14:15.660 INFO DiscoverStructuralVariantsFromAlignedContigsSAMSpark - 904 INS's; ```; @cwhelan Could you please to review?; @tedsharpe feel free to poke around and test run it. __UPDATE__:; This is to be merged after PR #2444.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2567:156,pipeline,pipeline,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"This `DS` annotation has been kicking around in the header for a long time, but I've never seen it in the wild. It doesn't show up in any of our integration tests. We only have the ability to add this annotation if the GenotypingEngine gets a non-null `Map<String, AlignmentContext> stratifiedContexts`, but that doesn't seem to be the case in *_any_* of our tests. Maybe it's a holdover from UnifiedGenotyper?. @davidbenjamin have you seen cases where we `calculateGenotypes` with stratifiedContexts (or refContext or rawContext or likelihoods)? Given that there's zero test coverage, how would you feel about ripping it out and seeing if anyone complains?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5678:145,integrat,integration,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5678,1,['integrat'],['integration']
Deployability,"This adds a hard filter for low variant allele fraction calls. We will not turn this on by default in any of our pipelines, but it will give users an easy option to filter everything below a certain VAF that they don't care about. It also adds a hard filter for low alt depth calls based on a threshold from the median autosomal coverage (that must be supplied as an argument). It takes the cutoff from a Poisson with a mean of 1.5 * median coverage (to account for NuMTs with 3 copies in the autosome) and is tuned to catch 99% of the false positives (which we know will also catch lots of true positives). . It also removes the Polymorphic NUMT annotation (since that's basically what's going into the filter at this point).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5842:113,pipeline,pipelines,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842,1,['pipeline'],['pipelines']
Deployability,"This adds a small test case for the WDL of the filtering pipeline. This still has indels and snps separated out. I can combine them if needed, but we'd like to use different annotations for each mode. This also doesn't actually apply the final filtering (with a threshold) since we still need to add a step to determine the correct threshold. The final VCFs from this workflow should have SCORE INFO annotations for each site. This takes in an array of VCFs (and outputs an array of VCFs) because this is an option for large callsets in the WARP joint genotyping WDL which is where this WDL will eventually be integrated. This test only ensures that the WDL runs and doesn't compare to expected results (the same as the other WDL tests in this repo).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7932:57,pipeline,pipeline,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7932,2,"['integrat', 'pipeline']","['integrated', 'pipeline']"
Deployability,This allow us to simplify our R installation in Travis. . R tests now only test Rscript executor; Using standard ubuntu R in travis instead of the more up to date one; Removing installation of R packages as part of gradle and travis builds; Updating readme,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/518:32,install,installation,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/518,2,['install'],['installation']
Deployability,"This along with the GVCF reblocking branch constitute the new code I'm using for gnomAD v3 on the Gnarly Pipeline. Some of the GDB hacks are gross, but I can't clean it up until after the protobuf update.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947:105,Pipeline,Pipeline,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947,2,"['Pipeline', 'update']","['Pipeline', 'update']"
Deployability,"This also includes:; * Added FPGA to ```Implementation``` enum of ```VectorLoglessPairHMM```; * Added FPGA to ```Implementation``` enum of ```PairHMM```; * Updated ```FASTEST_AVAILABLE``` value to try FPGA first (then AVX+OMP, then AVX, then regular)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2725:156,Update,Updated,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2725,1,['Update'],['Updated']
Deployability,"This applies to projects that import the GATK jar as part of the build process, but are not part of the GATK itself. All unit and integration tests are (by default) broken, since the BaseTest class requires the mini fasta, even when it should not be required. This causes breakage, since a project built on the GATK should not be expected to have that file at the exact correct place in the filesystem. The tests do not even start.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3029:130,integrat,integration,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3029,1,['integrat'],['integration']
Deployability,"This branch takes the version of gatk-public that gatk-protected currently depends on (4.alpha.2-188-g7332d10) and applies @davidbenjamin 's fix to the `TandemRepeat` annotation to it. The only purpose of this PR is to cause a snapshot to be generated -- do not merge!. This is necessary to unblock @davidbenjamin 's work, because the `HaplotypeCaller` tests are failing if we update protected to the latest public head, and although we've fixed some of the issues there are some unexplained failures in the concordance tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2569:377,update,update,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2569,1,['update'],['update']
Deployability,This brings in recently-merged changes related to headerless SAMRecords; that are needed in order to enable the faster SAMRecord serializer; (SAMRecordToGATKReadAdapterSerializer). Temporarily disabled the SAMRecordToGATKReadAdapterSerializerUnitTest; pending updates in PR #1127.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1153:260,update,updates,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1153,1,['update'],['updates']
Deployability,"This bumped some transitive dependencies which required a minor update in unrelated classes. We shouldn't merge this until we get a  from the SV team as well as running the jenkins spark tests. I think the SV team is already using 2.2.0 since they've gone to dataproc image 1.2. This will prevent the annoying adam log spam, closes #4186 ; closes #2555",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4314:64,update,update,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4314,1,['update'],['update']
Deployability,"This came up on PR #6351 after a rebase. The branch passes all tests locally (verified on my laptop and on @cmnbroad's) but fails `DocumentationGenerationIntegrationTest.documentationSmokeTest` in the docker integration CI job on Java 8 but not Java 11. The same test passed locally for me even after verifying Java 8 was enabled, refreshing the gradle project, rebasing again etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991:208,integrat,integration,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991,1,['integrat'],['integration']
Deployability,"This can improve some build configurations for GATK (ony noted the ones in 4.6, but not previos ones):. * `failFast` property for test tasks. This would be useful for PRs; * Declare reasons for dependency resolution rules and constraint dependencies. This could be useful for explaining why some dependencies are not the latest (e.g., protobuf).; * Allow options in the command line. This could be nice for the doc generation.; * Default jacoco is 0.8.0, which improves the coverage report by filtering out some empty constructors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4659:28,configurat,configurations,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659,1,['configurat'],['configurations']
Deployability,"This can wait until after release, but will need to be done to get gCNV into FireCloud.; Also, output blocks for Germline CNV calling WDL scripts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4025:26,release,release,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4025,1,['release'],['release']
Deployability,This causes integration tests to fail.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4839:12,integrat,integration,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4839,1,['integrat'],['integration']
Deployability,This change is dependent on [this recent change](https://github.com/samtools/htsjdk/commit/4f550e1f1afabf21467957fa672ca2a4ad457897#diff-b678735810949d4263df7bd0fffdecb8L42) in htsjdk (and the build will fail without it). Once htsjdk2.0 is available we'll upgrade it in this branch/pr so the two changes can go in together.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1243:256,upgrade,upgrade,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1243,1,['upgrade'],['upgrade']
Deployability,"This class is now needed by other pipelines, so let's make it generally available; in the engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/476:34,pipeline,pipelines,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/476,1,['pipeline'],['pipelines']
Deployability,"This code (building off of Louis' fixes) adds the following:; - AuthHolder, a replacement for the PipelineOptions. It stores the authentication info we need for GCS and supports both API_KEY and client-secrets.json. I adapted a few classes to accept an AuthHolder.; - BaseRecalibratorOptimizedSpark, a port of the ""shard"" approach I first did on the Dataflow side. Note that currently this code only performs reasonably for small inputs if you specify -L on the command line (for large inputs it doesn't matter).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987:98,Pipeline,PipelineOptions,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987,1,['Pipeline'],['PipelineOptions']
Deployability,"This contains a change to toggle the presorted flag used by GatherBAMFiles, and enables some CRAM tests that were previously broken due to htsjdk bugs. Fixes https://github.com/broadinstitute/gatk/issues/1138 and all but one from https://github.com/broadinstitute/gatk/issues/1141.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1255:26,toggle,toggle,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1255,1,['toggle'],['toggle']
Deployability,"This doesn't fix the integration test, but a bug in GvsBulkIngest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8736:21,integrat,integration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8736,1,['integrat'],['integration']
Deployability,This epic will track tickets related to work in the annotation engine for the 4.0 release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3274:82,release,release,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3274,1,['release'],['release']
Deployability,"This extends Variant Eval to compare AFs between variants in binned AF buckets based on Thousand Genomes VCF, between the expected AF from Thousand Genomes and the seen one in the actual VCF, to be used as a QC metric for our arrays pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6039:233,pipeline,pipeline,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6039,1,['pipeline'],['pipeline']
Deployability,This fixes a bug in handling the defaults for setting and using our default cluster initialization script for the SV pipeline. The master version will error if no init script parameters are specified.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3467:117,pipeline,pipeline,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3467,1,['pipeline'],['pipeline']
Deployability,"This fixes a bug in the `AlleleFrequencyCalculator` that was causing quality to be overestimated for sites with `*` alleles representing spanning deletions. The bug was causing the calculator to not include homozygous `*` genotypes in the sum of non-site specific variant allele probabilities that is the basis for the qual score. The bug was caused by an off-by-one index error: `IndexRange(0,2)` returns `[0,1]`, not `[0,1,2]` as intended. Not including this genotype inflated the quality score for these sites. . Due to interactions with QUAL-based variant and allele trimming, this causes slightly different behavior when HaplotyeCaller is run in modes where it is forced to emit variants for every locus, as can be seen in the `expected/gvcf.basepairResolution.includeNonVariantSites.vcf` test file for `GenotypeGVCFsIntegrationTest`: 1) Sites spanned by a deletion are now reported with a `*` alt allele and have QUAL 0 and a LowQual filter. Also added a mechanism to `GenotypeGCVFsIntegrationTest` to automatically update the expected result files, similar to what already exists in `HaplotypeCallerIntegrationTest` and `CombineGVCFsIntegrationTest`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6859:1022,update,update,1022,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6859,1,['update'],['update']
Deployability,This fixes the first day's downloads on a new release which previously were set to 0. @jonn-smith This is what I was talking about.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7794:46,release,release,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7794,1,['release'],['release']
Deployability,"This handles the case we saw in Shriner's beta files where variants on chrX (and presumably chrY) were represented with a call_GT of ""1."". NOTE: SOME of these changes fixed our code to work for any ploidy, but others only changed our pipeline to work for examples with a ploidy of 1. Specifically, the changes made to . `scripts/variantstore/wdl/extract/populate_alt_allele_table.py` and ; `src/main/resources/org/broadinstitute/hellbender/tools/gvs/filtering/feature_extract.sql`. have made it work for haploid and diploid values, but we'd need to generalize the code that explicitly lists the potential values for GT. Given that we're not seeing cases with a ploidy above 2 yet, doing that can be for a later ticket. Doc with steps I went through to test this:; https://docs.google.com/document/d/1F194j7OQh9ehs5pSdt5yHcsSWrm3WmqDlVVkDFkULuw/edit#heading=h.464spie271ew. Successful extract here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/a7cc6ffb-fd98-4142-a211-8235dea10b35. Successful integration run here:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8d09d70e-a6f3-42a8-9c81-95065c653f4d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8334:234,pipeline,pipeline,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8334,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"This includes wrappers to present `SAMRecords` to the tools; Also adding 4 simple tools as examples; `FlagStatsDataflow`; It makes use of dataflow's built in hierarchical aggregation; `CountBasesDataflow`; Simple walker that makes use of the SAMRecord conversion; `CountReadsDataflow`; Does what it says; `PrintReadsDataflow`; This is a very limited version of our print reads walker; It prints `SAMRecords` as strings to an unordered text file; It could potentially be useful as method for examining bam output before we have a proper bam writer. These tools exist in two parts:; A transform extending from `PTransformSAM` (A subclass of `PTransform<Read,O>` which facilitates conversion to `SAMRecord`; A command line tool implementing a complete pipeline; These pipelines can apply arbitrary `ReadFilter`s/ `ReadTransformer`s which are applied before the main transform; (a list of transforms and a list of filters can be applied, it's currently not handled very efficiently though, better to pre-comine them into a single meta transform). Currently, only tests which use local files are running on travis.; There is code included to run on files in buckets, but the tests for it are currently disabled due to travis configuration issues (will be resolved in a seperate ticket). Some changes were made to existing classes to make them Serialize properly; Some test files were moved to help normalize test data locations (although not all tests are normalized, should be done in separate ticket); the new storage locations are based on the complete package name rather than just the tool name",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/443:749,pipeline,pipeline,749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/443,3,"['configurat', 'pipeline']","['configuration', 'pipeline', 'pipelines']"
Deployability,This is a bit of a pain because the sam files produce by htsjdk now all say they're version 1.5. We need to rewrite our sam/bam files to be compliant with the new version (or at the very least update the version strings. ) Alternatively we could change the comparator to ignore versions when comparing.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/758:193,update,update,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/758,1,['update'],['update']
Deployability,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesnt really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1827:207,pipeline,pipelined,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827,1,['pipeline'],['pipelined']
Deployability,"This is a difference vs. GATK3, which did include reads with deletions in the isActive pileups for both HaplotypeCaller and Mutect2 ( @davidbenjamin take note ). The fix is trivial -- I'll have a patch submitted later today.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3830:196,patch,patch,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3830,1,['patch'],['patch']
Deployability,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8737:10,patch,patch,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737,3,"['integrat', 'patch']","['integration', 'patch']"
Deployability,"This is a prototype of the basic infrastructure that must go in to make the junction tree based Haplotype finding work. I have pulled out a toggle for the HaplotypeCaller that that enables a separate ReadThreadingAssembler codepath for haplotype finding. Right now when this mode is enabled `ExperimentalReadThreadingAssembler` is used in conjunction with `JuncitonTreeKBestHalotypeFinder` to extract only haplotypes that show up in our junction trees with evidence of > 3 reads. This still poses problems with dangling end recovery as definitionally those branches never include complete junction tree data. . I will continue to work on this branch (as it is in a somewhat rough state still) but I would like to at least get some eyes on it before i get too deep in the weeds to at least validate the structural approach I have chosen. . Currently known issues in this branch: ; - Tests are failing due to resolution of non-unique reference sink vertexes, I would solicit help as to how best to resolve the case where junction trees point to both a reference stop allele and a continued path.; - There is at least one very degenerate edge case that might cause the code to hang, I would also ask after what is the best way to close out of looping assembly structures that never have reads to close them (i.e. a ""dangling end"" hom-var that happens to point to a non-unique reference base). ; - Probably after discussion the threshold for discarding junction trees will be changed to instead use paths from the discarded tree first. . Resolves #5925",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6034:140,toggle,toggle,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034,1,['toggle'],['toggle']
Deployability,"This is a subtask of #133. Extra code is needed to switch between `""INPUT""` and `""input""` which is unfortunate. Replace ""INPUT""/""OUTPUT"" for `CleanSam` and `MarkDuplicates` (along with any others that I missed.) with `StandardArgumentDefinitions.INPUT_LONG_NAME` Then update `SamFileTester` to only use the standard names.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/805:268,update,update,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/805,1,['update'],['update']
Deployability,"This is a tool intended to evaluate the performance of genotyping (not sequencing/variant discovery), for example from a genotyping array or imputation pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7328:152,pipeline,pipeline,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7328,1,['pipeline'],['pipeline']
Deployability,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122:95,Integrat,IntegrationTestSpec,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122,2,['Integrat'],['IntegrationTestSpec']
Deployability,"This is an experiment to see if it's possible to run BWA-MEM on Spark. (Please don't merge.) The basic idea is that it uses JNI to call BWA-MEM's align function to align a batch of read pairs in one go. I think it should complement the work that @SHuang-Broad has been doing in #1701. It would be great to get your (and @akiezun's) feedback on the direction here. A few comments; - Building the native libraries is not integrated, and it's not using the Apache 2 licensed code. I think this could use some of the changes in #1701.; - The ref is assumed to be on the local FS for the moment - it should really be loaded from HDFS. Also, the output is a single SAM file on the local FS, not a sharded BAM as for the rest of the GATK Spark tools.; - It is assumed that read pairs are interleaved and reads in a pair are placed in the same split (by setting `hadoopbam.bam.keep-paired-reads-together`). However, that property only works for queryname sorted BAMs, which isn't the case here, so we need to relax that requirement in Hadoop-BAM.; - I haven't tried this on large inputs, so I don't know how well it performs. To run, I used the following on a cluster. ```; ./gatk-launch BwaSpark \; --ref /home/tom/workspace/jbwa/test/ref.fa \; --input hdfs:///user/$USER/bwa/R.bam \; --output /tmp/bwa.sam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; ```. The interesting bit is the use of Spark's `--archives` flag to copy a tarball of native libraries (which I built manually) to every executor, and unpacks it in the working directory. Then `spark.executor.extraLibraryPath` is set to add that path to the library path of the executor. This means that you don't have to rely on the native libraries being installed on every node in the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750:419,integrat,integrated,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750,2,"['install', 'integrat']","['installed', 'integrated']"
Deployability,"This is based on @davidadamsphd's initial work to port mark duplicates to Spark. It's not finished yet, but I wanted to post this for discussion. In particular 7 of the 56 mark duplicates integration tests are failing with ""Cannot get mate information for an unpaired read"" - I'm not sure how to address that. I'd appreciate some help on this one. The code currently has four shuffles: one groupBy in transformFragments (in MarkDuplicatesSparkUtils), two groupBys in transformReads, and one combine (foldByKey) in generateMetrics. The combine is more efficient than the others since it can run on the map side, reducing the amount of data that goes through the shuffle. I think it may be possible to merge the processing of the fragments and the reads to eliminate a shuffle - so there are only two shuffles for the main transform. A fragment would be represented as a pair with an empty second slot, so it can be handed in the processing separately from the true pairs that have both slots filled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/889:188,integrat,integration,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/889,1,['integrat'],['integration']
Deployability,"This is currently using a Barclay snapshot, and should not be merged until the released version is up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4270:79,release,released,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4270,1,['release'],['released']
Deployability,This is for [Issue 5397](https://github.com/broadinstitute/gatk/pull/5397). Got the tests to pass by generating new expected results for the following failing tests and use the GenomicsDBImports folder for expected results as that was updated as part of [PR 5170](https://github.com/broadinstitute/gatk/pull/5471). * testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData; * testGenomicsDBImportFileInputs_newMQ. Note that this PR also has merged with master.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5581:235,update,updated,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5581,1,['update'],['updated']
Deployability,"This is for demo purpose only, so the code is not ready yet to be merged:. ## Description. ### background & goal; Currently, there are two parallel code path for structural variation breakpoint location and type inference using local assembly contig alignment signatures in the pipeline `StructuralVariationDiscoveryPipelineSpark`. * the stable code path: scanning neighbor chimeric alignment pairs of a contig iteratively and outputs inversion breakpoints as symbolic variant `<INV>`, annotated with `INV55` and `INV33` for signaling if it is the left or right breakpoint of the assumed inversion.; * the experimental code path that separates the alignment pre-processing step from the inference step, and studying the alignments in whole; this code path, in addition to outputting insertion, deletion and small duplication calls as does the stable path, outputs ; * BND records representing assembled breakpoints for which type could not be completely determined using only the contig alignments; this includes supposedly inversion breakpoints; * complex (`<CPX>`) variants from assembly contigs with more than 2 alignments; ; The tool proposed in this PR is based on [manual review](https://github.com/broadinstitute/dsde-methods-sv/tree/sh_inv_filter_init/docs/knowledgeBase/variantReview/inversion/chm) of a callset generated a long time ago (but still useful for studying filtering inversion breakpoints), and is designed to be integrated with the experimental code path. ### proposed algo. #### input:; * the ""INV55/INV33""-annotated `BND` records output by the upstream experimental code path; * BND's have related concepts of `MATE` and `PARTNER` (see figure below, left); * `MATE`: novel adjacency, i.e. contiguity on sample that is absent on reference (e.g. mobile element insertions, deletions); * `PARTNER`: novel disruption, i.e. contiguity on reference disrupted on sample (e.g. insertions, deletions). ![inversion_demo](https://user-images.githubusercontent.com/16310888/40271739-6d999b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789:278,pipeline,pipeline,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789,1,['pipeline'],['pipeline']
Deployability,"This is for discussion... @jonn-smith @droazen . Currently, there seems to be a fair amount of code change in order to get Funcotator to work with a new gencode version. . For starters:; - the parser has to recognize the email address and version string; - the MAF output alias `MafOutputRendererConstants` needs to be updated. Is there a way to have this be more seamless? . Ideally, we would have no mandatory code changes. Maybe just warnings that would not block a user. Is this an issue at all? Are we going to scale back the flexibility of datasources?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4786:319,update,updated,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4786,1,['update'],['updated']
Deployability,"This is intended to alleviate transient issues with GermlineCNVCaller inference in which the ELBO converges to a NaN value, by calling the python gCNV code with an updated random seed input.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6866:164,update,updated,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6866,1,['update'],['updated']
Deployability,"This is the barest bones version of the reads preprocessing pipeline. This adds the following:; - loading reference bases from the Google Genomics API; - Joining overlapping variants with reads; - Joining reference bases with reads; - A pipeline outline for reads preprocessing. There PR is ready to get reviewed. Assigning to Louis, but JP will look at Variants and BQSR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/655:60,pipeline,pipeline,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/655,2,['pipeline'],['pipeline']
Deployability,This is very important for integration tests. Not being able to use large files is significantly slowing porting of existing tools to hellbender. Maybe this will work https://git-lfs.github.com/ maybe something else.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/388:27,integrat,integration,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/388,1,['integrat'],['integration']
Deployability,"This isn't just a matter of changing the number. [RegisterCoder was made more stringent](https://cloud.google.com/dataflow/release-notes/java) and this will force some code changes. Hopefully only little ones, but I got only as far as getting an internal Java error and I think that's a sign I should go to bed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/754:123,release,release-notes,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/754,1,['release'],['release-notes']
Deployability,"This issue is to keep a record that we're going to base the initial porting of Picard on version 1.130, released on 3/24/15. This will make it easy to re-sync the tools later on by just diffing against that version to see what changes need to be made.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/331:104,release,released,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/331,1,['release'],['released']
Deployability,This makes the Docker container more interoperable with other GATK containers that might not install to /gatk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3866:93,install,install,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3866,1,['install'],['install']
Deployability,This matches arguments of various tools used in gCNV pipeline to those used for running large exome cohorts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8234:53,pipeline,pipeline,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8234,1,['pipeline'],['pipeline']
Deployability,"This method is for unit/integration testing purposes only, and should not be called from tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4430:24,integrat,integration,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4430,1,['integrat'],['integration']
Deployability,"This micro-optimization fell out of profiling of the HaplotypeCaller in GVCF mode. . Profiler view over an Exome before this patch:; <img width=""906"" alt=""screen shot 2018-11-30 at 2 06 34 pm"" src=""https://user-images.githubusercontent.com/16102845/49310230-bc44a380-f4ab-11e8-98aa-1c0b321223c0.png"">. Profiler view over the same Exome after this patch:; <img width=""886"" alt=""screen shot 2018-11-30 at 2 20 39 pm"" src=""https://user-images.githubusercontent.com/16102845/49310291-e4cc9d80-f4ab-11e8-9fb3-4d819fbce43a.png"">. I suspect given the remaining 9% runtime could be reduced further by looking more closely at the array operations in `isReadInformativeAboutIndelsOfSize()` . (It should be noted that these profiler results lie within the ReferenceModelForNoVariation codepath which since this is over an Exome we expect the runtime to overall be skewed towards no-variation blocks). Resolves #5648",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5469:125,patch,patch,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5469,2,['patch'],['patch']
Deployability,"This moves us to a snapshot of google-cloud-java based off of a branch in my fork here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. This patch wraps many more operations within retries, and in our tests resolves the intermittent 503/SSL errors completely when running at scale. This PR also migrates us from setting retry settings per-Path to setting it globally, using a new API from that google-cloud-java branch. This fixes an issue where the number of reopens was getting set to 0 deep in the google-cloud-java library. Resolves #2749; Resolves #2685; Resolves #3118; Resolves #3120; Resolves #3253",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3295:187,patch,patch,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3295,1,['patch'],['patch']
Deployability,"This new PathSeq WDL redesigns the workflow for improved performance in the cloud. Downsampling can be applied to BAMs with high microbial content (ie >10M reads) that normally cause performance issues. . Other improvements include:. * Removed microbial fasta input, as only the sequence dictionary is needed.; * Broke pipeline down to into smaller tasks. This helps reduce costs by a) provisioning fewer resources at the filter and score phases of the pipeline and b) reducing job wall time to minimize the likelihood of VM preemption.; * Filter-only option, which can be used to cheaply estimate the number of microbial reads in the sample.; * Metrics are now parsed so they can be fed as output to the Terra data model.; * CRAM-to-BAM capability; * Updated WDL readme; * Deleted unneeded WDL json configuration, as the configuration can be provided in Terra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6536:319,pipeline,pipeline,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6536,5,"['Update', 'configurat', 'pipeline']","['Updated', 'configuration', 'pipeline']"
Deployability,This now outputs median coverage in addition to the mean coverage which was already being output before from the Mitochondria pipeline. @ahaessly Could you please take a look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7253:126,pipeline,pipeline,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7253,1,['pipeline'],['pipeline']
Deployability,This or these tool would be use to use piece meal bam files that we can use for local (laptop/desktop) development and integration tests. The input would be SV called variant VCF with and two intervals lists. The first one would indicate for what variants in input we want to gather the evidence reads and the second the supported reference interval in the output (anything outside that interval will be changed to unmapped). The second interval list must include the first interval list since otherwise it would not make any sense.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2504:119,integrat,integration,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2504,1,['integrat'],['integration']
Deployability,"This pr adds a step to the VAT pipeline which will export each chromosome of the VAT (TODO need to add X, Y, M) into it's own directory in GCS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7472:31,pipeline,pipeline,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7472,1,['pipeline'],['pipeline']
Deployability,"This pr adds the subpopulation AC/AN/AF calculations.; It does this by taking in the ancestry table and making sublists of each---then passing that list of samples into the SelectVariants GATK tool. Updated Lucid chart here: https://lucid.app/lucidchart/fee376a4-4b72-481e-a239-a027f7f6ab1f/edit?page=CsG3hy3S1zEH#. Design Doc for this work:; https://docs.google.com/document/d/1FnPu_Jkz2O9rElApAQld0v6iBEFGe22dKarVWcwNxGI/edit. misc:; how should I add the VAT validation to the VAT pipeline? Should it run automatically?. Anvil data version of this table: spec-ops-aou:anvil_100_for_testing.vat_aug19. <img width=""1379"" alt=""Screen Shot 2021-08-11 at 5 38 22 PM"" src=""https://user-images.githubusercontent.com/6863459/129606564-bfc20a68-119a-4072-88b4-aeaf011cc965.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7399:199,Update,Updated,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7399,2,"['Update', 'pipeline']","['Updated', 'pipeline']"
Deployability,"This produces a resource that will be used as input to an upcoming tool to filter intervals based on these annotations (as well as coverage statistics). Currently, we have an external python script performing this step in the gCNV pipeline. I also updated the AnnotateIntervals task and calls in WDL, but these changes are untested; the reviewer should check carefully for typos. Currently, all annotations are of double type, but I've added code that can support all types supported by the TSV code as well. Additional tracks can also be added relatively easily. Currently, allowed annotations and their corresponding types are hardcoded; we could possibly move this information to the SAM-style header in the future. For the Umap hg19 k100 single-read mappability track and the segmental-duplication track used by the Talkowski lab, annotation of 1kb bins on hg19 takes less than a minute with the default feature lookahead (which is exposed as a parameter). I tested using the Umap multi-read mappability track (which is orders of magnitude larger, but is actually what is used in the external script), but this is much slower (documentation indicates that the single-read track should be used to dissuade this). We should evaluate whether or not using the single-read track suffices for filtering.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5162:231,pipeline,pipeline,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5162,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"This pull request updates the environment to include `pytorch` to gatk conda environment. This required an update to numpy and consequently updates of PyMC3 and its dependencies, as well as parts of gCNV code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8094:18,update,updates,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8094,3,['update'],"['update', 'updates']"
Deployability,"This pulls the bulk of the pipeline into a separate subworkflow so that the validations (with the mixture samples) can be run. The mixtures have already been subset and tagged, which is why the rest of the pipeline needed to be extracted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5708:27,pipeline,pipeline,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5708,2,['pipeline'],['pipeline']
Deployability,"This read filter removes unnmapped reads and reads with unmapped mates. When used in combination with `MateOnSameContigOrNoMappedMateReadFilter` this subsets down to reads only on chrM whose mate is also on chrM. If we only used the `MateOnSameContigOrNoMappedMateReadFilter` we end up with reads whose mate is unmapped still in the BAM, but not the unmapped read, which causes problems downstream in the mitochondria pipeline. This read filter will make the subsetting step faster when we no longer need the NuMTs. I would really appreciate this getting in before the next release (on Tuesday). (fyi @droazen) @ldgauthier @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5826:418,pipeline,pipeline,418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5826,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"This release contains important bugfixes, including a fix for https://github.com/broadinstitute/gatk/issues/8141 (intermittent failure to properly compress outputs)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8409:5,release,release,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8409,1,['release'],['release']
Deployability,This release fixes (among other issues) a bug that could cause the; AVX compatibility check to hang on CentOS machines.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3392:5,release,release,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3392,1,['release'],['release']
Deployability,"This release fixes several bugs when using HttpPath resolve methods that have been affecting users. In particular, they were causing issues when trying to construct HTTP URIs for companion files such as fasta/bam indices.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8889:5,release,release,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8889,1,['release'],['release']
Deployability,This release potentially fixes a bug that the SV team (specifically @SHuang-Broad) encountered.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2277:5,release,release,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2277,1,['release'],['release']
Deployability,"This replaces a secret that requires a pr to fix, and updates the name of one of the others.; Requires 1 more step after this.; * Switch travis variable name from DOCKER_SERVICE_PASS -> DOCKER_SERVICE_TOKEN for clarity; * Replace gcloud encrypted key",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7521:54,update,updates,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7521,1,['update'],['updates']
Deployability,"This request was created from a contribution made by Duo Xie on August 20, 2022 16:16 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/community/posts/8235601014427-Issue-when-running-BaseRecalibrator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1 ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;   java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/uksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --know",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:610,pipeline,pipeline,610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,2,['pipeline'],['pipeline']
Deployability,"This request was created from a contribution made by Min-Hwan Sohn on March 05, 2020 01:00 UTC. Link: https://gatk.broadinstitute.org/hc/en-us/community/posts/360057956031-PathseqPipelineSpark-stop-with-error-message-regarding-com-esotericsoftware-kryo-KryoException-Buffer-underflow-. --. Hi GATK team. I recently used PathseqPipelineSpark embedded in GATK v4.1.4.1 (installed from anaconda) to identify potential microbial composition of human tissue Whole-Genome samples.. NovaSeq-sequenced paired-end reads (2X151bp) were aligned (onto hg19 reference), duplicate-removed, base quality score-recalibrated and BQSR-applied, which eventually used as an input to the PathseqPipelineSpark.. Since I failed to find hg19 host reference in the GATK resource bundle, first I created a BWA image file and a Kmer file originated from hg19 reference fasta with the command below. But for microbe-related files, I used ones that were contained in the bundle. . **'''** ; ; **gatk --java-options ""-Xmx50G"" BwaMemIndexImageCreator -I ./ref.fasta** ; **gatk --java-options ""-Xmx50G"" PathSeqBuildKmers --reference ./ref.fasta -O ref.hss** ; ; **'''**. . And then I ran PathSeq with the following command. . **'''** ; ; **gatk --java-options ""-Xmx200G"" PathSeqPipelineSpark \** ; **--input sample.bam \** ; **--filter-bwa-image ref.fasta.img \** ; **--kmer-file ref.hss \** ; **--is-host-aligned true \** ; **--min-clipped-read-length 70 \** ; **--microbe-fasta pathseq\_microbe.fa \** ; **--microbe-bwa-image pathseq\_microbe.fa.img \** ; **--taxonomy-file pathseq\_taxonomy.db \** ; **--output sample.pathseq.bam \** ; **--scores-output sample.pathseq.txt** ; ; ; **'''**. . and unfortunately it was shut down by this error message. **09:27:43.974 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so** ; **Mar 05, 2020 9:27",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:368,install,installed,368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['install'],['installed']
Deployability,"This requires a Barclay upgrade (that is not yet published) since most of the basic WDL generation code is in Barclay. WDL is only generated for tools that are annotated with @RuntimeProperties, and that have input/output files arguments that are annotated with@WorkflowResource. For each WDL generated, an accompanying JSON input file is generated that contains all the tool's arguments, with the optional arguments initialized to the tool's default values, and the required args initialized to a string that describes the required type. A temporary commit that contains a sample WDL/JSON generated by the WDL gen task in included to make it easier to see the WDL that results. The only commits in this branch that are directly WDL-gen related are the [WDL Gen](https://github.com/broadinstitute/gatk/pull/6504/commits/ffbd6ce5751924c76785c09baa3be04e37b7b3ac) commit itself, and the [sample output](https://github.com/broadinstitute/gatk/pull/6504/commits/ffbd6ce5751924c76785c09baa3be04e37b7b3ac) commit. The other commits are either related to GATKPathSpecifier migration (not required for WDL gen) or Barclay upgrade migration (required for WDL gen).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6504:24,upgrade,upgrade,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6504,2,['upgrade'],['upgrade']
Deployability,"This requires a Barclay upgrade (that is not yet published) since most of the basic WDL generation code is in Barclay. WDL is only generated for tools that are annotated with `@RuntimeProperties`, and that have input/output files arguments that are annotated with`@WorkflowResource`. For each WDL generated, an accompanying JSON input file is generated that contains all the tool's arguments, with the optional arguments initialized to the tool's default values, and the required args initialized to a string that describes the required type. A temporary commit that contains a sample WDL/JSON generated by the WDL gen task.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6503:24,upgrade,upgrade,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6503,1,['upgrade'],['upgrade']
Deployability,This reverts commit 8a366c7ba570c61338f7109b86c3284b80d5cf47. We noticed a major performance regression in `BaseRecalibratorSpark` and `HaplotypeCallerSpark` after we upgraded our ADAM dependency (see https://github.com/broadinstitute/gatk/issues/4376). This PR reverts that upgrade for now until we understand the underlying cause.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4428:167,upgrade,upgraded,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4428,2,['upgrade'],"['upgrade', 'upgraded']"
Deployability,This should be done after #5688 does in. Some Mutect2 integration tests should be deleted once this is done.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5763:54,integrat,integration,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5763,1,['integrat'],['integration']
Deployability,"This should fix the travis failure by forcing lfs to overwrite the existing commit hooks. The issue seems to be this:. We install lfs in the first part of the travis build, and then we run a docker build and mount the git folder into it. Docker then installs lfs again. The problem is occurring because git lfs 3.1.1 which released 2 days ago changed the format of the pre-push and other git hooks. Then it throws an error when it's installed again and there are hooks that look different than it expects already in place. Running install with `--force` fixes it. The lfs devs actually have a system for ignoring these differences, but they forgot to update their list of allowed differences ( or however they match it) in 3.1.1. They then released 3.1.2 today which fixes this. In most cases this would fix the issue, except the git-lfs installed INSIDE the docker image is on an ancient version and never updates since the ancient image ubuntu is pegged to an out of date one. While the one in travis outside of docker gets updated to the most recent one. So we have to manually force this. We should probably also update our ubuntu image to a newer one. Of note, we don't actually NEED lfs in the docker for the tests at all, since we've already downloaded the files outside of docker and are mounting them in. Here's a passing build where I remove it https://app.travis-ci.com/github/broadinstitute/gatk/builds/246595037. I'm afraid though that some other system depends on it so I don't want to change it. . Rebasing on this should fix the stuck branches. @droazen @jonn-smith @ldgauthier @jamesemery",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7682:122,install,install,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682,11,"['install', 'release', 'update']","['install', 'installed', 'installs', 'released', 'update', 'updated', 'updates']"
Deployability,"This should prevent the confusing case of accidentally corrupting an existing GenomicsDB. The next genomicsDB update is supposed to make this unnecessary, but until it happens lets avoid this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3975:110,update,update,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3975,1,['update'],['update']
Deployability,"This takes @takutosato's first commit in https://github.com/broadinstitute/gatk/pull/4035 and adds my updates to the PrintReads document, and uses the latest master branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4084:102,update,updates,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4084,1,['update'],['updates']
Deployability,"This test input is malformed. When I try to read it with the Dataflow code, I get this error:. htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Record 129, Read name 809R9ABXX101220:5:6:17918:145992, Mate Alignment start should be 0 because reference name = *. Here's the corresponding read:. 809R9ABXX101220:5:6:17918:145992 97 17 69400 37 67M9S \* 71202348 0 ACTCCCCACCTTACCTGACTCCTTCCAGGGTTTGTCGCCTTTCCGGTCCCTGACCCCAGTGGATGGGAGTCTGTCC ?ABDDEEABEECBDBDAB=DEDCDEEBFADABCEAD?EEEDCFE?ABEEE@FCDEEEBF@F?C<E@########## MD:Z:67 PG:Z:BWA RG:Z:809R9.5 AM:i:0 NM:i:0 SM:i:37 MQ:i:0 OQ:Z:DGEGGGGBFGGGGGDF8@@FGFBGGGBGCECCEEDFGGGFGFGGGBDGGF9DBFFGFBF;@>A4@@########## UQ:i:0. @droazen confirms that Picard's ValidateSAMFile utility reports that this bam has multiple errors. We should replace it with a clean input, and update the ""known good"" output accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/568:824,update,update,824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/568,1,['update'],['update']
Deployability,"This ticket set out to do VS-962 (one FOFN to rule them all) only, but along the way I found I also needed to do VS-984 (use Bulk Ingest from integration test) as the interface to `GvsAssignIds` had changed. After making these changes I realized I had basically done VS-982 as well (use Bulk Ingest in Beta WDL) due to the beta WDL picking up the changes to the unified WDL both it and Quickstart call. - [Successful integration run](https://job-manager.dsde-prod.broadinstitute.org/jobs/ab07dffd-a2ea-4b69-9c4c-eec0019e5b3b); - [Pending beta run](https://job-manager.dsde-prod.broadinstitute.org/jobs/37f6be29-5aae-42c4-a86b-c6d00c3caec5)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8404:142,integrat,integration,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8404,2,['integrat'],['integration']
Deployability,"This ticket was intended as a spike but in the process of spiking I was able to create a peer Hail VDS integration test to the VCF integration test. This seems like a viable candidate for Q4 MVP so putting it out for review as is. This creates a separate integration test for AoU Delta-style Hail VDS outputs with a tieout to regular GVS VCFs. The existing `GvsQuickstartIntegration` WDL has become an ""uber"" integration WDL that by default runs both the VCF and Hail VDS integration tests, but these can be toggled on or off separately with optional boolean inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8086:103,integrat,integration,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8086,6,"['integrat', 'toggle']","['integration', 'toggled']"
Deployability,"This tool copies single large files or directories from GCS into HDFS using Spark. Spark parallelization allows each task to copy a chunk in the size of the blocks of the target HDFS system simultaneously. When copying a directory containing a 120GB WGS bam and its index, this takes approximately 1 minute on a 10 worker / 160 core cluster, as opposed to approximately 20 minutes using Hadoop distcp. This may eventually be superseded by the NIO GCS integration work if that ends up performing comparably. @lbergelson would you like to review? Or feel free to nominate someone else.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2540:451,integrat,integration,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540,1,['integrat'],['integration']
Deployability,This update means the pipeline won't die on single-ended reads (it just filters them out).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5818:5,update,update,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5818,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,This upgrades Spark Dataflow to the correct version of Dataflow (Spark Dataflow 0.3.0 targets 0.4.150710). This fixes the runtime exception when running Spark tests due to library mismatches.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/785:5,upgrade,upgrades,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/785,1,['upgrade'],['upgrades']
